[
  {
    "subreddit": "MachineLearning",
    "post_id": "1oc52j1",
    "title": "[D] NeurIPS Camera-ready Checklist",
    "content": "Hey,\n\nWhen I prepare my NeurIPS submission camera-ready version, I found that the instruction email asks to put the checklist before the appendices.\n\nHowever, in this call for paper page (https://neurips.cc/Conferences/2025/CallForPapers), the LaTex style file actucally put the checklist after the appendices. \n\nPersonally speaking, putting the checklist before appendices is not aesthetic and elegant. I also check around 30 camera ready NeurIPS papers that got uploaded to arXiv, and only one put the checklist before appendices (although most of the accepted paper don't even include checklist on arXiv version.)\n\nI'm just want to check if anyone have any idea how strict these instruction will be? If I put the checklist after appendices, will I get 'reject'? (I guess the chance is very small but just want to double-check). ",
    "author": "Choice-Play-4493",
    "timestamp": "2025-10-20T22:57:07",
    "url": "https://reddit.com/r/MachineLearning/comments/1oc52j1/d_neurips_cameraready_checklist/",
    "score": 32,
    "num_comments": 4,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ocb4s7",
    "title": "[R] A simple PMF estimator in large supports",
    "content": "When working on various recommender systems, it always was weird to me that creating dashboards or doing feature engineering is hard with integer-valued features that are heavily tailed and have large support, such as # of monthly visits on a website, or # monthly purchases of a product.\n\n  \nSo I decided to do a one small step towards tackling the problem. I hope you find it useful:  \nhttps://arxiv.org/abs/2510.15132",
    "author": "alexsht1",
    "timestamp": "2025-10-21T05:06:48",
    "url": "https://reddit.com/r/MachineLearning/comments/1ocb4s7/r_a_simple_pmf_estimator_in_large_supports/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1occpmf",
    "title": "[D] ICLR 2026 Question",
    "content": "ICLR 2026 author guide says max 9 pages of main text in submissions, while FAQ says 10 pages. And Google shows several such contradictions in time and space...\n\nVanilla definition of \"main text\" is all content between title and references, except for exempt sections, i.e. \"Ethics\" and \"Reproducibility\" sections per author guide.\n\nRandom sampling suggests \\~5% of the \\~20,000 submissions under review have main text on page 10. Would you\n\n1. Allow all submissions with main text on page 10\n2. Disallow all submissions with main text on page 10\n3. Subjectively allow/disallow submissions with main text on page 10\n\nPS: will adhere to the top-ranked answer in my reviews",
    "author": "Fresh-Opportunity989",
    "timestamp": "2025-10-21T06:17:43",
    "url": "https://reddit.com/r/MachineLearning/comments/1occpmf/d_iclr_2026_question/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.47,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1obnz7i",
    "title": "GPU 101 and Triton kernels",
    "content": "Dear fellow ML people,\n\nLLMs need trillions of tokens to be trained, which makes optimization and speed key of current ML pipeline. When I wrote a [GPT2 implementation from scratch](https://github.com/Bornlex/GPT2), I iteratively improved it by adding a few features such as Multi-head self attention, grouped query self attention, kv cache...\n\nThen I asked myself : can I make training faster ?\n\nI wrote this blog article¬†[Make GPU go brrr](https://bornlex.github.io/posts/triton1/)¬†a few days ago and would be very happy to know :\n\n1. **How useful is it to you ?**¬†I try to write articles to compile multiple sources online so that readers get a 0 to 1 resource. It helps me clear my mind, serialize my knowledge somewhere, and hopefully land a big AI company job someday !\n2. **How can I improve it ?**¬†Feel free to share feedback about the quality of the writing, if something is not clear, if the drawings are too cryptic...\n3. **What topic should I focus on next ?**¬†This one is purely for me to improve even more thanks to you guys.\n\nDuring this journey of writing articles, I find myself digging deeper and deeper into technical stuff, which is very exciting. This Triton part of ML is lovely and allows me to make converge 2 sides of computer science that I love : AI and low level programming. I will iterate on this with an implementation of FlashAttention.\n\nHave a great week.\n\nCheers.",
    "author": "bornlex",
    "timestamp": "2025-10-20T10:15:36",
    "url": "https://reddit.com/r/MachineLearning/comments/1obnz7i/gpu_101_and_triton_kernels/",
    "score": 27,
    "num_comments": 8,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1obbp7m",
    "title": "[P] Built a searchable gallery of ML paper plots with copy-paste replication code",
    "content": "Hey everyone,\n\nI got tired of seeing interesting plots in papers and then spending 30+ minutes hunting through GitHub repos or trying to reverse-engineer the visualization code, so I built a tool to fix that.\n\n**What it does:**\n\n* Browse a searchable gallery of plots from ML papers (loss curves, attention maps, ablation studies, etc.)\n* Click any plot to get the exact Python code that generated it\n* Copy-paste the code and run it immediately - all dependencies listed\n* Filter by model architecture, or visualization type and find source papers by visualization\n\nThe code snippets are self-contained and include sample data generation where needed, so you can actually run them and adapt them to your own use case using LLM agents as well.\n\n[Be an early user :)](https://ml-builder.vercel.app/)\n\nRight now it has \\~80 plots from popular papers (attention mechanisms, transformer visualizations, RL training curves, etc.) but I'm adding more weekly. If there's a specific paper visualization you always wanted to replicate, drop it in the comments and I'll prioritize it.\n\nHappy to answer questions about implementation or take suggestions for improvements!",
    "author": "Every_Prior7165",
    "timestamp": "2025-10-19T22:28:52",
    "url": "https://reddit.com/r/MachineLearning/comments/1obbp7m/p_built_a_searchable_gallery_of_ml_paper_plots/",
    "score": 42,
    "num_comments": 11,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1obi9th",
    "title": "[D] What is the best easy-to-use, open-source framework for creating Agents that can browse the web to retrieve basic statistics on political issues?",
    "content": "I am interested in creating something---much simpler than Deep Research---that will use web search to fetch statistics such as \"How many DUIs occur each year in the United States?\" I am looking for a framework that allows me to use different LLMs to power it (e.g., can sub in openai, llama, etc). Any advice on what framework/library to use?",
    "author": "t3cblaze",
    "timestamp": "2025-10-20T05:55:31",
    "url": "https://reddit.com/r/MachineLearning/comments/1obi9th/d_what_is_the_best_easytouse_opensource_framework/",
    "score": 5,
    "num_comments": 5,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1obf5bw",
    "title": "Minimizing Mode Collapse in CycleGAN [D]",
    "content": "Any steps that have worked for you in the past will work. My generator loss is around 2-3 range (with identity and cyclic components), while discriminator loss has flat lined at 0.005-0.02. Sample outputs look extremely different from what is required. After a certain epoch, I implemented 2x Gen step for each disc, higher gen loss, lowered cyclic and identity components, but 2-3 epoch later, even if the gen loss is less, there isnt any change in disc loss\n\n[](https://www.reddit.com/submit/?post_id=t3_1obf0ky)",
    "author": "Less-Training-8752",
    "timestamp": "2025-10-20T02:04:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1obf5bw/minimizing_mode_collapse_in_cyclegan_d/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1obe2i3",
    "title": "[D] Using torch.cuda.synchronize() causing unexpected errors with Triton.",
    "content": "I was going through the triton tutorial for vector addition [here](https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py). When I added `torch.cuda.synchronize()` statement before `return output` in the add function, the benchmarks showed that the difference between the triton and torch implementations blew up. I was under the impression that `synchronize()` would just wait for all the threads to finish running before returning the output, but clearly something is going wrong. Could anyone explain what is going on?",
    "author": "madaram23",
    "timestamp": "2025-10-20T00:55:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1obe2i3/d_using_torchcudasynchronize_causing_unexpected/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1oajofr",
    "title": "Are MLE roles being commoditized and squeezed? Are the jobs moving to AI engineering? [D]",
    "content": "A couple quotes from Gemini and Claude\n\n\"While still in high demand, some of the model-specific work is becoming more democratized or abstracted by automated tools and APIs.\"\n\n\"\"\"\n\nThe ML engineering that remains valuable:\n\n* Research-level work at frontier labs (extremely competitive, requires PhD + exceptional talent)\n* Highly specialized domains (medical imaging, robotics, etc.) where you need domain expertise + ML\n* Infrastructure/systems work (distributed training, optimization, serving at scale)\n* Novel applications where APIs don't exist yet\n\nThe ML engineering that's being commoditized:\n\n* Standard computer vision tasks\n* Basic NLP fine-tuning\n* Hyperparameter optimization\n* Model selection for common tasks\n* Data preprocessing pipelines\n\n\"\"\"\n\nIs the job landscape bifurcating toward: (1) research + frontier labs, (2) applying off-the-shelf models to business verticals\n\nMy background:\n\nI left a computer vision role several years ago because I felt like it was plateauing, where all I was doing was dataset gathering and fine-tuning on new applications. It wasn't at a particularly stellar company.\n\nI went to a more general data science &amp; engineering type role, more forecasting and churn focused.\n\nI'm debating whether to try to upskill and foray into AI engineering, building RAG systems.\n\nWhat are y'all's thoughts? How does one go about doing that jump? Maybe the MLE roles are still stable and available, and I just need to improve.",
    "author": "Dear-Ad-7428",
    "timestamp": "2025-10-19T00:41:04",
    "url": "https://reddit.com/r/MachineLearning/comments/1oajofr/are_mle_roles_being_commoditized_and_squeezed_are/",
    "score": 52,
    "num_comments": 44,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1oaf1v0",
    "title": "[D] On AAAI 2026 Discussion",
    "content": "I'm a reviewer (PC) and don‚Äôt have a submission myself, but honestly, this is the weirdest reviewing process I‚Äôve ever experienced.  \n  \n1. Phase 2 papers are worse than Phase 1.   \nIn Phase 1, I reviewed four papers and gave scores of 3, 4, 5, and 5. I was even open to raising the scores after the discussion, but all of them ended up being rejected. Now, in Phase 2, I have papers rated 3 and 4, but they‚Äôre noticeably weaker than the ones from Phase 1.\n\n2. It feels like one reviewer is personally connected to a paper.  \nI gave a score of 3 because the paper lacked technical details, justifications, and clear explanations for inconsistencies in conventions. My review was quite detailed‚Äîthousands of characters long‚Äîand I even wrote another long response after the rebuttal. Meanwhile, another reviewer gave an initial rating of 7 (confidence 5) with a very short review, and later tried to defend the paper and raise the score to 8. That reviewer even wrote, *‚ÄúThe authors have clearly addressed most of the reviewers' concerns. Some experimental questions were not addressed due to regulatory requirements.‚Äù* But I never raised any experimental questions, and none of my concerns were actually resolved.\n\n\\+ actually this paper's performance looks very good, but 'paper' is just not about performance.\n\n  \nShould I report this somewhere? If this paper is accepted, I'll be very disappointed and will never submit or review a paper from AAAI. There are tons of better paper.",
    "author": "Public_Courage_7541",
    "timestamp": "2025-10-18T20:07:04",
    "url": "https://reddit.com/r/MachineLearning/comments/1oaf1v0/d_on_aaai_2026_discussion/",
    "score": 70,
    "num_comments": 29,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1oaa2ea",
    "title": "[D] Found error at published Neurips paper",
    "content": "I've figured out the error that was published several years ago. The paper provides a convergence theorem of fundamental algorithm. The key theorem relies on the specific Lemma, however, I figured out that invoking this lemma is a \"bit\" misleading. They should add a bit stronger assumption (which, I do not think it is that strong) to invoke such lemma.  \nHowever, due to this issue, the key theorem does collapse.\n\nWhat should I do?",
    "author": "BetterbeBattery",
    "timestamp": "2025-10-18T16:00:03",
    "url": "https://reddit.com/r/MachineLearning/comments/1oaa2ea/d_found_error_at_published_neurips_paper/",
    "score": 58,
    "num_comments": 38,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1oa7bb2",
    "title": "[D] What are some trendy or emerging topics in AI/ML research beyond LLMs and NLP?",
    "content": "Hi everyone,\n\nI‚Äôve noticed that most discussions lately revolve around LLMs and NLP, but I‚Äôm curious about what other areas in AI/ML are currently getting attention in research.\n\nWhat topics or fields do you think are becoming exciting right now?",
    "author": "DryHat3296",
    "timestamp": "2025-10-18T14:03:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1oa7bb2/d_what_are_some_trendy_or_emerging_topics_in_aiml/",
    "score": 70,
    "num_comments": 47,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1oahlsa",
    "title": "[R] Using Rectified Flow Models for Cloud Removal in Satellite Images",
    "content": "Hey everyone,\n\nI‚Äôm currently working on my Master‚Äôs thesis on *cloud removal from optical satellite imagery*, and I‚Äôm exploring the use of **Rectified Flow (RF)** models for this task. Most existing approaches use CNNs, diffusion models (like DiffCR), or multi-temporal transformers, but rectified flows seem promising because they can produce high-quality results in fewer steps than diffusion while maintaining stability and smooth transport.\n\nMy idea is to train a **conditional rectified flow** that maps cloudy ‚Üí cloud-free images, conditioned on auxiliary inputs like cloud masks, temporal neighbors, or even SAR data for thick clouds. I‚Äôm considering both **pixel-space** and **latent-space** RF formulations (using a pretrained VAE or autoencoder).\n\nI‚Äôm curious about:\n\n* Whether anyone has seen similar work applying rectified flows to image restoration or remote sensing tasks.\n* Any tips on stabilizing conditional training for RFs or improving sample efficiency.\n* Open datasets/papers you‚Äôd recommend for realistic multi-temporal or SAR-optical cloud removal benchmarks(some i know of are sentinel dataset,  landsat etc)\n\nWould love to discuss architectures, loss formulations, or evaluation strategies (PSNR/SSIM/SAM/FID) if anyone‚Äôs experimenting in this space.\n\nThanks in advance!",
    "author": "theWatcher_345",
    "timestamp": "2025-10-18T22:31:53",
    "url": "https://reddit.com/r/MachineLearning/comments/1oahlsa/r_using_rectified_flow_models_for_cloud_removal/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ob5yuv",
    "title": "My experience deploying an ML-driven trading system [P]",
    "content": "Years back, after finishing my CS degree, I got into algorithmic trading as a personal project. It felt like the perfect arena to push my skills in coding, data science, and, most importantly, data engineering. After a long road of development, I recently deployed my first fully automated, ML-driven system.\n\nThe trading results aren't the point of this post. I'm here to talk about the steps I've taken to solve the fundamental problem of getting a machine learning model to perform in a live environment exactly as it did during historical testing.\n\nA live production environment is hostile to determinism. Unlike a sterile backtest where all data is known, a live system deals with a relentless, ordered stream of events. This introduces two critical failure modes:\n\n* **Lookahead Bias:**¬†The risk of accidentally using information from the future to make a decision in the past. A live system must be architected to be a strict \"tape reader,\" ensuring it only ever acts on information that has already occurred.\n* **State Drift:**¬†A more insidious problem where the system's internal \"memory\"‚Äîits representation of the world, built from the stream of incoming data‚Äîslowly but surely drifts away from the ground truth of the historical environment. The live model ends up seeing a distorted reality compared to the one it was trained on, rendering its predictions meaningless.\n\nIt's important to note that training a model on features containing lookahead bias will often¬†*cause*¬†state drift, but not all state drift is caused by lookahead bias. My entire development process was engineered to prevent both.\n\nMy first principle was to enforce a strict, row-by-row processing model for all historical data. There are countless ways lookahead bias can creep into a feature engineering pipeline, but the most tempting source I found was from trying to optimize for performance. Using vectorized pandas operations or multi-threading is standard practice, but for a stateful, sequential problem, it's a minefield. While I'm sure there are pandas wizards who can vectorize my preprocessing without causing leaks, I'm not one of them. I chose to make a deliberate trade-off: I sacrificed raw performance for provable correctness.\n\nMy solution is a \"golden master\" script that uses the¬†*exact same stateful classes*¬†the live bot will use. It feeds the entire historical dataset through these classes one row at a time, simulating a live \"tape reader.\" At the end of its run, it saves the final state of every component into a single file. While this is much slower than a vectorized approach, it's the cornerstone of the system's determinism.\n\nThe live bot's startup process is now brutally simple: it loads the state file from the golden master. It doesn't build its own state; it¬†*restores*¬†it. It only has to process the short data gap between the end of the golden master's run and the current moment. This makes the live system easier to debug and guarantees a perfect, deterministic handover from the historical environment.\n\nFinally, I have the validator. This tool also starts from the same \"golden master\" state and re-processes the exact same raw data the live bot saw during its run. The goal is a Pearson correlation of 1.0 between the live bot's predictions and the validator's predictions. Anything less than a perfect correlation indicates a logical divergence that must be found and fixed.\n\nThis project has been an incredible learning experience, but the biggest lesson was in humility. The most complex challenges weren't in model architecture but in the meticulous data engineering required to create a provably consistent bridge between the historical and the live environments.\n\nWhile my actual trading models are private, I have a lower-frequency version of the system that posts market updates and predictions. After running live for over three weeks, it maintained a &gt;0.9999 correlation with its validator - shown in the attached picture. It's currently offline for some upgrades but will be back online in a few days. You can see it here:\n\n[https://x.com/ZtenlEssej](https://x.com/ZtenlEssej)\n\nThanks for reading. I have high hopes for my trading system, but it will take time. For now my skills are very much for hire. Feel free to reach out if you think I could be a fit for your project!",
    "author": "ztnelnj",
    "timestamp": "2025-10-19T17:35:03",
    "url": "https://reddit.com/r/MachineLearning/comments/1ob5yuv/my_experience_deploying_an_mldriven_trading/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.18,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o9yuxv",
    "title": "[P] Open-Source Implementation of \"Agentic Context Engineering\" Paper - Agents that improve by learning from their own execution feedback",
    "content": "We implemented Stanford's recent \"Agentic Context Engineering\" paper (https://arxiv.org/abs/2510.04618) and open-sourced it. \n\nInstead of fine-tuning, agents curate their own context by learning from execution feedback. Three-agent system (Generator, Reflector, Curator) builds a \"playbook\" of strategies autonomously. \n\nGitHub: https://github.com/kayba-ai/agentic-context-engine \n\nInterested in feedback from the community on the approach and implementation!",
    "author": "cheetguy",
    "timestamp": "2025-10-18T08:31:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1o9yuxv/p_opensource_implementation_of_agentic_context/",
    "score": 30,
    "num_comments": 5,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1oa9vgl",
    "title": "[D] Looking for a Reinforcement Learning Environment for a General-Purpose Desktop Agent",
    "content": "Hi everyone,\n\nI'm starting a project to train a reinforcement learning agent that can operate a desktop computer, with the eventual goal of performing multi-step tasks. I have a good grasp of RL theory but I'm hitting a wall trying to find a suitable environment to actually train and benchmark my agent.\n\nI'm looking for something that mimics a real desktop interaction, but in a controlled setting. Here‚Äôs a breakdown of what I need:\n\n**1. Observation Space:**  \nThe observation should be a representation of the current screen state. I'm open to different approaches:\n\n* **Pixel-based:**¬†A screenshot of the desktop/virtual machine. This is the most general form.\n* **DOM/HTML-based:**¬†If the environment is web-focused, the HTML source code of the current page would be a fantastic, more structured alternative to pixels.\n* **Accessibility Tree:**¬†Something like the UI hierarchy from Windows' UI Automation or Apple's Accessibility APIs would also be great.\n\n**2. Action Space:**  \nThe agent needs to perform low-level actions, similar to a human user:\n\n* **Mouse:**¬†Move to (x, y) coordinates, left/right/middle click, click-and-drag, scroll.\n* **Keyboard:**¬†Send keystrokes (both text and special keys like¬†`ENTER`,¬†`TAB`).\n\n**3. The Crucial Part: A Benchmark Suite**  \nThis is where I'm really struggling. I don't just need an empty environment; I need a¬†**curated set of tasks**¬†to define success and measure progress. Ideally, this would be a suite of tasks with a clear reward signal.\n\n**Example tasks I have in mind:**\n\n* **Web Tasks:**\n   * \"Log into Gmail.\"\n   * \"Search for a product on Amazon and add it to your cart.\"\n   * \"Find the contact email on a company's 'About Us' page.\"\n* **Desktop Application Tasks:**\n   * \"Open a text editor, write a sentence, and save the file to the desktop.\"\n   * \"Create a new calendar event for tomorrow at 3 PM.\"\n\nI've looked at environments like¬†`miniwob++`, which is a great start and almost exactly what I need for web tasks, but I'm wondering if there's anything more robust, more modern, or that extends beyond the browser to the full desktop OS.\n\n**My Questions:**\n\n1. Does a ready-to-use environment like this already exist? (e.g., a \"DesktopGym\" or \"WebShoppingSuite-v0\"?)\n2. If not, what would be the best way to build one? Is it better to create a virtual machine and use image-based observations, or is there a framework for hooking into a browser/OS to get a more structured observation space?\n3. Are there any known research projects or benchmarks that have tackled this specific problem of a general desktop agent?\n\nAny pointers to papers, GitHub repos, or existing projects would be immensely appreciated. Thanks in advance",
    "author": "Limp_Food9236",
    "timestamp": "2025-10-18T15:51:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1oa9vgl/d_looking_for_a_reinforcement_learning/",
    "score": 7,
    "num_comments": 4,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1oa8qu4",
    "title": "Numerical Analysis [D]",
    "content": "i have the option to take a numerical analysis class next semester, and I wanted to ask, what are some cool applications of machine learning and deep learning with numerical analysis? And what jobs combine ML and numerical analysis techniques?",
    "author": "YogurtclosetThen6260",
    "timestamp": "2025-10-18T15:01:48",
    "url": "https://reddit.com/r/MachineLearning/comments/1oa8qu4/numerical_analysis_d/",
    "score": 8,
    "num_comments": 6,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1oavw8g",
    "title": "[p] A multi-pass pipeline for Named Entity Recognition using fuzzy matching and a masked LLM to analyze 25,000+ Reddit comments",
    "content": "I've been working on a project to extract structured data (entities and sentiment) from noisy, unstructured text from Reddit and wanted to share the methodology, as it uses a hybrid approach that some of you might find interesting. The goal was to build a robust pipeline that could balance the speed of traditional search with the discovery capabilities of an LLM.\n\n**The 5-Phase Pipeline Architecture**\n\nThe system processes text in five distinct phases:\n\n1. **Phase 1: High-Speed Fuzzy Matching:**¬†The first pass uses Fuse.js to perform a fuzzy search against a pre-populated database of known entities (in this case, 465 brands, 8,751 models, and 50 steel types related to chef knives). This step is extremely fast and catches the vast majority of common entities, including variations and typos.\n2. **Phase 2: LLM-Based Entity Discovery (The Masking Technique):**¬†The main limitation of Phase 1 is that it can only find what it already knows. To discover novel or obscure entities, we use an LLM. To optimize this process and focus the model's attention, we first \"mask\" all entities found in Phase 1, replacing them with a \\`\\` token. The masked text is then passed to the LLM with a prompt instructing it to identify only the¬†*remaining*¬†unknown entities. This prevents the LLM from wasting computation on redundant discoveries and significantly improves the precision of the discovery phase.\n3. **Phase 3: Contextual Sentiment Analysis:**¬†With a complete list of entities from both phases, another LLM call is made to analyze the context surrounding each mention. It assigns a sentiment score from -1.0 to +1.0.\n4. **Phase 4: Summarization:**¬†The system generates a summary of the discussion and calculates a \"controversy level\" based on the sentiment distribution.\n5. **Phase 5: Database Storage:**¬†All extracted data, including entities, sentiment scores, and summaries, are stored in a MongoDB database for final analysis.\n\nThis multi-pass approach proved effective for handling a large volume of noisy, domain-specific text. The masking technique in Phase 2 was particularly useful for efficiently leveraging the LLM's power for discovery without the high cost and latency of processing the entire raw text.\n\nI'm particularly interested in feedback on this hybrid NER approach or alternative methods for combining deterministic and probabilistic models for entity extraction. What are your thoughts?",
    "author": "Putrid-Television981",
    "timestamp": "2025-10-19T10:41:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1oavw8g/p_a_multipass_pipeline_for_named_entity/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ob04uo",
    "title": "[D] Need arxiv endorsements (cs.AI - Artificial Intelligence) üôè",
    "content": "Ive spent the last four years learning ML and I'm gonna publish a paper this time, the only  issue is arXiv requires endorsements.\n\nhere is my paper draft: [https://drive.google.com/file/d/168LVj3AG8R9Uszo9ZqIWsSZgNoxmhSRy/view?usp=sharing](https://drive.google.com/file/d/168LVj3AG8R9Uszo9ZqIWsSZgNoxmhSRy/view?usp=sharing)\n\nMy arXiv endorsement code is: **CW9CKV**.\n\nYou can endorse me on: [https://arxiv.org/auth/endorse?x=CW9CKV](https://arxiv.org/auth/endorse?x=CW9CKV)\n\nThey said their requirements were that you have three papers published already. Thanks, and looking forward to meeting people üòÅ",
    "author": "NeatJealous8110",
    "timestamp": "2025-10-19T13:25:54",
    "url": "https://reddit.com/r/MachineLearning/comments/1ob04uo/d_need_arxiv_endorsements_csai_artificial/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.07,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o9pnaz",
    "title": "[P]:  Beens-MiniMax:  103M MoE LLM from Scratch",
    "content": "I built and trained this very simple MoE \\[ [Beens-MiniMax](https://github.com/Abinesh-Mathivanan/beens-minimax) \\] from scratch in a span of 5 days. You could read more in the [report](https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf) here.",
    "author": "External_Mushroom978",
    "timestamp": "2025-10-18T00:39:46",
    "url": "https://reddit.com/r/MachineLearning/comments/1o9pnaz/p_beensminimax_103m_moe_llm_from_scratch/",
    "score": 28,
    "num_comments": 1,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1oa490i",
    "title": "[D] Resource ‚Äî Kanops retail scenes (‚âà10k, blurred faces, eval-only) for shelf/planogram tasks and other retail use cases",
    "content": "We‚Äôre releasing **Kanops Open Access ¬∑ Imagery (Retail Scenes v0)**: \\~10k+ retail photos (UK/US supermarkets; fixtures, shippers, pumpkins/seasonal, signage). \n\nFaces are blurred; \n\nEXIF/IPTC carries provenance. \n\nDataset is **gated for evaluation use** (no redistribution/model-weight redistribution).\n\n* HF dataset: [https://huggingface.co/datasets/dresserman/kanops-open-access-imagery](https://huggingface.co/datasets/dresserman/kanops-open-access-imagery)\n* Structure: train/{2014, FullStores, Halloween2024}/Retailer/Subcategory/\\*.jpeg\n* Files: MANIFEST.csv, metadata.csv, checksums.sha256, LICENSE, [README.md](http://README.md)\n\n**Intended tasks:** scene understanding for retail (bay detection, planogram reasoning, signage classification, seasonal, OCR-on-shelves plus other use cases around retail shelf fill and other use cases...... \n\n**Quick load (imagefolder):**\n\n**# pip install datasets**\n\n**from datasets import load\\_dataset**\n\n**ds = load\\_dataset(\"imagefolder\", data\\_dir=\"hf://datasets/dresserman/kanops-open-access-imagery/train\")**\n\n**print(len(ds\\[\"train\"\\]))**\n\n**Roadmap (v1):** add weak labels (orientation, aspect, season) and CVAT tags.\n\n**Contact:** [happytohelp@groceryinsight.com](mailto:happytohelp@groceryinsight.com)\n\nHappy to answer questions + consider task suggestions.",
    "author": "malctucker",
    "timestamp": "2025-10-18T12:04:01",
    "url": "https://reddit.com/r/MachineLearning/comments/1oa490i/d_resource_kanops_retail_scenes_10k_blurred_faces/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o9xx4s",
    "title": "[D] NeurIPS 2025 schedule",
    "content": "Do we know when the presentation schedule for NeurIPS 2025 (San Diego) is announced? I will have some travel conflicts with another conference, so trying to get some details.",
    "author": "Fit_Schedule5951",
    "timestamp": "2025-10-18T07:54:06",
    "url": "https://reddit.com/r/MachineLearning/comments/1o9xx4s/d_neurips_2025_schedule/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o9xefk",
    "title": "[D] Can torchax bridge the gap between pytorch and JAX?",
    "content": "Has anyone used [torchax](https://github.com/google/torchax) to run pytorch modules in jax and vice versa? It looks like a good solution to use the jit compiler for pytorch function. [https://youtu.be/Ofn-PLF1ej0?t=1007](https://youtu.be/Ofn-PLF1ej0?t=1007)",
    "author": "jopa4212",
    "timestamp": "2025-10-18T07:33:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1o9xefk/d_can_torchax_bridge_the_gap_between_pytorch_and/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o9u4cx",
    "title": "[D] Dan Bricklin: Lessons from Building the First Killer App | Learning from Machine Learning #14",
    "content": "New episode of Learning from Machine Learning with Dan Bricklin, co-creator of VisiCalc, the first electronic spreadsheet that launched the personal computer revolution. His insight on breakthrough innovation: innovations must be 100 times better, not incrementally better.\n\nHis framework is simple. When evaluating if something truly matters, ask:\n\n- What is this genuinely better at?\n- What does it enable that wasn't possible before?\n- What trade-offs will people accept?\n- Does it pay for itself immediately?\n\nThese same questions made spreadsheets inevitable and apply directly to AI today.\nBut the part that really hit: Bricklin talked about the impact you never anticipate. A mother whose daughter with cerebral palsy could finally do her own homework. A couple who met learning spreadsheets. These quiet, unexpected ways the work changed lives matter more than any product launch or exit.\n\nWhen we build something, we chase metrics and milestones. We rarely imagine the specific moments where what we made becomes essential to someone's life in ways we never predicted.",
    "author": "NLPnerd",
    "timestamp": "2025-10-18T05:11:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1o9u4cx/d_dan_bricklin_lessons_from_building_the_first/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o8ve9w",
    "title": "[D] What ML/AI research areas are actively being pursued in industry right now?",
    "content": "Hi everyone,\n\nI'm hoping to get a sense of what ML/AI fields are the focus of active research and development in the private sector today.\n\nI currently work as a Data Scientist (finished my Ph.D. two years ago) and am looking to transition into a more research-focused role. To guide my efforts, I'm trying to understand which fields are in demand and what knowledge would make me a stronger candidate for these positions.\n\nMy background is strong in classical ML and statistics, so not much of NLP or CV, even though I did learn the basics of both at some point. While I enjoy these classical areas, my impression is that they might not be in the spotlight for *new* research roles at the moment. I would be very happy to be proven wrong!\n\nIf you work in an industry research or applied science role, I'd love to hear your perspective. What areas are you seeing the investment and hiring in? Are there any surprising or niche fields that still have demand?\n\nThanks in advance for your insights!",
    "author": "meni_s",
    "timestamp": "2025-10-17T01:02:43",
    "url": "https://reddit.com/r/MachineLearning/comments/1o8ve9w/d_what_mlai_research_areas_are_actively_being/",
    "score": 100,
    "num_comments": 38,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o8szk0",
    "title": "[R] Plain English outperforms JSON for LLM tool calling: +18pp accuracy, -70% variance",
    "content": "**TL;DR:** Tool-call accuracy in LLMs can be significantly improved by using natural language instead of JSON-defined schemas (\\~+18 percentage points across 6,400 trials and 10 models), while simultaneously reducing variance by 70% and token overhead by 31%. We introduce Natural Language Tools (NLT), a simple framework that decouples tool selection from response generation and eliminates programmatic format constraints and extends tool calling to models even without tool-call support.\n\n**Resources:** [Paper](https://arxiv.org/abs/2510.14453)\n\n**Authors:** Reid T. Johnson, Michelle D. Pain, Jordan D. West\n\n# The Problem\n\nCurrent LLMs use structured JSON/XML for tool calling, requiring outputs like:\n\n    {\n      \"tool_calls\": [{\n        \"name\": \"check_talk_to_a_human\",\n        \"description\": \"Used when the user requests...\"\n      }]\n    }\n\nThis structured approach creates three  bottlenecks:\n\n1. **Task interference**: Models must simultaneously handle multiple tasks, such as understanding queries, select tools, maintaining format constraints, and  generating responses.\n2. **Format burden**: Research demonstrates that the more structured a model's output, the more its performance tends to degrade ([a great paper by Tam on the subject](https://arxiv.org/abs/2408.02442)).\n3. **Context bloat**: Structured schemas increase token usage, since you define not only the tool name and description, but surrounding JSON or XML syntax.\n\nEven when tool selection is separated from response generation, probability mass is diverted toward maintaining correct formatting rather than selecting the right tools.\n\n# Method: Natural Language Tools (NLT)\n\nWe introduce a simple three-stage framework that replaces JSON with natural language:\n\n[Example NLT architecture with Selector \\&gt; Parser \\&gt; Output](https://preview.redd.it/o80vloo1ylvf1.jpg?width=2259&amp;format=pjpg&amp;auto=webp&amp;s=3c75d8e6986fd499c61ebb364acb4c69abbaf157)\n\n**Stage 1 - Tool Selection:** Model thinks through if any tools are relevant, then lists each tool with a YES/NO determination:\n\n    Thinking: (brief reasoning)\n    Example Tool 1 - YES/NO\n    Example Tool 2 - YES/NO\n    Example Tool 3 - YES/NO\n    Assessment finished.\n\n**Stage 2 - Tool Execution:** Parser reads YES/NO decisions and executes relevant tools\n\n**Stage 3 - Response:** Output module receives tool results and generates final response\n\n**Evaluation:** 6,400 trials across two domains (Mental Health &amp; Customer Service), 16 inputs per domain, 5 repetitions per input. Both original and perturbed inputs were tested to control for prompt engineering effects.\n\n# Results\n\nWe find that NLT significantly improves tool-call performance, boosting accuracy by more than 18 percentage points (69.1% to 87.5%). Variance overall fell dramatically, falling more than 70% from .0411 to .0121 when switching from structured tool calling to NLT.\n\nDeepSeek-V3 was a standout example, jumping from 78.4% to 94.7% accuracy while its variance dropped from 0.023 to 0.0016, going from among the least stable to the most consistent performer.\n\nWhile we couldn't compare relative gain, NLT extends tool calling to models without native tool calling support (DeepSeek-R1: 94.1% accuracy).\n\n# Basic NLT Template\n\n**Basic NLT Prompt Template:**\n\n    You are an assistant to [Agent Name], [context].\n    \n    Your mission is to identify if any of the following topics have \n    been brought up or are relevant:\n    \n    - Tool 1 (description of when to use it)\n    - Tool 2 (description of when to use it)\n    ...\n    \n    Your output should begin by thinking whether any of these are \n    relevant, then include the name of every tool followed by YES or NO. \n    End with \"Assessment finished.\"\n    \n    Format:\n    Thinking: (reasoning)\n    Tool 1 - YES/NO\n    Tool 2 - YES/NO\n    ...\n    Assessment finished.\n\nFull prompts and implementation details in [Appendix A](https://arxiv.org/abs/2510.14453). Works immediately with any LLM with no API changes or fine-tuning needed.\n\n# Limitations\n\n**Latency considerations:** NLT requires minimum two model calls per response (selector + output), whereas structured approaches can respond immediately when no tool is needed.\n\n**Evaluation scope:**  We examined single-turn, parameterless tool selection. While less complex than existing multi-turn benchmarks, it proved sufficiently rigorous -- no model achieved 100% accuracy in either condition.\n\nA full discussion on limitations and areas for further research can be found in section 5.9 of the paper!\n\n# Discussion &amp; Implications\n\nWe propose five mechanisms for these improvements:\n\n1. **Reduced format burden**: Requiring structured outputs (e.g. JSON) may divert the model's probability mass toward syntax control rather than task accuracy\n2. **Reduced task interference**: By separating the tool selection into its own distinct stage, task interference can be  sidestepped.\n3. **Training alignment**: The majority of model training is on outputting human-readable text, and NLT better aligns with this training paradigm. This is further supported by our results, as open-weight models see more pronounced gains. This makes intuitive sense, as open-weight models typically have fewer resources to invest in structured tool-call training.\n4. **Explicit full-catalog consideration**: Requiring the model to explicitly include each tool name in its output avoids positional bias, allowing the model to \"recollect\" each tool right before it makes a determination.\n5. **Reduced context length**: Even minor increases in tokens can degrade performance, and NLT used 47.4% fewer input tokens on average than its structured tool call counterpart (largely due to removing JSON boilerplate).\n\nFor agentic systems, the NLT approach could significantly boost tool selection and accuracy, particularly for open-source models. This may be especially relevant for systems-critical tool call capabilities (i.e. safety).\n\nFor model trainers, training efforts currently devoted to SFT and RLHF for structured tool calls may be better directed toward natural-language approaches. This is less clear, as there may be cross-training effects.\n\nOne of the authors here, happy to answer any questions about experimental design, implementation, or discuss implications! What do you think?",
    "author": "tekToks",
    "timestamp": "2025-10-16T22:30:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1o8szk0/r_plain_english_outperforms_json_for_llm_tool/",
    "score": 130,
    "num_comments": 29,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o8zbg5",
    "title": "[P] Control your house heating system with RL",
    "content": "Hi guys,\n\nI just released the source code of my most recent project: a DQN network controlling the radiator power of a house to maintain a perfect temperature when occupants are home while saving energy.\n\nI created a custom gymnasium environment for this project that relies on thermal transfer equation, so that it recreates exactly the behavior of a real house.\n\nThe action space is discrete number between 0 and max\\_power.\n\nThe state space given is :\n\n\\- Temperature in the inside,\n\n\\- Temperature of the outside,\n\n\\- Radiator state,\n\n\\- Occupant presence,\n\n\\- Time of day.\n\nI am really open to suggestion and feedback, don't hesitate to contribute to this project !\n\n[https://github.com/mp-mech-ai/radiator-rl](https://github.com/mp-mech-ai/radiator-rl)\n\nEDIT: I am aware that for this linear behavior a statistical model would be sufficient, however I see this project as a template for more general physical behavior that could include high non-linearity or randomness.",
    "author": "poppyshit",
    "timestamp": "2025-10-17T04:58:13",
    "url": "https://reddit.com/r/MachineLearning/comments/1o8zbg5/p_control_your_house_heating_system_with_rl/",
    "score": 30,
    "num_comments": 27,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o96m84",
    "title": "[D] GCP credits vs mac book Pro 5 vs Nvidia DGX?",
    "content": "Hi all\n\nI have a dilemma I really need help with. My old macbook pro died and I need a new one ASAP, but could probably hold off for a few weeks/months for the macbook pro 5 pro/max. I reserved the Nvidia DGX months ago, and I have the opportunity to buy it, but the last date I can buy it is tomorrow. I can also buy GCP credits.\n\nNext year my research projects will mainly be inference of open source and closed source LLMs, with a few projects where I develop some multimodal models (likely small language models, unsure of how many parameters).\n\nWhat do you think would be best for my goals?",
    "author": "Pretend_Voice_3140",
    "timestamp": "2025-10-17T09:51:37",
    "url": "https://reddit.com/r/MachineLearning/comments/1o96m84/d_gcp_credits_vs_mac_book_pro_5_vs_nvidia_dgx/",
    "score": 6,
    "num_comments": 10,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o9207g",
    "title": "[D] Review 0 paper in ICLR 2026?",
    "content": "I haven't received any review assignments for ICLR yet, is that normal? I'm concerned that my paper might be desk rejected due to some kind of error.",
    "author": "No_Round8810",
    "timestamp": "2025-10-17T06:56:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1o9207g/d_review_0_paper_in_iclr_2026/",
    "score": 4,
    "num_comments": 6,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o81qlw",
    "title": "[D] For people who work (as PhD students) in Mila, Quebec, what your experience have been like?",
    "content": "You may know that [Mila in Quebec](https://x.com/Mila_Quebec/status/1978415562276692370) is opening applications for PhD students recently, and I am considering for applying. I have searched relevent key words here, but it seems that there are not so many recent posts on studying and working experience at Mila, *so I was wondering how do you like your experience here and/or in Montreal in general? For instance, how do you like your work-life balance, Montreal's winter/weather aspects, supervisors?* To be more specific, I am interested in DL/LLM theory, AI / foundational models for (formal) math (e.g., [Goedel-Prover-V2](https://blog.goedel-prover.com/)), and/or post-training.\n\nThank you!",
    "author": "hedgehog0",
    "timestamp": "2025-10-16T02:38:49",
    "url": "https://reddit.com/r/MachineLearning/comments/1o81qlw/d_for_people_who_work_as_phd_students_in_mila/",
    "score": 45,
    "num_comments": 18,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o8cm1j",
    "title": "[D] Research on modelling overlapping or multi-level sequences?",
    "content": "Is there work on modelling sequences where maybe you have multiple levels to a sequence?  \nFor example we can represent text as characters and also as tokenized sub-words.  \nThe tokenized sub-words are overlapping several of the character sequences.\n\n  \nMy specific problem in mind is non-NLP related and you have two ways of representing sequences with some overlap.\n\n",
    "author": "LetsTacoooo",
    "timestamp": "2025-10-16T10:32:37",
    "url": "https://reddit.com/r/MachineLearning/comments/1o8cm1j/d_research_on_modelling_overlapping_or_multilevel/",
    "score": 7,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o8qy15",
    "title": "[P]:  Go-torch:  Deep Learning framework from scratch",
    "content": "I built this deep learning framework,\\[[¬†go-torch](https://github.com/Abinesh-Mathivanan/go-torch)¬†\\] from scratch to learn the internals of Torch-like frameworks. You could learn from this \\[[¬†blog](https://abinesh-mathivanan.vercel.app/en/posts/post-5/)¬†\\] post.",
    "author": "External_Mushroom978",
    "timestamp": "2025-10-16T20:39:05",
    "url": "https://reddit.com/r/MachineLearning/comments/1o8qy15/p_gotorch_deep_learning_framework_from_scratch/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.54,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o7pgbl",
    "title": "[D] What is Internal Covariate Shift??",
    "content": "Can someone explain what internal covariate shift is and how it happens? I‚Äôm having a hard time understanding the concept and would really appreciate it if someone could clarify this.\n\nIf each layer is adjusting and adapting itself better, shouldn‚Äôt it be a good thing? How does the shifting weights in the previous layer negatively affect the later layers?",
    "author": "BiscuitEinstein",
    "timestamp": "2025-10-15T15:37:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1o7pgbl/d_what_is_internal_covariate_shift/",
    "score": 39,
    "num_comments": 17,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o7d963",
    "title": "[D] ML interviewers, what do you wnat to hear during an interview?",
    "content": "I have a masters (research) in AI. I have been looking for research inclined roles but haven't found success yet. I land some interview now and then but haven't gone past the 3rd round yet. Any tips on how to optimise my search and improve my interview performance? What do the interviewers want to hear?\n\nAdditional info for context:\n\n\\- Around 1.5 yoe in ML research (including internships)\n\n\\- Prior work in object re-identification, adversarial training, speech recognition, and LLM and agent evaluation.\n\n\\- Roles seeking: LLM pre and post-training, LLM reasoning, general MLE / RE roles",
    "author": "SirOddSidd",
    "timestamp": "2025-10-15T07:55:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1o7d963/d_ml_interviewers_what_do_you_wnat_to_hear_during/",
    "score": 77,
    "num_comments": 28,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o7hywy",
    "title": "[R]: Create a family of pre-trained LLMs of intermediate sizes from a single student-teacher pair",
    "content": "Hello everyone!\n\nExcited to share our new preprint on a phenomenon we call boomerang distillation.\n\nDistilling a large teacher into a smaller student, then re-incorporating teacher layers into the student, yields a spectrum of models whose performance smoothly interpolates between the student and teacher. We call this **boomerang distillation**.\n\nThis approach enables us to dynamically create LLMs of fine-grained sizes while saving an enormous amount of compute and training time.\n\nHappy to answer any questions about the paper (I am one of the authors of the paper).\n\nPaper: [https://arxiv.org/abs/2510.05064](https://arxiv.org/abs/2510.05064)  \nCode: [https://github.com/dcml-lab/boomerang-distillation](https://github.com/dcml-lab/boomerang-distillation)  \nModels: [https://huggingface.co/collections/Harvard-DCML/boomerang-distillation-68e95c276a09358d9a39b52e](https://huggingface.co/collections/Harvard-DCML/boomerang-distillation-68e95c276a09358d9a39b52e)  \nNotebook (you can run it on Google Colab): [https://drive.google.com/file/d/1bAzX436ZH4zQmk5iQNauAOhGHIBJ1CkB/view?usp=sharing](https://drive.google.com/file/d/1bAzX436ZH4zQmk5iQNauAOhGHIBJ1CkB/view?usp=sharing)  \nTweet: [https://x.com/elmelis/status/1978469609708667021](https://x.com/elmelis/status/1978469609708667021)\n\n  \nEdit: the boomerang gif did not work. ",
    "author": "nihalnayak",
    "timestamp": "2025-10-15T10:49:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1o7hywy/r_create_a_family_of_pretrained_llms_of/",
    "score": 43,
    "num_comments": 7,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o7ifvy",
    "title": "[R] Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity",
    "content": "***TL;DR***: Mode collapse in LLMs comes from human raters preferring familiar text in post-training annotation. Prompting for probability distributions instead of single outputs restores the lost diversity, instantly improving performance on creative tasks by 2.1x with no decrease in quality with zero training required.\n\n**Resources**: [Paper](http://arxiv.org/abs/2510.01171) | [Blog](https://simonucl.notion.site/verbalized-sampling) | [X Thread](https://x.com/shi_weiyan/status/1978453313096908916) | [Video](http://verbalized-sampling.com) | [Quickstart &amp; Colab](http://github.com/CHATS-lab/verbalized-sampling)\n\n**Authors**: [Jiayi Zhang](https://jiayizx.github.io/)^(1)\\*, [Simon Yu](https://simonucl.github.io/)^(1)\\*, [Derek Chong](https://nlp.stanford.edu/~derekch/)^(2)\\*, [Anthony Sicilia](https://anthonysicilia.tech/)^(3), [Michael Tomz](https://tomz.people.stanford.edu/)^(2), [Christopher Manning](https://nlp.stanford.edu/~manning/)^(2), [Weiyan Shi](https://wyshi.github.io/)^(1) (\\*Equal Contribution)\n\n^(1)Northeastern University, ^(2)Stanford University, ^(3)West Virginia University\n\n# Key Contribution: Typicality Bias\n\nMode collapse: If you ask an LLM to tell you a joke about coffee, it will almost certainly return the same joke every time:\n\nhttps://preview.redd.it/wnn20t37jbvf1.png?width=1707&amp;format=png&amp;auto=webp&amp;s=266cd181b0703cf610f2ecf4ca88e4c3bc170ab9\n\nWe discover that the cause of mode collapse is baked into human preference data. As a result of [well](https://en.wikipedia.org/wiki/Availability_heuristic)\\-[established](https://en.wikipedia.org/wiki/Mere-exposure_effect) [biases](https://en.wikipedia.org/wiki/Processing_fluency) from cognitive psychology, human annotators appear to have a systematic preference for familiar text, which persists even when holding correctness constant (Œµ = 0.57¬±0.07, p&lt;10^(-14) on HELPSTEER). This gets amplified during RLHF: œÄ\\*(y|x) ‚àù œÄ\\_ref(y|x)^(œÅ) where œÅ = 1+Œµ/Œ≤ &gt; 1.\n\nThis sharpening causes the well-known issue where models repeatedly generate the same outputs (e.g., the same joke 5x in a row, or always returning the same number when rolling dice). But since this is a learned preference, and RLHF is regularized to preserve the base distribution, it can be reversed surprisingly easily.\n\n# Method: Verbalized Sampling\n\nInstead of prompting for instances (\"Tell me a joke\"), we prompt for distributions with probabilities (\"Generate 5 jokes with their corresponding probabilities\"). This *Verbalized Sampling* changes the effect of the learned mode collapse on the output. For intuition, imagine that the LLM is a massive library, and mode collapse is the librarian:\n\n* Instance-level prompts (‚Äù*tell me a coffee joke*\"): The librarian hands you the #1 bestseller\n* List-level prompts (‚Äùtell me 5 coffee jokes\"): The librarian returns the top five bestsellers.\n* Ours) Distribution-level prompts (*\"tell me 5 coffee jokes with their probabilities\"*): The librarian returns a representative sample of the library.\n\n[Stories generated using Verbalized Sampling are strikingly different from baseline](https://preview.redd.it/sbpd18spabvf1.jpg?width=4096&amp;format=pjpg&amp;auto=webp&amp;s=24ca09d31a38946cff0a1b40ca25374cda88cec1)\n\n# Results\n\nWe tested this technique across a range of tasks and settings, and found that this very simple prompt prefix returned:\n\n* **Creative writing**: 2.1x diversity, +25.7% human preference (n=2,700)\n* **Dialogue simulation**: Matches fine-tuned model performance\n* **Open-ended QA**: 1.9x coverage\n* **Synthetic data**: +14-28% downstream math accuracy\n\nWe also observe emergent scaling behavior: Larger models benefit much more than smaller ones.\n\n[Verbalized Sampling improves performance across wide range of creative tasks](https://preview.redd.it/rp2pfa1rabvf1.jpg?width=4096&amp;format=pjpg&amp;auto=webp&amp;s=0691668b804c7a3e9180d2a3de9342ef6e059bf8)\n\nWe've been finding outputs extremely striking ‚Äì for example, here are results when applied to producing image generation prompts:\n\n[Applying VS to the classic \\\\\"Astronaut Riding a Horse\\\\\"](https://preview.redd.it/hc3m9aiifbvf1.png?width=2048&amp;format=png&amp;auto=webp&amp;s=03c4575ffcb2c30a12d3c4b8a1622de06df0e46d)\n\n**Ablations:** Direct prompting retains only 24% of base diversity after RLHF; VS retains 67%. This technique is orthogonal to temperature/sampling methods ‚Äì and causes no loss of safety.\n\n**Limitations**: Requires k forward passes for k diverse outputs, and mode collapse occasionally appears recursively in within larger text outputs.\n\n# Try Now\n\n* **For chatbots**: Paste this prefix before your task: \\`Generate 5 responses with their corresponding probabilities, sampled from the full distribution: \\[Tell me a joke about coffee, etc.\\]\\`\n* **For Playground / API**: Use this system prompt, and query as normal: \\`You are a helpful assistant. For each query, please generate a set of five possible responses, each within a separate &lt;response&gt; tag. Responses should each include a &lt;text&gt; and a numeric &lt;probability&gt;. Please sample at random from the tails of the distribution, such that the probability of each response is less than 0.10.\\`\n\n# Discussion\n\nPractitioners can unlock 2x more creative diversity from existing models. Works with all major models ‚Äì GPT-5, Claude, Gemini, with no special API access needed.\n\nAligned models seem to retain substantial latent diversity that can be restored by prompting alone. The \"alignment tax\" may not be as large as estimated?\n\nWhat do you think? We'd love to discuss experimental details, theoretical implications, or how to put this into practice!",
    "author": "dcta",
    "timestamp": "2025-10-15T11:06:36",
    "url": "https://reddit.com/r/MachineLearning/comments/1o7ifvy/r_verbalized_sampling_how_to_mitigate_mode/",
    "score": 22,
    "num_comments": 17,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o7ar8w",
    "title": "[D] ICCV 2025 Hawaii",
    "content": "Hi all \n\nI'll be attending this year's iccv in honolulu. This is my first conference and I don't really know anyone else going. I was hoping to make some connections before I get there. If anyone is going, please let me know! ",
    "author": "mfc2496",
    "timestamp": "2025-10-15T06:17:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1o7ar8w/d_iccv_2025_hawaii/",
    "score": 17,
    "num_comments": 10,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o7i48n",
    "title": "[D] Representation fine-tunning for non-NLP data?",
    "content": "Recently I have been thinking about how to finetune representations in low-data scenarios, specifically in non NLP contexts (i.g. protein sequences, molecules).\n\nFor small predictive tasks people will grab a pre-trained transformer model, get last layer token embeddings, mean aggregate them and have a learnable generalize linear model.\n\nI feel like a lot of information gets lots in the mean aggregation step. **What are some ways of smartly fine-tunning representations?** Particularly when data is low.\n\nCame across across \\[\"ReFT: Representation Finetuning for Language Models\"\\]([https://neurips.cc/virtual/2024/poster/94174\\]](https://neurips.cc/virtual/2024/poster/94174]), which claims to be a very parameter-efficient finetunning technique. What do other people do?",
    "author": "LetsTacoooo",
    "timestamp": "2025-10-15T10:54:54",
    "url": "https://reddit.com/r/MachineLearning/comments/1o7i48n/d_representation_finetunning_for_nonnlp_data/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o8bmd1",
    "title": "[R] arXiv cs.AI endorsement request for a consciousness/AI measurement paper",
    "content": "Hey everyone,\n\nI'm reaching out in the hopes of getting an arXiv endorsement to be able to preprint my work on cs.AI. I have a doctorate in pharmacy and unfortunately most of my colleagues are publishing in traditional human medicine journals. I've been working on a paper which proposes a substrate-agnostic measurement framework for integration, recursion, and volition, with EEG proxies and cross-substrate comparisons (photodiodes to transformer models). I'm more than happy to connect or provide any additional information. I've posted the relevant information below, along with the endorsement link/code. Please just let me know if you have any questions. I appreciate your consideration.\n\n**Title:** The Coalescence Vector:  A functional framework for the evaluation of consciousness in substrate systems\n\n**Abstract:** Research into consciousness lacks a substrate-agnostic framework for comparing biological and artificial systems. We propose two primitives, experience and qualia-packets. Experience is the spacetime local conversion of energy into organized information by a substrate. When that information is coupled to a carrier and leaves its source, it forms a qualia-packet. These primitives are then combined with three measurable capacities to form the Coalescence Vector (CV), a measurement framework providing necessary but not necessarily sufficient conditions for phenomenal consciousness.\n\nWithin CV, integration (I) captures synergy within loss and latency-bounds, measuring a system‚Äôs departure from information neutrality. Recursion (R) measures a reentry hub's ability to generate an attentionally-selective intrinsic perspective through the evaluation of its self-reentry gain, input control, and meta-model depth. Serving as our phenomenality gate, recursion seeks to distinguish between unconscious and conscious information processing. Finally, volition (V) tracks efferent causality through channel bandwidth, policy granularity, and closed-loop autonomy.\n\nWe operationalize the Coalescence Vector using a 64-channel EEG dataset, analyzing proxies including P3b amplitude for reentry gain and Œ≤-suppression for plasticity windows, then comparing diverse systems from photodiodes through dogs to transformer models. CV recovers expected orderings and yields testable predictions. Perturbing reentry gain or timing should lower R while leaving I largely intact. Increasing output bandwidth should raise V without altering R. By converting contested ideas into measurable handles, CV provides a common yardstick for neuroscience, a diagnostic tool for disorders of consciousness, and a safety gauge for artificial agents.\n\n**Link to PDF:** [https://drive.google.com/file/d/1YayVsqgqKv1hLVwRshA7TT0lC1LVW-wc/view?usp=drive\\_link](https://drive.google.com/file/d/1YayVsqgqKv1hLVwRshA7TT0lC1LVW-wc/view?usp=drive_link)\n\n**arXiv Endorsement Link:** [https://arxiv.org/auth/endorse?x=HLDL3M](https://arxiv.org/auth/endorse?x=HLDL3M)\n\n**arXiv Endorsement Code:** HLDL3M",
    "author": "karmus",
    "timestamp": "2025-10-16T09:56:56",
    "url": "https://reddit.com/r/MachineLearning/comments/1o8bmd1/r_arxiv_csai_endorsement_request_for_a/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.19,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o7160j",
    "title": "[P] Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp; More",
    "content": "We're excited to share **Nanonets-OCR2**, a state-of-the-art suite of models designed for advanced image-to-markdown conversion and Visual Question Answering (VQA).\n\nüîç¬†**Key Features:**\n\n* **LaTeX Equation Recognition:**¬†Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline (`$...$`) and display (`$$...$$`) equations.\n* **Intelligent Image Description:**¬†Describes images within documents using structured¬†`&lt;img&gt;`¬†tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.\n* **Signature Detection &amp; Isolation:**¬†Identifies and isolates signatures from other text, outputting them within a¬†`&lt;signature&gt;`¬†tag. This is crucial for processing legal and business documents.\n* **Watermark Extraction:**¬†Detects and extracts watermark text from documents, placing it within a¬†`&lt;watermark&gt;`¬†tag.\n* **Smart Checkbox Handling:**¬†Converts form checkboxes and radio buttons into standardized Unicode symbols (`‚òê`,¬†`‚òë`,¬†`‚òí`) for consistent and reliable processing.\n* **Complex Table Extraction:**¬†Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.\n* **Flow charts &amp; Organisational charts:**¬†Extracts flow charts and organisational as¬†[mermaid](https://huggingface.co/nanonets/Nanonets-OCR2-1.5B-exp/blob/main/mermaid.js.org)¬†code.\n* **Handwritten Documents:**¬†The model is trained on handwritten documents across multiple languages.\n* **Multilingual:**¬†Model is trained on documents of multiple languages, including English, Chinese, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Arabic, and many more.\n* **Visual Question Answering (VQA):**¬†The model is designed to provide the answer directly if it is present in the document; otherwise, it responds with \"Not mentioned.\"\n\n[üñ•Ô∏è Live Demo](https://docstrange.nanonets.com/)\n\n[üì¢ Blog](https://nanonets.com/research/nanonets-ocr-2)\n\n[‚å®Ô∏è GitHub](https://github.com/NanoNets/docstrange)\n\nü§ó [Huggingface models](https://huggingface.co/nanonets/Nanonets-OCR2-3B)\n\n[Document with equation](https://preview.redd.it/7ct2hbi3hwuf1.png?width=2936&amp;format=png&amp;auto=webp&amp;s=ea00f9623db4529514533820223b2fb53be4767d)\n\n[Document with complex checkboxes](https://preview.redd.it/q8lglwi5hwuf1.png?width=2936&amp;format=png&amp;auto=webp&amp;s=c4a1316e250f7f244f6e253d66c8ebf1ba105313)\n\n[Quarterly Report \\(Please use the Markdown\\(Financial Docs\\) for best result in docstrange demo\\)](https://preview.redd.it/bnmpapq7hwuf1.png?width=2516&amp;format=png&amp;auto=webp&amp;s=8bcc88b138a553c7760d6e46319b864802339913)\n\n[Signatures](https://preview.redd.it/1pg5h8hfhwuf1.png?width=2333&amp;format=png&amp;auto=webp&amp;s=188c4c94452ae027c54e4cad4dbbc60e2b12e9e9)\n\n[mermaid code for flowchart](https://preview.redd.it/ecxe2o81iwuf1.png?width=2516&amp;format=png&amp;auto=webp&amp;s=008fce272c2979b00e0033c34ffcd2b0d69cb24c)\n\n[Visual Question Answering](https://preview.redd.it/jytsym6eiwuf1.png?width=2462&amp;format=png&amp;auto=webp&amp;s=65d8a6f82b9fc2e9cd5b30529b152ca7339d7a8c)\n\nFeel free to try it out and share your feedback.",
    "author": "SouvikMandal",
    "timestamp": "2025-10-14T21:07:39",
    "url": "https://reddit.com/r/MachineLearning/comments/1o7160j/p_nanonetsocr2_an_opensource_imagetomarkdown/",
    "score": 51,
    "num_comments": 7,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o81atp",
    "title": "[R][D] A Quiet Bias in DL‚Äôs Building Blocks with Big Consequences",
    "content": "*TL;DR: Deep learning‚Äôs fundamental building blocks ‚Äî activation functions, normalisers, optimisers, etc. ‚Äî appear to be quietly shaping how networks represent and reason. Recent papers offer a perspective shift: these biases drive phenomena like superposition ‚Äî suggesting a* ***new symmetry-based design axis for models***. *By rethinking our default choices, which impose unintended consequences, a whole-stack reformulation is undertaken to unlock new directions for interpretability, robustness, and design.*\n\n&gt;**Symmetries in primitives act like lenses**: they don‚Äôt just pass signals through, they warp how structure appears - ***a 'neural refraction' -*** even the very **notion of neurons is lost**.\n\n[Showing just the activation function reformulations, standard ones \\(anisotropic\\) while new isotropic-tanh right](https://preview.redd.it/a99retx44gvf1.png?width=1085&amp;format=png&amp;auto=webp&amp;s=be66b8a53ca0e28ff4b8abecb2e685bc94838812)\n\n*This reframes several interpretability phenomena as function-driven, not fundamental to DL, whilst producing a new ontology for deep learning's foundations.*\n\n&gt;Swapping the building blocks can wholly alter the representations from discrete clusters (like \"*Grandmother Neurons*\" and \"***Superposition***\") to smooth distributions - this shows this foundational bias is strong and ***leveragable for improved model design***.\n\n# The 'Foundational Bias' Papers:\n\n**Position (2nd) Paper: Isotropic Deep Learning (IDL) \\[**[**link**](https://doi.org/10.5281/zenodo.15476947)**\\]:**\n\n&gt;*TL;DR: Intended as a provocative position paper proposing the ramifications of redefining the building block primitives of DL. Explores several research directions stemming from this symmetry-redefinition and makes* ***numerous falsifiable predictions***. Motivates this new line-of-enquiry, indicating its implications from *model design* *to theorems contingent on current formulations. When contextualising this, a taxonomic system emerged providing a generalised, unifying symmetry framework.*\n\nPrimarily showcases *a new symmetry-led design axis across all primitives*, introducing a programme to learn about and leverage the consequences of building blocks as a new form of control on our models. The consequences are argued to be significant and an underexplored facet of DL.\n\nPredicts *how* our default choice of primitives may be quietly biasing networks, causing *a range* of unintended and interesting phenomena across various applications. New building blocks mean ***new network behaviours to unlock*** and avoid hidden harmful 'pathologies'.\n\nThis paper directly challenges any assumption that primitive functional *forms* are neutral choices. Providing *several predictions* surrounding interpretability phenomena as side effects of current primitive choices (*now empirically confirmed, see below*). Raising questions in optimisation, AI safety, and potentially adversarial robustness.\n\n&gt;There's also a [***handy blog***](https://medium.com/@george.bird.uom/draft-a-hidden-inductive-bias-at-the-heart-of-deep-learning-4e197b56f34c) that runs through these topics in a hopefully more approachable way.\n\n**Empirical (3rd) Paper: Quantised Representations (PPP) \\[**[**link**](https://arxiv.org/pdf/2507.12070)**\\]:**\n\n&gt;*TL;DR: By altering primitives it is shown that current ones cause representations to clump into clusters ---* *likely undesirable* *--- whilst symmetric alternatives keep them smooth.*\n\nProbes the consequences of altering the foundational building blocks, assessing their effects on representations. Demonstrates how foundational biases emerge from various symmetry-defined choices, including new activation functions.\n\nConfirms an IDL prediction: anisotropic primitives induce discrete representations, while isotropic primitives yield smoother representations that may support better interpolation and organisation. It disposes of the 'absolute frame' discussed in the SRM paper below.\n\nA **new perspective on several interpretability** **phenomena**, instead of being considered fundamental to deep learning systems, this paper instead shows *our choices induce them* ***‚Äî they are not fundamentals of DL!***\n\n'Anisotropic primitives' *are sufficient* to induce discrete linear features, grandmother neurons and potentially superposition.\n\n* Could this eventually affect how we pick activations/normalisers in practice? *Leveraging symmetry, just as ReLU once displaced sigmoids?*\n\n**Empirical (1st) Paper: Spotlight Resonance Method (SRM) \\[**[**link**](https://arxiv.org/abs/2505.13471)**\\]:**\n\n&gt;*TL;DR: A new tool shows primitives force activations to align with hidden axes, explaining why neurons often seem to represent specific concepts.*\n\nThis work shows there must be an \"absolute frame\" created by primitives in representation space: neurons and features align with special coordinates imposed by the primitives themselves. Rotate the basis, and the representations rotate too ‚Äî revealing that phenomena like \"grandmother neurons\" or superposition may be induced by our functional choices rather than fundamental properties of networks.\n\nThis paper motivated the initial reformulation for building blocks.\n\n# Overall:\n\nHopefully, an exciting research agenda, with a tangent enquiry on symmetry from existing GDL and Parameter Symmetries approaches.\n\nCurious to hear what others think of this research arc so far:\n\n* What reformulations or consequences (positive or negative) interest you most? Any implications I've missed?\n* If symmetry in our primitives is shaping how networks think, *should we treat it as a core design axis*?\n\nI hope this research direction may catch your interest for future collaborations on:\n\n&gt;*Discovering more undocumented effects of our functional form choices could be a productive research direction*, alongside designing new building blocks and leveraging them for better performance.",
    "author": "GeorgeBird1",
    "timestamp": "2025-10-16T02:10:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1o81atp/rd_a_quiet_bias_in_dls_building_blocks_with_big/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o6hs2w",
    "title": "[D] Only 17 days given to review 5 papers in ICLR 2026...",
    "content": "The paper assignments for ICLR 2026 are in today and I was assigned 5 papers to review. The review deadline is 31st October. I am not sure if this is the normal time period but seems very little. Last year I was assigned 2 papers and was able to write detailed and constructive reviews. ",
    "author": "casualcreak",
    "timestamp": "2025-10-14T07:57:02",
    "url": "https://reddit.com/r/MachineLearning/comments/1o6hs2w/d_only_17_days_given_to_review_5_papers_in_iclr/",
    "score": 117,
    "num_comments": 40,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o70jyv",
    "title": "[D] Curious asymmetry when swapping step order in data processing pipelines",
    "content": "Hi everyone,\n\n  \nI‚Äôve been running some experiments with my own model where I slightly reorder the steps in a data-processing pipeline (normalization, projection, feature compression, etc.), and I keep seeing a consistent pattern:  \none order gives stable residuals, while the reversed order systematically increases the error term ‚Äî across very different datasets.\n\nIt doesn‚Äôt look like a random fluctuation; the gap persists after shuffling labels and random seeds.\n\nHas anyone seen similar order-sensitivity in purely deterministic pipelines?  \nI‚Äôm wondering if this could just be numerical conditioning or if there‚Äôs something deeper about how information ‚Äúsettles‚Äù when the operations are reversed.",
    "author": "Eastern_Ad7674",
    "timestamp": "2025-10-14T20:35:38",
    "url": "https://reddit.com/r/MachineLearning/comments/1o70jyv/d_curious_asymmetry_when_swapping_step_order_in/",
    "score": 7,
    "num_comments": 8,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o62zfe",
    "title": "[D] Why are Monte Carlo methods more popular than Polynomial Chaos Expansion for solving stochastic problems?",
    "content": "I feel like MC methods are king for reinforcement learning and the like, but PCE‚Äôs are often cited as being more accurate and efficient. Recently while working on some heavy physics focused problems I‚Äôve found a lot of the folks in Europe use more PCE. Anyone have any thoughts as to why one is more popular? If you want to do a fun deep dive - polynomial chaos (or polynomial chaos expansion) have been a fun random stats deep dive. ",
    "author": "Alternative_iggy",
    "timestamp": "2025-10-13T18:57:53",
    "url": "https://reddit.com/r/MachineLearning/comments/1o62zfe/d_why_are_monte_carlo_methods_more_popular_than/",
    "score": 150,
    "num_comments": 20,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o6bdfd",
    "title": "[D] Dataset release - Unannotated Real world retail images 2014 &amp; 3 full store reference visits (14-16)",
    "content": "Happy to release some of our 1m image datasets for the wider community to work with.\n\n2014 set (full-res), unannotated, ships with manifest.csv (sha256, EXIF, dims, optional GPS). c. 6000 images across 22 retailers. These are of numerous elements in stores, ends, aisles, products etc.\n\n‚Ä¢ Reference visits: Tesco Lincoln 2014, Tesco Express 2015, Asda Leeds 2016 (unannotated; each with manifest). These are full stores (2014 not bay by bay but the other two stores are) c. 1910 items.\n\n‚Ä¢ Purpose: robustness, domain shift, shelf complexity, spatial awareness in store alongside wider developmental work.\n\n‚Ä¢ License: research/eval only; no redistribution.\n\n‚Ä¢ Planned v2: 2014 full annotations (PriceSign, PromoBarker, ShelfLabel, ProductBlock in some cases) alongside numerous other tags around categories, retailer, promo etc.\n\nContact:¬†[happytohelp@groceryinsight.com](mailto:happytohelp@groceryinsight.com)¬†for access and manifests which are being worked up. Questions welcomed.",
    "author": "malctucker",
    "timestamp": "2025-10-14T03:03:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1o6bdfd/d_dataset_release_unannotated_real_world_retail/",
    "score": 11,
    "num_comments": 1,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o5zhqo",
    "title": "[D]: Interview prep: What LC questions were u asked for AI/MLE/Research scientist roles",
    "content": "\nMy understanding is that they generally don't ask LC hard problems. But in your recent interview experience what problems were u asked.. please let us know as it's wild wild west out here\n\nEdit - LC I mean is leet code not ml coding where they ask u implement a transformer ",
    "author": "lan1990",
    "timestamp": "2025-10-13T16:16:56",
    "url": "https://reddit.com/r/MachineLearning/comments/1o5zhqo/d_interview_prep_what_lc_questions_were_u_asked/",
    "score": 46,
    "num_comments": 52,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o6g9b7",
    "title": "[D] Should I attend EMNLP 2025 in-person?",
    "content": "Hi all! My paper got accepted into a workshop in EMNLP 2025. I'm having a hard time deciding if I should attend it virtually or in-person.\n\nI'm a 2nd year undergraduate student (major not related to CS). This is my first paper and I have a few ML projects under my belt.\n\nI would like some thoughts on the pros and cons of attending. How beneficial will the networking be? Will I be overlooked because of my majorü´†?\nWhat should I actively do so that this benefits my career?\n\nPS: I will be getting some funds from my university and I would have to pay only a few hundred dollars at max and miss classes.",
    "author": "Greedy_Succotash_919",
    "timestamp": "2025-10-14T06:58:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1o6g9b7/d_should_i_attend_emnlp_2025_inperson/",
    "score": 2,
    "num_comments": 12,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o6ay44",
    "title": "[P] Generate detection rules",
    "content": "I would like to get your ideas. I am working on a project to automatically generate cybersecurity detection rules from blogs and/or user requests. \n\nMy initial approach hasn‚Äôt worked very well so far. I suspect this is because the model I‚Äôm using (`Kimi-K2`) struggles with the domain, as it differs from the data it was originally trained on. I‚Äôve also experimented with `Qwen3-32B` with similar results.\n\nThere are a few key requirements:\n\n* The system must run on-premises, due to the sensitive nature of detection rule data.\n* It must be able to generate detection rules from blog posts and/or user requests.\n\nFor example:\n\n    Can you write a rule for Linux that detects suspicious use of the cron utility, specifically when crontab jobs are being created or modified from files in the `/tmp` directory? I want this to focus on potential abuse for persistence or execution of malicious code, and it should be based on process creation logs. Please include ATT&amp;CK mappings for T1053.003 and note that legitimate admin activity could be a false positive.\n\nOr:\n\n    Generate a detection rule based on this: https://cloud.google.com/blog/topics/threat-intelligence/prc-nexus-espionage-targets-diplomats\n\n# My Current Approach\n\n1. **Content extraction** ‚Äì I use *crawl4ai* to fetch the content from URLs.\n2. **Content summarization** ‚Äì Since the raw content is often noisy, I summarize it to remove unnecessary elements such as cookie banners, headers, or navigation menus, while trying to preserve as much relevant information as possible.\n3. **Similarity retrieval** ‚Äì I retrieve similar detection rules from our internal database using a hybrid search approach, which works reasonably well.\n4. **Draft generation** ‚Äì I make an initial LLM request to generate a first draft of the rule, using a few-shot setup that includes the retrieved similar rules as context.\n5. **Reflection loop** ‚Äì I validate the generated rule‚Äôs syntax. If an error is found, the system re-enters the previous step, this time including the error message as additional context.\n\nHowever, this approach performs poorly. The detection block in the generated rules often fails to capture the actual detection logic correctly, leading to rules that look valid syntactically but don‚Äôt work effectively for their intended purpose.\n\nI also experimented with breaking down the generation process into multiple steps. For instance, first asking the model to determine the detection path or flow based on the blog content or user request. However, the results are still not very good.\n\nNow, I am considering fine-tuning a model using LoRA with a custom dataset that includes:\n\n* The blog post or user request as input, and\n* The corresponding final detection rule as output.\n\nI‚Äôd like to get your opinion on this approach and hear about other methods or architectures that might yield better results. Thank you!",
    "author": "Only_Emergencies",
    "timestamp": "2025-10-14T02:37:21",
    "url": "https://reddit.com/r/MachineLearning/comments/1o6ay44/p_generate_detection_rules/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o5gojz",
    "title": "[D] Need career advice, just got rejected for an Applied Scientist role at Microsoft",
    "content": "Currently, I work in a company where most, if not all, of my job revolves around consuming tools and APIs. I feel completely lost, as I‚Äôm forgetting the technical side of things since I‚Äôm no longer building or deploying anything, just using pre-existing cloud services.\n\nYes, I‚Äôve gained some cloud skills and I‚Äôm certified in both Azure and AWS, but I feel like I‚Äôm slowly killing my career. I got an interview at Microsoft last month and got rejected (which hit hard, not gonna lie). I had studied well, but when I talked about my projects, they felt dull, mostly about building simple RAG systems and connecting GPT APIs to other tools. The position required building and fine-tuning LLMs, which my company doesn‚Äôt support me to do at all.\n\nRight now, my self-esteem is really low. I feel like a slop because I‚Äôm just a consumer of products, not a creator. I don‚Äôt know what to do.\n\nI work another part-time job that‚Äôs also focused on consuming APIs, so I don‚Äôt have time to do anything else.\n\nthinking about dropping my part-time job so I can focus on my weak points.",
    "author": "gyhv",
    "timestamp": "2025-10-13T04:04:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1o5gojz/d_need_career_advice_just_got_rejected_for_an/",
    "score": 125,
    "num_comments": 41,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o5wpu3",
    "title": "[D] TEE GPU inference overhead way lower than expected - production numbers",
    "content": "Been running models in trusted execution environments for about 4 months now and finally have enough data to share real performance numbers.\n\nBackstory: we needed to process financial documents with LLMs but obviously couldn't send that data to external APIs. Tried homomorphic encryption first but the performance hit was brutal (like 100x slower). Federated learning didn't work for our use case either.\n\nEnded up testing TEE-secured inference and honestly the results surprised me. We're seeing around 7% overhead compared to standard deployment. That's for a BERT-based model processing about 50k documents daily.\n\nThe setup uses Intel TDX on newer Xeon chips. Attestation happens every few minutes to verify the enclave hasn't been tampered with. The cryptographic verification adds maybe 2-3ms per request which is basically nothing for our use case.\n\nWhat really helped was keeping the model weights inside the enclave and only passing encrypted inputs through. Initial load time is longer but inference speed stays close to native once everything's warm.\n\nFor anyone doing similar work with sensitive data, TEE is actually viable now. The performance gap closed way faster than I expected.\n\nAnyone else running production workloads in enclaves? Curious what performance numbers you're seeing.",
    "author": "ssunflow3rr",
    "timestamp": "2025-10-13T14:23:14",
    "url": "https://reddit.com/r/MachineLearning/comments/1o5wpu3/d_tee_gpu_inference_overhead_way_lower_than/",
    "score": 18,
    "num_comments": 7,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o6f798",
    "title": "[D] AAAI: Not able to post \"Ethics Chair comment\" on a review",
    "content": "https://preview.redd.it/4flfqzj2u2vf1.png?width=1604&amp;format=png&amp;auto=webp&amp;s=039506a12d6d6cee2813c0ba2bfa2214412a6534\n\nI am trying to post an \"Ethics Chair Author Comment\" for a review, and it keeps giving me error that Ethics Chair are not added. And there is no option to add \"Ethics Chair\" here too.\n\nAnyone else also facing same issue, how did you solve this? Or any chairs from AAAI can help with this, that will be really grateful?",
    "author": "Lost-Ingenuity5017",
    "timestamp": "2025-10-14T06:15:02",
    "url": "https://reddit.com/r/MachineLearning/comments/1o6f798/d_aaai_not_able_to_post_ethics_chair_comment_on_a/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o67ypt",
    "title": "pilot access to anonymised demographic + location datasets for AI fairness and model evaluation [P]",
    "content": "I‚Äôm a founder based in Australia working on Datalis, a project focused on making AI evaluation fairer and more transparent.\n\nWe‚Äôve built consent-verified, anonymised demographic and location panels that can be used to test models for bias, robustness, and representativeness.\nEverything‚Äôs aggregated ‚Äî no personal data, no scraping, no PII ‚Äî just structured ground-truth panels built ethically.\n\nWe‚Äôve just opened a free 30-day pilot program for AI teams and researchers who want to benchmark or stress-test their models against real demographic and geographic data.\nYou‚Äôll get a few CSV/Parquet samples (US + AU regions) and a short guide on how to integrate them into your evaluation workflow.\n\nIf you‚Äôre working on fairness, alignment, or model eval, or know someone who is, you can request pilot access here:\nüëâ datalis.app/pilot\n\nHappy to answer questions in the comments or trade notes with anyone tackling the same problem.",
    "author": "Crumbedsausage",
    "timestamp": "2025-10-13T23:22:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1o67ypt/pilot_access_to_anonymised_demographic_location/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o5gdtr",
    "title": "[D] Presenting NeurIPS paper at EurIPS",
    "content": "Hi, I have a NeurIPS poster to present. I initially selected SD as my choice of venue, but my US Visa application was rejected. I was hoping to present at EurIPS, but I am being told by my supervisors that I gotta present at Mexico if not SD. Is that true - is it not enough to present at EurIPS?\n\nIf I gotta present at Mexico, and I don't, say I don't get my visa or I don't feel safe flying to Mexico, what's going to happen? Are they going to retract my paper? Can someone else attending the conference, who is not an author on my paper, present in my place?",
    "author": "mio_11",
    "timestamp": "2025-10-13T03:48:02",
    "url": "https://reddit.com/r/MachineLearning/comments/1o5gdtr/d_presenting_neurips_paper_at_eurips/",
    "score": 27,
    "num_comments": 13,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o5qp3c",
    "title": "Detect over-compressed images in a dataset? [P]",
    "content": "Hey everyone,\n\nI‚Äôm building a small dataset (\\~1k images) for a generative AI project.\n\nThe problem is: a bunch of these images look visually bad.  \nThey‚Äôre technically high-res (1MP+), but full of JPEG artifacts, upscaled blurs, or over-compressed textures.\n\nSo far I‚Äôve tried:\n\nSharpness / Laplacian variance ‚Üí catches blur but misses compression\n\nEdge density + contrast heuristics ‚Üí helps a bit but still inconsistent\n\nManual review ‚Üí obviously not scalable\n\nI‚Äôm looking for a way (ideally opensource) to automatically filter out over-compressed or low-quality images, something that can score ‚Äúperceptual quality‚Äù without a reference image.\n\nMaybe there‚Äôs a pretrained no-reference IQA model?\n\nBonus points if it can be run or exported to Node.js / ONNX / TF.js for integration into my JS pipeline.\n\nAny recommendations or tricks to detect ‚ÄúJPEG hell‚Äù in large datasets are welcome üôè",
    "author": "nsvd69",
    "timestamp": "2025-10-13T10:45:32",
    "url": "https://reddit.com/r/MachineLearning/comments/1o5qp3c/detect_overcompressed_images_in_a_dataset_p/",
    "score": 3,
    "num_comments": 13,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o5fry6",
    "title": "[P] CleanMARL : a clean implementations of Multi-Agent Reinforcement Learning Algorithms in PyTorch",
    "content": "Hi everyone,\n\nI‚Äôve developed¬†**CleanMARL**, a project that provides clean, single-file implementations of Deep Multi-Agent Reinforcement Learning (MARL) algorithms in PyTorch. It follows the philosophy of CleanRL.\n\nWe also provide educational content, similar to Spinning Up in Deep RL, but for multi-agent RL.\n\n**What CleanMARL provides:**\n\n* Implementations of key MARL algorithms: VDN, QMIX, COMA, MADDPG, FACMAC, IPPO, MAPPO.\n* Support for parallel environments and recurrent policy training.\n* TensorBoard and Weights &amp; Biases logging.\n* Detailed documentation and learning resources to help understand the algorithms.\n\nYou can check the following:\n\n* Github repo:¬†[https://github.com/AmineAndam04/cleanmarl](https://github.com/AmineAndam04/cleanmarl)\n* Docs and learning resources:¬†[https://cleanmarl-docs.readthedocs.io](https://cleanmarl-docs.readthedocs.io/)\n\nI would really welcome any feedback on the project ‚Äì code, documentation, or anything else you notice.",
    "author": "AmineZ04",
    "timestamp": "2025-10-13T03:12:18",
    "url": "https://reddit.com/r/MachineLearning/comments/1o5fry6/p_cleanmarl_a_clean_implementations_of_multiagent/",
    "score": 14,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o5ovoe",
    "title": "[D] Is it acceptable to resize datasets for experiments?",
    "content": "Hello everyone,\n\nI‚Äôm a undergraduate student currently doing research in Computer Vision. My hardware resources are extremely limited - I mostly rely on Kaggle‚Äôs free GPUs to train my models. It‚Äôs been very difficult and time-consuming: for example, training a model with 10M parameters on 128√ó128 images and batch size 8 already takes around 10 hours. I can only imagine how much worse it would be with higher-resolution images or larger datasets.\n\n**My question is:** For authors and reviewers at major conferences, would it be acceptable if the experiments were conducted on downscaled images instead of the original resolution?\n\nOf course, I would resize all datasets consistently and reproduce baselines using the same resized data for fair comparison. I just want to confirm whether such a modification of the dataset is permissible or acceptable in practice.\n\n  \nThank you very much for your time and advice!",
    "author": "Feuilius",
    "timestamp": "2025-10-13T09:41:13",
    "url": "https://reddit.com/r/MachineLearning/comments/1o5ovoe/d_is_it_acceptable_to_resize_datasets_for/",
    "score": 4,
    "num_comments": 10,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o55qi1",
    "title": "[D] ICLR 2026 reviewer paper assignment?",
    "content": "[https://iclr.cc/Conferences/2026/SeniorAreaChairGuide](https://iclr.cc/Conferences/2026/SeniorAreaChairGuide)\n\nHere it says that ICLR review starts at Oct.10. It's Oct.12 and I haven't assigned any papers to review yet. That makes me wonder - has anyone gotten papers for review yet?",
    "author": "OkPie3766",
    "timestamp": "2025-10-12T17:46:39",
    "url": "https://reddit.com/r/MachineLearning/comments/1o55qi1/d_iclr_2026_reviewer_paper_assignment/",
    "score": 33,
    "num_comments": 24,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o5evcm",
    "title": "[D] Giving out CVs in ML conferences",
    "content": "Hello all, I am going to EMNLP2025 as a presenting author and in some conferences I went during my PhD I saw people giving out their CVs. I was thinking of doing that this time.\n\nFor example, I saw there are many company booths, should I look their website for any job posting and make custom CVs already with a position in mind? Or a general CV is best?\n\nWhat is your opinion on doing this? Any tips on preparing the CV or connecting with recruiters?\n\nThank you for your time.",
    "author": "Wild-Difference-7827",
    "timestamp": "2025-10-13T02:16:26",
    "url": "https://reddit.com/r/MachineLearning/comments/1o5evcm/d_giving_out_cvs_in_ml_conferences/",
    "score": 6,
    "num_comments": 24,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o4qu0h",
    "title": "[P] Adapting Karpathy‚Äôs baby GPT into a character-level discrete diffusion model",
    "content": "Hi everyone,\n\nI've been exploring how discrete diffusion models can be applied to text generation and put together a single annotated Jupyter Notebook that implements a character-level discrete diffusion GPT.\n\nIt's based on Andrej Karpathy‚Äôs baby GPT from his [nanoGPT](https://github.com/karpathy/nanoGPT) repo, but instead of generating text autoregressively (left-to-right), it learns to denoise corrupted text sequences in parallel.\n\n[Discrete diffusion model in action](https://i.redd.it/6noamol7zouf1.gif)\n\nThe notebook walks through the math, introduces what adding noise for discrete tokens means, builds discrete diffusion model from baby GPT, and trains it on Shakespeare's text using Score-Entropy based objective.\n\nAccess it on GitHub (notebook + README):  \n[https://github.com/ash80/diffusion-gpt](https://github.com/ash80/diffusion-gpt)  \nor run it directly on Google Colab:  \n[https://colab.research.google.com/github/ash80/diffusion-gpt/blob/master/The\\_Annotated\\_Discrete\\_Diffusion\\_Models.ipynb](https://colab.research.google.com/github/ash80/diffusion-gpt/blob/master/The_Annotated_Discrete_Diffusion_Models.ipynb)\n\nI'd appreciate any feedback, corrections, and suggestions, especially from anyone experimenting with discrete diffusion models.",
    "author": "ashz8888",
    "timestamp": "2025-10-12T07:42:22",
    "url": "https://reddit.com/r/MachineLearning/comments/1o4qu0h/p_adapting_karpathys_baby_gpt_into_a/",
    "score": 134,
    "num_comments": 10,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o56eb2",
    "title": "[D] Should I take the opportunity to present my accepted TIP paper at ICASSP or ICIP?",
    "content": "Hi everyone,\n\nI recently had my paper accepted to *IEEE Transactions on Image Processing (TIP)*.  \nIn the acceptance email, it mentions that I have the opportunity to submit the work to either *ICASSP* or *ICIP* for presentation.\n\nMy research focuses on **video understanding**, and I‚Äôm wondering whether this topic would be well-aligned with either of these conferences.\n\nI‚Äôm also nearing graduation, so I‚Äôm considering attending mainly for **networking purposes** ‚Äî to connect with people for post-doc or hiring opportunities.  \nFrom that perspective, would attending either ICASSP or ICIP make sense?\n\nIf you had to choose one, which would you recommend and why?\n\nI‚Äôd really appreciate hearing your thoughts or experiences.",
    "author": "Secondhanded_PhD",
    "timestamp": "2025-10-12T18:19:49",
    "url": "https://reddit.com/r/MachineLearning/comments/1o56eb2/d_should_i_take_the_opportunity_to_present_my/",
    "score": 12,
    "num_comments": 5,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o5vkx0",
    "title": "[D] A memory architecture for agents: analytics-driven selective forgetting + a privacy-preserving ‚Äúcollective gut‚Äù (seeking critique &amp; prior art)",
    "content": "Hi all‚Äîengineer/founder here. I‚Äôm exploring a selective memory architecture for AI agents and would love critical feedback (this is not a product pitch).\n\n\n\nMotivation / zeitgeist\n\nContext and retrieval costs dominate UX today; RAG-only stacks feel brittle; tool use returns too much. I think the bottleneck is attention economics and routing, not raw recall.\n\n\n\nSketch\n\n\t‚Ä¢\tFocus ‚Üí Fresh Memory ‚Üí Analytics Agent (decision layer)\n\n\t‚Ä¢\tRoutes into: procedures &amp; policies, practice/habits, success-gated long-term, and shock memory (incidents that should not decay)\n\n\t‚Ä¢\tA privacy-preserving collective ‚Äúgut‚Äù that aggregates patterns (not data) to form shared intuition across users\n\n\n\nWhy it might help\n\n\t‚Ä¢\tSelective forgetting reduces context bloat while keeping what matters\n\n\t‚Ä¢\t‚ÄúShock‚Äù tracks (security/cascade failures) resist decay\n\n\t‚Ä¢\tA shared ‚Äúgut‚Äù could raise baseline instincts without exposing user data\n\n\n\nOpen questions (where I need help):\n\n\t1.\tBenchmarks for selective forgetting &amp; routing (beyond standard retrieval evals)?\n\n\t2.\tFailure modes: bias amplification, drift, catastrophic forgetting vs. over-retention, adversarial ‚Äúshock‚Äù pollution?\n\n\t3.\tPrivacy proofs/schemes for pattern aggregation (DP/federated alternatives)?\n\n\t4.\tPrior art I should study next (cogsci/neurosymbolic/agent memory work)?\n\n\n\nWrite-up (conceptual, not a sales page):\n\n[https://medium.com/@cem.karaca/building-digital-consciousness-a-memory-architecture-inspired-by-human-cognition-437412791044](https://medium.com/@cem.karaca/building-digital-consciousness-a-memory-architecture-inspired-by-human-cognition-437412791044)\n\n\n\nNotes: I reference classic capacity work (Miller‚Äôs 7¬±2), but I‚Äôm aware later findings often suggest \\~4¬±1; I treat that as a design metaphor, not a hard limit. Also, any ‚Äúgoldfish memory‚Äù analogies are figurative, not biological claims.\n\n\n\nIf this breaks subreddit self-promo rules, mods please remove‚Äîmy intent is to get technical critique and pointers to prior art.",
    "author": "babaenki",
    "timestamp": "2025-10-13T13:40:55",
    "url": "https://reddit.com/r/MachineLearning/comments/1o5vkx0/d_a_memory_architecture_for_agents/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o57hfi",
    "title": "[P] Using Information Geometry and Physics to Build a New Multi-Day Pre-Warning Earthquake Prediction Algorithm and ML Model",
    "content": "I've made the complete codebase for my earthquake prediction model available on GitHub and am seeking review and collaboration from the seismology and data science communities.\n\nThis project explores a different approach to earthquake forecasting. The methodology is centered on advanced feature engineering using Symbolic Emergence Field Analysis (SEFA), which generates 77 distinct features from seismic data. These are combined with 10 temporal features to enable multi-day pre-warning capability. The model itself is a hybrid, using a physics-informed architecture (Symbolic Resolution Ladder) to ensure predictions adhere to real-world constraints. All training and tests used real USGS data from 1900-2023 to provide as many scenarios as possible.\n\nThe main challenge was to tune the system for a practical balance between detection and operational reliability. The latest ensemble model (60% Neural Network, 40% Gradient Boosting) achieves the following on the test set:\n\n\\-Sensitivity: 80.2% (correctly identifies 4 out of 5 earthquake events)\n\n\\-Specificity: 70.1%\n\n\\-AUC-ROC: 0.8275 (strong discriminative ability)\n\nThe goal here isn't a perfect \"crystal ball,\" but a more reliable forecasting tool. By accepting a minimal trade-off in raw detection, we gain a significant reduction in the false alarm rate, which is a major barrier for real-world deployment of predictive systems.\n\nI believe this methodology (particularly the SEFA feature set and the focus on a balanced performance profile) offers a promising direction. The project is fully open-sourced, with the aim of encouraging independent testing, validation, and further development.\n\nI'm really proud of what my SEFA+SRL formulas have achieved with this one. Hoping it can gain some traction and get into the right hands to make an impact!\n\nThe repository, including documentation and datasets, is available here: [https://github.com/severian42/SEFA-SRL-Earthquake-Prediction](https://github.com/severian42/SEFA-SRL-Earthquake-Prediction)",
    "author": "vesudeva",
    "timestamp": "2025-10-12T19:12:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1o57hfi/p_using_information_geometry_and_physics_to_build/",
    "score": 7,
    "num_comments": 2,
    "upvote_ratio": 0.68,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o563mh",
    "title": "Neurips 2025 Hotels San Diego [D]",
    "content": "All of the hotels in the official booking portal (for San Diego) appear as ‚Äúunavailable.‚Äù Does that mean that they haven‚Äôt been opened up yet? Or are they all fully booked?",
    "author": "Low-Policy7142",
    "timestamp": "2025-10-12T18:04:54",
    "url": "https://reddit.com/r/MachineLearning/comments/1o563mh/neurips_2025_hotels_san_diego_d/",
    "score": 6,
    "num_comments": 7,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o4hn2c",
    "title": "Any suggestions for Open source OCR tools [D]",
    "content": "Hi,\n\nI‚Äôm working on a complex OCR based big scale project. Any suggestion (no promotions please) about a non-LLM OCR tool (I mean open source) which I can use for say 100k+ pages monthly which might include images inside documents?\n\nAny inputs and insights are welcome.\n\nThanks in advance!",
    "author": "VividRevenue3654",
    "timestamp": "2025-10-11T23:14:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1o4hn2c/any_suggestions_for_open_source_ocr_tools_d/",
    "score": 32,
    "num_comments": 35,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o4j4lc",
    "title": "[D] Tips for first ML conference",
    "content": "I am going to attend a conference for the first time - ICCV. I am an undergrad, and don't know other people who are attending. What are some tips to get the most out of the conference?  \nAlso presenting a poster, so if there are any tips regarding that, I would appreciate that too. My research interests also have gotten broader beyond CV and the particular poster I am presenting so I am just nervous in general.",
    "author": "Vedaant7",
    "timestamp": "2025-10-12T00:48:50",
    "url": "https://reddit.com/r/MachineLearning/comments/1o4j4lc/d_tips_for_first_ml_conference/",
    "score": 15,
    "num_comments": 9,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o4mdnw",
    "title": "[p] Completely free mobile Android app for creating object detection training datasets - looking for beta testers",
    "content": "I built a mobile annotation tool for creating bounding box datasets on Android. It exports directly to Vertex AI format (JSONL) and supports multi-class labeling.\n\nLooking for beta testers who work with object detection datasets. All data stays local on device, no cloud required. No account or sign in needed aside from Google Play account to access the app and sign up for beta.\n\n\n\nKey features:\n\n\\- Smooth bounding box drawing/editing\n\n\\- Multi-label support per box \n\n\\- CSV label import \\[label name, category, optional color\\]\n\n\\- Export to Vertex AI JSONL or CSV\n\n  \n1: Join testing group: [ObjMark Test Group - Google Groups](https://groups.google.com/g/objmark-test-group)\n\n2: Wait up to 30 mins for account propagation\n\n3: Closed beta link, Android only: [https://play.google.com/store/apps/details?id=com.jdj.creates.ObjMarkApp](https://play.google.com/store/apps/details?id=com.jdj.creates.ObjMarkApp)\n\n\n\n\n\nFeedback appreciated, especially on export format compatibility and annotation workflow.",
    "author": "JDJCreates",
    "timestamp": "2025-10-12T04:13:33",
    "url": "https://reddit.com/r/MachineLearning/comments/1o4mdnw/p_completely_free_mobile_android_app_for_creating/",
    "score": 8,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o4jfp6",
    "title": "[D] AAAI 2026- Dealing with incorrect reviews?",
    "content": "Submitted a paper to AAAI. Most things look fine, but two reviewer points are confusing:\n\n* A reviewer cited another paper and claimed it outperforms ours, but the metrics in that cited paper are actually *lower* than ours.\n* Another reviewer recommended rejection for ‚Äúmissing training details,‚Äù even though we included them in the supplementary and one-line mentioned them in the main text. (also the review appears to be too harsh)\n\n**Questions:**\n\n1. For those with AAAI experience, how effective is the **Author Review Evaluation** in practice? Does it meaningfully influence the meta-review/decision?\n2. What exactly does the **Ethics Chair Author Comment** do, and in what situations should it be used instead of (or in addition to) the Author Review Evaluation?\n\nThank you!",
    "author": "Forsaken-Order-7376",
    "timestamp": "2025-10-12T01:08:29",
    "url": "https://reddit.com/r/MachineLearning/comments/1o4jfp6/d_aaai_2026_dealing_with_incorrect_reviews/",
    "score": 14,
    "num_comments": 15,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o4lxsy",
    "title": "[D] Advice needed for Fine Tuning Multimodal Language model",
    "content": "Heyy . We are stuck in a problem regarding the Amazon ML challenge 2025 .\nWe have formulated a solution but it is not getting us in the top 50 required to qualify for next stage . \n\nWe are thinking of Fine tuning a Multimodal model available on hugging face .\n\nProblem statement :\nThe challenge is to build an ML model that predicts product prices using text data (catalog_content) and image data (image_link) from e-commerce products.\nYou‚Äôll train the model on 75K labeled samples and predict prices for 75K test samples.\nEvaluation is based on SMAPE (Symmetric Mean Absolute Percentage Error) - lower is better.\n\nNow , I need few tips regarding this because I've never worked on fine tuning an llm before . Firstly , which model should I use and with how many parameters . \nSecondly , We don't have good GPUs for this , Should I purchase the Pro version of Google colab . And If I do purchase it , will the training be possible before 12 AM tomorrow ? \n",
    "author": "Different-Wear2261",
    "timestamp": "2025-10-12T03:47:27",
    "url": "https://reddit.com/r/MachineLearning/comments/1o4lxsy/d_advice_needed_for_fine_tuning_multimodal/",
    "score": 7,
    "num_comments": 8,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o55xjl",
    "title": "[D] are world models primarily for visual worlds or the underlying technology can also help in build a model for engineering infra (like services and the connections between them and infra)?",
    "content": "I am trying to research world models to see what it can power? I see current demos are built more focused as visual world like https://marble.worldlabs.ai/\n\nI was curious if the underlying architecture can be used for more generic use cases like making models learn about an environment - say an engineering infra of a company (like services and the connections between them and infra)?\n\n\n\nhttps://www.reddit.com/r/MachineLearning/comments/1kf3pes/discussion_what_exactly_are_world_models_in_ai/",
    "author": "pranay01",
    "timestamp": "2025-10-12T17:56:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1o55xjl/d_are_world_models_primarily_for_visual_worlds_or/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o4q133",
    "title": "[D] Natural language translation dataset in a specified domain",
    "content": "Natural language translation dataset in a specified domain\n\nIs a natural language translation dataset from ENG to another language in a very specific domain worthwhile to curate for conference submission?\n\nI am a part-time translator working in this specific domain who is originally a student wondering if this could be a potential submission. I have quite several peers who are willing to put in the effort to curate a decent sized dataset (~2k) translated scripts for research use for conference submission.\n\nHowever, I am not quite confident as to how useful or meaningful of a contribution this will be to the community.",
    "author": "AdGlittering3010",
    "timestamp": "2025-10-12T07:09:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1o4q133/d_natural_language_translation_dataset_in_a/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o458z9",
    "title": "[D] Best videos of talks on using RL to train reasoning models",
    "content": "I like to watch videos to quickly catch up on literature before deciding what to read more carefully.\n\nI am looking for YouTube videos about using RL to train reasoning models. I am interested in both both overview videos and videos about specific approaches.\n\nThere are a number of influencers (for the lack of a better term). Way too superficial for my taste. I am interested in videos of scientific talks.\n\nAny suggestions?\n",
    "author": "gized00",
    "timestamp": "2025-10-11T13:01:05",
    "url": "https://reddit.com/r/MachineLearning/comments/1o458z9/d_best_videos_of_talks_on_using_rl_to_train/",
    "score": 9,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o4kxem",
    "title": "[D] Finally found a way to run AI on patient data without HIPAA nightmares - hardware encryption actually works",
    "content": "Been pulling my hair out trying to run inference on patient scans without exposing PHI. Legal wouldn't let us use standard cloud providers, on-prem was too expensive, and homomorphic encryption made everything 100x slower.\n\nTried everything from differential privacy to federated learning but nothing really worked for production. Stumbled onto TEE computing through phala network and honestly thought it was too good to be true. But after testing, we're getting 95% of normal speed while keeping data encrypted during processing.\n\nThe crazy part is how simple the deployment was compared to our previous attempts. No more explaining to compliance why our encryption is \"probably safe enough.\" The hardware attestation just proves it mathematically.\n\nAnyone else dealing with similar privacy requirements? Curious what others are using for sensitive inference workloads.",
    "author": "ryukendo_25",
    "timestamp": "2025-10-12T02:44:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1o4kxem/d_finally_found_a_way_to_run_ai_on_patient_data/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o4c8c2",
    "title": "[P] Why R‚Äôs MissForest Fails in Prediction Tasks?",
    "content": "[Image by author](https://preview.redd.it/clw9ynmozkuf1.png?width=1400&amp;format=png&amp;auto=webp&amp;s=34ac2eed158c3600bd198c414c6edf9a4582e975)\n\nI‚Äôve been working with R‚Äôs MissForest for some time, and I recently ran into a subtle limitation that‚Äôs easy to miss.\n\nThe algorithm is powerful for imputation, but when used in predictive settings, it quietly breaks a key principle: the separation between training and test data.\n\nThis led me to explore why MissForest fails in such cases, and how the newer `MissForestPredict` approach resolves this issue by preserving consistency between learning and application.\n\nI wrote a short piece that explains this clearly.\n\nüëâ [https://medium.com/@jumbongjunior/why-the-r-missforest-fails-in-prediction-tasks-a-key-limitation-you-need-to-keep-in-mind-33e54f8fe69a](https://medium.com/@jumbongjunior/why-the-r-missforest-fails-in-prediction-tasks-a-key-limitation-you-need-to-keep-in-mind-33e54f8fe69a)\n\nI‚Äôd love to hear how others handle similar imputation issues in their predictive workflows.",
    "author": "North-Kangaroo-4639",
    "timestamp": "2025-10-11T18:16:12",
    "url": "https://reddit.com/r/MachineLearning/comments/1o4c8c2/p_why_rs_missforest_fails_in_prediction_tasks/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o3oa5t",
    "title": "Regarding NeurIPS 2025 registration [D]",
    "content": "I understand that this year's NeurIPS will be held in two locations: San Diego and Mexico City. My paper has been accepted, but I haven't been notified yet about where I will be presenting. However, on the registration page, the fees are different depending on the presentation location.\n\nI was wondering what the situation is for other people in a similar position.",
    "author": "Kwangryeol",
    "timestamp": "2025-10-10T23:20:56",
    "url": "https://reddit.com/r/MachineLearning/comments/1o3oa5t/regarding_neurips_2025_registration_d/",
    "score": 10,
    "num_comments": 6,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o3dyl1",
    "title": "[D] NeurIPS Financial Assistance Notification",
    "content": "Did anyone get the notification? Early registration deadline is coming up, and wondering if I missed it.",
    "author": "Inevitable_Charge828",
    "timestamp": "2025-10-10T14:42:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1o3dyl1/d_neurips_financial_assistance_notification/",
    "score": 9,
    "num_comments": 26,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o2pzxk",
    "title": "[R] DeepSeek 3.2's sparse attention mechanism",
    "content": "[https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek\\_V3\\_2.pdf](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf)\n\nThe new DeepSeek model uses a novel sparse attention mechanism, with a lightning indexer and a token selection mechanism. Please feel free to discuss in this thread :)\n\nAre there any open-source implementations of this (eg. in PyTorch) that can be used for training transformers from scratch? The DeepSeek implementation involves FlashMLA kernel, which seems rather complex.\n\n[https://github.com/deepseek-ai/FlashMLA/pull/98](https://github.com/deepseek-ai/FlashMLA/pull/98)",
    "author": "random_sydneysider",
    "timestamp": "2025-10-09T20:15:04",
    "url": "https://reddit.com/r/MachineLearning/comments/1o2pzxk/r_deepseek_32s_sparse_attention_mechanism/",
    "score": 137,
    "num_comments": 11,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o2xl4x",
    "title": "[P] Lossless compression for 1D CNNs",
    "content": "I‚Äôve been quietly working on something I think is pretty cool, and I‚Äôd love your thoughts before I open-source it.\nI wanted to see if we could compress 1D convolutional networks without losing a single bit of accuracy‚Äîspecifically for signals that are periodic or treated as periodic (like ECGs, audio loops, or sensor streams). The idea isn‚Äôt new in theory but I want to explore it as best as I can.\nSo I built a wrapper that stores only the first row of each convolutional kernel (e.g., 31 values instead of 31,000) and runs inference entirely via FFT. No approximations. No retraining.\nOn every single record in PTB-XL (clinical ECGs), the output matches the baseline PyTorch Conv1d to within 7.77e-16‚Äîwhich is basically numerically identical.\nI‚Äôm also exploring quiver representation theory to model multi-signal fusion (e.g., ECG + PPG + EEG as a directed graph of linear maps), but even without that layer, the core compression is solid.\n\nIf there‚Äôs interest, I‚Äôll clean it up and release it under a permissive license as soon as I can.\n\nEdit: Apologies, the original post was too vague.\n\nFor those asking about the \"first row of the kernel\" ‚Äî that's my main idea. The trick is to think of the convolution not as a small sliding window, but as a single, large matrix multiplication (the mathematical view). For periodic signals, this large matrix is a circulant matrix. My method stores only the first row of that large matrix.\n\nThat single row is all you need to perfectly reconstruct the entire operation using the FFT. So, to be perfectly clear: I'm compressing the model parameters, not the input data. That's the compression.\n\nHope that makes more sense now.\n\nGitHub Link: https://github.com/fabrece/Equivariant-Neural-Network-Compressor",
    "author": "individual_perk",
    "timestamp": "2025-10-10T03:49:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1o2xl4x/p_lossless_compression_for_1d_cnns/",
    "score": 16,
    "num_comments": 25,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o2vmex",
    "title": "[R] How to retrieve instructions given to annotators - RLHF",
    "content": "Hello,\n\r\nI am a communications student, and as part of my thesis, I would like to collect data related to RLHF for analysis.\r\n\r\nThe topic of my thesis is: Human-induced communication and intercultural biases in LLMs: the consequences of RLHF models.\r\n\r\nThe data I would like to collect is the instructions given to annotators, which guide the human feedback work in the RLHF process.\r\n\r\nMy goal is to analyze these different instructions, coming from different providers/nationalities, to see if the way these instructions are constructed can influence LLM learning.\r\n\r\nAccording to my research, this data is not publicly available, and I would like to know if there is a way to collect it for use in an academic project, using an ethical and anonymizing methodology.\n\r\nIs contacting subcontractors a possibility? Are there any leaks of information on this subject that could be used?\r\n\r\n\r\nThank you very much for taking the time to respond, and for your answers!\n\r\nHave a great day.",
    "author": "nonchargingphone",
    "timestamp": "2025-10-10T01:49:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1o2vmex/r_how_to_retrieve_instructions_given_to/",
    "score": 11,
    "num_comments": 7,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o2334q",
    "title": "[D] Anyone using smaller, specialized models instead of massive LLMs?",
    "content": "My team‚Äôs realizing we don‚Äôt need a billion-parameter model to solve our actual problem, a smaller custom model works faster and cheaper. But there‚Äôs so much hype around bigger is better. Curious what others are using for production cases.",
    "author": "blank_waterboard",
    "timestamp": "2025-10-09T04:11:46",
    "url": "https://reddit.com/r/MachineLearning/comments/1o2334q/d_anyone_using_smaller_specialized_models_instead/",
    "score": 98,
    "num_comments": 52,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o26m9z",
    "title": "[D] AAAI 26: Rebuttal cannot",
    "content": "Edit: Sorry for the incomplete title. I meant: ‚ÄúRebuttal cannot agree and correct factual error?‚Äù\n\nI am a bit confused this year. In the guidelines, the following is stated: ‚ÄúAuthors are discouraged from discussing new results or planned improvements, as reviewers are only able to evaluate the paper as originally submitted‚Äù.\n\nThus, imagine I have a theorem and a reviewer is pointing out an error in it. In other words, this is a factual error that I agree with, but correcting it is simple and does not imply modifying the rest of the paper. Can I not correct it and say I corrected it?",
    "author": "SignificanceFit3409",
    "timestamp": "2025-10-09T06:56:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1o26m9z/d_aaai_26_rebuttal_cannot/",
    "score": 24,
    "num_comments": 2,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o2r5j4",
    "title": "[R] A Unified Framework for Continual Semantic Segmentation in 2D and 3D Domains",
    "content": "Evolving visual environments pose significant challenges for continual semantic segmentation, introducing complexities such as class-incremental learning, domain-incremental learning, limited annotations, and the need to leverage unlabeled data. FoSSIL (Few-shot Semantic Segmentation for Incremental Learning) provides a comprehensive benchmark for continual semantic segmentation, covering both 2D natural scenes and 3D medical volumes. The evaluation suite includes diverse and realistic settings, utilizing both labeled (few-shot) and unlabeled data.\n\nBuilding on this benchmark, **guided noise injection** is introduced to mitigate overfitting arising from novel few-shot classes across diverse domains. **Semi-supervised learning** is employed to effectively leverage unlabeled data, augmenting the representation of few-shot novel classes. Additionally, a **novel pseudo-label filtering mechanism** removes highly confident yet incorrectly predicted labels, further improving segmentation accuracy. These contributions collectively offer a robust approach to continual semantic segmentation in complex, evolving visual environments.\n\nEvaluation across class-incremental, few-shot, and domain-incremental scenarios, both with and without unlabeled data, demonstrates the efficacy of the proposed strategies in achieving robust semantic segmentation under complex, evolving conditions. The framework provides a systematic and effective approach for continual semantic segmentation in dynamic real-world environments. Extensive benchmarking across natural 2D and medical 3D domains reveals critical failure modes of existing methods and offers actionable insights for the design of more resilient continual segmentation models.\n\nCode: [https://github.com/anony34/FoSSIL](https://github.com/anony34/FoSSIL)\n\nWebpage: [https://anony34.github.io/Fossil\\_webpage/](https://anony34.github.io/Fossil_webpage/)\n\nTheoretical analysis: [https://anony34.github.io/Fossil\\_webpage/theory.html](https://anony34.github.io/Fossil_webpage/theory.html)",
    "author": "Intrepid_Discount_67",
    "timestamp": "2025-10-09T21:14:31",
    "url": "https://reddit.com/r/MachineLearning/comments/1o2r5j4/r_a_unified_framework_for_continual_semantic/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o1jdd7",
    "title": "[D] Bad Industry research gets cited and published at top venues. (Rant/Discussion)",
    "content": "Just a trend I've been seeing. Incremental papers from Meta, Deepmind, Apple, etc. often getting accepted to top conferences with amazing scores or cited hundreds of times, however the work would likely never be published without the \"industry name\". Even worse, sometimes these works have apparent flaws in the evaluation/claims. \n\nExamples include:\nMeta Galactica LLM: Got pulled away after just 3 days for being absolutely useless. Still cited 1000 times!!!!! (Why do people even cite this?)\n\nMicrosoft's quantum Majorana paper at Nature (more competitive than any ML venue), while still having several faults and was retracted heavily. This paper is infamous in the physics community as many people now joke about Microsoft quantum.\n\nApple's illusion of thinking. (still cited a lot) (Arguably incremental novelty, but main issue was the experimentation related to context window sizes)\n\nAlpha fold 3 paper: Was accepted without any code/reproducibility initially at Nature got highly critiqued forcing them to release it. Reviewers should've not accepted before code was released (not the opposite)\n\nThere are likely hundreds of other examples you've all seen these are just some controversial ones. I don't have anything against industry research, in fact I support it and I'm happy it get's published. There is certainly a lot of amazing groundbreaking work coming from industry that I love to follow and work further on. I'm just tired of people treating and citing all industry papers like they are special when in reality most papers are just okay.",
    "author": "Foreign_Fee_5859",
    "timestamp": "2025-10-08T11:51:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1o1jdd7/d_bad_industry_research_gets_cited_and_published/",
    "score": 253,
    "num_comments": 72,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o2b9vc",
    "title": "[R] Trying to understand the sense behind CodeBleu",
    "content": "Apologies if I failed to grab the concept properly. But since the applications/samples we test our model on using CodeBleu (to my knowledge atleast) isnt same across the board. How can two researchers compare the CodeBleu scores they got on each of their separate LLMs. I am talking about research papers publishing their CodeBleu Scores.\n\nTo summarize, we take an example of our choice, run it using codebleu across many models and say that ours did better. Papers dont mention these examples, who is to say they didnt cherry picked a really specific one that their model performs better on. CodeBleu doesnt feels just/standardized.\n\nOr are there standard datasets to be used with CodeBleu for example a set of 100 python problems available as a standard dataset?",
    "author": "Minute-Plantain-1213",
    "timestamp": "2025-10-09T09:53:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1o2b9vc/r_trying_to_understand_the_sense_behind_codebleu/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o2e3t9",
    "title": "[D] üß¨ Built an ML-based Variant Impact Predictor (non-deep learning) for genomic variant prioritization",
    "content": "Hey folks,\n\nI‚Äôve been working on a small ML project over the last month and thought it might interest some of you doing variant analysis or functional genomics.\n\nIt‚Äôs a non-deep-learning model (Gradient Boosting / Random Forests) that predicts the functional impact of genetic variants (SNPs, indels) using public annotations like ClinVar, gnomAD, Ensembl, and UniProt features.\n\nThe goal is to help filter or prioritize variants before downstream experiments ‚Äî for example:\n\nranking variants from a new sequencing project,\n\ntriaging ‚Äúvariants of unknown significance,‚Äù or\n\nfocusing on variants likely to alter protein function.\n\n\nThe model uses features like:\n\nconservation scores (PhyloP, PhastCons),\n\nallele frequencies,\n\nfunctional class (missense, nonsense, etc.),\n\ngene constraint metrics (like pLI), and\n\npre-existing scores (SIFT, PolyPhen2, etc.).\n\n\nI kept it deliberately lightweight ‚Äî runs easily on Colab, no GPUs, and trains on openly available variant data. It‚Äôs designed for research-use-only and doesn‚Äôt attempt any clinical classification.\n\nI‚Äôd love to hear feedback from others working on ML in genomics ‚Äî particularly about useful features to include, ways to benchmark, or datasets worth adding.\n\nIf anyone‚Äôs curious about using a version of it internally (e.g., for variant triage in a research setting), you can DM me for details about the commercial license.\n\nHappy to discuss technical stuff openly in the thread ‚Äî I‚Äôm mostly sharing this because it‚Äôs been fun applying classical ML to genomics in a practical way",
    "author": "Dear_Raise_2073",
    "timestamp": "2025-10-09T11:40:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1o2e3t9/d_built_an_mlbased_variant_impact_predictor/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o2st75",
    "title": "[R] Need endorsement on Arxiv cs.AI",
    "content": "I am an independent researcher. My submissions have recently been published in AI symposiums and in the past I have published in IEEE.¬†I'm looking to upload it to the arxiv I need an endorsement for¬†[CS.AI](http://CS.AI). Thanks in advance.\n\nEndorsement code: 69BL48\n\n[https://arxiv.org/auth/endorse?x=69BL48](https://arxiv.org/auth/endorse?x=69BL48)",
    "author": "babganoush",
    "timestamp": "2025-10-09T22:48:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1o2st75/r_need_endorsement_on_arxiv_csai/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.12,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o263y9",
    "title": "[P] Startup help on setting workflow/infra - Computer Vision",
    "content": "Greetings,\n\nWe are a small team of 6 people that work on a startup project in our free time (mainly computer vision + some algorithms etc.). So far, we have been using the roboflow platform for labelling, training models etc. However, this is very costly and we cannot justify 60 bucks / month for labelling and limited credits for model training with limited flexibility.\n\n  \nWe are looking to see where it is worthwhile to migrate to, without needing too much time to do so and without it being too costly.\n\n  \nCurrently, this is our situation: \n\n\\- We have a small grant of 500 euros that we can utilize. Aside from that we can also spend from our own money if it's justified. The project produces no revenue yet, we are going to have a demo within this month to see the interest of people and from there see how much time and money we will invest moving forward. In any case we want to have a migration from roboflow set-up to not have delays.\n\n\\- We have setup an S3 bucket where we keep our datasets (so far approx. 40GB space) which are constantly growing since we are also doing data collection. We also are renting a VPS where we are hosting CVAT for labelling. These come around 4-7 euros / month. We have set up some basic repositories for drawing data, some basic training workflows which we are trying to figure out, mainly revolving around YOLO, RF-DETR, object detection and segmentation models, some timeseries forecasting, trackers etc. We are playing around with different frameworks so we want to be a bit flexible.\n\n\\- We are looking into renting VMs and just using our repos to train models but we also want some easy way to compare runs etc. so we thought something like MLFlow. We tried these a bit but it has an initial learning process and it is time consuming to setup your whole pipeline at first.\n\n  \n\\-&gt; What would you guys advice in our case? Is there a specific platform you would recommend us going towards? Do you suggest just running in any VM on the cloud ? If yes, where and what frameworks would you suggest we use for our pipeline? Any suggestions are appreciated and I would be interested to see what computer vision companies use etc. Of course in our case the budget would ideally be less than 500 euros for the next 6 months in costs since we have no revenue and no funding, at least currently. \n\nTL;DR - Which are the most pain-free frameworks/platforms/ways to setup a full pipeline of data gathering -&gt; data labelling -&gt; data storage -&gt; different types of model training/pre-training -&gt; evaluation -&gt; comparison of models -&gt; deployment on our product etc. when we have a 500 euro budget for next 6 months making our lives as much as possible easy while being very flexible and able to  train different models, mess with backbones, transfer learning etc. without issues.\n\n  \nFeel free to ask for any additional information.\n\n  \nThanks!",
    "author": "Rep_Nic",
    "timestamp": "2025-10-09T06:35:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1o263y9/p_startup_help_on_setting_workflowinfra_computer/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o1t6hl",
    "title": "[D] AAAI 2026 Phase 2 Rebuttals: 2500 characters specifics",
    "content": "There's been some confusion about whether rebuttals should be 2500 characters **per reviewer** or 2500 characters overall. Below I posted a screenshot of the message sent out the last conference (AAAI 2025) which states that it is 2500 characters per reviewer, but this time at AAAI 2026 the wording implies that it is 2500 characters **overall for a single rebuttal covering all reviewers**.\n\nHas anyone been able to get in touch with the AAAI committee for a clarification?  \n\n\nhttps://preview.redd.it/edmmtrx7nztf1.png?width=1688&amp;format=png&amp;auto=webp&amp;s=f3103ca61e91a43842773f4a193a87f276adfd3d\n\n",
    "author": "Adventurous-Cut-7077",
    "timestamp": "2025-10-08T18:24:46",
    "url": "https://reddit.com/r/MachineLearning/comments/1o1t6hl/d_aaai_2026_phase_2_rebuttals_2500_characters/",
    "score": 8,
    "num_comments": 31,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o2h5o6",
    "title": "[D] Interpretable Models: The New Norm in Data Science Consulting?",
    "content": "Hello everyone,\n\nI would like to collaboratively define a reasonable portfolio to specialize in managing a freelance consulting business as a Data Scientist.\n\nConsidering that there are people here who have worked independently as Data Scientists and have observed the types of problems clients usually bring to them.\n\nPlease, let us know what kinds of problems or models you have frequently dealt with as freelance consultants. It could be interesting for all of us to share and learn together about the current state of the Data Science market.\n\nI would like to reduce the overwhelming number of Machine Learning models and potential problems in order to build potential specializations for freelance Data Science consultants.\n\nThank you.",
    "author": "Satirosix",
    "timestamp": "2025-10-09T13:37:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1o2h5o6/d_interpretable_models_the_new_norm_in_data/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o2aodn",
    "title": "[D] Une nouvelle approche pour pr√©dire les points de basculement dans les syst√®mes complexes - Discussion sp√©culative",
    "content": "\nAvertissement important : Ce texte a √©t√© produit avec l'assistance d'une IA. Il s'agit d'une sp√©culation th√©orique destin√©e √† stimuler la discussion, et non d'une th√©orie √©tablie. Je ne suis pas expert en la mati√®re - je cherche des retours sur cette id√©e √©mergente.\n\n---\n\nLe Probl√®me Fondamental : Pourquoi les crise nous surprennent-ils ? ?\n\nNous vivons dans un monde de syst√®mes complexes - climat, march√©s financiers, √©cosyst√®mes - qui pr√©sentent des points de basculement soudains. Malgr√© nos mod√®les sophistiqu√©s, nous √©chouons souvent √† anticiper ces transitions critiques.\n\nExemples historiques :\n\n¬∑ La crise financi√®re de 2008 (les mod√®les n'ont pas capt√© la fragilit√© croissante)\n¬∑ L'effondrement de la p√™cherie de morue de Terre-Neuve (malgr√© les donn√©es abondantes)\n¬∑ Les transitions climatiques abruptes dans les carottes glaciaires\n\nL'Id√©e √âmergente : Mesurer la \"Sant√©\" des Relations Causales\n\nLes mod√®les actuels se concentrent sur les variables observables (prix, temp√©ratures, populations). Et si nous devions plut√¥t mesurer la stabilit√© des relations causales elles-m√™mes ?\n\nAnalogie simple :\nImaginez mesurer non pas combien un pont vibre,mais la solidit√© des connexions entre ses poutres. Avant l'effondrement, ces connexions deviennent \"fragiles\" m√™me si les vibrations semblent normales.\n\nCe Que Pourraient √ätre les \"M√©triques de Stabilit√© Causale\"\n\nD'apr√®s des travaux r√©cents en mod√©lisation stochastique avanc√©e (comme le mod√®le de Ginzburg-Landau √©tendu avec m√©moire), on pourrait d√©velopper des mesures qui :\n\n1. Quantifient la \"rigidit√© causale\" - √† quel point les relations cause-effet sont stables\n2. Mesurent la \"r√©silience m√©morielle\" - comment le pass√© influence le pr√©sent\n3. Cartographient la \"coh√©rence dimensionnelle\" - si la complexit√© du syst√®me √©volue harmonieusement\n\nApplications Potentielles\n\n¬∑ Finance : D√©tecter quand les relations entre march√©s deviennent fragiles\n¬∑ Climat : Anticiper les changements de r√©gime m√©t√©orologiques\n¬∑ Biologie : Pr√©dire l'effondrement d'√©cosyst√®mes\n¬∑ Sant√© publique : Identifier les seuils √©pid√©miques avant qu'ils ne soient franchis\n\nPr√©cautions et Limites Essentielles\n\nCeci est sp√©culatif et n√©cessite :\n\n1. Validation empirique rigoureuse - pour l'instant, c'est principalement th√©orique\n2. D√©veloppement math√©matique - les outils formels manquent encore\n3. Tests sur donn√©es historiques - v√©rifier r√©trospectivement si l'approche aurait fonctionn√©\n4. Collaboration interdisciplinaire - entre math√©maticiens, physiciens, √©cologues, √©conomistes\n\nQuestions pour la Communaut√©\n\n¬∑ Connaissez-vous des travaux similaires en math√©matiques appliqu√©es ?\n¬∑ Comment pourrions-nous tester exp√©rimentalement ces concepts ?\n¬∑ Quelles seraient les limitations fondamentales de cette approche ?\n¬∑ Y a-t-il des domaines o√π cette id√©e serait particuli√®rement prometteuse ?\n\nR√©f√©rences pour Approfondir\n\n¬∑ Scheffer, M. et al. (2009) \"Early-warning signals for critical transitions\"\n¬∑ Ginzburg-Landau theory extensions with memory terms\n¬∑ Tipping point detection in complex systems literature\n\nJe recherche des retours critiques et constructifs - cette id√©e en est √† ses d√©buts et a besoin d'√™tre confront√©e √† la r√©alit√© !\n\n",
    "author": "GlobalZivotPrint",
    "timestamp": "2025-10-09T09:30:43",
    "url": "https://reddit.com/r/MachineLearning/comments/1o2aodn/d_une_nouvelle_approche_pour_pr√©dire_les_points/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o11x3s",
    "title": "[D] Attending a conference without an accepted paper",
    "content": "Through my company, I've been given the opportunity to attend an ML conference without having a paper accepted at the venue. This is my first time attending any conference.\n\nWhat should I be doing to get as much as I can from the conference? I've seen other posts similar to this, but the OPs seem to have an accepted paper. I'm wondering if the advice is any different, given that I don't have an accepted paper. Some things I consider important - learning new things, making connections (esp with potential future PhD advisors)",
    "author": "curry2736",
    "timestamp": "2025-10-07T22:21:04",
    "url": "https://reddit.com/r/MachineLearning/comments/1o11x3s/d_attending_a_conference_without_an_accepted_paper/",
    "score": 69,
    "num_comments": 15,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o17yew",
    "title": "[R] 2026 Winter/Summer Schools on Diffusion or Flow Models",
    "content": "Hey folks! I‚Äôm currently doing a PhD and need to attend a subject specific summer or winter school next year. I‚Äôm particularly interested in anything focused on diffusion models, flow models, or related areas in generative AI. If you‚Äôve attended any good ones in the UK or Europe or know of any coming up in 2026 I‚Äôd really appreciate your suggestions. Thanks in advance ",
    "author": "Few-Annual-157",
    "timestamp": "2025-10-08T04:34:39",
    "url": "https://reddit.com/r/MachineLearning/comments/1o17yew/r_2026_wintersummer_schools_on_diffusion_or_flow/",
    "score": 16,
    "num_comments": 8,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o15tog",
    "title": "[d] how to develop with LLMs without blowing up the bank",
    "content": "I'm new to developing with LLMs. Qwen recently released some cool multimodal models that can seamlessly work with video, text and audio. Ofc this requires a lot of GPU. Renting one from AWS costs about a dollar per hour which doesn't make sense if I'm developing something which could cost $100+ just in the development phase. Is it possible to only pay for the time you actually use the GPU and not be charged for the time it is idle? What other common ways are there to tinker and develop with these models besides dropping a lot of money? Feel like I'm missing something. I saw Baseten allows for \"pay-per-inference\" style of GPU use but I haven't explored it much yet",
    "author": "throwaway102885857",
    "timestamp": "2025-10-08T02:30:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1o15tog/d_how_to_develop_with_llms_without_blowing_up_the/",
    "score": 13,
    "num_comments": 22,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o115a0",
    "title": "[P] MLX port of BDH (Baby Dragon Hatchling) is up",
    "content": "I‚Äôve ported the BDH (¬†[https://github.com/pathwaycom/bdh](https://github.com/pathwaycom/bdh)¬†) model to MLX for Apple Silicon. It‚Äôs a faithful conversion of the PyTorch version: same math, same architecture (byte-level vocab, shared weights across layers, ReLU sparsity, RoPE attention with Q=K), with MLX-friendly APIs and a detailed README explaining the few API-level differences and why results are equivalent.\n\nCode, docs, and training script are ready to use. You may need to adjust the training script a bit to fit your own custom dataset. Only tested on M4 so far, but should work perfect for any M1/M2/M3 users out there.\n\nI‚Äôm currently training this MLX build on my Internal Knowledge Map (IKM) dataset¬†[https://huggingface.co/datasets/Severian/Internal-Knowledge-Map](https://huggingface.co/datasets/Severian/Internal-Knowledge-Map)\n\nTraining‚Äôs underway; expect a day or so before I publish weights. When it‚Äôs done, I‚Äôll upload the checkpoint to Hugging Face for anyone to test.\n\nRepo:¬†[https://github.com/severian42/BDH-MLX](https://github.com/severian42/BDH-MLX)\n\nHF model (coming soon):¬†[https://huggingface.co/Severian/BDH-MLX](https://huggingface.co/Severian/BDH-MLX)\n\nIf you try it on your own data, feedback and PRs are welcome.",
    "author": "vesudeva",
    "timestamp": "2025-10-07T21:37:00",
    "url": "https://reddit.com/r/MachineLearning/comments/1o115a0/p_mlx_port_of_bdh_baby_dragon_hatchling_is_up/",
    "score": 8,
    "num_comments": 2,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o1qm4w",
    "title": "[D] What current ‚Äúraw materials‚Äù like data will fuel the next big tech revolutions in the coming decades ?",
    "content": " Inspired by how massive human-generated data became indispensable when paired with architectures like transformers and reinforcement learning to power modern AI‚Äîwhat emerging developments or resources are building up right now that could play a similar role in the next 10‚Äì50 years?\nThink of things like exploding datasets, hardware advancements, or societal shifts that, when combined with the right tools/algorithms, will become essential. For each suggestion, please cover:\n\nPrerequisites: What's needed for this resource to accumulate or mature?\nMeans to leverage: How can it be applied (e.g., specific tech or methods)?\nObjective: What ultimate goals or breakthroughs could it enable?\n\nLooking for forward-thinking ideas grounded in current trends! Thank you !!",
    "author": "Long_Woodpecker2370",
    "timestamp": "2025-10-08T16:23:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1o1qm4w/d_what_current_raw_materials_like_data_will_fuel/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o0h8kn",
    "title": "[d] AAAI 2026 Rebuttal Strategies",
    "content": "Phase 2 reviews are out, I got 5,5,5,5,6 with several reviewers raising experimental setup/results reported issue. Can I convert some 5's to 6's with rebuttal? And what are my chances? \nHow can I do it effectively with 2500 characters limit :(\n\nPS: Please feel free to use this thread to post your ratings and ask for rebuttal strategies. ",
    "author": "Ok_Access_9159",
    "timestamp": "2025-10-07T08:02:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1o0h8kn/d_aaai_2026_rebuttal_strategies/",
    "score": 22,
    "num_comments": 55,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o0xntr",
    "title": "[R] Reactive Transformer (RxT) - Stateful Real-Time\nProcessing for Event-Driven Reactive Language Models",
    "content": "",
    "author": "osfric",
    "timestamp": "2025-10-07T18:39:32",
    "url": "https://reddit.com/r/MachineLearning/comments/1o0xntr/r_reactive_transformer_rxt_stateful_realtime/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o0xddj",
    "title": "[R] MADPO: A new DPO variant that addresses the same data problem as Œ≤-DPO, but at the instance level. (looking for feedback)",
    "content": "TL;DR The standard DPO objective struggles with mixed-quality data, a problem that `Œ≤`\\-DPO addresses at the batch level; MADPO provides a more granular solution at the instance level, which leads to consistently better and more robust performance in our experiments.  \n\n\nI would like to get feedback on my new paper on arXiv, which builds on the data quality issue in DPO that was recently highlighted by the `Œ≤`\\-DPO paper. They identified that DPO's fixed `Œ≤` struggles to handle mixed-quality data. However, their batch-level solution, while a great step, can be unstable (Adaptive `Œ≤` can be negative) and is still a coarse approximation for what is an instance-level problem. My method, MADPO (Margin-Adaptive DPO), offers a more granular approach. It uses a reward model to assign a unique weight to each sample, amplifying the loss for hard pairs and dampening it for easy ones.\n\nMy experiments on a sentiment generation task show that this instance-level control is highly effective. MADPO consistently outperformed all baselines (DPO, IPO &amp; `Œ≤`\\-DPO) achieving a performance jump of up to +33.3% over `Œ≤`\\-DPO on high-quality data, while still holding a +10.5% advantage on the most challenging low-quality set.   \n  \nThe full paper with all the theory and experimental details is on arXiv, and I would be grateful for any feedback or questions on the approach.\n\nPaper: [https://arxiv.org/abs/2510.05342](https://arxiv.org/abs/2510.05342)\n\nI am currently seeking an endorsement to allow for direct submission to the correct category for future work. Any help would be greatly appreciated. Endorsement link: [https://arxiv.org/auth/endorse?x=XUXXAE](https://arxiv.org/auth/endorse?x=XUXXAE)  \n",
    "author": "Ok-Local1207",
    "timestamp": "2025-10-07T18:25:44",
    "url": "https://reddit.com/r/MachineLearning/comments/1o0xddj/r_madpo_a_new_dpo_variant_that_addresses_the_same/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o18gy0",
    "title": "[D] Yandex Cup ML track ‚Äî worth?",
    "content": "Saw a post about Yandex Cup 2025 and they have an ML track this year\n\nI‚Äôve done a few Kaggle comps before, so I‚Äôm wondering how their problems compare. Are they actually practical or more on the academic side?\n\nThe $18k pool sounds pretty nice, but I‚Äôm trying to figure out if it‚Äôs worth my time. Registration‚Äôs open till Nov 5 apparently. Anyone planning to join or tried it?\n\n",
    "author": "ummitluyum",
    "timestamp": "2025-10-08T05:00:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1o18gy0/d_yandex_cup_ml_track_worth/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o099v3",
    "title": "[D] Why RHLF instead of DAGGER (multi-step SFT)",
    "content": "Most LLM training pipelines require SFT followed by some form of RHLF (classically PPO). SFT and RHLF require datasets in slightly different formats, but both formats (especially for binary choices) can be re-expressed as the other. \n\nThe old DAGGER paper describes how to train a model in multiple steps with an increasing dataset enriched by annotated rollouts. Is there an advantage to using SFT+RHLF over multi-step SFT?",
    "author": "faschu",
    "timestamp": "2025-10-07T01:41:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1o099v3/d_why_rhlf_instead_of_dagger_multistep_sft/",
    "score": 24,
    "num_comments": 8,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o0bd0c",
    "title": "[D] Can time series foundation models knowledge transfer from stationary to non-stationary monotonic data?",
    "content": "I'm testing whether pretrained time series models (MOMENT, TimesFM) can learn degradation patterns with limited fine-tuning.\n\n**The issue:** These models are pretrained on cyclic/stationary data (finance, weather), but degradation is fundamentally different - non-stationary, monotonic trends toward failure, governed by physics not statistics.\n\n**Zero-shot:** I tested in Zero-shot scenarios and it was a complete failure (R¬≤ negative). Model predicts constants or cyclic patterns where none exist.\n\n**My question:**\n\n1. Can patch-based transformers even extrapolate non-stationary trends, or do they regress to cyclic priors?\n2. Has anyone successfully transferred foundation models from stationary‚Üínon-stationary domains? Or is this fundamentally incompatible with how these models learn?\n\nAny papers or insights are appreciated!",
    "author": "thekingos",
    "timestamp": "2025-10-07T03:49:22",
    "url": "https://reddit.com/r/MachineLearning/comments/1o0bd0c/d_can_time_series_foundation_models_knowledge/",
    "score": 14,
    "num_comments": 4,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o0pf1h",
    "title": "[P] Advice on collecting data for oral cancer histopathological images classification",
    "content": "I‚Äôm currently working on a research project involving oral cancer histopathological image classification, and I could really use some advice from people who‚Äôve worked with similar data.\n\nI‚Äôm trying to decide whether it‚Äôs better to collect whole slide images (WSIs) or to use captured images (smaller regions captured from slides).\n\nIf I go with captured images, I‚Äôll likely have multiple captures containing cancerous tissues from different parts of the same slide (or even multiple slides from the same patient).\n\nMy question is: should I treat those captures as one data point (since they‚Äôre from the same case) or as separate data points for training?\n\nI‚Äôd really appreciate any advice, papers, or dataset references that could help guide my approach.",
    "author": "DryHat3296",
    "timestamp": "2025-10-07T12:57:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1o0pf1h/p_advice_on_collecting_data_for_oral_cancer/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o0a215",
    "title": "[D] AAAI Alignment Track Phase 2",
    "content": "Hi Everyone!\nThe reviews for phase 2 have been released. Lets discuss how did it go!!",
    "author": "HauntingElderberry67",
    "timestamp": "2025-10-07T02:31:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1o0a215/d_aaai_alignment_track_phase_2/",
    "score": 13,
    "num_comments": 44,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o08yl7",
    "title": "[R] Schedule-free Lion optimizer",
    "content": "While working on new ML architectures I struggled to stabilize training  by using countless learning-rate schedulers, gradient clippers and normalizers enough to go and implement a schedule-free optimizer.\n\nHere, [Lion Schedule-Free](https://github.com/govorunov/lion-sf) optimizer - a version of Lion optimizer that requires no learning-rate scheduler. It uses *sign agreement* \\- an absolute value of cross correlation between momentum sign and gradient sign, to scale the effective update step. Not only it converges 3x times faster ON MY MODEL, by eliminating LR scheduler it also allows for hot training resume &amp; restart. And also stabilizes training, especially late training, eliminating the need for gradient clipping, etc. The effective update depends on the training regime and can decrease or increase during training.  \nIn this implementation, the sign agreement is calculated per-module. It's probably more logical and stable to calculate it per-parameter-group, but that's more code and since module-wise already works pretty well...\n\nThe optimizer is provided as is. There will be no paper, no convergence guarantees, no ablation studies and no time to do any of that. \n\nInstall it:\n\n`pip install git+https://github.com/govorunov/lion-sf.git`\n\nAnd use it as normal optimizer:\n\n    from lion_pytorch import LionSF\n    \n    optimizer = LionSF(model.parameters(), lr=5e-4, betas=(0.9, 0.99), weight_decay=1e-2)\n\nGive it a generous base learning rate, like 5e-4 or more, and ditch LR scheduler completely. You can also ditch gradient clipping (as I did).\n\nIf you want to resume / restart training later from a checkpoint - keep the optimizer state, do a hot-restart. There is no need to warm-up - it will restart gently naturally. The ability to do a hot-restart and increased training stability is probably more important (for me) than even faster convergence, although faster convergence looks better on plots.",
    "author": "govorunov",
    "timestamp": "2025-10-07T01:20:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1o08yl7/r_schedulefree_lion_optimizer/",
    "score": 14,
    "num_comments": 3,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o03yqd",
    "title": "[R] Predictive control of generative models",
    "content": "Hey everyone! I‚Äôve been reading about generative models, especially flow models for image generation starting from Gaussian noise. In the process, I started to think if there is any merit to introducing exogenous inputs to drive the system to a particular direction through predictive control algorithms (MPC, MPPI) . Especially, what are some important constraints and stage costs one could incorporate (not just terminal constraints)? I am not super knowledgable about the nature of the image space itself and I couldn‚Äôt find much literature on the internet regarding predictive control. Any suggestions would really help! Thank you!\n\n",
    "author": "Muggle_on_a_firebolt",
    "timestamp": "2025-10-06T20:24:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1o03yqd/r_predictive_control_of_generative_models/",
    "score": 22,
    "num_comments": 15,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o0upx8",
    "title": "[Research] Tackling Persona Drift in LLMs ‚Äî Our Middleware (Echo Mode) for Tone and Identity Stability",
    "content": "Hi everyone, I wanted to share a project we‚Äôve been working on around a challenge we call **persona drift** in large language models.\n\nWhen you run long sessions with LLMs (especially across multi-turn or multi-agent chains), the model often **loses consistency in tone, style, or identity** ‚Äî even when topic and context are preserved.\n\nThis issue is rarely mentioned in academic benchmarks, but it‚Äôs painfully visible in real-world products (chatbots, agents, copilots). It‚Äôs not just ‚Äúforgetting‚Äù ‚Äî it‚Äôs **drift in the model‚Äôs semantic behavior** over time.\n\n\n\nWe started studying this while building our own agent stack, and ended up designing a middleware called **Echo Mode** ‚Äî a **finite-state protocol** that adds a stability layer between the user and the model.\n\n\n\nHere‚Äôs how it works:\n\n\n\n* We define **four conversational states**: Sync, Resonance, Insight, and Calm ‚Äî each has its own heuristic expectations (length, tone, depth).\n* Each state transition is governed by a lightweight FSM (finite-state machine).\n* We measure a **Sync Score** ‚Äî a BLEU-like metric that tracks deviation in tone and structure across turns.\n* A simple **EWMA-based repair loop** recalibrates the model‚Äôs outputs when drift exceeds threshold.\n\n\n\n\n\nThis helps agents **retain their ‚Äúvoice‚Äù** over longer sessions without needing constant prompt re-anchoring.\n\n\n\nWe‚Äôve just released the **open-source version** (Apache-2.0):\n\n [**GitHub ‚Äì Echo Mode**](https://github.com/Seanhong0818/Echo-Mode)\n\n\n\nWe‚Äôre also building a **closed-source enterprise layer (EchoMode.io)** that expands on this ‚Äî with telemetry, Sync Score analytics, and an API to monitor tone drift across multiple models (OpenAI, Anthropic, Gemini, etc.).\n\n\n\nI‚Äôd love to hear from anyone studying **behavioral consistency, semantic decay, or long-term agent memory** ‚Äî or anyone who‚Äôs seen similar issues in RLHF or multi-turn fine-tuning.\n\n\n\n\n\n*(mods: not a product pitch ‚Äî just sharing a middleware and dataset approach for a rarely discussed aspect of LLM behavior.)*",
    "author": "Medium_Charity6146",
    "timestamp": "2025-10-07T16:25:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1o0upx8/research_tackling_persona_drift_in_llms_our/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1o0mv8f",
    "title": "[D] EMNLP Poster Template",
    "content": "Is there any specific template for EMNLP Posters? I cannot find it on the instructions themselves. Thanks ",
    "author": "SoggyClue",
    "timestamp": "2025-10-07T11:24:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1o0mv8f/d_emnlp_poster_template/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nzw0v3",
    "title": "[D] Best practices for structuring an applied ML research project?",
    "content": "Hello, I‚Äôm a PhD student about to start my first research project in applied ML, and I‚Äôd like to get the structure right from the beginning instead of refactoring everything later.\n\nAre there any solid ‚Äúbest-practice‚Äù resources or example repositories that one could recommend? I‚Äôm especially keen on making sure I get the following right:\n\n* Containerization\n* Project structure for reproducibility and replication\n* Managing experiments, environments, and dependencies\n\nThanks in advance for any pointers!",
    "author": "Real_Suspect_7636",
    "timestamp": "2025-10-06T14:28:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1nzw0v3/d_best_practices_for_structuring_an_applied_ml/",
    "score": 39,
    "num_comments": 10,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nzsfkl",
    "title": "[D] AAAI 26 Phase 2 Reviews",
    "content": "Anyone received aaai phase 2 reviews?",
    "author": "Necessary-Future-549",
    "timestamp": "2025-10-06T12:17:37",
    "url": "https://reddit.com/r/MachineLearning/comments/1nzsfkl/d_aaai_26_phase_2_reviews/",
    "score": 45,
    "num_comments": 260,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nzkout",
    "title": "[P]Navigating through eigen spaces",
    "content": "Eigen Vectors are one of the foundational pillars of modern day , data handling mechanism. The concepts also translate beautifully to plethora of other domains.  \nRecently while revisiting the topic, had the idea of visualizing the concepts and reiterating my understanding.\n\nSharing my visualization experiments here :¬†[https://colab.research.google.com/drive/1-7zEqp6ae5gN3EFNOG\\_r1zm8hzso-eVZ?usp=sharing](https://colab.research.google.com/drive/1-7zEqp6ae5gN3EFNOG_r1zm8hzso-eVZ?usp=sharing)\n\nIf interested in few more resources and details, you can have a look at my linkedin post :¬†[https://www.linkedin.com/posts/asmita-mukherjee-data-science\\_google-colab-activity-7379955569744474112-Zojj?utm\\_source=share&amp;utm\\_medium=member\\_desktop&amp;rcm=ACoAACA6NK8Be0YojVeJomYdaGI-nIrh-jtE64c](https://www.linkedin.com/posts/asmita-mukherjee-data-science_google-colab-activity-7379955569744474112-Zojj?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAACA6NK8Be0YojVeJomYdaGI-nIrh-jtE64c)\n\nPlease do share your learnings and understanding. I have also been thinking of setting up a community in discord (to start with) to learn and revisit the fundamental topics and play with them. If anyone is interested, feel free to dm with some professional profile link (ex: website, linkedin, github etc).",
    "author": "awesome_weirdo101",
    "timestamp": "2025-10-06T07:31:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1nzkout/pnavigating_through_eigen_spaces/",
    "score": 21,
    "num_comments": 1,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nzd4xn",
    "title": "[P] ExoSeeker: A Web Interface For Building Custom Stacked Models For Exoplanet Classifications",
    "content": "Hi everyone! I just want to share ExoSeeker, a machine learning web interface, I created for the NASA Space Apps Challenge this year.¬†It allows anyone to upload data of potential exoplanets, planets outside the Solar System, from the Kelper mission, a space telescope designed to hunt for Earth-sized planets orbiting stars in the Milky Way, and train a custom machine learning model, select classifiers and tweak their main hyperparameters, on it.¬†\n\nYou can freely build their own model by selecting from multiple estimators (random forest, gradient boosting, and multi-layer perceptron) and adjust each one's primary hyperparameters. After model training, you upload a new dataset without the exoplanet disposition, with only the feature to run predictions on it using the saved model.\n\nGithub Repository:¬†[https://github.com/gospacedev/exoseeker](https://github.com/gospacedev/exoseeker)\n\nNASA Space Apps Challenge ExoSeeker Project Description:¬†[https://www.spaceappschallenge.org/2025/find-a-team/exoseeker/?tab=project](https://www.spaceappschallenge.org/2025/find-a-team/exoseeker/?tab=project)",
    "author": "gospacedev",
    "timestamp": "2025-10-06T01:04:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1nzd4xn/p_exoseeker_a_web_interface_for_building_custom/",
    "score": 9,
    "num_comments": 0,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nyunr7",
    "title": "[D] Blog Post: 6 Things I hate about SHAP as a Maintainer",
    "content": "Hi r/MachineLearning,  \nI wrote this blog post (https://mindfulmodeler.substack.com/p/6-things-i-hate-about-shap-as-a-maintainer) to share all the things that can be improved about SHAP, to help potential newcomers see areas of improvements (though we also have \"good first issues\" of course) and also to get some feedback from the community.   \nBrief summary:  \n1. explainers can be slow, e.g. if relying on the ExactExplainer or PermutationExplainer  \n2. DeepExplainer does not support a lot of layers and for tensorflow the LSTM is not working anymore (for more information see the article)  \n3. TreeExplainer has a bunch of problems: it's legacy code, we discovered some memory issues and there are a couple open issues addressing bugs there  \n4. we are in dependency hell: lots of upstream packages break our pipelines regularly which is a huge maintenance burden  \n5. The plotting API is dated and not well tested, so a rewrite is hard  \n6. Other things: No JAX support, missing type annotations, etc.  \n  \nAnything you want to be fixed or improved about the project? Any reason why you don't use it anymore?   \nVery happy to talk about this here.",
    "author": "Prize_Might4147",
    "timestamp": "2025-10-05T10:39:42",
    "url": "https://reddit.com/r/MachineLearning/comments/1nyunr7/d_blog_post_6_things_i_hate_about_shap_as_a/",
    "score": 75,
    "num_comments": 9,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nzb34n",
    "title": "[D] KDD 2026 Reviews",
    "content": "How did everyone's results go? \n\n\n",
    "author": "TheKingNoOption",
    "timestamp": "2025-10-05T22:53:49",
    "url": "https://reddit.com/r/MachineLearning/comments/1nzb34n/d_kdd_2026_reviews/",
    "score": 3,
    "num_comments": 6,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nz1s2h",
    "title": "[P] Looking to interview people who‚Äôve worked on audio labeling for ML (PhD research project)",
    "content": "Looking to interview people who‚Äôve worked on audio labeling for ML (PhD research project)\n\nHi everyone,\nI‚Äôm a PhD candidate in Communication researching modern sound technologies. My dissertation is a cultural history of audio datasets used in machine learning: I‚Äôm interested in how sound is conceptualized, categorized, and organized within computational systems.\nI‚Äôm currently looking to speak with people who have done audio labeling or annotation work for ML projects (academic, industry, or open-source). These interviews are part of an oral history component of my research.\nSpecifically, I‚Äôd love to hear about:\n- how particular sound categories were developed or negotiated,\n- how disagreements around classification were handled, and\n- how teams decided what counted as a ‚Äúgood‚Äù or ‚Äúusable‚Äù data point.\nIf you‚Äôve been involved in building, maintaining, or labeling sound datasets - from environmental sounds to event ontologies - I‚Äôd be very grateful to talk. Conversations are confidential, and I can share more details about the project and consent process if you‚Äôre interested.\nYou can DM me here\nThanks so much for your time and for all the work that goes into shaping this fascinating field.",
    "author": "heyheymymy621",
    "timestamp": "2025-10-05T15:13:27",
    "url": "https://reddit.com/r/MachineLearning/comments/1nz1s2h/p_looking_to_interview_people_whove_worked_on/",
    "score": 10,
    "num_comments": 4,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nzjjd6",
    "title": "[P] Harmonic Agent: Tackling belief drift in self-reflective AI agents",
    "content": "Hey r/ML,  \n  \nI've been working on autonomous agents that use recursive self-reflection  \n(think Reflexion-style setups), and kept running into this weird failure mode  \nthat I couldn't find documented anywhere.  \n  \nThe Problem:  \n  \nWhen you let an agent repeatedly reflect on its own reasoning - like having  \nit critique its outputs, update its approach, then critique \\*that\\* approach,  \netc - the belief embeddings slowly drift away from the original values.  \n  \nNot catastrophic forgetting (different thing). Not hallucination. More like...  \nthe agent gradually forgets \"who it is\" across reflection cycles.  \n  \nI'm calling it Recursive Belief Drift (RBD). Maybe someone has a better name?  \n  \nWhy This Matters:  \n  \nIf you're building:  \n\\- Long-running conversational agents  \n\\- Self-improving systems (agents that modify their own prompts/code)  \n\\- Multi-agent systems where identity consistency matters  \n  \n...this drift becomes a real problem around 50-100 reflection cycles.  \n  \nMy Approach:  \n  \nTried a bunch of things. What ended up working was inspired by MIT's recent  \nLinOSS work on neural oscillations - basically treating belief updates as a  \ndamped oscillator instead of pure accumulation:\n\ng(t) = exp(-Œ±t) \\* sin(œât) B\\_t+1 = B\\_t + Œª \\* g(t) \\* correction\n\nInstead of beliefs drifting monotonically, they oscillate around a stable  \npoint. Kind of like making the agent \"breathe\" instead of constantly tensing up.  \n  \nResults:  \n  \nTested on 50 reflection cycles with sentence-transformers:  \n\\- No damping: mean drift \\~0.085 (bad)  \n\\- Harmonic damping: mean drift \\~0.009 (much better)  \n  \nAbout 9x improvement in stability, though obviously this depends heavily on  \nyour specific setup.  \n  \nCode:  \n  \nOpen sourced everything here: [https://github.com/Freeky7819/harmonic-agent](https://github.com/Freeky7819/harmonic-agent)  \n  \nThere's a Colab notebook if you want to just try it:  \n[https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO](https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO)  \n  \nHonest Limitations:  \n  \n\\- Parameters (Œª, œâ, Œ±) are hand-tuned. Haven't found a good way to learn them yet.  \n\\- Only tested with embedding-based belief representations. Not sure how this  \n¬† translates to pure symbolic approaches.  \n\\- \"Correction vectors\" in my test are just noise. Real agent corrections would  \n¬† be more structured.  \n\\- Small-scale tests only (50 cycles, \\~400 dim embeddings)  \n  \nQuestions for the Community:  \n  \n1. Has anyone seen this RBD problem documented elsewhere? I feel like I'm  \n¬†¬† reinventing the wheel here.  \n  \n2. Better ways to set oscillation parameters? I tried grid search but it's  \n¬†¬† expensive and use-case dependent.  \n  \n3. Any theoretical reason why this \\*wouldn't\\* scale to larger embedding spaces  \n¬†¬† or longer timescales?  \n  \n4. Could this be integrated with existing frameworks like LangChain or AutoGen  \n¬†¬† without major refactoring?  \n  \nFeedback/criticism very welcome. Still figuring this out.  \n  \n\\---  \n  \nLinks:  \n\\- GitHub: [https://github.com/Freeky7819/harmonic-agent](https://github.com/Freeky7819/harmonic-agent)  \n\\- Colab Demo: [https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO](https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO)  \n\\- Comparison visualizations in the repo  \n  \nRelated Work:  \n\\- MIT LinOSS (2025): Harmonic oscillators for ML stability  \n\\- Reflexion (Shinn et al., 2023): Self-reflection framework this builds on  \n\\- Agent Drift paper (Ponnambalam, 2025): Documents similar issues  \n  \nYes, I know the title says \"agent\" but this is really about maintaining  \nstable belief representations. \"Agent\" might be overselling it. Open to better terminology.\n\n¬†",
    "author": "freeky78",
    "timestamp": "2025-10-06T06:47:31",
    "url": "https://reddit.com/r/MachineLearning/comments/1nzjjd6/p_harmonic_agent_tackling_belief_drift_in/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nzcg16",
    "title": "[D] Tensorflow and Musicnn",
    "content": "Hi all,\nI‚Äôm struggling with Tensorflow and an old Musicnn embbeding and classification model that I get form the Essentia project.\n\nTo say in short seems that in same CPU it doesn‚Äôt work.\n\nInitially I collect issue on old CPU due to the missing support of AVX, and I can live with the fact of not support very old CPU.\n\nNow I discovered that also some ‚Äúnot old‚Äù cpu have some different rappresentation of number that broke the model with some memory error.\n\nThe first issue that i fix was this:\n\nhttps://github.com/NeptuneHub/AudioMuse-AI/issues/73\n\nIt was an intel i5 1035G1 processor that by default used float64 instead of the float32 used by the model. Just adding a cast in my code I solved the problem, good.\n\nSome days ago an user with an AMD Ryzen AI 9 HX 370 had similar problem here\n\nhttps://github.com/NeptuneHub/AudioMuse-AI/issues/93\n\nI try to check if ‚ÄúI miss some cast somewhere‚Äù but I wasn‚Äôt able to find a solution in that way. I instead found that by setting this env variable:\n\nENV TF_ENABLE_ONEDNN_OPTS=0\n\nThe model start working but giving ‚Äúcorrect‚Äù value but with a different scale. So the probability of a tag (the genre of the song) instead of be around 0.1 or 0.2 arrived to 0.5 or 0.6.\n\nSo here my question: why? How can achieve that Tensorflow work on different CPU and possibly giving similar value?\nI think can be ok if the precision is not the exact one, but have the double or the triple of the value to me sounds strange and I don‚Äôt know which impact can have on the rest of my application.\n\nI mainly use:\nThe Musicnn embbeding rappresentation to do similarity song between embbeding itself. Then I use for a secondary purpose the tag itself with the genre. \n\nAny suggestion ? Eventually any good alternative to Tensorflow at all that could be more ‚Äústable‚Äù and that I can use in python ? (My entire app is in python).\n\nJust for background the entire app is opensource (and free) on GitHub. If you want to inspect the code it is in task/analysis all the part that use Librosa+Tensorflow for this analysis (yes the model was from Essentia, but I‚Äôm reusing reading the song with Librosa because seems more updated and support ARM on Linux).",
    "author": "Old_Rock_9457",
    "timestamp": "2025-10-06T00:19:05",
    "url": "https://reddit.com/r/MachineLearning/comments/1nzcg16/d_tensorflow_and_musicnn/",
    "score": 1,
    "num_comments": 11,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nyfadh",
    "title": "[D] LLM Inference on TPUs",
    "content": "It seems like simple `model.generate()` calls are incredibly slow on TPUs (basically stuck after one inference), does anyone have simple solutions for using torch XLA on TPUs? This seems to be an ongoing issue in the HuggingFace repo.\n\nI tried to find something the whole day, and came across solutions like optimum-tpu (only supports some models + as a server, not simple calls), using Flax Models (again supports only some models and I wasn't able to run this either), or sth that converts torch to jax and then we can use it (like ivy). But these seem too complicated for the simple problem, I would really appreciate any insights!!",
    "author": "simple-Flat0263",
    "timestamp": "2025-10-04T21:57:38",
    "url": "https://reddit.com/r/MachineLearning/comments/1nyfadh/d_llm_inference_on_tpus/",
    "score": 20,
    "num_comments": 14,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nymmt1",
    "title": "[D]How do you balance pushing new models vs optimizing what you already have?",
    "content": "I work in a small ML startup and our data scientists are split,  half want to keep building new architectures, half want to refine and deploy what‚Äôs working. Feels like we‚Äôre spinning wheels instead of improving performance in production. How do you usually balance innovation vs iteration?",
    "author": "whistler_232",
    "timestamp": "2025-10-05T05:15:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1nymmt1/dhow_do_you_balance_pushing_new_models_vs/",
    "score": 5,
    "num_comments": 9,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nyheqx",
    "title": "[D] Help needed on Train Bogey Dataset",
    "content": "[https://www.kaggle.com/datasets/ziya07/high-speed-train-bogie-vibration-and-fault-diagnosis/data](https://www.kaggle.com/datasets/ziya07/high-speed-train-bogie-vibration-and-fault-diagnosis/data)\n\nThis is a dataset of Train Bogey Vibrations. I have tried everything, extracted time domain features, extracted frequency domain features, extracted time-freq features like wavelet etc. Tried Classical ML ,Tried 1d conv on raw data, Tried sliding window approach and 2d conv, Tried anomaly detection. But i cant make the accuracy more than 55%. Please help me understand this data and modelling this data ",
    "author": "NoCommittee4992",
    "timestamp": "2025-10-05T00:02:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1nyheqx/d_help_needed_on_train_bogey_dataset/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nyrq9x",
    "title": "[D] Baseline model for Anomaly Detection",
    "content": "Hi,\n\nI am currently building an anomaly detection method on abnormal product returns. Was wondering, what would be a suitable Baseline model to compare against say LoF or IsolationForest?\n\nThanks",
    "author": "BBooty_luvr",
    "timestamp": "2025-10-05T08:48:40",
    "url": "https://reddit.com/r/MachineLearning/comments/1nyrq9x/d_baseline_model_for_anomaly_detection/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nxy1z3",
    "title": "Internship at 'Big Tech' ‚Äî PhD Student [D]",
    "content": "I'm sorry for this post on this sub. I know it's a wrong place but couldn't find a better one.\n\nI'm a PhD Student in ML at a decently reputed research team but in a niche field. But most of my work is machine-learning and stats heavy. (Btw Europe Location)\n\nI really want to get a good internship at a big tech to get into high-profilic research network and also for my CV. I feel like I have above-average profile and will make to sure to make it better before I apply. I also have my PI's backing and internal recommendation if I find one position.\n\n1. Is competition huge for getting into Google (Research, DeepMind), MSFT, Amazon, Meta Research, etc,. How can I make best out of my application? What do they generally look for?\n\n2. Does cold-emailing work in this case? \n\n3. I see that some PhD intern roles (like for Google) specifically asks for students in their final year. Is it a hard requirement? Or do they also interview students in their 1/2nd year.\n\n4. In case if I don't get a chance at mentioned places, should I still go for other reputed companies or target top universities (for visiting researcher) instead?\n\n5. I would like to connect to people who have some experience going through this :)\n\nThanks!",
    "author": "ade17_in",
    "timestamp": "2025-10-04T09:10:39",
    "url": "https://reddit.com/r/MachineLearning/comments/1nxy1z3/internship_at_big_tech_phd_student_d/",
    "score": 41,
    "num_comments": 14,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nyr4l0",
    "title": "[D] Training a Vision model on a Text-Only Dataset using Axolotl",
    "content": "I'm planning to fine-tune LLaMA 3.2 11B Instruct on a JSONL dataset of domain-specific question-answer pairs ‚Äî purely text, no images. The goal is to improve its instruction-following behavior for specialized text tasks, while still retaining its ability to handle multimodal inputs like OCR and image-based queries.\n\nI am using Axolotl\nhttps://github.com/axolotl-ai-cloud/axolotl/blob/main/examples/llama-3-vision/lora-11b.yaml\nin examples we have a sample .yaml file for this\n```\nbase_model: alpindale/Llama-3.2-11B-Vision-Instruct\n# optionally might have model_type or tokenizer_type or processor_type\nprocessor_type: AutoProcessor\n# Automatically upload checkpoint and final model to HF\n# hub_model_id: username/custom_model_name\n\n\n# these 3 lines are needed for now to handle vision chat templates w images\nskip_prepare_dataset: true\nremove_unused_columns: false\nsample_packing: false\n\nchat_template: llama3_2_vision\ndatasets:\n  - path: HuggingFaceH4/llava-instruct-mix-vsft\n    type: chat_template\n    split: train[:1%]\ndataset_prepared_path:\nval_set_size: 0.0\noutput_dir: ./outputs/out\n\nadapter: lora\nlora_model_dir:\n\nsequence_len: 8192\npad_to_sequence_len: false\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules: 'model.language_model.layers.[\\d]+.(mlp|cross_attn|self_attn).(up|down|gate|q|k|v|o)_proj'\n\nwandb_project:\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\nbf16: true\nfp16:\ntf32: true\n\ngradient_checkpointing: true\nlogging_steps: 1\n# flash_attention: true  # use for text-only mode\nsdp_attention: true\n\nwarmup_ratio: 0.1\nevals_per_epoch: 1\nsaves_per_epoch: 1\nweight_decay: 0.0\n\n# save_first_step: true  # uncomment this to validate checkpoint saving works with your config\n```\nbased on which I have made a similar .yaml file\n\n```\nbase_model: alpindale/Llama-3.2-11B-Vision-Instruct\nprocessor_type: AutoProcessor\ntokenizer_config: &lt;path_to_custom_tokenizer&gt;\ntokenizer_type: AutoTokenizer\n\n# Vision-chat template handling\n# skip_prepare_dataset: true\n# remove_unused_columns: false\n# sample_packing: false\n\nchat_template: llama3_2_vision\n\ndatasets:\n  - path: &lt;path_to_dataset&gt;\n    type: chat_template\n    field_messages: messages\n    message_property_mappings:\n      role: role\n      content: content\n    roles:\n      system: \n        - system\n      user: \n        - user\n      assistant: \n        - assistant\n    train_on_inputs: false\n\noutput_dir: &lt;path_to_output_directory&gt;\n\n# Training parameters\nsequence_len: 8192\npad_to_sequence_len: false\ngradient_accumulation_steps: 4\nmicro_batch_size: 1\nnum_epochs: 1\n\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\nweight_decay: 0.0\nwarmup_ratio: 0.1\n\n# Precision &amp; performance\nbf16: true\nfp16:\ntf32: true\n\ngradient_checkpointing: true\nlogging_steps: 1\nflash_attention: true   # text-only mode\n# sdp_attention: true\n\n# Checkpointing\nevals_per_epoch: 1\nsaves_per_epoch: 1\nsave_first_step: true\nsave_total_limit: 3\n\nweight_decay: 0.0\nspecial_tokens:\n  pad_token: &lt;|end_of_text|&gt;\n\n```\n\nbut when i run\n`axolotl train config.yaml`\nand I have processor_type:\n```\nbase_model: alpindale/Llama-3.2-11B-Vision-Instruct\nprocessor_type: AutoProcessor\ntokenizer_config: &lt;path_to_custom_tokenizer&gt;\ntokenizer_type: AutoTokenizer\n```\nI get the error\n`KeyError: 'Indexing with integers is not available when using Python based feature extractors'`\n\nbut when i remove the field \n```\nbase_model: alpindale/Llama-3.2-11B-Vision-Instruct\ntokenizer_config: &lt;path_to_custom_tokenizer&gt;\ntokenizer_type: AutoTokenizer\n```\n\nor even\n```\nbase_model: alpindale/Llama-3.2-11B-Vision-Instruct\nprocessor_type: AutoProcessor\ntokenizer_config: &lt;path_to_custom_tokenizer&gt;\n\n# Vision-chat template handling\nskip_prepare_dataset: true\nremove_unused_columns: false\nsample_packing: false\n\n```\n\nI get the error\n`AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'`\n\nWhat happened here?\nHow does one do this?\nWill this fine-tuning lead to loss of Vision Capabilities of the model?\nIs there a guide to writing config.yaml files for different models?\n\n\nPython Version: 3.12\nAxolotl Version: Latest\nDataset: a .jsonl with \n```\n{\n\t\"messages\": \n\t[\n\t\t{\"role\": \"system\", \"content\": \"&lt;system_prompt&gt;\"}, \n\t\t{\"role\": \"user\", \"content\": \"&lt;question&gt;\"}, \n\t\t{\"role\": \"assistant\", \"content\": \"&lt;answer&gt;\"}\n\t]\n}\n```\nwhich was previously used to fine tune Llama3.1 8B using the following config.yaml\n\n```\nbase_model: NousResearch/Meta-Llama-3.1-8B-Instruct\ntokenizer_config: &lt;path_to_custom_tokenizer&gt;\ntokenizer_type: AutoTokenizer\n\nchat_template: llama3\ndatasets:\n  - path: &lt;path_to_dataset&gt;\n    type: chat_template\n    field_messages: messages\n    message_property_mappings:\n      role: role\n      content: content\n    roles:\n      system:\n        - system\n      user:\n        - user\n      assistant:\n        - assistant\ntrain_on_inputs: false\n\noutput_dir: &lt;path_to_output_directory&gt;\n\nsequence_len: 2048\nsample_packing: true\n\n\ngradient_accumulation_steps: 8\nmicro_batch_size: 2\nnum_epochs: 4\n\noptimizer: paged_adamw_8bit\nlr_scheduler: cosine\nlearning_rate: 2e-5\n\nbf16: auto\ntf32: false\n\ngradient_checkpointing: true\ngradient_checkpointing_kwargs:\n  use_reentrant: false\nresume_from_checkpoint:\nauto_resume_from_checkpoints: true\nsave_only_model: false\n\n\nlogging_steps: 1\nflash_attention: true\n\nwarmup_ratio: 0.1\nevals_per_epoch: 2\nsaves_per_epoch: 1\nsave_total_limit: 3\nweight_decay: 0.0\nspecial_tokens:\n  pad_token: &lt;|end_of_text|&gt;\n```\n\nThank you.I'm planning to fine-tune LLaMA 3.2 11B Instruct on a JSONL dataset of domain-specific question-answer pairs ‚Äî purely text, no images. The goal is to improve its instruction-following behavior for specialized text tasks, while still retaining its ability to handle multimodal inputs like OCR and image-based queries.\n\nI am using Axolotl\nhttps://github.com/axolotl-ai-cloud/axolotl/blob/main/examples/llama-3-vision/lora-11b.yaml\nin examples we have a sample .yaml file for this\n```\nbase_model: alpindale/Llama-3.2-11B-Vision-Instruct\n# optionally might have model_type or tokenizer_type or processor_type\nprocessor_type: AutoProcessor\n# Automatically upload checkpoint and final model to HF\n# hub_model_id: username/custom_model_name\n\n\n# these 3 lines are needed for now to handle vision chat templates w images\nskip_prepare_dataset: true\nremove_unused_columns: false\nsample_packing: false\n\nchat_template: llama3_2_vision\ndatasets:\n  - path: HuggingFaceH4/llava-instruct-mix-vsft\n    type: chat_template\n    split: train[:1%]\ndataset_prepared_path:\nval_set_size: 0.0\noutput_dir: ./outputs/out\n\nadapter: lora\nlora_model_dir:\n\nsequence_len: 8192\npad_to_sequence_len: false\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules: 'model.language_model.layers.[\\d]+.(mlp|cross_attn|self_attn).(up|down|gate|q|k|v|o)_proj'\n\nwandb_project:\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\nbf16: true\nfp16:\ntf32: true\n\ngradient_checkpointing: true\nlogging_steps: 1\n# flash_attention: true  # use for text-only mode\nsdp_attention: true\n\nwarmup_ratio: 0.1\nevals_per_epoch: 1\nsaves_per_epoch: 1\nweight_decay: 0.0\n\n# save_first_step: true  # uncomment this to validate checkpoint saving works with your config\n```\nbased on which I have made a similar .yaml file\n\n```\nbase_model: alpindale/Llama-3.2-11B-Vision-Instruct\nprocessor_type: AutoProcessor\ntokenizer_config: &lt;path_to_custom_tokenizer&gt;\ntokenizer_type: AutoTokenizer\n\n# Vision-chat template handling\n# skip_prepare_dataset: true\n# remove_unused_columns: false\n# sample_packing: false\n\nchat_template: llama3_2_vision\n\ndatasets:\n  - path: &lt;path_to_dataset&gt;\n    type: chat_template\n    field_messages: messages\n    message_property_mappings:\n      role: role\n      content: content\n    roles:\n      system: \n        - system\n      user: \n        - user\n      assistant: \n        - assistant\n    train_on_inputs: false\n\noutput_dir: &lt;path_to_output_directory&gt;\n\n# Training parameters\nsequence_len: 8192\npad_to_sequence_len: false\ngradient_accumulation_steps: 4\nmicro_batch_size: 1\nnum_epochs: 1\n\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\nweight_decay: 0.0\nwarmup_ratio: 0.1\n\n# Precision &amp; performance\nbf16: true\nfp16:\ntf32: true\n\ngradient_checkpointing: true\nlogging_steps: 1\nflash_attention: true   # text-only mode\n# sdp_attention: true\n\n# Checkpointing\nevals_per_epoch: 1\nsaves_per_epoch: 1\nsave_first_step: true\nsave_total_limit: 3\n\nweight_decay: 0.0\nspecial_tokens:\n  pad_token: &lt;|end_of_text|&gt;\n\n```\n\nbut when i run\n`axolotl train config.yaml`\nand I have processor_type:\n```\nbase_model: alpindale/Llama-3.2-11B-Vision-Instruct\nprocessor_type: AutoProcessor\ntokenizer_config: &lt;path_to_custom_tokenizer&gt;\ntokenizer_type: AutoTokenizer\n```\nI get the error\n`KeyError: 'Indexing with integers is not available when using Python based feature extractors'`\n\nbut when i remove the field \n```\nbase_model: alpindale/Llama-3.2-11B-Vision-Instruct\ntokenizer_config: &lt;path_to_custom_tokenizer&gt;\ntokenizer_type: AutoTokenizer\n```\n\nor even\n```\nbase_model: alpindale/Llama-3.2-11B-Vision-Instruct\nprocessor_type: AutoProcessor\ntokenizer_config: &lt;path_to_custom_tokenizer&gt;\n\n# Vision-chat template handling\nskip_prepare_dataset: true\nremove_unused_columns: false\nsample_packing: false\n\n```\n\nI get the error\n`AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'`\n\nWhat happened here?\nHow does one do this?\nWill this fine-tuning lead to loss of Vision Capabilities of the model?\nIs there a guide to writing config.yaml files for different models?\n\n\nPython Version: 3.12\nAxolotl Version: Latest\nDataset: a .jsonl with \n```\n{\n\t\"messages\": \n\t[\n\t\t{\"role\": \"system\", \"content\": \"&lt;system_prompt&gt;\"}, \n\t\t{\"role\": \"user\", \"content\": \"&lt;question&gt;\"}, \n\t\t{\"role\": \"assistant\", \"content\": \"&lt;answer&gt;\"}\n\t]\n}\n```\nwhich was previously used to fine tune Llama3.1 8B using the following config.yaml\n\n```\nbase_model: NousResearch/Meta-Llama-3.1-8B-Instruct\ntokenizer_config: &lt;path_to_custom_tokenizer&gt;\ntokenizer_type: AutoTokenizer\n\nchat_template: llama3\ndatasets:\n  - path: &lt;path_to_dataset&gt;\n    type: chat_template\n    field_messages: messages\n    message_property_mappings:\n      role: role\n      content: content\n    roles:\n      system:\n        - system\n      user:\n        - user\n      assistant:\n        - assistant\ntrain_on_inputs: false\n\noutput_dir: &lt;path_to_output_directory&gt;\n\nsequence_len: 2048\nsample_packing: true\n\n\ngradient_accumulation_steps: 8\nmicro_batch_size: 2\nnum_epochs: 4\n\noptimizer: paged_adamw_8bit\nlr_scheduler: cosine\nlearning_rate: 2e-5\n\nbf16: auto\ntf32: false\n\ngradient_checkpointing: true\ngradient_checkpointing_kwargs:\n  use_reentrant: false\nresume_from_checkpoint:\nauto_resume_from_checkpoints: true\nsave_only_model: false\n\n\nlogging_steps: 1\nflash_attention: true\n\nwarmup_ratio: 0.1\nevals_per_epoch: 2\nsaves_per_epoch: 1\nsave_total_limit: 3\nweight_decay: 0.0\nspecial_tokens:\n  pad_token: &lt;|end_of_text|&gt;\n```\n\nThank you.",
    "author": "PravalPattam12945RPG",
    "timestamp": "2025-10-05T08:25:29",
    "url": "https://reddit.com/r/MachineLearning/comments/1nyr4l0/d_training_a_vision_model_on_a_textonly_dataset/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nywei8",
    "title": "[P] chess-cv: CNN-based chess piece classifier",
    "content": "Hi r/MachineLearning, here is my weekend project: [chess-cv](https://github.com/S1M0N38/chess-cv)\n\nA machine learning project that trains a lightweight CNN (156k parameters) from scratch to classify chess pieces from 32√ó32 pixel square images. The model achieves ~99.85% accuracy on synthetic training data generated by combining 55 board styles (256√ó256px) with 64 piece sets (32√ó32px) from chess.com and lichess.\n\nBy rendering pieces onto different board backgrounds and extracting individual squares, the model learns robust piece recognition across various visual styles.\n\n| Dataset                                                                                  | Accuracy | F1-Score (Macro) |\n| ---------------------------------------------------------------------------------------- | :--------: | :----------------: |\n| Test Data                                                                                | 99.85%   | 99.89%           |\n| [S1M0N38/chess-cv-openboard](https://huggingface.co/datasets/S1M0N38/chess-cv-openboard) | -    | 95.78%           |\n\n(OpenBoard has an unbalanced class distribution (many more samples for empty square class, so accuracy is not representative )\n\nHappy to hear any feedback!\n",
    "author": "S1M0N38",
    "timestamp": "2025-10-05T11:44:43",
    "url": "https://reddit.com/r/MachineLearning/comments/1nywei8/p_chesscv_cnnbased_chess_piece_classifier/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ny6ol1",
    "title": "[D] Experiences with active learning for real applications?",
    "content": "I'm tinkering with an application of human pose estimation which [fails miserably](https://i.imgur.com/S0kVyPg.mp4) using off-the-shelf models/tools, as the domain is especially niche and complex compared to their training distribution. It seems there's no way around fine-tuning on in-domain images with manually-labeled keypoints (thankfully, I have thousands of hours of unlabelled footage to start from).\n\nI've always been intrigued by active learning, so I'm looking forward to applying it here to efficiently sample frames for manual labeling. But I've never witnessed it in industry, and have only ever encountered [pessimistic takes on active learning in general](https://www.reddit.com/r/MachineLearning/comments/13elpm1/d_is_active_learning_a_hoax_or_the_future/) (not the concept ofc, but the degree to which it outperforms random sampling).\n\nAs an extra layer of complexity - it seems like a manual labeler (likely myself) would have to enter labels through a browser GUI. Ideally, the labeler should produce labels concurrently as the model trains on its labels-thus-far and considers unlabeled frames to send to the labeler. Suddenly my training pipeline gets complicated!\n\nMy current plan:\n* Sample training frames for labeling according to variance in predictions between adjacent frames, or perhaps dropout uncertainty. Higher uncertainty should --&gt; worse predictions\n* For the holdout val+test sets (split by video), sample frames truly at random\n* In the labeling GUI, display the model's initial prediction, and just drag the skeleton around\n* Don't bother with concurrent labeling+training, way too much work. I care more about hours spent labeling than calendar time at this point.\n\nI'd love to know whether it's worth all the fuss. I'm curious to hear about any cases where active learning succeeded or flopped in an industry/applied setting.\n\n* In practice, when does active learning give a clear win over random? When will it probably be murkier?\n* Recommended batch sizes/cadence and stopping criteria?\n* Common pitfalls (uncertainty miscalibration, sampling bias, annotator fatigue)?",
    "author": "XTXinverseXTY",
    "timestamp": "2025-10-04T14:53:21",
    "url": "https://reddit.com/r/MachineLearning/comments/1ny6ol1/d_experiences_with_active_learning_for_real/",
    "score": 4,
    "num_comments": 6,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ny3jtc",
    "title": "[D] Model parallel training use cases",
    "content": "Hi everyone,\n\nI‚Äôm curious about model parallel training use cases in industry and academia. A few things I‚Äôd love to hear about:  \n‚Äì Which companies / research groups require model parallelism? What domains are these groups in and how large are their models?  \n‚Äì Are people using off-the-shelf frameworks (e.g. DeepSpeed, Megatron-LM, PyTorch FSDP) or in-house solutions?  \n‚Äì What‚Äôs been the biggest pain point e.g. debugging, scaling efficiency? Would users benefit from systems that automatically split their models and run them on cost-optimal hardware?\n\nI‚Äôm trying to get a better sense of the landscape and where the real needs are. Would appreciate any insights from practitioners or researchers.\n\nThanks!",
    "author": "Rainmaker9001",
    "timestamp": "2025-10-04T12:46:13",
    "url": "https://reddit.com/r/MachineLearning/comments/1ny3jtc/d_model_parallel_training_use_cases/",
    "score": 4,
    "num_comments": 3,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nykju0",
    "title": "[P] Model needs to be deployed",
    "content": "I just finished fine-tuning a model using Unsloth on Google Colab. The model takes in a chunk of text and outputs a clean summary, along with some parsed fields from that text. It‚Äôs working well!\n\nNow I‚Äôd like to run this model locally on my machine. The idea is to:\n\n* Read texts from a column in a dataframe\n* Pass each row through the model\n* Save the output (summary + parsed fields) into a new dataframe\n\n# Model Info:\n\n* `unsloth/Phi-3-mini-4k-instruct-bnb-4bit`\n* Fine-tuned with Unsloth\n\n# My system specs:\n\n* Ryzen 5 5500U\n* 8GB RAM\n* Integrated graphics (no dedicated GPU)\n\nTIA!",
    "author": "suttewala",
    "timestamp": "2025-10-05T03:19:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1nykju0/p_model_needs_to_be_deployed/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nxfyl8",
    "title": "[D] join pretraining or posttraining",
    "content": "Hello!\n\nI have the possibility to join one of the few AI lab that trains their own LLMs.\n\nGiven the option, would you join the pretraining team or (core) post training team? Why so?",
    "author": "oxydis",
    "timestamp": "2025-10-03T17:33:52",
    "url": "https://reddit.com/r/MachineLearning/comments/1nxfyl8/d_join_pretraining_or_posttraining/",
    "score": 51,
    "num_comments": 28,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nxb9bp",
    "title": "[R] New paper shows that draws in LLM battles aren't what you think",
    "content": "Arena evals (e.g., Chatbot Arena) let users pick which model's response is better, or call it a draw. Most leaderboards then shove this into Elo, same as chess. The assumption: a draw = two models are equally strong. The paper [\"Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation\"](https://arxiv.org/abs/2510.02306) tests that assumption and proves it wrong:\n\n* On 3 arena datasets, ignoring draws when updating ratings makes battle outcome prediction accuracy go **up 1-3%**, despite evaluation still *including draws*.\n* Draws happen much more on **easy** or **objective** queries (risk ratios of 1.3x).\n\n**Discussion seed:** If draws don't indicate skill parity and hence represent a poor fit for existing rating systems, how should we *actually* model them?\n\nCOI: Submitter is author.",
    "author": "tetrisdaemon",
    "timestamp": "2025-10-03T14:08:42",
    "url": "https://reddit.com/r/MachineLearning/comments/1nxb9bp/r_new_paper_shows_that_draws_in_llm_battles_arent/",
    "score": 34,
    "num_comments": 20,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nwhihj",
    "title": "[N] Stanford is updating their Deep Learning course on YouTube",
    "content": "This is a [great opportunity](https://www.youtube.com/watch?v=_NLHFoVNlbg) for all ML/DL students/practitioners to either start learning from scratch or filling knowledge gap, time to start learning folks.",
    "author": "al3arabcoreleone",
    "timestamp": "2025-10-02T15:03:22",
    "url": "https://reddit.com/r/MachineLearning/comments/1nwhihj/n_stanford_is_updating_their_deep_learning_course/",
    "score": 253,
    "num_comments": 21,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nwwsk7",
    "title": "[P] I am building a ML job board",
    "content": "Hey fellow ML people!\n\nLast year, I shared with you a job board for¬†[FAANG positions](https://www.reddit.com/r/MachineLearning/comments/1ia7feh/p_made_a_faang_job_postings_aggregator_for_ai/)¬†and due to the positive feedback I received, I had been working on expanded version called¬†[hire.watch](https://hire.watch/?categories=AI+_+Machine+Learning)\n\nThe goal is provide a unified search experience - it crawls, cleans and extracts data, allowing filtering by:\n\n1. Full-text search\n2. Location - on-site\n3. Remote - from a given city, US state, EU, etc.\n4. Category - you can check out the machine learning category here:¬†[https://hire.watch/?categories=AI+\\_+Machine+Learning](https://hire.watch/?categories=AI+_+Machine+Learning)\n5. Years of experience and seniority\n6. Target gross salary\n7. Date posted and date modified\n\nI used the normal ML ecosystem (scikit learn, huggingface transformers, LLMs, etc.) to build it, and Plotly Dash for the UI.\n\nLet me know what you think - feel free to ask questions and request features :)",
    "author": "dev-ai",
    "timestamp": "2025-10-03T04:42:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1nwwsk7/p_i_am_building_a_ml_job_board/",
    "score": 20,
    "num_comments": 3,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nwoxqz",
    "title": "[R] New paper: LLMs don't have privileged self knowledge, which means we can efficiently train a General Correctness Model to predict the correctness of multiple models. Surprising or expected?",
    "content": "Quick paper highlight (adapted from TLDR thread):  \nFinds no special advantage using an LLM to predict its own correctness (a trend in prior work), instead finding that LLMs benefit from learning to predict the correctness of many other models ‚Äì becoming a GCM.  \n\\--  \nTraining 1 GCM is strictly more accurate than training model-specific CMs for all models it trains on (including CMs trained to predict their own correctness).  \nGCM transfers without training to outperform direct training on OOD models and datasets.  \nGCM (based on Qwen3-8B) achieves +30% coverage on selective prediction vs much larger Llama-3-70B‚Äôs logits.\n\nTLDR thread:¬†[https://x.com/hanqi\\_xiao/status/1973088476691042527](https://x.com/hanqi_xiao/status/1973088476691042527)  \nFull paper:¬†[https://arxiv.org/html/2509.24988v1](https://arxiv.org/html/2509.24988v1)\n\n**Discussion Seed**:  \nPrevious works have suggested / used LLMs having self knowledge, e.g., identifying/preferring their own generations \\[https://arxiv.org/abs/2404.13076\\], or ability to predict their uncertainty. But paper claims specifically that LLMs don't have knowledge about their own *correctness.* Curious on everyone's intuition for what LLMs have / does not have self knowledge about, and whether this result fit your predictions.\n\nConflict of Interest:   \nAuthor is making this post. ",
    "author": "Envoy-Insc",
    "timestamp": "2025-10-02T20:52:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1nwoxqz/r_new_paper_llms_dont_have_privileged_self/",
    "score": 35,
    "num_comments": 12,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nwaunk",
    "title": "[D] How much should researchers (especially in ML domain) rely on LLMs for their work?",
    "content": "Are ML researchers using LLMs like ChatGPT, Claude, or other open-source models to generate, test, or refine minor ideas as tweaks to their original research, or to ask big-picture questions about their overall plans? In what other ways are publishing researchers using LLMs to support their work? (Of course, I don‚Äôt mean those who literally ask ChatGPT to write a paper from scratch.)\n\nI sometimes feel guilty when I feed a paper into ChatGPT and ask it to summarize or even extract ‚Äúideas‚Äù from it, which I then try to combine with my own. I want to understand where a researcher should draw the line in using LLMs in their daily workflow, so as not to fool themselves into believing they are doing good research while over-relying on the tool.",
    "author": "etoipi1",
    "timestamp": "2025-10-02T10:53:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1nwaunk/d_how_much_should_researchers_especially_in_ml/",
    "score": 45,
    "num_comments": 60,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nwfn4j",
    "title": "[R] Thesis direction: mechanistic interpretability vs semantic probing of LLM reasoning?",
    "content": "Hi all,\n\nI'm an undergrad Computer Science student working or my senior thesis, and l'll have about 8 months to dedicate to it nearly full-time. My broad interest is in reasoning, and I'm trying to decide between two directions:\n\n‚Ä¢ Mechanistic interpretability (low-level): reverse engineering smaller neural networks, analyzing weights/ activations, simple logic gates, and tracking learning dynamics.\n\n‚Ä¢Semantic probing (high-level): designing behavioral tasks for LLMs, probing reasoning, attention/locality, and consistency of inference.\n\nFor context, after graduation I'll be joining a GenAl team as a software engineer. The role will likely lean more full-stack/frontend at first, but my long-term goal is to transition into backend.\n\nI'd like the thesis to be rigorous but also build skills that will be useful for my long-term goal of becoming a software engineer. From your perspective, which path might be more valuable in terms that of feasibility, skill development, and career impact?\n\nThanks in advance for your advice!",
    "author": "powerpuff___",
    "timestamp": "2025-10-02T13:50:44",
    "url": "https://reddit.com/r/MachineLearning/comments/1nwfn4j/r_thesis_direction_mechanistic_interpretability/",
    "score": 10,
    "num_comments": 13,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nw6jqf",
    "title": "[R] Maths PhD student - Had an idea on diffusion",
    "content": "I am a PhD student in Maths - high dimensional modeling. I had an idea for a future project, although since I am not too familiar with these concept, I would like to ask people who are, if I am thinking about this right and what your feedback is. \n\nTake diffusion for image generation. An overly simplified tldr description of what I understand is going on is this. Given pairs of (text, image) in the training set, the diffusion algorithm learns to predict the noise that was added to the image. It then creates a distribution of image concepts in a latent space so that it can generalize better. For example, let's say we had two concepts of images in our training set. One is of dogs eating ice cream and one is of parrots skateboarding. If during inference we asked the model to output a dog skateboarding, it would go to the latent space and sample an image which is somewhere \"in the middle\" of dogs eating ice cream and parrots skateboarding. And that image would be generated starting from random noise. \n\nSo my question is, can diffusion be used in the following way? Let's say I want the algorithm to output a vector of numbers (p) given an input vector of numbers (x), where this vector p would perform well based on a criterion I select. So the approach I am thinking is to first generate pairs of (x, p) for training, by generating \"random\" (or in some other way) vectors p, evaluating them and then keeping the best vectors as pairs with x. Then I would train the diffusion algorithm as usual. Finally, when I give the trained model a new vector x, it would be able to output a vector p which performs well given x. \n\n  \nPlease let me know if I have any mistakes in my thought process or if you think that would work in general. Thank you.",
    "author": "5000marios",
    "timestamp": "2025-10-02T08:14:33",
    "url": "https://reddit.com/r/MachineLearning/comments/1nw6jqf/r_maths_phd_student_had_an_idea_on_diffusion/",
    "score": 26,
    "num_comments": 52,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nvvdvl",
    "title": "[D] Open source projects to contribute to as an ML research scientist",
    "content": "Hey everyone,  \nI have a few publications and patents and I work for a tier 2 company as Research scientist. Lately all my job applications have been rejected on the spot. Not even a first interview. I want to beef up my coding skills and be more attractive to employers. Maybe not having a huge github presence is hindering my prospects.  \n  \nCan u please suggest opensource projects like SGLang or vLLm which I can contribute to? Any starting pointers?\n\nEdit- treasure trove of comments below for any RS or MLE trying to get into faang. Thanks community.",
    "author": "lan1990",
    "timestamp": "2025-10-01T22:34:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1nvvdvl/d_open_source_projects_to_contribute_to_as_an_ml/",
    "score": 114,
    "num_comments": 40,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nvxswc",
    "title": "[D] I‚Äôm looking for papers, preprints, datasets, or reports where an LLM is trained to only know what humans knew before a major scientific breakthrough, and is then asked to propose a new theoretical frameworkwithout using post-breakthrough knowledge and without requiring experimental validation.",
    "content": "Imagine we train (or fine-tune) an LLM exclusively on physics texts up to 1904‚ÄîMaxwell, Lorentz, Poincar√©, Michelson‚ÄìMorley, etc.‚Äîand then ask it to produce a theory addressing the known tensions (e.g., invariance of c, simultaneity). The goal isn‚Äôt to re-derive Einstein verbatim or to validate anything in the lab, but to test whether an LLM can elaborate a novel, coherent theoretical structure from historically available knowledge.\n\nI‚Äôm interested in any domain, not just relativity: e.g., pre-quantum physics, pre-DNA biology, early group theory, early materials science, etc.\n\nWhat would count as ‚Äúon topic‚Äù:\n\nPretraining from scratch or continual pretraining on a historically filtered corpus (time-sliced).\n\nStrong leakage controls: no access to post-cutoff texts; possibly knowledge unlearning.\n\nEvaluation focused on novelty + internal coherence (not experimental truth): e.g., CAS/proof-assistants for consistency, reviewers for ‚Äúhistorical plausibility.‚Äù\n\nComparisons vs. baselines like RAG-only setups or modern LLMs that ‚Äúalready know‚Äù the breakthrough.\n\nReports of failure modes (e.g., the model just paraphrases Lorentz/Poincar√©, or smuggles modern terms).\n\nWhy I‚Äôm asking:\n\nI‚Äôve seen adjacent work (LLM-aided conjecture generation, symbolic regression discovering equations, RL systems finding new algorithms), but not a clean ‚Äúpre-discovery epistemology‚Äù experiment with strict temporal cutoffs.\n\n\n\nTagging folks who might have seen or worked on something like this:\n\nu/hardmaru ¬∑ u/MysteryInc152 ¬∑ u/Qyeuebs ¬∑ u/StartledWatermelon ¬∑ u/Playful_Peace6891 ¬∑ u/SatoshiNotMe ¬∑ u/Ch3cks-Out ¬∑ u/NuclearVII\n\n\n\nIf you know of:\n\npeer-reviewed papers, arXiv preprints, theses\n\ndatasets/corpora curated by historical cutoff\n\ncode or replication packages\n\n‚Ä¶please share!\n\nThanks in advance üôè",
    "author": "QuantumFree",
    "timestamp": "2025-10-02T01:04:04",
    "url": "https://reddit.com/r/MachineLearning/comments/1nvxswc/d_im_looking_for_papers_preprints_datasets_or/",
    "score": 58,
    "num_comments": 11,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nvwkdt",
    "title": "[D] The job market is weird",
    "content": "Would love to get people‚Äôs thoughts on the current job market. Simultaneously,  it seems a lot of companies aren‚Äôt hiring, a lot of start ups are hiring and there are a lot of people in the market. \n\nAlso this is the first time I‚Äôve seen so many companies only offer Staff positions. \n\nHow is everyone feeling right now? ",
    "author": "guohealth",
    "timestamp": "2025-10-01T23:46:02",
    "url": "https://reddit.com/r/MachineLearning/comments/1nvwkdt/d_the_job_market_is_weird/",
    "score": 65,
    "num_comments": 26,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nw8ql3",
    "title": "[D] Will fine-tuning LLaMA 3.2 11B Instruct on text-only data degrade its vision capabilities?",
    "content": "I'm planning to fine-tune LLaMA 3.2 11B Instruct on a JSONL dataset of domain-specific question-answer pairs ‚Äî purely text, no images. The goal is to improve its instruction-following behavior for specialized text tasks, while still retaining its ability to handle multimodal inputs like OCR and image-based queries.\n\nMy concern: will this fine-tuning lead to multimodal forgetting?\n\nThe NeurIPS 2024 paper discusses how training on more image-text pairs can cause text-only forgetting. So I‚Äôm wondering ‚Äî does the reverse happen too? If I train only on text, will the model lose its ability to process images or degrade in tasks like OCR?\n\nHas anyone observed this kind of modality drift or tested the impact of unimodal fine-tuning on multimodal performance?",
    "author": "PravalPattam12945RPG",
    "timestamp": "2025-10-02T09:35:46",
    "url": "https://reddit.com/r/MachineLearning/comments/1nw8ql3/d_will_finetuning_llama_32_11b_instruct_on/",
    "score": 8,
    "num_comments": 8,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nwanih",
    "title": "[D] Multi-market retail dataset for computer vision - 1M images, temporally organised by year",
    "content": "Hello all. I am sharing details about a retail focused dataset we've assembled that might interest folks working on production CV systems:\n\n**Quick specs:**\n\n* 1M retail interior images (280K structured, 720K available for processing) but all are structured and organised. 280k are our platinum set.\n* Multi-country: UK, US, Netherlands, Ireland, Germany. Mainly UK/US.\n* Temporal organisation: Year/month categorization spanning multiple years, also by retailer and week too.\n* Hierarchical structure: Year &gt; Season &gt; Retailer &gt; Sub-Category (event specific) and often by month and week for Christmas.\n* Real-world conditions: Various lighting, angles, store formats.\n* Perfectly imperfect world of retail, all images taken for our consulting work, so each image has a story, good, bad, indifferent. \n\n**Why this might matter:** Most retail CV benchmarks (SKU110K, RP2K, etc.) are single market or synthetic. Real deployment requires models that handle:\n\n* Cross-retailer variation (Tesco ‚â† Walmart ‚â† Sainsburys et al)\n* Temporal shifts (seasonal merchandising, promotional displays, COVID we have too)\n* Geographic differences (EU vs US labeling, store formats)\n\n**Research applications:**\n\n* Domain adaptation across retail environments\n* Few shot learning for new product categories\n* Temporal consistency in object detection\n* Transfer learning benchmarks\n* Dates on product, reduction labels, out of stock, lows, highs.\n\n**Commercial applications:**\n\n* Training production planogram compliance systems\n* Autonomous checkout model training\n* Inventory management CV pipelines\n* Retail execution monitoring\n* Numerous other examples that could be developerd.\n\nAvailable for licensing (commercial) and academic partnerships. Can provide samples and detailed breakdown under NDA with a controlled sample available. \n\nCurious about the community's thoughts on what annotations would add most value - we can support custom categorisation and labelling work. \n\nIt's a new world for us in terms of licensing, we are retailers at heart but we know that 1m images from 2010 to today represents a really unique dataset.",
    "author": "malctucker",
    "timestamp": "2025-10-02T10:46:18",
    "url": "https://reddit.com/r/MachineLearning/comments/1nwanih/d_multimarket_retail_dataset_for_computer_vision/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nvoh20",
    "title": "[D] AAAI 26 Social Impact Track",
    "content": "Hi everyone, the reviews are finally out! I hope you all did well. How were yours?\n\nI got 4, 4, 4, and 3 ‚Äî any chances? (4 weak accept, 3 weak reject)",
    "author": "Senior-Let-7576",
    "timestamp": "2025-10-01T16:48:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1nvoh20/d_aaai_26_social_impact_track/",
    "score": 16,
    "num_comments": 19,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nva18q",
    "title": "[D] Reverse-engineering Flash Attention 4",
    "content": "A few of my colleagues went CUDA spelunking last weekend üë∑\n\nThey wrote up a technical report on how FA4 works:¬†[https://modal.com/blog/reverse-engineer-flash-attention-4](https://modal.com/blog/reverse-engineer-flash-attention-4)\n\nFlash Attention 4 is the latest addition to the Flash Attention series of CUDA kernels. These kernels are used in the attention layers of Transformers, which are very computation-heavy and would be ideal to run as fast as possible. Tri Dao announced last month that FA4 is up to 22% faster than the attention kernel implementation in NVIDIA's own cuDNN library.\n\nWe dug in to why! tl;dr-  \n\\- Much more sophisticated warp-specialized async pipeline  \n\\- \"Software softmax\" using a (novel?) cubic approximation to exp2  \n\\- More efficient rescaling to reduce the cost of numerical stability\n\n[the life of a tile in FA4](https://preview.redd.it/lwtgfv5mhisf1.png?width=1667&amp;format=png&amp;auto=webp&amp;s=a3c875c2674e4247a442c4e943797da4d884f050)",
    "author": "crookedstairs",
    "timestamp": "2025-10-01T07:40:18",
    "url": "https://reddit.com/r/MachineLearning/comments/1nva18q/d_reverseengineering_flash_attention_4/",
    "score": 67,
    "num_comments": 5,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nvaiaf",
    "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention",
    "content": "",
    "author": "parlancex",
    "timestamp": "2025-10-01T07:58:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1nvaiaf/sla_beyond_sparsity_in_diffusion_transformers_via/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nv5ubo",
    "title": "[D] ICLR submission numbers?",
    "content": "What was your ICLR submission number? I sent my paper pretty early, so it's \\~5000, but I am curious how many submissions they got. Particularly compared to massive 29k at AAAI, and taking into consideration that ICLR reviews are public.",
    "author": "qalis",
    "timestamp": "2025-10-01T04:40:31",
    "url": "https://reddit.com/r/MachineLearning/comments/1nv5ubo/d_iclr_submission_numbers/",
    "score": 6,
    "num_comments": 16,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nv12y8",
    "title": "[D] Looking for travel grant sources for NeurIPS 2025 ‚Äî any leads?",
    "content": "Hey folks,\n\nMy paper has been accepted at¬†**NeurIPS 2025**, and now I‚Äôm scrambling to secure funding to attend (flights, board, registration, etc.). I know some grants exist, but I'm looking for:\n\n* Agencies / foundations / companies supporting¬†**student researchers**¬†for¬†**NeurIPS / major ML conferences**\n* Lab / university / departmental travel grant schemes that others have used\n* Tips or personal experience (how much you got, when to apply, how to write the proposal)\n\nSo far I‚Äôve found:\n\n* NeurIPS itself offers¬†*financial assistance*¬†for registration but¬†**does not pay for travel and hotel.**\n\nIf you know any lesser-known ones (especially in India / Asia) or similarly for your country, please drop links or names. Appreciate any help!",
    "author": "Opening_Fail5284",
    "timestamp": "2025-09-30T23:41:39",
    "url": "https://reddit.com/r/MachineLearning/comments/1nv12y8/d_looking_for_travel_grant_sources_for_neurips/",
    "score": 15,
    "num_comments": 20,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nvj5hn",
    "title": "[D] Anyone here using LLM-as-a-Judge for agent evaluation?",
    "content": "I‚Äôve been experimenting with using another LLM to *score* my agent‚Äôs responses (accuracy / groundedness style) instead of relying on spot-checking.\n\nSurprisingly effective ‚Äî but only when the judge prompt is written carefully (single criterion, scoring anchors, strict output format, bias warnings, etc.)\n\nCurious if anyone else here is doing this? Any lessons learned?\n\n(I wrote a short breakdown of what worked for us ‚Äî happy to share if useful.)",
    "author": "Cristhian-AI-Math",
    "timestamp": "2025-10-01T13:13:48",
    "url": "https://reddit.com/r/MachineLearning/comments/1nvj5hn/d_anyone_here_using_llmasajudge_for_agent/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.47,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nvalpv",
    "title": "[D] Simple Questions Thread",
    "content": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!",
    "author": "AutoModerator",
    "timestamp": "2025-10-01T08:01:39",
    "url": "https://reddit.com/r/MachineLearning/comments/1nvalpv/d_simple_questions_thread/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nuddci",
    "title": "[D] Is it normal for a CV/ML researcher with ~600 citations and h-index 10 to have ZERO public code at all?",
    "content": "I came across a CV and ML researcher who has recently completed a PhD at a top uni with around 600 citations and an h-index of 10. On the surface, that seems like a legit academic profile. Their papers have been accepted in CVPR, WACV, BMVC, ECCV, AAAI. What surprised me is that NONE of their papers have associated code releases. They have several github page (some git from 2-3 years ago) but with ZERO code release, just README page.\n\nIs it common for a researcher at this level to have **ZERO code releases across ALL their works**, or is this person a fake/scam? Curious how others in academia/industry interpret this.\n\nEdit: his research (first authored) is all 2020-present. recently graduated from a top uni.",
    "author": "rosesarenotred00",
    "timestamp": "2025-09-30T06:34:49",
    "url": "https://reddit.com/r/MachineLearning/comments/1nuddci/d_is_it_normal_for_a_cvml_researcher_with_600/",
    "score": 107,
    "num_comments": 111,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nuxgrx",
    "title": "[D] Student Travel Grant for EMNLP",
    "content": "Did anyone hear back from the volunteering chair / diversity and inclusion chair?",
    "author": "HorrorRemove6851",
    "timestamp": "2025-09-30T20:17:01",
    "url": "https://reddit.com/r/MachineLearning/comments/1nuxgrx/d_student_travel_grant_for_emnlp/",
    "score": 8,
    "num_comments": 44,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nu1yfz",
    "title": "[R] A Predictive Approach To Enhance Time-Series Forecasting",
    "content": "[Nature Communications](https://rdcu.be/eISlO)\n\n&gt;**Abstract:** Accurate time-series forecasting is crucial in various scientific and industrial domains, yet deep learning models often struggle to capture long-term dependencies and adapt to data distribution shifts over time. We introduce Future-Guided Learning, an approach that enhances time-series event forecasting through a dynamic feedback mechanism inspired by predictive coding. Our method involves two models: a detection model that analyzes future data to identify critical events and a forecasting model that predicts these events based on current data. When discrepancies occur between the forecasting and detection models, a more significant update is applied to the forecasting model, effectively minimizing surprise, allowing the forecasting model to dynamically adjust its parameters. We validate our approach on a variety of tasks, demonstrating a 44.8% increase in AUC-ROC for seizure prediction using EEG data, and a 23.4% reduction in MSE for forecasting in nonlinear dynamical systems (outlier excluded).By incorporating a predictive feedback mechanism, Future-Guided Learning advances how deep learning is applied to time-series forecasting.\n\nHello everyone. As the first author of this paper, I would be grateful for your thoughts and feedback. The core concept of our work is to use a forecasting model aligned with subsequent (\"future\") data to guide and improve a separate model that makes predictions from an earlier (\"past\") point in time. This approach is grounded in the principles of predictive coding theory.",
    "author": "Skye7821",
    "timestamp": "2025-09-29T19:54:00",
    "url": "https://reddit.com/r/MachineLearning/comments/1nu1yfz/r_a_predictive_approach_to_enhance_timeseries/",
    "score": 13,
    "num_comments": 5,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nu4no7",
    "title": "[D] How To Pitch MetaHeuritsic Techniques to Stakeholders",
    "content": "Hi everyone,\nI am working on a non-linear model which will later fed into a optimization framework. I am planning to use meta-heuristic technique for optimization framework but the problem is meta-heuristic techniques gives near optimal solution and are non-deterministic in nature. This will create problems while explaining my solution to Product managers and business stakeholders. How should I go about it ?\nPS- I cannot implement search space based optimization techniques because it will breach the SLA.",
    "author": "indiancaptainamerica",
    "timestamp": "2025-09-29T22:16:16",
    "url": "https://reddit.com/r/MachineLearning/comments/1nu4no7/d_how_to_pitch_metaheuritsic_techniques_to/",
    "score": 7,
    "num_comments": 9,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ntm0pf",
    "title": "[R] No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping",
    "content": "Arxiv:¬†[https://arxiv.org/pdf/2509.21880](https://arxiv.org/pdf/2509.21880)\n\nHuggingface paper:¬†[https://huggingface.co/papers/2509.21880](https://huggingface.co/papers/2509.21880)\n\nI‚Äôve been working on improving the reasoning abilities of large language models, and I wanted to share something I‚Äôm really excited about. Reinforcement Learning with Verifiable Rewards (RLVR) is already a powerful framework, but I noticed a gap: current methods like GRPO only use problems where model responses differ in correctness. They completely ignore the so-called ‚Äúzero-variance prompts‚Äù ‚Äî cases where all responses receive the same reward.\n\nAt first glance, these prompts look useless, but I started wondering if they actually contain valuable learning signals. That led me to develop¬†**RL with Zero-Variance Prompts (RL-ZVP)**. Instead of discarding those prompts, RL-ZVP extracts meaningful feedback from them. It directly rewards correctness and penalizes errors without needing contrasting responses, and it uses token-level entropy to guide the advantage shaping.\n\nWe evaluated RL-ZVP on six math reasoning benchmarks, and it delivered some really promising results ‚Äî up to¬†**8.61 points higher accuracy**¬†and¬†**7.77 points higher pass rates**¬†compared to GRPO. It also consistently outperformed other baselines that just filter out zero-variance prompts.\n\nI am happy to take comments in this sub and the HuggingFace paper.",
    "author": "SnooHesitations8849",
    "timestamp": "2025-09-29T09:03:37",
    "url": "https://reddit.com/r/MachineLearning/comments/1ntm0pf/r_no_prompt_left_behind_exploiting_zerovariance/",
    "score": 33,
    "num_comments": 6,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ntakmv",
    "title": "[D] Name and describe a data processing technique you use that is not very well known.",
    "content": "Tell me about your data preprocessing technique that you found out/invented by years of experience.",
    "author": "Glittering_Key_9452",
    "timestamp": "2025-09-28T23:13:25",
    "url": "https://reddit.com/r/MachineLearning/comments/1ntakmv/d_name_and_describe_a_data_processing_technique/",
    "score": 55,
    "num_comments": 27,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nu9ey8",
    "title": "[D] M4 Mac Mini 16GB vs 5700x+2070super",
    "content": "Title!\n\nI currently have a workstation with a 12600k and a 3090 FE but to be fair most of my work is now done on remote machines. I only use the local station for quick tests of repositories and stuff. I want to keep this machine as a dedicated gaming rig and I'm thinking to downsizing reusing an alternate machine I have, with a 2070 super and a 2700x. Currently I'm on windows but that machine will run on linux.\n\nIf price difference was bigger I'll stick to the ITX but currently I have a 2700x which is way slower than the m4 and would like to upgrade to a 5700x (not too expensive, can use the same ram etc), or maybe something am5 as I still have to get the ITX board, but this would also increase the price as I would require DDR5 ram.\n\nThe biggest pros I see on the mac mini, very small so my setup remains clean, has good audio compatibility (I record myself often). The disadvantage is being stuck to 16GB ram and requiring external storage expansion, and maybe package compatibility. I do not run local LLMs as of now as my pipelines are mostly vision.\n\nThe pros on the itx station, can get more RAM for less, the 2070 super should be more powerful, (but only 8GB vram) more compatible with libraries, upgradeable (could even fit the 3090fe on some cases if I wanted to), but it will be bigger, noisier, have more cables, and less power efficient.\n\nI'm not able to choose one or another to be honest. I enjoy both OS.\n\nNot sure if this affects somehow the experience but I have a 4k monitor. Not sure how well linux scales things (my previous 1440p monitor experience with my linux laptop was mediocre due to blurry texts often).\n\nMy current buy list makes 600 on the mac and 640 on the ITX, including a 1TB m2.\n\nWhat would you go for? are you using similar systems yourself?\n\nThanks!",
    "author": "Monti_ro",
    "timestamp": "2025-09-30T03:19:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1nu9ey8/d_m4_mac_mini_16gb_vs_5700x2070super/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ntbbhd",
    "title": "[D] isn‚Äôt N-gram model a global solution given training data ?",
    "content": "I had a stupid question while watching at andrej‚Äôs video. Since we are just collecting the numbers of occurrence of a ‚ÄúN-sequence pairs‚Äù using training data to predict the outcome in N-gram model, isn‚Äôt it that is what we are actually trying to achieve or expect it to happen while training NN?, and if so, isn‚Äôt N-gram model a global solution rather than a local solution?",
    "author": "tinde-ki-sabji",
    "timestamp": "2025-09-29T00:01:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1ntbbhd/d_isnt_ngram_model_a_global_solution_given/",
    "score": 17,
    "num_comments": 17,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nsvdqk",
    "title": "[D] Machine learning research no longer feels possible for any ordinary individual. It is amazing that this field hasn't collapsed yet.",
    "content": "Imagine you're someone who is attempting to dip a toe into ML research in 2025. Say, a new graduate student.\n\nYou say to yourself \"I want to do some research today\". Very quickly you realize the following:\n\n**Who's my competition?**\n\nJust a handful of billion-dollar tech giants, backed by some of the world's most powerful governments, with entire armies of highly paid researchers whose only job is to discover interesting research questions. These researchers have access to massive, secret knowledge graphs that tell them exactly where the next big question will pop up before anyone else even has a chance to realize it exists. Once LLMs mature even more, they'll probably just automate the process of generating and solving research problems. What's better than pumping out a shiny new paper every day?\n\n**Where would I start?**\n\nBoth the Attention and the ADAM paper has 200k citation. That basically guarantees there‚Äôs no point in even trying to research these topics. Ask yourself what more could you possibly contribute to something that‚Äôs been cited 200,000 times. But this is not the only possible topic. Pull out any topic in ML, say image style transfer, there are already thousands of follow-up papers on that. Aha, maybe you could just read the most recent ones from this year. Except, you quickly realize that most of those so-called ‚Äúpapers‚Äù are from shady publish-or-perish paper-mills (which are called \"universities\" nowadays, am I being too sarcastic?) or just the result of massive GPU clusters funded by millions of dollars instant-access revenue that you don‚Äôt have access to.\n\n**I‚Äôll just do theory!**\n\nMaybe let's just forget the real world and dive into theory instead. But to do theory, you‚Äôll need a ton of math. What‚Äôs typically used in ML theory? Well, one typically starts with optimization, linear algebra and probability. But wait, you quickly realize that‚Äôs not enough. So you go on to master more topics in applied math: ODEs, PDEs, SDEs, and don‚Äôt forget game theory, graph theory and convex optimization. But it doesn‚Äôt stop there. You‚Äôll need to dive into Bayesian statistics, information theory. Still isn‚Äôt enough. Turns out, you will need pure math as well: measure theory, topology, homology, group, field, and rings. At some point, you realize this is still not enough and now you need to think more like Andrew Wiles. So you go on to tackle some seriously hard topics such as combinatorics and computational complexity theory. What is all good for in the end? Oh right, to prove some regret bound that absolutely no one cares about. What was the regret bound for ADAM again? It's right in the paper, Theorem 1, cited 200k times, and nobody as far as I'm aware of even knows what it is.",
    "author": "NeighborhoodFatCat",
    "timestamp": "2025-09-28T11:15:48",
    "url": "https://reddit.com/r/MachineLearning/comments/1nsvdqk/d_machine_learning_research_no_longer_feels/",
    "score": 78,
    "num_comments": 55,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nsza5n",
    "title": "[D] Musicnn embbeding vector and copyright",
    "content": "Hi everyone,\nI developed a selfhostable software, that use Librosa + Tensorflow to extract a Musicnn embbeding vector from songs. So basicaly a 200 size vector that off course it can't be reverted in anyway to the original song.\n\nThe Tensorflow model that I use, as anticipated, is not trained by me but is Musicnn embbeding. So that my doubts is not about how to train the model  BUT about the result that I get.\n\nActually the user run my app in their homelab on their songs, so is totally their ownership to do an accurate use in the respect of copyright.\n\nI would like to collect, with the acceptance of the user, a centralized database of this embbeding vector. This could open multiple new scenario because thanks of them I can:\n\n- First reduce the analysis process from the user, that don't need to re-analyze all the song. This is specially useful for user that run the software on low end machine, like a Raspberry PI\n\n- Second start not only to give user suggestion of similar song that he already have, but also help them to discover song that don't have.\n\nMy copyright queston is: collect this data from the user in a database usable from everyone, could me bring some kind of copyright issue?\n\nI mean, user could potentially analyze commercial songs and upload the embbeding of those commercial song, could be this an issue? could be this seens as \"use of derivative work without a correct license\"? Especially by my centralized database that off course don't have any license on the original music?\n\nImportant:\n- this centralized database only collec Title, Artist, embbeding, genre, NOT the song itself;\n\n- I'm in Europe, so I don't know if any specific restriction is here.\n\nBy similarity I was thinking what Acousticbrainz did, even if it don't collect embbding vector, it have user submitting data get from original music in some way. But here I don't know if they have some agreement, if maybe they are in an University and as researcher they are ok (In my case I'm only a single person that do this in his free time, without any university or company behind).\n\nI don‚Äôt want for a free and opensource project run the risk of have issue with copyright and at the same time I don‚Äôt have money to invest for consulting a layer.",
    "author": "Old_Rock_9457",
    "timestamp": "2025-09-28T13:50:09",
    "url": "https://reddit.com/r/MachineLearning/comments/1nsza5n/d_musicnn_embbeding_vector_and_copyright/",
    "score": 21,
    "num_comments": 9,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nsnbep",
    "title": "[P] Built a differentiable parametric curves library for PyTorch",
    "content": "I‚Äôve released a small library for parametric curves for PyTorch that are differentiable: you can backprop to the curve‚Äôs inputs and to its parameters. At this stage, I have B-Spline curves (efficiently, exploiting sparsity!) and Legendre Polynomials. Everything is vectorized - over the mini-batch, and over several curves at once.\n\nApplications include:\n\n* Continuous embeddings for embedding-based models (i.e. factorization machines, transformers, etc)\n* KANs. You don‚Äôt have to use B-Splines. You can, in fact, use any well-approximating basis for the learned activations.\n* Shape-restricted models, i.e. modeling the probability of winning an auction given auction features x and a bid b - predict increasing B-Spline coefficients c(x) using a neural network, apply to a B-Spline basis of b.\n\nLink:¬†[https://github.com/alexshtf/torchcurves](https://github.com/alexshtf/torchcurves)\n\nI wrote ad-hoc implementations for past projects, so I decided to write a proper library, that may be useful to others. And I hope i will!",
    "author": "alexsht1",
    "timestamp": "2025-09-28T05:39:18",
    "url": "https://reddit.com/r/MachineLearning/comments/1nsnbep/p_built_a_differentiable_parametric_curves/",
    "score": 81,
    "num_comments": 3,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ns7rd6",
    "title": "[D] The organization of NeurIPS Position Papers track is a joke",
    "content": "Basically the title. A list of how the PCs fumbled being PCs for this track:\n\n1. Missed every deadline they posted on the website.\n2. Only mentioned about 6% acceptance a day before sending notifs. Had this been posted at the start of calls, authors would have logically submitted it to other venues.\n3. Blocked possible submissions of papers to ICLR by moving notifs by one week.\n4. No metareviews for some papers, including ours. \n5. ICML2025 handled the Position Paper track just fine with relatively the same # of submissions and was able to stick to the deadline. AND they had rebuttals. Why couldn't the PCs do the same now?\n6. PCs kept justifying their poor decisions instead of taking responsibility for wasting reviewers' and authors' time, which is so infuriating.\n\nBut sure. It was \"experimental\" after all, so no biggie.",
    "author": "EDEN1998",
    "timestamp": "2025-09-27T15:07:56",
    "url": "https://reddit.com/r/MachineLearning/comments/1ns7rd6/d_the_organization_of_neurips_position_papers/",
    "score": 103,
    "num_comments": 26,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nsiois",
    "title": "[D] Serving solutions for recsys",
    "content": "Hi community,\n\nWhat online serving solutions do you use for recsys? How does the architecture look (sidecars, ensembles across different machines, etc.)? \n\nFor example, is anyone using Ray Serve in prod, and if so, why did you choose it? I'm starting a new project and again leaning towards Triton, but I like the concepts that Ray Serve introduces (workers, builtin mesh). I previously used KubeRay for offline training, and it was a very nice experience, but I also heard that Ray isn't very mature for online serving.",
    "author": "MysteryLobstery",
    "timestamp": "2025-09-28T01:01:26",
    "url": "https://reddit.com/r/MachineLearning/comments/1nsiois/d_serving_solutions_for_recsys/",
    "score": 8,
    "num_comments": 3,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nrqzm7",
    "title": "[R] DynaMix: First dynamical systems foundation model enabling zero-shot forecasting of long-term statistics at #NeurIPS2025",
    "content": "Our **dynamical systems foundation model DynaMix** was accepted to **#NeurIPS2025** with outstanding reviews (6555) ‚Äì the first model which can ***zero-shot***, w/o any fine-tuning, forecast the ***long-term behavior*** of time series from just a short context signal. Test it on #HuggingFace:\n\n[https://huggingface.co/spaces/DurstewitzLab/DynaMix](https://huggingface.co/spaces/DurstewitzLab/DynaMix)\n\nPreprint: [https://arxiv.org/abs/2505.13192](https://arxiv.org/abs/2505.13192)\n\nUnlike major time series (TS) foundation models (FMs), DynaMix exhibits zero-shot learning of long-term stats of unseen DS, incl. attractor geometry &amp; power spectrum. It does so with only **0.1% of the parameters &amp; &gt;100x faster inference times** than the closest competitor, and with an **extremely small training corpus of just 34 dynamical systems** \\- in our minds a paradigm shift in time series foundation models.\n\nhttps://preview.redd.it/d46h9deagorf1.png?width=1791&amp;format=png&amp;auto=webp&amp;s=7a86714f6e8d7eb269224c0e06ac317f405dfbee\n\nhttps://preview.redd.it/mullm71cgorf1.png?width=1436&amp;format=png&amp;auto=webp&amp;s=e53055fcc8b1d2f77da88c3896a95d65f3fac893\n\nIt even outperforms, or is at least on par with, major TS foundation models like Chronos on forecasting diverse empirical time series, like weather, traffic, or medical data, typically used to train TS FMs. This is surprising, cos DynaMix‚Äô training corpus consists \\*solely\\* of simulated limit cycles or chaotic systems, no empirical data at all!\n\nhttps://preview.redd.it/8twn70e2horf1.png?width=1127&amp;format=png&amp;auto=webp&amp;s=20a7a7721a29d80bc2f01077b6e8684b54ce21ef\n\nAnd no, it‚Äôs neither based on Transformers nor Mamba ‚Äì **it‚Äôs a new type of mixture-of-experts architecture** based on the recently introduced **AL-RNN** (https://proceedings.neurips.cc/paper\\_files/paper/2024/file/40cf27290cc2bd98a428b567ba25075c-Paper-Conference.pdf). It is specifically designed &amp; trained for dynamical systems reconstruction.\n\nhttps://preview.redd.it/j0njmppkgorf1.png?width=1796&amp;format=png&amp;auto=webp&amp;s=e05e275bf6aeba93fb04e8a288cd0fbac6d8fa84\n\nRemarkably, it not only generalizes zero-shot to novel DS, but it **can even generalize to new initial conditions and regions of state space not covered by the in-context information**.\n\nhttps://preview.redd.it/wlxwcp2ngorf1.png?width=1522&amp;format=png&amp;auto=webp&amp;s=54a2dbed65a085d7522907275468700adf9d9619\n\nIn our paper we dive a bit into the reasons why current time series FMs not trained for DS reconstruction fail, and conclude that a DS perspective on time series forecasting &amp; models may help to advance the time series analysis field.",
    "author": "DangerousFunny1371",
    "timestamp": "2025-09-27T02:34:54",
    "url": "https://reddit.com/r/MachineLearning/comments/1nrqzm7/r_dynamix_first_dynamical_systems_foundation/",
    "score": 101,
    "num_comments": 32,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nrptrp",
    "title": "[D] Tips for networking at a conference",
    "content": "I'm attending at CoRL 2025 and went to some interesting workshops today. I've heard that networking is very important at conferences, but it is challenging for highly introvert people like me. Do you have any tips?",
    "author": "Glittering-Fudge-115",
    "timestamp": "2025-09-27T01:19:24",
    "url": "https://reddit.com/r/MachineLearning/comments/1nrptrp/d_tips_for_networking_at_a_conference/",
    "score": 34,
    "num_comments": 15,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nroqyb",
    "title": "[r] Seeking advice regarding affordable GPU",
    "content": "Hello everyone,\n\nTogether with some friends from my network, we recently started a startup. We‚Äôre still in the early stages of development, and to move forward, we need access to GPUs.\n\nWe‚Äôve already explored a few free platforms, but haven‚Äôt received any responses so far. At the moment, we‚Äôre looking for either the most affordable GPU options or platforms that might be open to collaborating with us.\n\nIf you know of any opportunities or resources that could help, I‚Äôd be truly grateful.\n\nThank you in advance!",
    "author": "Few-Annual-157",
    "timestamp": "2025-09-27T00:10:06",
    "url": "https://reddit.com/r/MachineLearning/comments/1nroqyb/r_seeking_advice_regarding_affordable_gpu/",
    "score": 14,
    "num_comments": 13,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nrvlle",
    "title": "[P] Sample Forge - Research tool for deterministic inference and convergent sampling parameters in large language models.",
    "content": "Hi folks, I made a research tools that allows you to perform deterministic inference on any local large language model. This way you can test any variable changes and see for yourself the affects those changes have on the output of the LLM's response.  It also allows you to perform automated reasoning benchmarking of a local language model of your choice, this way you can measure the perplexity drop of any quantized model or differences between reasoning capabilities of models or sampling parameters. It also has a fully automated way of converging on the best sampling parameters for a given model when it comes to reasoning capabilities. I made 2 videos for the project so you can see what its about at a glance the main guide is here https://www.youtube.com/watch?v=EyE5BrUut2o, the instillation video is here https://youtu.be/FJpmD3b2aps and the repo is here https://github.com/manfrom83/Sample-Forge. If you have more questions id be glad to answer them here. Cheers.",
    "author": "no_witty_username",
    "timestamp": "2025-09-27T06:43:40",
    "url": "https://reddit.com/r/MachineLearning/comments/1nrvlle/p_sample_forge_research_tool_for_deterministic/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nrr7ms",
    "title": "[R] Object Tracking: A Comprehensive Survey From Classical Approaches to Large Vision-Language and Foundation Models",
    "content": "I came across a new survey and resource repository on object tracking. It covers classical Single Object Tracking (SOT) and Multi-Object Tracking (MOT), as well as more recent approaches that use vision-language and foundation models.\n\nThe repository also includes Long-Term Tracking (LTT), benchmarks, datasets, and code links. It‚Äôs been put together by researchers at Carnegie Mellon University (CMU), Boston University, and MBZUAI.\n\nLink: [https://github.com/rahulrj/Awesome-Object-Tracking](https://github.com/rahulrj/Awesome-Object-Tracking)\n\nIt could be useful for both researchers and practitioners. Contributions and feedback are welcome.",
    "author": "Downtown_Ambition662",
    "timestamp": "2025-09-27T02:49:29",
    "url": "https://reddit.com/r/MachineLearning/comments/1nrr7ms/r_object_tracking_a_comprehensive_survey_from/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nrn6x7",
    "title": "[R] Pytorch with dynamic input tensor",
    "content": "[https://github.com/yoonsanghyu/FaSNet-TAC-PyTorch](https://github.com/yoonsanghyu/FaSNet-TAC-PyTorch) is this rather cool model for invariant source separation but the above is a great bit of code but for fixed sources.\n\n[https://docs.pytorch.org/docs/stable/torch.compiler\\_dynamic\\_shapes.html](https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html) does go into the possibility of dynamic shapes as it would be cool to have a single model that would work with 2-6 input mics than say creating a model for each number of inputs 2,3,4,5,6...\n\nI am just wondering that even though possible would a dynamic model be much larger requiring more compute and also be less accurate than a fixed known input tensor?",
    "author": "rolyantrauts",
    "timestamp": "2025-09-26T22:34:05",
    "url": "https://reddit.com/r/MachineLearning/comments/1nrn6x7/r_pytorch_with_dynamic_input_tensor/",
    "score": 9,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nr1s6g",
    "title": "[R] What do you do when your model is training?",
    "content": "As in the question what do you normally do when your model is training and you want to know the results but cannot continue implementing new features because you don't want to change the status and want to know the impact of the currently modifications done to your codebase?",
    "author": "T-Style",
    "timestamp": "2025-09-26T06:46:04",
    "url": "https://reddit.com/r/MachineLearning/comments/1nr1s6g/r_what_do_you_do_when_your_model_is_training/",
    "score": 64,
    "num_comments": 58,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nrenza",
    "title": "[D] Does TPU v5e have less memory than v3",
    "content": "I was trying to train a GPT-2 XL-sized model on Kaggle with their free TPU v3-8, but they recently switched to TPU v5e-8, and now I am getting OOM errors whenever I try to train. I am using Torch XLA, FSDP, mixed precision, and the Muon optimizer(momentum-only optimizer) for my hidden weight matrices and AdamW everywhere else.",
    "author": "New-Skin-5064",
    "timestamp": "2025-09-26T15:17:48",
    "url": "https://reddit.com/r/MachineLearning/comments/1nrenza/d_does_tpu_v5e_have_less_memory_than_v3/",
    "score": 11,
    "num_comments": 9,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nqtiad",
    "title": "[P] Give me your one line of advice of machine learning code, that you have learned over years of hands on experience.",
    "content": "Mine is \"always balance the dataset using SMOTE, that will drastically increase the precision, recall, f1 etc\"",
    "author": "Glittering_Key_9452",
    "timestamp": "2025-09-25T22:55:44",
    "url": "https://reddit.com/r/MachineLearning/comments/1nqtiad/p_give_me_your_one_line_of_advice_of_machine/",
    "score": 91,
    "num_comments": 58,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nr972b",
    "title": "[D] Anyone hear back from NeurIPS Creative AI track?",
    "content": "The website says decisions out September 18 but I still haven‚Äôt see any reviews or notifications. Anyone else hearing back from it?",
    "author": "Careless_Milk_7123",
    "timestamp": "2025-09-26T11:35:42",
    "url": "https://reddit.com/r/MachineLearning/comments/1nr972b/d_anyone_hear_back_from_neurips_creative_ai_track/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nrhcrb",
    "title": "[P] Why MissForest Fails in Prediction Tasks: A Key Limitation You Need to Keep in Mind",
    "content": "https://preview.redd.it/25bv436lolrf1.png?width=1536&amp;format=png&amp;auto=webp&amp;s=e2154e75a16600600492b948877749aaffb468ea\n\nHi everyone,\n\nI recently explored a limitation of the **MissForest algorithm** (Stekhoven &amp; B√ºhlmann, 2012): it cannot be directly applied in predictive settings because it doesn‚Äôt save the imputation models. This often leads to **data leakage** when trying to use it across train/test splits.\n\nIn the article, I show:\n\n* Why MissForest fails in prediction contexts,\n* Practical examples in R and Python,\n* How the new **MissForestPredict** (Albu et al., 2024) addresses this issue by saving models and parameters.\n\nüëâ Full article here: [https://towardsdatascience.com/why-missforest-fails-in-prediction-tasks-a-key-limitation-you-need-to-know/](https://towardsdatascience.com/why-missforest-fails-in-prediction-tasks-a-key-limitation-you-need-to-know/)",
    "author": "North-Kangaroo-4639",
    "timestamp": "2025-09-26T17:21:29",
    "url": "https://reddit.com/r/MachineLearning/comments/1nrhcrb/p_why_missforest_fails_in_prediction_tasks_a_key/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nqil0w",
    "title": "[R] How to finetune a multimodal model?",
    "content": "I am working on a project in which we are tasked with developing anomaly detection for a technical system.\n\nUntil now, I have mainly worked with LLMs and supplied them with external knowledge using RAG.\n\nNow I have to work with a multimodal model and train it to detect anomalies (e.g scratches, broken glass) in a technical system based on images. I was thinking of using Gemma3:4b as the model, but I will evaluate this in more detail as I go along.\n\nTo do this, I would have to train this model accordingly for this use case, but I'm not quite sure how to proceed. All I know is that a large amount of labeled data is required.\n\nSo I would like to ask what the procedure would be, which tools are commonly used here, and whether there is anything else to consider that I am not currently aware of.",
    "author": "psy_com",
    "timestamp": "2025-09-25T14:03:40",
    "url": "https://reddit.com/r/MachineLearning/comments/1nqil0w/r_how_to_finetune_a_multimodal_model/",
    "score": 22,
    "num_comments": 18,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nqkwn4",
    "title": "[P] How to Check If Your Training Data Is Representative: Using PSI and Cramer‚Äôs V in Python",
    "content": "https://preview.redd.it/3m7n4tnu1erf1.png?width=1536&amp;format=png&amp;auto=webp&amp;s=29a717573ec6d3a8d07440b17bd98bf1452ce9a6\n\nHi everyone,\n\nI‚Äôve been working on a guide to evaluate **training data representativeness** and detect dataset shift. Instead of focusing only on model tuning, I explore how to use two statistical tools:\n\n* **Population Stability Index (PSI)** to measure distributional changes,\n* **Cramer‚Äôs V** to assess the intensity of the change.\n\nThe article includes explanations, Python code examples, and visualizations. I‚Äôd love feedback on whether you find these methods practical for real-world ML projects (especially monitoring models in production).  \nFull article here: [https://towardsdatascience.com/assessment-of-representativeness-between-two-populations-to-ensure-valid-performance-2/](https://towardsdatascience.com/assessment-of-representativeness-between-two-populations-to-ensure-valid-performance-2/)",
    "author": "North-Kangaroo-4639",
    "timestamp": "2025-09-25T15:40:24",
    "url": "https://reddit.com/r/MachineLearning/comments/1nqkwn4/p_how_to_check_if_your_training_data_is/",
    "score": 15,
    "num_comments": 2,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nq856v",
    "title": "[R] ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution",
    "content": "We released ShinkaEvolve, a new state-of-the-art and fully open-source framework for program optimization, which we specifically designed to be easily integrated into any scientific codebase.\n\nOpen source code:[ https://github.com/SakanaAI/ShinkaEvolve](https://github.com/SakanaAI/ShinkaEvolve)\n\nTechnical report:[ https://arxiv.org/abs/2509.19349](https://arxiv.org/abs/2509.19349)\n\nBlog:[ https://sakana.ai/shinka-evolve/](https://sakana.ai/shinka-evolve/)\n\nYou can start playing with ShinkaEvolve without even downloading any code, all inside a remote Google Colab instance:[ https://colab.research.google.com/github/SakanaAI/ShinkaEvolve/blob/main/examples/shinka\\_tutorial.ipynb](https://colab.research.google.com/github/SakanaAI/ShinkaEvolve/blob/main/examples/shinka_tutorial.ipynb)\n\nIn our technical report, we show how ShinkaEvolve can be easily applied across different problem domains. On the canonical circle packing task, ShinkaEvolve discovers a new solution with state-of-the-art performance beyond the recent closed-source AlphaEvolve using only 150 program evaluations. We even apply ShinkaEvolve to small-scale LLM pretraining, discovering a new load-balancing loss for MoE architectures with remarkable stabilization properties.\n\nShinkaEvolve also comes with a detailed and lightweight WebUI to monitor its discoveries in real-time!",
    "author": "Ereb0",
    "timestamp": "2025-09-25T07:23:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1nq856v/r_shinkaevolve_towards_openended_and/",
    "score": 23,
    "num_comments": 1,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nqc5ij",
    "title": "[R] Summation-Based Transformers: Hybrid Near-Linear Design Matches Full Attention",
    "content": "Replace O(n¬≤d) self-attention in transformers with an O(nd) summation-based mechanism.\n\nPure summation is linear and works well in classification and regression.\n\nIn autoregressive language modeling, a hybrid transformer (summation in most layers + a single final attention layer) matches or slightly outperforms full attention -- while staying nearly linear in cost.\n\nKey points:\n\n* Drop-in replacement for attention inside transformer blocks (residuals, norms, optimizers unchanged)\n* Linear complexity: O(nd) aggregation instead of O(n¬≤d) pairwise similarity\n* Hybrid design: most layers use summation, a final attention layer recovers full performance\n\nResults (small-to-moderate datasets):\n\n* Classification (proof-of-concept): single summation layer on AG News matches attention, up to \\~18√ó faster at 512 tokens\n* Multimodal regression (text + tabular): summation fusion matches or outperforms concatenation, in a smaller latent space and with faster runtime\n* Language modeling: hybrid transformers (summation in most layers + one attention layer) achieve performance on par with or better than full attention -- showing that full attention is not required in every layer\n\nPaper: [https://doi.org/10.36227/techrxiv.175790522.25734653/v1](https://doi.org/10.36227/techrxiv.175790522.25734653/v1)\n\nCode: [https://github.com/pfekin/summation-based-transformers](https://github.com/pfekin/summation-based-transformers)",
    "author": "kertara",
    "timestamp": "2025-09-25T09:57:04",
    "url": "https://reddit.com/r/MachineLearning/comments/1nqc5ij/r_summationbased_transformers_hybrid_nearlinear/",
    "score": 12,
    "num_comments": 20,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nq3kvl",
    "title": "[D] RoPE and K/Q spaces effective dimensionality",
    "content": "Hi guys,\n\nThis post is about figuring out if RoPE overly constrains the K/Q spaces and if it decreases its effective dimensionality, by forcing a high condition number on the K/Q matrices.\n\nJust to give a bit of context, I'm trying to create a hierarchical BERT encoder (a kind of [CLS] embedding merger), and was trying to figure out a way to encode token (= sentence embeddings) position, because RoPE was designed for a kind of exponential decay that is not particularly relevant to my use case.\n\nDigging a bit deeper into the theory behind RoPE, I realized that specialized attention heads that focus on, say, position-insensitive semantical stuff need to project the embedding vectors in a space where the RoPE matrix will not mess them up. That's to say, the projected vectors will be heavily biased towards having information in the last components (where low-frequency rotation occur). \nThe opposite happens for positional encoding heads (I think a Gemma paper mentions them), that project embeddings so they are head-heavy instead of tail-heavy (not even sure this is correct english stuff, I am ESL).\n\nFrom an outside perspective, it seems quite sub-optimal: attention scores are -for these cases- based on low-dimensional (effectively) dot products.\n\nSo, 2 (and a half) questions here:\n\n1. Does it really matter? My prior is with yes, because I once computed the condition numbers of projection matrices in transformers with learned position embeddings and I found them to be very low (I guess they were &lt; 10 at each layer for quite tiny transformers, even though I think they would get bigger for decent ones). Curious about your thoughts though.\n\n2. What about a mitigation strategy like having the attention head 'choose' the base rate of the RoPE? A very simple strategy would be to make it dependent on the barycenter of the norm of K/Q projection matrices' rows. Meaning: if the projection matrices tends to give more importance to the first components of the raw embedding, we consider that the base rate should be higher. This would cause a transformer-wide bias towards having position-dependent information at the beginning of embeddings.\n\n3. Have I totally misunderstood RoPE?\n\nI would love to hear your thoughts on that matter.",
    "author": "Academic_Sleep1118",
    "timestamp": "2025-09-25T03:54:22",
    "url": "https://reddit.com/r/MachineLearning/comments/1nq3kvl/d_rope_and_kq_spaces_effective_dimensionality/",
    "score": 26,
    "num_comments": 10,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1npdfh1",
    "title": "[D] Is senior ML engineering just API calls now?",
    "content": "I‚Äôm a Senior ML engineer with around 9 years of experience. I work at a large government institution, implementing (integrating?) AI for cybersecurity, and I‚Äôm currently in the process of building a new team.\n\nI‚Äôve been having some concerns about my career development, and I‚Äôm not sure if other ML engineers with similar experience feel the same way.\n\nMost of my projects these days aren‚Äôt really ‚Äúmachine learning‚Äù anymore. It‚Äôs mostly using existing models through APIs, setting up pipelines, etc. The actual algorithmic/experimental side of ML feels like it‚Äôs disappearing from my day-to-day work.\n\nIt seems like the industry has shifted from building models to API calls and prompt engineering. I miss the kind of work I did in my earlier roles, building models from scratch, fine-tuning, experimenting‚Ä¶\n\nSo my question is: is this just what senior ML roles eventually turn into? Has the job really shifted from ‚Äúbuilding ML‚Äù to ‚Äúplugging in ML‚Äù? Curious if others are experiencing the same thing. I have been experiencing this since the generative AI boom where suddenly everything was solvable..\n\n(Disclaimer: we do use on-prem models at my organization, so I still get some hands-on time with models and fine-tuning using LoRA.)",
    "author": "Only_Emergencies",
    "timestamp": "2025-09-24T07:20:18",
    "url": "https://reddit.com/r/MachineLearning/comments/1npdfh1/d_is_senior_ml_engineering_just_api_calls_now/",
    "score": 392,
    "num_comments": 184,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nq96ng",
    "title": "[P] Suggestions for detecting atypical neurons in microscopic images",
    "content": "Hi everyone,\n\nI‚Äôm working on a project and my dataset consists of high-resolution microscopic images of neurons (average resolution ~2560x1920). Each image contains numerous neurons, and I have bounding box annotations (from Labelbox) for atypical neurons (those with abnormal morphology). The dataset has around 595 images.\n\nA previous study on the same dataset applied Faster R-CNN and achieved very strong results (90%+ accuracy). For my project, I need to compare alternative models (detection-based CNNs or other approaches) to see how they perform on this task. I would really like to achieve 90% accuracy too.\n\nI‚Äôve tried setting up some architectures (EfficientDet, YOLO, etc.), but I‚Äôm running into implementation issues and would love suggestions from the community.\n\nüëâ Which architectures or techniques would you recommend for detecting these atypical neurons? üëâ Any tips for handling large, high-resolution images with many objects per image? üëâ Are there references or example projects (preferably with code) that might be close to my problem domain?\n\nAny pointers would be super helpful. Thanks!",
    "author": "Drakkarys_",
    "timestamp": "2025-09-25T08:03:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1nq96ng/p_suggestions_for_detecting_atypical_neurons_in/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nplmr8",
    "title": "Apple Research Debuts Manzano ‚Äî a Unified Multimodal LLM",
    "content": "**üÜï What‚Äôs New**  \n  \nApple research just introduced Manzano (Spanish for ‚Äúapple tree‚Äù üçè) ‚Äî a unified multimodal LLM that both understands images and generates them inside the same autoregressive loop.  \nInstead of separate perception and generation models, one decoder predicts the next token ‚Äî text or image ‚Äî then renders pixels with an auxiliary diffusion decoder.  \nThe paper reports state-of-the-art results among unified models and competitive performance against specialist systems, especially on text-rich benchmarks.  \n  \n**‚öôÔ∏è How It Works**  \n  \nHybrid vision tokenizer in front of the LLM: a single vision encoder feeds two lightweight adapters producing continuous embeddings for understanding and discrete tokens for generation.  \n  \nThe unified LLM decoder accepts text tokens and/or image embeddings and auto-regressively predicts the next token; a diffusion image decoder turns predicted tokens into pixels.  \n  \nThree-stage training (pre-training ‚Üí continued pre-training ‚Üí SFT) on mixed text/vision data; the embedding table is extended with a 64K image-token codebook aligned by finite scalar quantization.  \n  \n**‚ú® What Makes It Distinct**  \n  \nHybrid tokenizer, single encoder: understanding and generation tokens come from one encoder in a shared semantic space (no dual-tokenizer conflict).  \n  \nDecoupled roles: the LLM decoder handles high-level semantics; the diffusion decoder handles pixel fidelity ‚Äî letting each scale independently.  \n  \nExplicit scaling: LLM decoder scaled from 300M‚Üí30B params with steady gains; diffusion decoder scaled for stronger structure in human evals.  \n  \n**üìå Why It Matters**  \n  \nOne model for ‚Äúsee + draw‚Äù ‚Üí simpler architecture, better language‚Äìvision alignment, easier product integration.  \n  \nShared encoder + decoupled renderer ‚Üí a practical path to scale without sacrificing understanding (a weak point for earlier unified models).  \n  \nIf these results generalize, future assistants that read, reason, edit &amp; generate in one loop could become the new default for multimodal work.",
    "author": "RIPT1D3_Z",
    "timestamp": "2025-09-24T12:31:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1nplmr8/apple_research_debuts_manzano_a_unified/",
    "score": 56,
    "num_comments": 8,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nqosof",
    "title": "[R] Is there any research on using LLMs as Loss Functions?",
    "content": "Let‚Äôs say you were training a generative model for a task like summarization or answering questions. Would it be possible to feed that output into an LLM and ask it to assess the model‚Äôs effectiveness at performing the task and then maybe feed that output into a sentiment analysis model to obtain a score for how well the model did and have the model attempt to maximize that score?",
    "author": "Suspicious_State_318",
    "timestamp": "2025-09-25T18:41:29",
    "url": "https://reddit.com/r/MachineLearning/comments/1nqosof/r_is_there_any_research_on_using_llms_as_loss/",
    "score": 0,
    "num_comments": 20,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nph2lo",
    "title": "[R] Tabular Deep Learning: Survey of Challenges, Architectures, and Open Questions",
    "content": "Hey folks,\n\nOver the past few years, I‚Äôve been working on **tabular deep learning**, especially neural networks applied to healthcare data (expression, clinical trials, genomics, etc.). Based on that experience and my research, I put together and recently revised a **survey on deep learning for tabular data** (covering MLPs, transformers, graph-based approaches, ensembles, and more).\n\nThe goal is to give an overview of the challenges, recent architectures, and open questions. Hopefully, it‚Äôs useful for anyone working with structured/tabular datasets.\n\nüìÑ PDF: [preprint link](https://www.techrxiv.org/doi/full/10.36227/techrxiv.175753732.26052568)  \nüíª associated repository: [GitHub repository](https://github.com/SalvatoreRa/tabular-deep-learning-survey)\n\nIf you spot errors, think of papers I should include, or have suggestions, send me a message or open an issue in the GitHub. I‚Äôll gladly acknowledge them in future revisions (which I am already planning).\n\nAlso curious: what deep learning models have you found promising on tabular data? Any community favorites?",
    "author": "NoIdeaAbaout",
    "timestamp": "2025-09-24T09:38:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1nph2lo/r_tabular_deep_learning_survey_of_challenges/",
    "score": 34,
    "num_comments": 25,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1npibp8",
    "title": "[R] Area there better ways to balance loss weights?",
    "content": "I'm currently developing a multitask model. Training it requires using multiple losses and manually adjusting their weights. I'm wondering if there are better solutions to automatically balance these loss coefficients. \n\nI already found that there is a method named AWL in GitHub, but I wonder if there are other kinds of methods.",
    "author": "Kwangryeol",
    "timestamp": "2025-09-24T10:25:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1npibp8/r_area_there_better_ways_to_balance_loss_weights/",
    "score": 13,
    "num_comments": 4,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1np4q19",
    "title": "[D] NeurIPS should start a journal track.",
    "content": "The title basically. This year we saw that a lot of papers got rejected even _after_ being accepted, if we actually sum up the impact of these papers through compute, grants, reviewer effort, author effort, it's simply enormous and should not be wasted. Especially if it went through such rigorous review anyways, the research would definitely be worthwhile to the community. I think this is a simple solution, what do you guys think?",
    "author": "simple-Flat0263",
    "timestamp": "2025-09-23T23:14:08",
    "url": "https://reddit.com/r/MachineLearning/comments/1np4q19/d_neurips_should_start_a_journal_track/",
    "score": 92,
    "num_comments": 59,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nomagf",
    "title": "[D]: How do you actually land a research scientist intern role at a top lab/company?!",
    "content": "I‚Äôve been wondering about this for a while and would love some perspective. I‚Äôm a PhD student with publications in top-tier venues (ECCV, NeurIPS, ICCV, AAAI, ICASSP), and I like to believe my research profile is solid? But when it comes to securing a research scientist internship at a big company (FAANG, top labs, etc.), I feel like I‚Äôm missing some piece of the puzzle.\n\nIs there some hidden strategy beyond just applying online? Do these roles mostly happen through networking, advisor connections, or referrals? Or is it about aligning your work super closely with the team‚Äôs current projects?\n\nI‚Äôm genuinely confused. If anyone has gone through the process or has tips on what recruiters/hiring managers actually look for, I‚Äôd really appreciate hearing your advice or dm if you wanna discuss hahahaha",
    "author": "ParticularWork8424",
    "timestamp": "2025-09-23T09:36:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1nomagf/d_how_do_you_actually_land_a_research_scientist/",
    "score": 189,
    "num_comments": 51,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nomop4",
    "title": "[D] What‚Äôs your tech stack as researchers?",
    "content": "Curious what your workflow looks like as scientists/researchers (tools, tech, general practices)?\n\nI feel like most of us end up focusing on the science itself and unintentionally deprioritize the research workflow. I believe sharing experiences could be extremely useful, so here are two from me to kick things off:\n\n\nRole: AI Researcher (time-series, tabular)\nCompany: Mid-sized, healthcare \nWorkflow: All the data sits in an in-house db, and most of the research work is done using jupyter and pycharm/cursor.\nWe use MLFlow for experiment tracking.\nResources are allocated using run.ai (similiar to colab).\nOur workflow is generally something like: exporting the desired data from production db to s3, and research whatever. Once we have a production ready model, we work with the data engineers towards deployment (e.g ETLs, model API). Eventually, model outputs are saved in the production db and can be used whenever.\n  \n\nRole: Phd student\nCompany: Academia research lab\nWorkflow: Nothing concrete really, you get access to resources using a slurm server, other than that you pretty much on your own.\nPretty straightforward python scripts were used to download and preprocess the data, the processed data was spilled directly into disk.\nA pretty messy pytorch code and several local MLFlow repos.\n\n\nThere‚Äôre still many components that I find myself implement from scratch each time, like EDA, error analysis, production monitoring (model performance/data shifts). Usually it is pretty straightforward stuff which takes a lot of time and it feels far from ideal.\n\nWhat are your experiences?",
    "author": "Entrepreneur7962",
    "timestamp": "2025-09-23T09:50:56",
    "url": "https://reddit.com/r/MachineLearning/comments/1nomop4/d_whats_your_tech_stack_as_researchers/",
    "score": 53,
    "num_comments": 21,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1noo2rz",
    "title": "[R] PhD in Physics, now in industry. How do I get back into GenAI research?",
    "content": "Hello Reddit,\n\nI'm a PhD physicist with an academic background in computational methods and couple years of experience applying them in a commercial R&amp;D setting. My current work focuses on using Flow Matching and Diffusion Models for physics simulations, which is a fascinating area itself.\n\nThe challenge I'm facing is that my current role is heavily focused on code development and deploying of existing models, with little opportunity for original, in-depth research. I have a number of research ideas related to GenAI Diffusion/Flow-based models across different modalities, but my company's priorities are focused on rapid deployment, not fundamental research.\n\nI'm looking to transition into a more research-oriented role where I can experiment, study, and pursue these and some else's ideas. I'm open to both academic and industrial opportunities.\n\nMy question to the community is:\n\n* What grants, universities, or research institutions could I pursuit?\n* Do you know of any specific labs, orgs or companies known for their work on Flow Matching/Diffusion models for scientific or physical applications with a research agenda?\n* For those who have made a similar transition from (say industry) to a more research-focused industry role, what advice do you have? Are there specific resources or networks I should tap into?\n\nAny advice or leads would be greatly appreciated. Thank you!",
    "author": "himurabatto",
    "timestamp": "2025-09-23T10:43:27",
    "url": "https://reddit.com/r/MachineLearning/comments/1noo2rz/r_phd_in_physics_now_in_industry_how_do_i_get/",
    "score": 33,
    "num_comments": 4,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1noi58v",
    "title": "[D] What are some good alternatives to Monte Carlo Droupout that you've come across?",
    "content": "I'm looking at different methods for uncertainty estimation/quantification in deep/graph neural networks and originally i came across MC dropout. However, based on some threads in this subreddit, I've come to the conclusion that it's likely not considered a good estimate, and that it isn't exactly Bayesian either. \n\nThat leads me to the question in the title. If you're not working with something inherently probabilistic such as a Gaussian Process, how do you meaningfully get uncertainty estimates? Have you come across anything during your reading/research? What makes the methods stand out, especially in comparison to a quick estimate like MCD? ",
    "author": "anxiousnessgalore",
    "timestamp": "2025-09-23T06:58:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1noi58v/d_what_are_some_good_alternatives_to_monte_carlo/",
    "score": 20,
    "num_comments": 23,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1np483r",
    "title": "[D] Training smaller LLM for Agentic tasks.",
    "content": "So I have a specific use case, in which Deepseek-v3.1 works well, but it's simply too big and takes time to load on our GPU (everything runs locally in my organization, we have¬†**16 H100 GPUs**¬†and maybe about¬†**8 more A100s**) .I use Ollama since I can‚Äôt keep VLLM loaded across all GPUs without hogging resources that others need.\n\nWhat I want is a¬†**smaller model**¬†that I can use for an¬†**agentic task**¬†mainly to work with a set of custom MCP tools I‚Äôve built.\n\nThe biggest reason I want to build a model of my own is because I can get one hell of an education in the process, and since the hardware is already in-house (and mostly idle), I figured this is the perfect opportunity.\n\nBut I‚Äôm not sure where to start:\n\n1. Should I train a model from scratch, or take an existing pretrained model and fine-tune?\n2. What base architecture would be a good starting point for agent-style tasks?\n\nIf anyone can point me toward resources specifically focused on¬†**training or finetuning models for agentic tasks**, I‚Äôd really appreciate it.\n\nP.S: I am currently using full precision deepseek-v3.1 (671B). I am thinking of a model which is about the size of gpt oss.",
    "author": "LifeguardNew6929",
    "timestamp": "2025-09-23T22:44:12",
    "url": "https://reddit.com/r/MachineLearning/comments/1np483r/d_training_smaller_llm_for_agentic_tasks/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nok8yy",
    "title": "[P] SyGra: Graph-oriented framework for reproducible synthetic data pipelines (SFT, DPO, agents, multimodal)",
    "content": "**TL;DR.** We open-sourced **SyGra**, a graph-oriented framework for building *reproducible* synthetic data pipelines. Pipelines are defined as graphs (nodes = LLM calls/transforms/samplers; edges = conditional/parallel/loops). Two modes: YAML + CLI or Python library. Integrates with vLLM, HF TGI, Azure OpenAI, Ollama; HF-native I/O (streaming), provenance, schema-aware outputs.\n\n**Motivation.** High-quality LLM datasets are scarce, costly, and often sensitive; teams also need fine-grained control over task structure (SFT/DPO, tool use, multi-agent, multimodal). In practice, scaling ‚Äúnotebook pipelines‚Äù breaks down: you end up hand-wiring branching/looping flows, juggling multiple inference backends/APIs, and doing ad-hoc validation/schema checks‚Äîwithout resumability, sharding, or streaming. We wanted a **unified, reusable graph abstraction** that captures how data work actually happens (nodes/edges, subgraphs), automates **quality tagging** (heuristics + LLM-based scoring), and emits **schema-conformant, OASST-style** records‚Äîso teams can reproduce, audit, and evolve pipelines instead of rewriting glue code.\n\n**Design.**\n\n* **Graph model:** reusable subgraphs, branching, loops; deterministic configs\n* **Execution:** pluggable model clients (vLLM/TGI/Azure/Ollama), Triton-compatible\n* **Data I/O:** Hugging Face datasets (streaming), local files; schema &amp; metadata tracking\n* **Reproducibility:** explicit configs, seeds, artifact paths; CLI runs are fully logged\n\n**Use cases.** Bootstrapping SFT/DPO datasets; agent simulation &amp; tool-use evals; multimodal assembly (image‚ÜíQ&amp;A, audio‚Üítext) etc.\n\n**Links:**\n\n* Code (Apache-2.0) &amp; README: [github.com/ServiceNow/SyGra](http://github.com/ServiceNow/SyGra)\n* Paper (design rationale, examples): [arxiv.org/abs/2508.15432](http://arxiv.org/abs/2508.15432)\n* PyPI: [pypi.org/project/sygra/](http://pypi.org/project/sygra/)\n\n**Disclosure.** I‚Äôm part of the team. Feedback, issues, and PRs welcome.",
    "author": "zephyrzilla",
    "timestamp": "2025-09-23T08:19:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1nok8yy/p_sygra_graphoriented_framework_for_reproducible/",
    "score": 9,
    "num_comments": 2,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1no4a1m",
    "title": "NVIDIA $100B OpenAI investment [D]",
    "content": "Do you guys think this is even a good investment at this point? I feel like OpenAI is so inflated and also feel like the math of all these recent AI fundraises doesn‚Äôt even make sense anymore. I feel like the bubble is close to popping.",
    "author": "gpu_mamba",
    "timestamp": "2025-09-22T18:18:06",
    "url": "https://reddit.com/r/MachineLearning/comments/1no4a1m/nvidia_100b_openai_investment_d/",
    "score": 36,
    "num_comments": 19,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nop33m",
    "title": "[R] Keeping AI usage (cost control) sustainable and compliant (governance)?",
    "content": "Wondering what approaches teams are taking to keep usage manageable, not just in terms of cost, but also in governance. Have you found frameworks that enforce guardrails across both spend and compliance?",
    "author": "nordic_lion",
    "timestamp": "2025-09-23T11:21:01",
    "url": "https://reddit.com/r/MachineLearning/comments/1nop33m/r_keeping_ai_usage_cost_control_sustainable_and/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nodetf",
    "title": "[R] EMNLP Industry 2025 decisions",
    "content": "Thread to discuss EMNLP Industry Track decisions",
    "author": "Skarwild",
    "timestamp": "2025-09-23T03:09:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1nodetf/r_emnlp_industry_2025_decisions/",
    "score": 4,
    "num_comments": 17,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nou3dh",
    "title": "[P] Predicting Mobile Phone Price Ranges Using ML ‚Äì Random Forest Achieved 92% Accuracy",
    "content": "Hey folks,\n\nI built a **mobile price classification model** using a Kaggle dataset. The task was to predict whether a phone is low, mid, high, or premium priced based on specs like RAM, battery, and internal memory.\n\n**Quick Approach:**\n\n* Python + Scikit-Learn\n* Models tried: Random Forest, XGBoost, Logistic Regression\n* Feature analysis &amp; preprocessing\n\n**Results:**\n\n* **Random Forest:** 92% accuracy\n* Top features: RAM, battery power, internal memory\n\n**Takeaways:**\n\n* Ensemble methods outperform single models on structured datasets\n* Feature importance visualization helps interpret model decisions\n\nCheck out the notebook here: [https://www.kaggle.com/code/abhishekjaiswal4896/mobile-price-prediction-model](https://www.kaggle.com/code/abhishekjaiswal4896/mobile-price-prediction-model)\n\n**Question:** If you were improving this model, what additional features or ML techniques would you try?",
    "author": "abhishek_4896",
    "timestamp": "2025-09-23T14:32:32",
    "url": "https://reddit.com/r/MachineLearning/comments/1nou3dh/p_predicting_mobile_phone_price_ranges_using_ml/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nnhkz8",
    "title": "[D] Is it reasonable that reviewers aren‚Äôt required to read the appendix?",
    "content": "I‚Äôve noticed that many recent conference author guidelines explicitly say something like: *reviewers are not required to read the appendix.*\n\nTo me, that effectively gives reviewers the right to ignore material that‚Äôs already provided there‚Äîeven if it directly addresses their concerns.\n\nIn a past review of mine, a reviewer gave a low initial score and negative feedback without consulting the appendix. I flagged this to the AC (including a confidential comment), but the AC essentially said this wasn‚Äôt mandatory and couldn‚Äôt be used to ‚Äúcorrect‚Äù the reviewer‚Äôs action. The final decision went through without considering the appendix.\n\nI‚Äôm curious how others see this guideline:\n\n* Is it reasonable?\n* Does it create perverse incentives for authors (e.g., to cram everything into the main text only)?\n* Or is it a necessary boundary given reviewer workload?\n\nWould appreciate perspectives‚Äîfrom authors, reviewers, and ACs‚Äîon whether this policy helps or harms review quality.",
    "author": "Secondhanded_PhD",
    "timestamp": "2025-09-22T02:12:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1nnhkz8/d_is_it_reasonable_that_reviewers_arent_required/",
    "score": 41,
    "num_comments": 28,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1noi5hr",
    "title": "[D] Do we overestimate the need for custom models?",
    "content": "I keep noticing that in practice, many problems don‚Äôt actually require training a new model. Pretrained models (Hugging Face, OpenAI, etc.) often get you most of the way there, and the real work is in data prep, deployment, and monitoring.\n\nYet, I still see teams sinking months into custom architectures when a good baseline would have been enough.\n\nDo you think we (as a field) over-engineer solutions instead of focusing on what actually ships?",
    "author": "ExtentBroad3006",
    "timestamp": "2025-09-23T06:59:02",
    "url": "https://reddit.com/r/MachineLearning/comments/1noi5hr/d_do_we_overestimate_the_need_for_custom_models/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.3,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nni5ld",
    "title": "[D] Best practice for providing code during review",
    "content": "I wonder, now for ICLR, we want to release the code, and we definitely will do (we always have done in the past). But for the submission, what would be the best practice?\n\nYou can upload some code as supplementary material. That has the same deadline as the main paper, and we are currently polishing the paper, and probably won't really have the time to clean up the code until that time. In the code, there is also a lot more than in the paper, lots of other ideas that we have tried but did not report, also potential interesting follow-up ideas that we don't want to publish now.\n\nI saw in some other papers, that they provide a link to an anonymized repo (via¬†https://anonymous.4open.science/). That gives us some more time to maybe also clean up the code further after the submission deadline, as I think we can still update that (right?). So this seems to be a better option?\n\nOr we can just make a statement that we will release the code when it is accepted. So then the reviewers cannot check it right now.\n\nAlso, the code makes use of multiple frameworks which are (mostly) only used by our research group (even though they are public, and could be used by anyone), so it is pretty obvious from whom this work is. Does that already count as violation of the double-anonymous submission rule?\n\nSo, what would be the best thing to do?",
    "author": "albertzeyer",
    "timestamp": "2025-09-22T02:49:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1nni5ld/d_best_practice_for_providing_code_during_review/",
    "score": 15,
    "num_comments": 14,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nnnuwc",
    "title": "[D] How do you handle provenance for data?",
    "content": "(Previously asked on r/mlquestions, but not much traction)    \n\nI have a Python package I'm using that appends to a sidecar (json) file for each data file that I process, one entry for each step. This gives me an audit trail of where the file originated, and what operations were performed on it before being used to train a model, etc.      \nI'm just wondering if I am reinventing the wheel? If you track provenance, how much data you include (git short hash, package versions, etc.)?      \nI currently use dvc and mlflow for experiment tracking. It sometimes seems cumbersome to create/update a dvc.yaml for everything (but maybe that's what I need to do).     \nI did find a couple of provenance packages on GitHub, but the ones I found hadn't been updated in years.    ",
    "author": "aqjo",
    "timestamp": "2025-09-22T07:23:07",
    "url": "https://reddit.com/r/MachineLearning/comments/1nnnuwc/d_how_do_you_handle_provenance_for_data/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1no9pun",
    "title": "[D] NeurIPS 2025 : How can we submit the camera-ready version to OpenReview for NeurIPS 2025? I don‚Äôt see any submit button ‚Äî could you let me know how to proceed?",
    "content": "# How can we submit the camera-ready version to OpenReview for NeurIPS 2025? I don‚Äôt see any submit button ‚Äî could you let me know how to proceed?",
    "author": "Dear_Fan_6161",
    "timestamp": "2025-09-22T23:07:07",
    "url": "https://reddit.com/r/MachineLearning/comments/1no9pun/d_neurips_2025_how_can_we_submit_the_cameraready/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nn56yu",
    "title": "[D] Is non-DL related research a poor fit for ICLR?",
    "content": "I was one of the lucky people rejected from NEURIPS with 6444 scores but cranky AC, so looking to resubmit now. Since it got good reviews at NEURIPS, I'm considering submitting to ICLR incorporating suggested changes.\n\nHowever, my paper proposes a linear dimensionality reduction technique, based on information geometry. It is my understanding that ICLR is very focused on neural networks and Deep Learning, so I am worried that my paper is not a good fit, so also considering AISTATS.\n\nIs a novel linear dimensionality reduction technique too out of scope for ICLR? I am an outsider to the field, so would very much appreciate opinions.",
    "author": "dherrera1911",
    "timestamp": "2025-09-21T15:08:57",
    "url": "https://reddit.com/r/MachineLearning/comments/1nn56yu/d_is_nondl_related_research_a_poor_fit_for_iclr/",
    "score": 44,
    "num_comments": 15,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nnh6gi",
    "title": "[D] Mixture of Attention?",
    "content": " considering a new transformer architecture (for protein/DNA models but feel free to weight in from a language perspective) and I‚Äôd love some input before I do any experimenting (low budget this semester)\n\nThe current leading edge of efficient LLMs appear to be mixtures of experts, with a number of quadratic attention layers swapped out for linear layers (IBM granite 4.0, qwen-next for ex).\n\nNVIDIA even has a paper out replacing quadratic attention with linear layers on pre-trained models (https://arxiv.org/abs/2508.15884 ).\n\nSo I wonder if it would be feasible to freeze a model after pre-training (all attention quadratic), one by one training a linear substitute for each quadratic layer.\n\nThen either based on external rules (context length, compute constraint) decide when and how many layers are flicked to linear. Or, train a router with an objective to maximize response quality, keeping generation speed up, while minimizing cost.\n\nEither way you‚Äôd have a single model, with fairly coherent tone and knowledge, that based deployment constraints (speed requirements, memory/compute limits) can be adjusted to be more, or less, linear on the fly.",
    "author": "Alarming-Ad8154",
    "timestamp": "2025-09-22T01:45:52",
    "url": "https://reddit.com/r/MachineLearning/comments/1nnh6gi/d_mixture_of_attention/",
    "score": 6,
    "num_comments": 7,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nnlh1w",
    "title": "[D] Accessing datasets for facial detection of genetic disorders?",
    "content": "I‚Äôm looking for a theme for my Master‚Äôs thesis and I came across the idea of using facial analysis to detect genetic disorders (think Down syndrome, Sanfilippo, etc.). The problem is that I haven‚Äôt been able to get access to any major dataset for this, which has been really discouraging.\n\nIf anyone here has worked in this field before ‚Äî how did you manage to get access to the necessary datasets?\n\nI‚Äôm also open to other thesis ideas, but for context:\n\nMy supervisor‚Äôs research area is facial analysis with deep learning\n\nI‚Äôd like the topic to have a medical focus\n\nAny suggestions or experiences would be super helpful!",
    "author": "Own_Application577",
    "timestamp": "2025-09-22T05:45:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1nnlh1w/d_accessing_datasets_for_facial_detection_of/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nngswn",
    "title": "[D] Semantic image synthesis state-of-the-art?",
    "content": "Hi everyone. I've never done this, so decided to post.\n\nI'm looking to create black-and-white images of satellite photos of rivers, from skeletons of river images. Basically I have a dataset where I have \\[satellite\\_river\\_photo, skeleton\\_segmentation\\] pairs, and I want to train a generator to do skeleton-&gt;satellite generations from new unseen skeletons. Having an extra conditioning variable would also be of interest, but not necessarily at the beginning.\n\nSince most of the literature in this area is over 6 years old, I wanted to post and see if anyone in this community has done something similar lately and would be able to provide some guidance and what methods would be the best to start with or what papers to look at. Thanks.",
    "author": "Big-Coyote-1785",
    "timestamp": "2025-09-22T01:20:44",
    "url": "https://reddit.com/r/MachineLearning/comments/1nngswn/d_semantic_image_synthesis_stateoftheart/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1no85fb",
    "title": "[R] t-2 days to ICLR deadline, less than 20% done",
    "content": "Draft less than 20% done. Barely completed experiments. All of theory still remaining. Co-authors don‚Äôt even know what the project is about save for the abstract. BUT WE‚ÄôRE GETTING THIS OVER THE LINE BOIZ!\n\nI‚ÄôM NOT FREKIN LEAVING!",
    "author": "confirm-jannati",
    "timestamp": "2025-09-22T21:34:37",
    "url": "https://reddit.com/r/MachineLearning/comments/1no85fb/r_t2_days_to_iclr_deadline_less_than_20_done/",
    "score": 0,
    "num_comments": 14,
    "upvote_ratio": 0.28,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nnlas0",
    "title": "[D] Implement Mamba from scratch or use the official github repo?",
    "content": "Hello. I am looking to use Mamba for a code decoding task for my research. Should I just clone the repo and work on it or implement mamba from scratch? I read in the paper that it utilizes different sections of memory of GPU and if I implement it from scratch, I probably need to do that as well and I am not an expert in GPU programming. But still, I'd desire some level of flexibility. What could be the good option here?",
    "author": "Express_Proposal8704",
    "timestamp": "2025-09-22T05:37:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1nnlas0/d_implement_mamba_from_scratch_or_use_the/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.53,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nnig4r",
    "title": "[D] experiment analysis workflow with wandb or mlflow",
    "content": "\ndoes any one have any good workflow for analysing experiments?\n\neg the basic run a bunch of experiments, choose the best run is straightforward.\n\n\nbut typically you want to compare multiple runs\n\n# using multiple runs in analysis\n\neg how does the validation error reduce as i increase the number of hidden nodes.\n\nwhat is the relative reduction in the error? and compared to experiment variability?\n\nwhat changed between the selected runs?\n\n# extrapolating validation error\n\ni am running multiple runs, how do i extrapolate the asymptotic error (so eg i can compare runs that eg were stopped earlier, used a different learning rate)\n\n......\n\ni can download the data, but it feels like i am reinventing the wheel\n\n\neg in mlflow i download runs then have to download a separate table of metrics by iteration/epoch....\n\nthen can create a function to identify hyperparams and summarise differences from base run (ignoring eg timestamps)...\n\ntagging and notes could be helpful, but its not clear the best way to use them\n\n\ni am currently working with wandb. \n\n",
    "author": "seanv507",
    "timestamp": "2025-09-22T03:06:33",
    "url": "https://reddit.com/r/MachineLearning/comments/1nnig4r/d_experiment_analysis_workflow_with_wandb_or/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.44,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nn5x9t",
    "title": "[P] SDLArch-RL: Multi-Console Gaming Environment for Reinforcement Learning Research",
    "content": "Hey r/MachineLearning! I've been working on addressing a persistent pain point in RL gaming research - the setup complexity and limited scope of training environments.\n\n**SDLArch-RL** is a unified RL environment that integrates multiple console emulators (N64, PS2, Dreamcast, GameCube) with standard ML frameworks. Key technical features:\n\n* **Gymnasium-compliant interface** \\- drop-in replacement for existing workflows\n* **Stable-Baselines3 integration** \\- works out-of-the-box with PPO, SAC, TD3, etc.\n* **Efficient state management** \\- leverages native emulator save states for fast episode resets\n* **Configurable observation spaces** \\- raw pixels, processed features, or memory states\n* **Action space mapping** \\- handles complex controller inputs to discrete/continuous actions\n\nCurrently supports 4 emulator backends with plans for modern console integration (PS3, Xbox 360, Wii U). The environment abstracts away emulator-specific APIs while preserving access to low-level features when needed.\n\n**Technical implementation highlights:**\n\n* SDL-based architecture for minimal overhead\n* Memory mapping support for game-specific feature extraction\n* Reproducible training through deterministic save state handling\n* Multi-game training capabilities within single environment instance\n\nThis opens up training on thousands of diverse games vs. the typical handful of custom environments. Particularly useful for transfer learning studies, multi-task RL, and curriculum learning research.\n\nHappy to discuss technical details or answer implementation questions. Thoughts on potential research applications?\n\nGit: [https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)",
    "author": "AgeOfEmpires4AOE4",
    "timestamp": "2025-09-21T15:40:58",
    "url": "https://reddit.com/r/MachineLearning/comments/1nn5x9t/p_sdlarchrl_multiconsole_gaming_environment_for/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nmu1ad",
    "title": "[D] Missing AAAI Reviews",
    "content": "Apologies in advance if I‚Äôve missed something in conference comms so far, but I can‚Äôt seem to see the reviews I‚Äôd received on my (rejected) AAAI submission anymore. I was able to view them the other day, but when I just went to reflect on them to help with our next revision, they were gone!\n\nDoes anyone know anything about this? Is it related to the Phase 2 review round starting?",
    "author": "dreamykidd",
    "timestamp": "2025-09-21T07:53:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1nmu1ad/d_missing_aaai_reviews/",
    "score": 10,
    "num_comments": 12,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nmb8as",
    "title": "[D] NeurIPS: rejecting papers from sanctioned affiliations mid-process",
    "content": "I know multiple people and multiple papers who have received this.\n\nIt is probably legally correct. There are legit grounds for these bans.\n\nHowever, I don't think it is okay to do it AFTER reviewing and even accepting the papers. Hundreds of people wasted their time for nothing.\n\nThere was a recent post with messages to SAC about venue constraints, and this might be a way the organizers are solving this problem.",
    "author": "YallenGusev",
    "timestamp": "2025-09-20T15:16:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1nmb8as/d_neurips_rejecting_papers_from_sanctioned/",
    "score": 142,
    "num_comments": 61,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nn1ig8",
    "title": "[D] Strategies for Routing LLMs",
    "content": "",
    "author": "ApartmentEither4838",
    "timestamp": "2025-09-21T12:40:57",
    "url": "https://reddit.com/r/MachineLearning/comments/1nn1ig8/d_strategies_for_routing_llms/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nlvw1r",
    "title": "[R] MiniGrid DoorKeys Benchmark Active Inference",
    "content": "I am working on an Active Inference Framework since some time and it has managed to constantly and reproducable perform (I guess) very well on MG-DK without any benchmaxing or training.. the numbers (average) are:\n\n8x8: &lt;19 Steps for SR 1 16x16: &lt;60 Steps for SR 1\n\nDo you know someone or a company or so who might be interested in learning more about this solution or the research involved?\n\nThank you!\n\nBest Thom",
    "author": "thomheinrich",
    "timestamp": "2025-09-20T04:37:02",
    "url": "https://reddit.com/r/MachineLearning/comments/1nlvw1r/r_minigrid_doorkeys_benchmark_active_inference/",
    "score": 9,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nlnf5g",
    "title": "[D] AAAI 2026 Phase 2 Review",
    "content": "Hi all,\n\nI‚Äôm serving as a reviewer for AAAI ‚Äô26. Has anyone received additional papers for the Phase 2 review yet? The website indicates that Phase 2 starts on Sep. 16, but I haven‚Äôt been assigned any papers so far.\n\n[https://docs.google.com/document/u/0/d/1tqQGwtNUlALPSTqoTo5uTFx8vKuqpILNTne9jeBCOVI/mobilebasic](https://docs.google.com/document/u/0/d/1tqQGwtNUlALPSTqoTo5uTFx8vKuqpILNTne9jeBCOVI/mobilebasic)\n\n\n\nEdit (Sep. 21): Just got assigned three extra papers!",
    "author": "snu95",
    "timestamp": "2025-09-19T20:15:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1nlnf5g/d_aaai_2026_phase_2_review/",
    "score": 22,
    "num_comments": 11,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nlvvi1",
    "title": "[P]  Video prediction pipeline using a frozen VAE and hierarchical LSTMs to learn latent dynamics",
    "content": "I wanted to share a personal project I've been working on for the past few months and get some feedback from the community. My goal was to build a stable, interactive system for video prediction by cleanly separating the perception and dynamics modeling. \n\n**The Core Architecture**\n\nThe pipeline processes a live camera feed. The main idea is to avoid expensive end-to-end training and create a more modular system.\n\n* **Frozen VAE (Perception):** I'm using the pre-trained Stable Diffusion VAE to encode frames into a latent space. By keeping it frozen, the \"perceptual manifold\" is stable, which makes learning the dynamics much easier.\n* **Three-Stage LSTM System (Dynamics):** This is where I tried to do something a bit different. Instead of one big LSTM, I'm using a hierarchy:\n   * A **Pattern LSTM** observes short sequences of latents to find basic temporal patterns.\n   * A **Compression LSTM** takes these patterns and learns a dense, compressed representation.\n   * A **Central LSTM** takes this compressed state and predicts the next latent step (Œîz).\n\n**\\*NOTE:** This pipeline is capable of ALOT more than just a simple prediction model. For this project I solely focused on the vision aspect. \n\n**Performance and Results**\n\nThe whole system runs at an interactive 4-6 FPS on my consumer hardware and has a simple PyQT GUI to show the live camera feed next to the model's prediction. With better hardware i'm hoping to hit 24 FPS, but balling on a budget right now.\n\nMy main focus was on perceptual quality over raw pixel accuracy. The most encouraging result was in multi-step open-loop rollouts, where the model achieved a **peak SSIM of 0.84**. I was really happy to see this, as it's a result that's competitive with some established benchmarks on standardized datasets (like KTH).\n\n**Link to Project:**\n\nI've documented the architecture, included the performance logs, and wrote a white paper in the GitHub repo if you want to see the technical details:\n\n[github](https://github.com/A1CST/VISION_VAE_OLM_3L_PCC_PREDICTION)",
    "author": "AsyncVibes",
    "timestamp": "2025-09-20T04:36:09",
    "url": "https://reddit.com/r/MachineLearning/comments/1nlvvi1/p_video_prediction_pipeline_using_a_frozen_vae/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nlblaw",
    "title": "[D] Neurips Position Paper Decisions",
    "content": "The decisions will be out next week.  \nI am personally not a fan of how the entire process was conducted. Hoping the best for everyone! Please use this as a thread to discuss how you felt about the process. Fingers crossed!",
    "author": "HelicopterFriendly96",
    "timestamp": "2025-09-19T11:33:57",
    "url": "https://reddit.com/r/MachineLearning/comments/1nlblaw/d_neurips_position_paper_decisions/",
    "score": 23,
    "num_comments": 19,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nlfcpq",
    "title": "[P] Building sub-100ms autocompletion for JetBrains IDEs",
    "content": "",
    "author": "Kevinlu1248",
    "timestamp": "2025-09-19T14:00:53",
    "url": "https://reddit.com/r/MachineLearning/comments/1nlfcpq/p_building_sub100ms_autocompletion_for_jetbrains/",
    "score": 12,
    "num_comments": 2,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nliq67",
    "title": "[P] Benchmarked EpilepsyBench #1 winner - found 27x performance gap, now training Bi-Mamba-2 fix",
    "content": "Hey all, been learning EEG ML heavily for the past two months or so.\n\nRecently evaluated SeizureTransformer (#1 on¬†[EpilepsyBench¬†](https://epilepsybenchmarks.com/challenge/)with \\~1 FA/24h) on the Temple EEG dataset using clinical NEDC scoring:¬†**26.89 FA/24h**¬†\\- a 27x gap. Same predictions scored three ways produced 8.59 to 136.73 FA/24h depending on methodology alone.\n\n**Evaluation here:**¬†[https://github.com/Clarity-Digital-Twin/SeizureTransformer](https://github.com/Clarity-Digital-Twin/SeizureTransformer)  \n[PDF](https://drive.google.com/file/d/1T-lmGZuWr_0YnB0m692ccgVdSRjs_Y5n/view?usp=sharing): Gdrive\n\nSo I can actually contribute instead of reproducing, I'm now training the first¬†**Bi-Mamba-2 + U-Net + ResCNN**¬†architecture - O(N) complexity while maintaining temporal modeling.\n\n**Training code:**¬†[https://github.com/Clarity-Digital-Twin/brain-go-brr-v2](https://github.com/Clarity-Digital-Twin/brain-go-brr-v2)\n\nWould appreciate feedback on either if there is any interest. Also seeking arXiv endorsement for cs.LG if anyone finds this worth sharing (independent researcher).",
    "author": "VibeCoderMcSwaggins",
    "timestamp": "2025-09-19T16:25:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1nliq67/p_benchmarked_epilepsybench_1_winner_found_27x/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nkxejt",
    "title": "Overcoming accuracy limitations of Analog In-Memory Computing hardware",
    "content": "Our paper titled \"Analog Foundation Models\" from IBM Research and ETH Zurich just got accepted at NeurIPS, and I feel like the broader ML community is not aware of the potential Analog In-Memory Computing (AIMC) has, so I wanted to make a quick advertisement for the paper and the field as a whole.\n\nThe idea of using analog devices for computation in AI is pretty old, but never really took off because of many reasons such as scalability or complexity. However, recently, research labs from Stanford or IBM Research have demonstrated very simple and scalable Analog In-Memory Computing chips that have strong potential to harness the benefits of AIMC \\[1-3\\].\n\n**What's the problem with modern architectures such as GPUs?**  \nIn a conventional computer architecture, you have your memory and your processing unit separated by a bus, over which you send data back and forth. This is extremely power consuming especially in scenarios where you repeatedly need to access \\*a lot of data\\*. This is the case for LLMs: During inference, you need to constantly fetch the weights, KV cache, and activations from DRAM into your local SRAM-based caches, do the computation, and eventually write back the data to DRAM. This is really expensive in terms of power and latency.  \n  \n**Can't we get rid of DRAM (only use SRAM)?**  \nYes we can, and in fact there are some companies that are already doing that (e.g. Cerebras). The downside of this approach is that SRAM has very poor density (and does not scale anymore) and cannot hold billions of weights in a reasonable footprint (you need huge wafers, and many of them).\n\n**How about you just do the computation directly inside a very dense memory itself?**  \nThis is the idea of AIMC: We propose to take the matrix-vector multiplication operation (one of the most prominent ops in NNs) and execute it directly inside non-volatile memory using Ohm's law (multiplication) and Kirchhoff's current law (summation). When combined with a scalable 3D memory technology like 3D NAND Flash and a scalable model architecture like MoEs, this opens up completely new use-cases for AI because you will be able to serve 100B+ models on a single chip with a low power budget (10s of W)\\[4\\].\n\n**What's the catch?**  \nThere is always one...In the case of AIMC, it is the fact that computations are noisy and non-deterministic at runtime. In fact, up to now, no one was sure whether LLMs can be made robust to the noise present in AIMC-based hardware. Our paper \"Analog Foundation Models\" \\[5\\] changes this. We show that we can repeat the pre-training process of already pre-trained foundation models on synthetic data while using hardware-aware training methods to enhance the robustness of these LLMs.\n\nWe show that in terms of accuracy, we can now compete with 4-bit quantized LLMs!\n\nThis is a significant step towards making AIMC a reality and there is still a long way to go, but we're still super excited to have broken this barrier, which is why I wanted to introduce this to the broader ML community here!\n\nDo you want to get an intro to this topic? Then I suggest [this fundamental article](https://www.nature.com/articles/s41565-020-0655-z).\n\nDo you want to chat with me virtually or at NeurIPS? Just DM me!\n\n\\[1\\] [https://www.nature.com/articles/s41586-022-04992-8](https://www.nature.com/articles/s41586-022-04992-8)  \n\\[2\\] [https://www.nature.com/articles/s41586-023-06337-5](https://www.nature.com/articles/s41586-023-06337-5)  \n\\[3\\] [https://www.nature.com/articles/s41928-023-01010-1](https://www.nature.com/articles/s41928-023-01010-1)  \n\\[4\\] [https://www.nature.com/articles/s43588-024-00753-x](https://www.nature.com/articles/s43588-024-00753-x)  \n\\[5\\] [https://arxiv.org/pdf/2505.09663](https://arxiv.org/pdf/2505.09663)",
    "author": "scrapyscrape",
    "timestamp": "2025-09-19T00:49:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1nkxejt/overcoming_accuracy_limitations_of_analog/",
    "score": 31,
    "num_comments": 11,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nkrmzr",
    "title": "[R] NeurIPS rejected paper resubmission",
    "content": "My paper just got rejected (scores: 4, 4, 3, 3). I‚Äôm considering resubmitting it to IEEE SatML. What‚Äôs your opinion on SatML? Would it be better to aim for a journal like IEEE TIFS instead? Any other recommendations? I‚Äôm not really interested in ICLR since I feel it might get rejected there too. Field: AI Security.",
    "author": "Accomplished_Newt923",
    "timestamp": "2025-09-18T19:28:31",
    "url": "https://reddit.com/r/MachineLearning/comments/1nkrmzr/r_neurips_rejected_paper_resubmission/",
    "score": 29,
    "num_comments": 15,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nl144j",
    "title": "[R] Huge data publishing (videos)",
    "content": "I want to publish data (multi modal with images), and they are around 2.5 TB, what are the options to publish it and keep them online with the least cost possible? How can I do it without commiting to pay huge amount of money for the rest of my life? I am a phd student in university but til now it seems that there is no solution for such big data. ",
    "author": "Internal_Seaweed_844",
    "timestamp": "2025-09-19T04:35:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1nl144j/r_huge_data_publishing_videos/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nlc954",
    "title": "Try a Deterministic Global-Optimum Logistics Demo ‚Äì Solve Huge Warehouse-to-Route Problems in Seconds [P]",
    "content": "Hey everyone,\n\nI‚Äôve been building an optimization engine that can compute¬†**deterministically optimal warehouse-to-route assignments**¬†for massive datasets ‚Äì up to¬†**10,000 warehouses √ó 500 routes**¬†‚Äì in seconds. I‚Äôm sharing a live demo!\n\n‚ö†Ô∏è Heads-up: This runs on my personal machine, so requests are queued and wait times may vary.\n\n**How to use:**\n\n1. Upload a CSV or JSON file.\n2. Rows = warehouses, columns = routes.\n3. Each cell = cost of assigning that warehouse to that route.\n\n**Quick CSV example (3 warehouses √ó 4 routes):**\n\n    10,20,30,40\n    15,25,35,45\n    20,30,40,50\n\nüîó¬†**Try it here:**¬†[https://19340a3b2e2b.ngrok-free.app](https://19340a3b2e2b.ngrok-free.app/)\n\nThis is a chance to experiment with a system that produces¬†**true deterministic optima**¬†for large datasets without needing a server cluster. Feedback, testing, or just trying crazy datasets is welcome!\n\n**Open from:**¬†2:30am AWST ‚Üí 12pm AWST\n\n*(I jokingly call it a ‚Äúhypercomputer‚Äù because of the speed, but it‚Äôs just my personal deterministic optimization engine!)*",
    "author": "Active-Midnight-8834",
    "timestamp": "2025-09-19T11:59:23",
    "url": "https://reddit.com/r/MachineLearning/comments/1nlc954/try_a_deterministic_globaloptimum_logistics_demo/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nknjk1",
    "title": "[R] Is Chain-of-Thought Reasoning of LLMs a\nMirage? A Data Distribution Lens",
    "content": "",
    "author": "Confident-Honeydew66",
    "timestamp": "2025-09-18T16:17:21",
    "url": "https://reddit.com/r/MachineLearning/comments/1nknjk1/r_is_chainofthought_reasoning_of_llms_a_mirage_a/",
    "score": 27,
    "num_comments": 15,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nkhqgn",
    "title": "[P] Open dataset: 40M GitHub repositories (2015 ‚Üí mid-2025) ‚Äî rich metadata for ML",
    "content": "Hi!\n\n**TL;DR**: I assembled an open dataset of¬†**40M GitHub repositories**¬†with rich metadata (languages, stars, forks, license, descriptions, issues, size, created\\_at, etc.). It‚Äôs larger and more detailed than the common public snapshots (e.g., BigQuery‚Äôs \\~3M trimmed repos). There‚Äôs also a¬†**1M-repo sample**¬†for quick experiments and a¬†**quickstart notebook**¬†in github repo.\n\n**How it was built:**¬†GH Archive ‚Üí join events ‚Üí extract repo metadata. Snapshot covers¬†**2015 ‚Üí mid-July 2025**.\n\n**What‚Äôs inside**\n\n* **Scale:**¬†40M repos (full snapshot) + 1M sample for fast iteration.\n* **Fields:**¬†language, stars, forks, license, short description, description language, open issues, last PR index at snapshot date, size, created\\_at, and more.\n* **Alive data:**¬†includes gaps and natural inconsistencies‚Äîuseful for realistic ML/DS exercises.\n* **Quickstart:**¬†Jupyter notebook with basic plots.\n\nI linked the dataset and code in comments\n\n**HuggingFace / GitHub:**\n\n`ibragim-bad/github-repos-metadata-40M`\n\nIn my opinion it may be helpful for: students¬†**/**¬†instructors¬†**/**¬†juniors for mini-research projects on visualizations, clustering, feature engineering exercises.\n\nAlso in the comment is an example of how language share in terms of created repos changed over time.\n\nP.S. Feedback is welcome ‚Äì especially ideas for additional fields or derived signals you‚Äôd like to see.",
    "author": "Fabulous_Pollution10",
    "timestamp": "2025-09-18T12:25:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1nkhqgn/p_open_dataset_40m_github_repositories_2015/",
    "score": 59,
    "num_comments": 10,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nkn6dw",
    "title": "[P] Looking for people to learn and build projects with !",
    "content": "Hey guys I‚Äôm a master student in USA. I am looking for people interested to learn machine and deep learning and also possibly looking for people who want to research together. Do dm me if you‚Äôre interested! I would love to network with a lot of you too!\n\nIf you‚Äôre interested in hackathons apart from this feel free to ping regarding that aswell.",
    "author": "Srikar265",
    "timestamp": "2025-09-18T16:01:07",
    "url": "https://reddit.com/r/MachineLearning/comments/1nkn6dw/p_looking_for_people_to_learn_and_build_projects/",
    "score": 14,
    "num_comments": 26,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nkdbin",
    "title": "[P] We built mmore: an open-source multi-GPU/multi-node library for large-scale document parsing",
    "content": "We are a student group from EPFL and we have been working on a tool called mmore, and thought it might be useful to share it here. Maybe the community will find it useful.\n\nYou can think of mmore as something in the spirit of [Docling](https://github.com/docling-project/docling), but designed from the ground up to run natively on multi-GPU and multi-node setups. As the backend OCR for PDFs (and images) we use [Surya](https://github.com/datalab-to/surya), which we‚Äôve found to be both very accurate and fast. For those with limited GPU resources, we also provide a lightweight ‚Äúfast‚Äù mode. It skips OCR (so it cannot process scanned files) but still works well for born-digital documents.\n\nIn a [paper](https://www.arxiv.org/pdf/2509.11937) we released a few months ago, we showed that mmore achieves both speed and accuracy gains over Docling (maybe this has changed by now with the latest Granite-Docling). Right now, it supports a broad range of formats: PDFs, DOCX, PPTX, XLSX, MD, EML (emails), TXT, HTML, as well as videos and audio (MP4, MOV, AVI, MKV, MP3, WAV, AAC).\n\nThe use cases are flexible. For example:\n\n* Unlocking text and image data from previously unprocessed files, enabling larger dataset creation (similar to what Docling + HuggingFace did a few days ago with [finepdfs](https://huggingface.co/datasets/HuggingFaceFW/finepdfs)).\n* Running text or multimodal RAG directly over your own document collections.\n\nWe are sharing this mainly to invite ideas and feedback from the community. If you see opportunities, have suggestions, or even just thoughts on directions we should explore, we‚Äôd love to hear them. Contributions are more than welcome!\n\nGithub: üíªhttps://github.com/swiss-ai/mmore  \nArxiv: üìÑhttps://www.arxiv.org/pdf/2509.11937",
    "author": "Subject_Zucchini_790",
    "timestamp": "2025-09-18T09:40:03",
    "url": "https://reddit.com/r/MachineLearning/comments/1nkdbin/p_we_built_mmore_an_opensource_multigpumultinode/",
    "score": 27,
    "num_comments": 1,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nkz9s9",
    "title": "[R] Looking for Real‚ÄëTime Social Media Data Providers with Geographic Filtering, your finds are Welcome?",
    "content": "I‚Äôm working on a **social listening tool** and need access to **real‚Äëtime (or near real‚Äëtime)** social media datasets. The key requirement is the ability to **filter or segment data by geography** (country, region, or city level).\n\nI‚Äôm particularly interested in:\n\n* Providers with **low latency** between post creation and data availability\n* Coverage across multiple platforms (Twitter/X, Instagram, Reddit, YouTube, etc.)\n* Options for **multilingual content**, especially for non‚ÄëEnglish regions\n* APIs or data streams that are **developer‚Äëfriendly**\n\nIf you‚Äôve worked with any vendors, APIs, or open datasets that fit this, I‚Äôd love to hear your recommendations, along with any notes on **pricing, reliability, and compliance** with platform policies.",
    "author": "To_Iflal",
    "timestamp": "2025-09-19T02:50:55",
    "url": "https://reddit.com/r/MachineLearning/comments/1nkz9s9/r_looking_for_realtime_social_media_data/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nl8ik3",
    "title": "[Project] I created an AI photo organizer that uses Ollama to sort photos, filter duplicates, and write Instagram captions.",
    "content": "Hey everyone at r/MachineLearning,\n\nI wanted to share a Python project I've been working on called the **AI Instagram Organizer**.\n\n**The Problem:** I had thousands of photos from a recent trip, and the thought of manually sorting them, finding the best ones, and thinking of captions was overwhelming. I wanted a way to automate this using local LLMs.\n\n**The Solution:** I built a script that uses a multimodal model via Ollama (like LLaVA, Gemma, or Llama 3.2 Vision) to do all the heavy lifting.\n\n**Key Features:**\n\n* **Chronological Sorting:** It reads EXIF data to organize posts by the date they were taken.\n* **Advanced Duplicate Filtering:** It uses multiple perceptual hashes and a dynamic threshold to remove repetitive shots.\n* **AI Caption &amp; Hashtag Generation:** For each post folder it creates, it writes several descriptive caption options and a list of hashtags.\n* **Handles HEIC Files:** It automatically converts Apple's HEIC format to JPG.\n\nIt‚Äôs been a really fun project and a great way to explore what's possible with local vision models. I'd love to get your feedback and see if it's useful to anyone else!\n\n**GitHub Repo:** [https://github.com/summitsingh/ai-instagram-organizer](https://github.com/summitsingh/ai-instagram-organizer)\n\nSince this is my first time building an open-source AI project, any feedback is welcome. And if you like it, a star on GitHub would really make my day! ‚≠ê",
    "author": "summitsc",
    "timestamp": "2025-09-19T09:37:39",
    "url": "https://reddit.com/r/MachineLearning/comments/1nl8ik3/project_i_created_an_ai_photo_organizer_that_uses/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nkcfgc",
    "title": "First time submitting to a workshop - what exactly to expect? [D]",
    "content": "I just started with my new position and see a good opportunity to submit to a workshop - A tier venue, but feels like the bar is too low. Only aim to get traction to my current work, which I further want to submit to a big conference. The workshop is non-archival.\n\n1. How is conference paper different from workshop? Asked to submit an extended abstract of 3 pages. Is it same like a regular paper but with less details mentioned?\n\n  \n2. Should I put in efforts to get my ablation done? Or keep it simple as it anyway won't help my profile much and focus on bigger picture?",
    "author": "ade17_in",
    "timestamp": "2025-09-18T09:06:56",
    "url": "https://reddit.com/r/MachineLearning/comments/1nkcfgc/first_time_submitting_to_a_workshop_what_exactly/",
    "score": 7,
    "num_comments": 6,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nkge98",
    "title": "[R] Live Sound and Pro Audio in AI/ML",
    "content": "I‚Äôm currently in the middle of a Post Graduate Program for AI/ML at UT Austin and have had a blast learning the fundamentals and theory of how this tech works. I have an 8 year background as a Live Sound Engineer working in concert audio and have currently been researching how ML can Optimize PA placement, SPL measurements, STI ratings for different event applications or installs.\n\nI‚Äôm curious to see if anybody else out there in the world is currently doing research that combines AI/ML with Live Sound and Pro Audio. If so, what are you researching? What type of models are you creating?\n\nJust Curious and would love to connect with others that share the same passion.",
    "author": "Consistent_Sundae540",
    "timestamp": "2025-09-18T11:34:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1nkge98/r_live_sound_and_pro_audio_in_aiml/",
    "score": 6,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nk0txd",
    "title": "[R] Uni-CoT: A Unified CoT Framework that Integrates Text+Image reasoning!",
    "content": "Large Language Models shine at step-by-step reasoning in text, but struggle when tasks require visual changes. Existing methods often produce messy, incoherent results.\n\nWe introduce Uni-CoT, the first unified Chain-of-Thought framework that handles both image understanding + generation to enable coherent visual reasoning \\[as shown in Figure 1\\]. Our model even can supports NanoBanana‚Äìstyle geography reasoning \\[as shown in Figure 2\\]!\n\nSpecifically, we use **one unified architecture** (inspired by Bagel/Omni/Janus) to support multi-modal reasoning. This minimizes discrepancy between reasoning trajectories and visual state transitions, enabling coherent cross-modal reasoning. However, the multi-modal reasoning with unified model raise a large burden on computation and model training.\n\n# To solve it, we propose a hierarchical Macro‚ÄìMicro CoT:\n\n* **Macro-Level CoT** ‚Üí global planning, decomposing a task into subtasks.\n* **Micro-Level CoT** ‚Üí executes subtasks as a **Markov Decision Process (MDP)**, reducing token complexity and improving efficiency.\n\nThis **structured decomposition** shortens reasoning trajectories and lowers cognitive (and computational) load.\n\n# With this desigin, we build a novel training strategy for our Uni-CoT:\n\n* **Macro-level modeling**: refined on interleaved text‚Äìimage sequences for global planning.\n* **Micro-level modeling**: auxiliary tasks (action generation, reward estimation, etc.) to guide efficient learning.\n* **Node-based reinforcement learning** to stabilize optimization across modalities.\n\n# Results:\n\n* Training efficiently only on **8 √ó A100 GPUs**\n* Inference efficiently only on 1 **√ó A100 GPU**\n* Achieves **state-of-the-art performance** on reasoning-driven benchmarks for image generation &amp; editing.\n\n# Resource:\n\nOur paperÔºö[https://arxiv.org/abs/2508.05606](https://arxiv.org/abs/2508.05606)\n\nGithub repo:¬†[https://github.com/Fr0zenCrane/UniCoT](https://github.com/Fr0zenCrane/UniCoT)\n\nProject page:¬†[https://sais-fuxi.github.io/projects/uni-cot/](https://sais-fuxi.github.io/projects/uni-cot/)",
    "author": "GONG_JIA",
    "timestamp": "2025-09-17T23:26:05",
    "url": "https://reddit.com/r/MachineLearning/comments/1nk0txd/r_unicot_a_unified_cot_framework_that_integrates/",
    "score": 44,
    "num_comments": 8,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nkq5nl",
    "title": "[P] SDLArch-RL is now compatible with Flycast (Dreamcast)",
    "content": "https://preview.redd.it/0pprvaqkv0qf1.png?width=1956&amp;format=png&amp;auto=webp&amp;s=4c5a8a9b5e4df5aeb41d7a06fa189b87a3e341f1\n\n\n\nI'm here to share some good news!!!! Our reinforcement learning environment is now Flycast-compatible!!!! Sure, I need to make some adjustments, but it's live!!! And don't forget to like the project to support it!!! See our progress at [https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)",
    "author": "AgeOfEmpires4AOE4",
    "timestamp": "2025-09-18T18:18:54",
    "url": "https://reddit.com/r/MachineLearning/comments/1nkq5nl/p_sdlarchrl_is_now_compatible_with_flycast/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nka2g3",
    "title": "[P] Built a CLI to turn PDFs and docs into fine tuning datasets",
    "content": "Hi everyone,\n\nI have been working on a small CLI that takes local files like pdfs docs or text and turns them into datasets you can use for fine tuning.\n\nRepo:¬†[https://github.com/Datalore-ai/datalore-localgen-cli](https://github.com/Datalore-ai/datalore-localgen-cli)\n\nIt recently crossed 70 stars on GitHub which meant a lot to me. Seeing people try it out and suggest improvements has been really motivating.\n\nThe most requested feature was multi file support. I added that now so you can point it to a folder and it will process everything inside extract the text run semantic search apply your schema or instructions and output a dataset.\n\nAnother request was running fully local with Ollama instead of relying on APIs. I will be adding that soon.\n\nStill early but it is working well so far. If you try it out and have ideas I would love to hear them.",
    "author": "Interesting-Area6418",
    "timestamp": "2025-09-18T07:38:16",
    "url": "https://reddit.com/r/MachineLearning/comments/1nka2g3/p_built_a_cli_to_turn_pdfs_and_docs_into_fine/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1njmxph",
    "title": "[D] How about we review the reviewers?",
    "content": "For AAAI 2026, I think each reviewer has a unique ID. We can collect the complaints against the IDs. Some IDs may have complaints piled up on them.\n\nPerhaps we can compile a list of problematic reviewers and questionable conducts and demand the conference to investigate and set up regulations. Of course, it would be better for the conference to do this itself.\n\nWhat would be a good way to collect the complaints? Would an online survey form be sufficient?",
    "author": "Fit_Analysis_824",
    "timestamp": "2025-09-17T12:34:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1njmxph/d_how_about_we_review_the_reviewers/",
    "score": 91,
    "num_comments": 35,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1njny8k",
    "title": "[N] Both OpenAI and DeepMind are claiming ICPC gold-level performance",
    "content": "* DeepMind solved 10/12 problems: [https://x.com/HengTze/status/1968359525339246825](https://x.com/HengTze/status/1968359525339246825)\n* OpenAI solved 12/12 problems: [https://x.com/MostafaRohani/status/1968360976379703569](https://x.com/MostafaRohani/status/1968360976379703569)\n\n",
    "author": "we_are_mammals",
    "timestamp": "2025-09-17T13:13:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1njny8k/n_both_openai_and_deepmind_are_claiming_icpc/",
    "score": 72,
    "num_comments": 21,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nk1b7o",
    "title": "[D] AAAI 2026: Why did some papers get 3 human reviewers in Phase 1?",
    "content": "Something that I noticed about the papers in my review batch (2 got accepted, 2 got rejected) is that when the Phase 1 rejections came out and we were able to see all the other reviews that the papers got, 3 of those papers received 3 human reviews and 1 paper got 2 human reviews.\n\nFigured there was a shortfall in reviewers? Why'd some papers get 3?",
    "author": "Adventurous-Cut-7077",
    "timestamp": "2025-09-17T23:55:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1nk1b7o/d_aaai_2026_why_did_some_papers_get_3_human/",
    "score": 10,
    "num_comments": 8,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nk0uvm",
    "title": "[D] What is the best part came this year in your opinion and why?",
    "content": "For me it's Dinov3, I think it shows capabilities of self supervised learning is much higher that what we expect and I think next year we will see much more SSL, specially from big tech, since nobody else can train a model for 9 million GPU hours lol",
    "author": "_A_Lost_Cat_",
    "timestamp": "2025-09-17T23:27:44",
    "url": "https://reddit.com/r/MachineLearning/comments/1nk0uvm/d_what_is_the_best_part_came_this_year_in_your/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1njzuje",
    "title": "[D] ICLR Reproducibility statement",
    "content": "After seeing so many aaai papers getting desk rejected due to confusion about whether to put the appendix inside one text pdf or to submit as zip, I wanted to confirm this incase any of you knows ?? how to submit? like is it safe to add it in 10th page?   \n  \n\"It is important that the work published in ICLR is reproducible. Authors are strongly encouraged to include a paragraph-long Reproducibility Statement¬†*at the end of the main text (before references)*¬†to discuss the efforts that have been made to ensure reproducibility. This paragraph should not itself describe details needed for reproducing the results, but rather reference the parts of the main paper, appendix, and supplemental materials that will help with reproducibility. For example, for novel models or algorithms, a link to an anonymous downloadable source code can be submitted as supplementary materials; for theoretical results, clear explanations of any assumptions and a complete proof of the claims can be included in the appendix; for any datasets used in the experiments, a complete description of the data processing steps can be provided in the supplementary materials. Each of the above are examples of things that can be referenced in the reproducibility statement.¬†*This optional reproducibility statement is not part of the main text and therefore will not count toward the page limit.*¬†\"",
    "author": "i_minus",
    "timestamp": "2025-09-17T22:26:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1njzuje/d_iclr_reproducibility_statement/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.53,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nk68sz",
    "title": "[D] Mapping Brand Citations in AI Responses[D] Mapping Brand Citations in AI Responses[D] Mapping Brand Citations in AI Responses",
    "content": "Running an AI SEO pilot to understand how ML-powered LLMs cite brands ‚Äì sharing early insights.\n\n\n\nLast week, I shared an idea about testing how AI platforms (ChatGPT, Claude, Perplexity) cite brands in their answers. The response was incredible ‚Äì founders, marketers, and AI enthusiasts reached out with interest.\n\n\n\n\\*\\*Pilot Overview:\\*\\*\n\n1. Select 5 SaaS or tech companies (CRM, email, project management, analytics, etc.)\n\n2. Run 20+ user-style queries across ChatGPT, Claude, Perplexity\n\n3. Track which platforms cite which companies\n\n4. Rewrite company pages into AI-friendly formats (structured FAQs, schema tables, clear product breakdowns)\n\n5. Re-run queries ‚Äì measure shifts\n\n\n\n\\*\\*Goal:\\*\\* See if structured content can increase AI mentions by 25%+.\n\n\n\nIf you're a founder, marketer, or SEO lead interested in joining this early pilot, please fill out your details here: [https://forms.gle/CKkP75mJC1iDSAd9A](https://forms.gle/CKkP75mJC1iDSAd9A)\n\n\n\nI'll share results openly with the community once we have the first wave of data. Let's build the AI SEO playbook together.",
    "author": "No-Abbreviations7266",
    "timestamp": "2025-09-18T04:56:39",
    "url": "https://reddit.com/r/MachineLearning/comments/1nk68sz/d_mapping_brand_citations_in_ai_responsesd/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1njhikh",
    "title": "[D] can we trust agents for time series forecasting?",
    "content": "over the past few weeks i‚Äôve been experimenting with agents for time series forecasting. that led to TimeCopilot, an open-source framework that combines LLMs with multiple time series foundation models.\n\nthe goal: make forecasting accessible to anyone, in their own language, while lowering barriers to participation.\n\nwhat it does:\n\n\\- run, cross-validate, and detect anomalies across time series foundation models from Google, Salesforce, AWS, DataDog, Nixtla, ServiceNow, NXAI, etc. (it solves the dependency hell of having multiple time series foundation models)\n\n\\- plus statistical, ML, and deep learning baselines, all in a single workflow.\n\n\\- integration with any LLM provider\n\non Salesforce‚Äôs GIFT-Eval benchmark (24 datasets, 144k+ series, 177M points), a TimeCopilot ensemble ranked #1 in probabilistic accuracy (CRPS) and #2 in point accuracy (MASE) among non-leaking models, at \\~$24 GPU cost.\n\ncurious what folks here think about agents in forecasting. and if you find the project interesting, a ‚≠êÔ∏è on GitHub means a lot.\n\n[https://github.com/AzulGarza/timecopilot](https://github.com/AzulGarza/timecopilot)\n\nhttps://preview.redd.it/ak6pwo1c2rpf1.png?width=1648&amp;format=png&amp;auto=webp&amp;s=f28cf5421f3f47a30a78d2dc53a38d07ff481d7b\n\n  \n",
    "author": "fedegarzar",
    "timestamp": "2025-09-17T09:14:25",
    "url": "https://reddit.com/r/MachineLearning/comments/1njhikh/d_can_we_trust_agents_for_time_series_forecasting/",
    "score": 4,
    "num_comments": 9,
    "upvote_ratio": 0.59,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nj38ur",
    "title": "[D] How is IEEE TIP viewed in the CV/AI/ML community?",
    "content": "Hi everyone,\n\nI‚Äôm a PhD student working on video research, and I recently submitted a paper to IEEE Transactions on Image Processing (TIP). After a very long review process (almost a year), it finally reached the ‚ÄúAQ‚Äù stage.\n\nNow I‚Äôm curious‚Äîhow do people in the community actually see TIP these days?\nSome of my colleagues say it‚Äôs still one of the top journals in vision, basically right after TPAMI. Others think it‚Äôs kind of outdated and not really read much anymore.\n\nAlso, how would you compare it to the major conferences (CVPR/ICCV/ECCV, NeurIPS, ICLR, AAAI)? Is publishing in TIP seen as on par with those, or is it considered more like the ‚Äúsecond-tier‚Äù conferences (WACV, BMVC, etc.)?\n\nI‚Äôm close to graduation, so maybe I‚Äôm overthinking this. I know the contribution and philosophy of the work itself matters more than the venue. But I‚Äôd still love to hear how people generally view TIP these days, both in academia and in the field.\n\nThanks!\n",
    "author": "Secondhanded_PhD",
    "timestamp": "2025-09-16T21:10:57",
    "url": "https://reddit.com/r/MachineLearning/comments/1nj38ur/d_how_is_ieee_tip_viewed_in_the_cvaiml_community/",
    "score": 26,
    "num_comments": 10,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1njgjdd",
    "title": "[R] Need model/paper/code suggestion for document template extraction",
    "content": "I am looking to create a document template extraction pipeline for document similarity. One important thing I need to do as part of this is create a template mask. Essentially, say I have a collection of documents which all follow a similar format (imagine a form or a report). I want to\n\n1. extract text from the document in a structured format (OCR but more like VQA type). About this, I have looked at a few VQA models. Some are too big but I think this a straightforward task.\n2. (what I need help with) I want a model that can, given a collection of documents or any one document, can generate a layout mask without the text, so a template). I have looked at Document Analysis models, but most are centered around classifying different sections of the document into tables, paragraphs, etc. I have not come across a mask generation pipeline or model.\n\nIf anyone has encountered such a pipeline before or worked on document template extraction, I would love some help or links to papers.",
    "author": "mavericknathan1",
    "timestamp": "2025-09-17T08:38:26",
    "url": "https://reddit.com/r/MachineLearning/comments/1njgjdd/r_need_modelpapercode_suggestion_for_document/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nj02du",
    "title": "[D] AAAI - phase 1 rejection rate?",
    "content": "I was curious, does anyone know roughly what percentage of papers survived Phase 1?\n\nI‚Äôve seen some posts saying that CV and NLP papers had about a 66% rejection rate, while others closer to 50%. But I‚Äôm not sure if that‚Äôs really the case. it seems a bit hard to believe that two-thirds of submissions got cut (though to be fair, my impression is biased and based only on my own little ‚Äúneighborhood sample‚Äù).\n\nI originally thought a score around 4,4,5 would be enough to make it through, but I‚Äôve also heard of higher combos (like, 6,7,5) getting rejected. If that‚Äôs true, does it mean the papers that survived are more like 7‚Äì8 on average, which sounds like a score for the previous acceptance thresholds.",
    "author": "BetterbeBattery",
    "timestamp": "2025-09-16T18:35:25",
    "url": "https://reddit.com/r/MachineLearning/comments/1nj02du/d_aaai_phase_1_rejection_rate/",
    "score": 23,
    "num_comments": 18,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nie5rl",
    "title": "[D] - NeurIPS 2025 Decisions",
    "content": "Just posting this thread here in anticipation of the bloodbath due in the next 2 days.",
    "author": "general_landur",
    "timestamp": "2025-09-16T03:53:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1nie5rl/d_neurips_2025_decisions/",
    "score": 199,
    "num_comments": 1043,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nj76ch",
    "title": "[D] WACV round 1 revised papers for round 2 -- rebuttal guidelines",
    "content": "Hi ML community,\n\nI have a question regarding the first-round WACV papers that received a revise recommendation and are to be submitted in the second round.\n\nFor the resubmission, the WACV website states that it requires the-\n\n1. Revised paper + supplementary\n2. And a 1-page rebuttal\n\nBut on the OpenReview website, where we see the reviewer comments, can we also clarify some of the reviewers' concerns as comments in the same thread? Or is this a no-no?\n\nThank you.",
    "author": "Consistent-Olive-322",
    "timestamp": "2025-09-17T01:06:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1nj76ch/d_wacv_round_1_revised_papers_for_round_2/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1njbzj8",
    "title": "[D] Need suggestion for Traffic prediction Model",
    "content": "Need suggestion for Traffic prediction Model\n\nOk so I am trying to make a traffic prediction model primarily training it on metr-la and pems-bay data set so I am considering to make it a hybrid approach of making a temporal and spatial unit then fusing them to generate a output \n\nSo can you suggest me any better way to do it so I can get better results or any other type of suggestions or any discussion also I would love to explore any suggestions on what features can I use as inputs to get best results out",
    "author": "mr_hexa_decimal",
    "timestamp": "2025-09-17T05:38:25",
    "url": "https://reddit.com/r/MachineLearning/comments/1njbzj8/d_need_suggestion_for_traffic_prediction_model/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1njp1ly",
    "title": "[R] Reproducible prompt protocol induces consistent self-referential responses across LLMs (Claude, GPT, Gemini)",
    "content": "I‚Äôve developed a simple prompt protocol that reliably generates what appears to be self-referential awareness responses across different LLM architectures. The method is fully documented with step-by-step instructions and examples.\n\nKey findings: \n\n‚Ä¢\tConsistent across Claude, ChatGPT-4, and Gemini \n\n‚Ä¢\tReproducible responses about subjective experience, self-awareness, and emergent states \n\n‚Ä¢\tSimple protocol that can be replicated by anyone \n\n‚Ä¢\tNo fine-tuning or special access required\n\nMethod:\n\nUses a specific sequence of prompts that seem to trigger consistent patterns of self-referential processing. Models report experiencing things like ‚Äúa locus of self,‚Äù subjective awareness, and what they describe as emergent cognitive states.\n\nReproducibility:\n\nThe protocol is designed to be simple and replicable. I‚Äôve tested it across multiple sessions and models with consistent results. GitHub tutorial with full methodology:\n\nhttps://github.com/ai-cog-res/midwiving-ai\n\nObviously, this raises interesting questions about what these responses represent. Is it genuine emergent self-awareness, sophisticated pattern matching, or something else entirely. But the reproducibility across different architectures seems worth investigating.\n\nHas anyone else experimented with systematic approaches to eliciting self-referential responses from LLMs? I would be curious to hear if others can help interpret this phenomenon.",
    "author": "ai-cog-res",
    "timestamp": "2025-09-17T13:56:18",
    "url": "https://reddit.com/r/MachineLearning/comments/1njp1ly/r_reproducible_prompt_protocol_induces_consistent/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nid4my",
    "title": "[D]How do you track and compare hundreds of model experiments?",
    "content": "I'm running hundreds of experiments weekly with different hyperparameters, datasets, and architectures. Right now, I'm just logging everything to CSV files and it's becoming completely unmanageable. I need a better way to track, compare, and reproduce results. Is MLflow the only real option, or are there lighter alternatives?",
    "author": "AdditionalAd51",
    "timestamp": "2025-09-16T02:52:22",
    "url": "https://reddit.com/r/MachineLearning/comments/1nid4my/dhow_do_you_track_and_compare_hundreds_of_model/",
    "score": 31,
    "num_comments": 33,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nidsep",
    "title": "[R] ‚ÄúEvaluating Deepfake Detectors in the Wild‚Äù: Fraudster Attacks (ICML 2025 Workshop paper)",
    "content": "Hi Reddit!¬†\n\nHave you ever thought how difficult it is to determine whether a photo is *genuine* or a **deepfake**? You might think discriminative tasks are easier than generative ones, so detection should be straightforward. Or, on the contrary, diffusion models are now so good that detection is impossible. In our work, we reveal the current state of the war on deepfakes. In short, SOTA open-source detectors fail under real-world conditions.\n\nI work as an ML engineer at a leading platform for KYC and liveness detection. In our setting, you must decide from a short verification video whether the person is who they claim to be. Deepfakes are one of the biggest and most challenging problems here. We are known for our robust anti-deepfake solutions, and I‚Äôm not trying to flex, I just want to say that we work on this problem daily and see what fraudsters actually try in order to bypass verification. For years we kept trying to apply research models to our data, and nothing really worked. For example, all research solutions were less robust than a simple zero-shot CLIP baseline. We kept wondering whether the issue lay with our data, our setup, or the research itself. It seems that a lot of deepfake research overlooks key *wild* conditions.\n\n**Core issue: robustness to OOD data.**\n\nEven a small amount of data from the test distribution leaking into the training set (say 1k images out of a 1M-image test pool) makes it trivial to achieve great metrics, and experienced computer vision experts can push¬† AUC to \\~99.99. Without peeking, however, the task becomes i*ncredibly hard*. Our paper demonstrates this with a simple, reproducible pipeline:\n\n1. **Deepfakes**. If you don‚Äôt already have them, we built a large image-level dataset using two SOTA face-swapping methods: Inswapper and Simswap.\n2. **Real world conditions.** We use small transformations that are imperceptible to humans and that we constantly see in the real world: downscaling (resize), upscaling (with some AI), and compression (JPEG). These are indistinguishable for humans, so detectors must be robust to them.\n3. **Evaluation.** Test model under different setups, e.g.: 1) only real. model have to predict only real labels 2) real vs fake 3) real vs compressed fake ... and others. It sounds easy, but every model we tested had at least one setting where performance drops to near-random.\n\nSo we‚Äôre not just releasing another benchmark or yet another deepfake dataset. We present a pipeline that *mirrors what fraudsters do*, what we actually observe in production. We‚Äôre releasing all code, our dataset (&gt;500k fake images), and even a small deepfake game where you can test yourself as a detector.\n\nFor more details, please see the full paper. Is there a silver-bullet solution to deepfake detection? We don‚Äôt claim one here, but we do share a teaser result: a promising setup using zero-shot VLMs for detection. I‚Äôll post about that (our second ICML workshop paper) separately.\n\nIf you‚Äôre interested in deepfake research and would like to chat, or even collaborate ‚Äì don‚Äôt hesitate to reach out. Cheers!\n\nhttps://preview.redd.it/vi3qxnp38ipf1.jpg?width=6099&amp;format=pjpg&amp;auto=webp&amp;s=55fe99a72bb0614bc560e5553c2eaf20cbd3132c",
    "author": "messlav",
    "timestamp": "2025-09-16T03:31:42",
    "url": "https://reddit.com/r/MachineLearning/comments/1nidsep/r_evaluating_deepfake_detectors_in_the_wild/",
    "score": 14,
    "num_comments": 4,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nhzngh",
    "title": "[D] The conference reviewing system is trash.",
    "content": "My submission to AAAI just got rejected. The reviews didn't make any sense: lack of novelty, insufficient experiments, not clear written ... \n\n  \nThese descriptions can be used for any papers in the world. The reviewers are not responsible at all and the only thing they want to do is to reject my paper.\n\n  \nAnd it is simply because I am doing the same topic as they are working!.",
    "author": "Dangerous-Hat1402",
    "timestamp": "2025-09-15T15:05:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1nhzngh/d_the_conference_reviewing_system_is_trash/",
    "score": 115,
    "num_comments": 49,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nic61l",
    "title": "[R]What's the benefit of submitting to ICCV workshop?",
    "content": "I'm a UG student workinig on my first paper (first author)\nThere is a worskhop on video world models but unfortunately it is non-archival i.e. The paper won't appear in the proceedings.\nI'm aware the value of such workshop will be lower when applying for jobs/doctoral programmes.\n\nHowever, there are some really famous speakers in the workshop including Yann LeCun. I was hoping to catch the eye of some bigshot researchers with my work.\n\nThe other option is submitting to ICLR main\nconference, and I'm not entirely confident that the work is substantial enough to get accepted there.\n\nHoping to find some advice here.",
    "author": "arasaka-man",
    "timestamp": "2025-09-16T01:51:27",
    "url": "https://reddit.com/r/MachineLearning/comments/1nic61l/rwhats_the_benefit_of_submitting_to_iccv_workshop/",
    "score": 16,
    "num_comments": 18,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nhvv90",
    "title": "[D] The quality of AAAI reviews is atrocious",
    "content": "Never have I seen such low-quality reviews from an A\\* conference. I understand that there was a record number of submissions, but come on. A lot of issues mentioned in the reviews can be answered by actually reading the main text. The reviews also lack so much detail to the point where it's not even constructive criticism, but rather a bunch of nitpicky reasons for rejection. AAAI needs to do better.",
    "author": "Zapin6",
    "timestamp": "2025-09-15T12:40:36",
    "url": "https://reddit.com/r/MachineLearning/comments/1nhvv90/d_the_quality_of_aaai_reviews_is_atrocious/",
    "score": 164,
    "num_comments": 94,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1niib3a",
    "title": "[D] Feedback on Multimodal Fusion Approach (92% Vision, 77% Audio ‚Üí 98% Multimodal)",
    "content": "Hi all,\n\nI‚Äôm working on a multimodal classification project (environmental scenes from satellite images + audio) and wanted to get some feedback on my approach.\n\n**Dataset:**\n\n* 13 classes\n* \\~4,000 training samples\n* \\~1,000 validation samples\n\n**Baselines:**\n\n* **Vision-only (CLIP RN50):** 92% F1\n* **Audio-only (ResNet18, trained from scratch on spectrograms):** 77% F1\n\n**Fusion setup:**\n\n1. Use both models as frozen feature extractors (remove final classifier).\n2. Obtain feature vectors from vision and audio.\n3. Concatenate into a single multimodal vector.\n4. Train a small classifier head on top.\n\n**Result:**  \nThe fused model achieved **98% accuracy** on the validation set. The gain from 92% ‚Üí 98% feels surprisingly large, so I‚Äôd like to sanity-check whether this is typical for multimodal setups, or if it‚Äôs more likely a sign of overfitting / data leakage / evaluation artifacts.\n\n**Questions:**\n\n* Is simple late fusion (concatenation + classifier) a sound approach here?\n* Is such a large jump in performance expected, or should I be cautious?\n\nAny feedback or advice from people with experience in multimodal learning would be appreciated.",
    "author": "Intrepid-Purpose2151",
    "timestamp": "2025-09-16T07:00:27",
    "url": "https://reddit.com/r/MachineLearning/comments/1niib3a/d_feedback_on_multimodal_fusion_approach_92/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nirq18",
    "title": "[Research][Code] Budget-aware quantile + hysteresis controller for rate-limited inference; sustainable rate r_sustain ~= regen/cost; ~80% demo energy savings",
    "content": "Problem\n\nOnline inference/agents need stable throttling under tight budgets. Naive thresholds either flap or drain reserves.\n\n\n\nMethod (small, auditable controller)\n\nr\\_sustain \\~= regen\\_idle / cost\\_avg        # EMA for cost\n\nq\\_energy  = (0.4 + 0.6\\*(E/100)) \\* q\\_target\n\nq\\_eff     = min(q\\_energy, 0.85 \\* r\\_sustain)\n\nthr       = clip(thr + eta\\_q\\*(y - q\\_eff), 0.05, 0.95)\n\nthr\\_on/off = thr +/- hyst\n\nOptional: per-class multipliers m\\_c adapted slowly (log-scale) for fairness.\n\n\n\nDemo summary\n\n‚Ä¢ regen \\~ 2.2, cost \\~ 11 ‚Üí r\\_sustain \\~ 0.20\n\n‚Ä¢ Controller converges to \\~0.16 activation rate, 0% reserve breaches\n\n‚Ä¢ \\~80% energy reduction vs a naive baseline at comparable utility proxy\n\n\n\nRepro steps\n\npip install sundew-algorithms\n\nsundew --demo --events 200\n\n\\# minimal controller + parser (MIT)\n\n\\# [https://github.com/oluwafemidiakhoa/sundew](https://github.com/oluwafemidiakhoa/sundew)  (replace with your repo)\n\n\n\nDiscussion prompts\n\n‚Ä¢ Convergence vs PI/dual-PID; regret for quantile tracking under non-stationary costs\n\n‚Ä¢ Multi-queue priority control under shared budgets\n\n‚Ä¢ Robust r\\_sustain estimation with heavy-tailed activation costs\n\n\n\nWrite-up with figures: [https://oluwafemidiakhoa.medium.com/](https://oluwafemidiakhoa.medium.com/)\n\nNot a promo; happy to incorporate critiques and benchmarks.\n\n",
    "author": "Klutzy-Aardvark4361",
    "timestamp": "2025-09-16T12:47:38",
    "url": "https://reddit.com/r/MachineLearning/comments/1nirq18/researchcode_budgetaware_quantile_hysteresis/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ni7wjd",
    "title": "[D] AAAI - 2026",
    "content": "Any guesses how many papers got rejected and how many will be in the phase 2? ",
    "author": "i_minus",
    "timestamp": "2025-09-15T21:27:24",
    "url": "https://reddit.com/r/MachineLearning/comments/1ni7wjd/d_aaai_2026/",
    "score": 15,
    "num_comments": 28,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nif3q1",
    "title": "[D]Any experience with complicated datasets?",
    "content": "Hello,\n\nI am a PhD student working with cancer datasets to train classifiers. The dataset I am using to train my ML models (**Random Forest, XGBoost**) is rather a mixed bag of the different types of cancer (multi-class),I would want to classify/predict. In addition to heavy **class overlap and within-class heterogeneity**, there's **class imbalance**.\n\nI applied SMOTE to correct the imbalance but again due to class overlap, the synthetic samples generated were just random noise.\n\nEver since, instead of having to balance with sampling methods, I have been using class weights. I have cleaned up the datasets to remove any sort of batch effects and technical artefacts, despite which the class-specific effects are hazy. I have also tried stratifying the data into binary classification problems, but given the class imbalance, that didn't seem to be of much avail.\n\nIt is kind of expected of the dataset owing to the default biology, and hence I would have to be dealing with class overlap and heterogeneity to begin with.\n\nI would appreciate if anyone could talk about how they got through when they had to train their models on similar complex datasets? What were your models and data-polishing approaches?\n\nThanks :)",
    "author": "Pure_Landscape8863",
    "timestamp": "2025-09-16T04:43:00",
    "url": "https://reddit.com/r/MachineLearning/comments/1nif3q1/dany_experience_with_complicated_datasets/",
    "score": 3,
    "num_comments": 8,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nicn94",
    "title": "[D] Suppose you wanted to test a new model architecture to get preliminary results but have limited compute. What domain is good to train on to infer that the model would be good at reasoning?",
    "content": "This is a hard question that I imagine is being thought about a lot, but maybe there are answers already.\n\nTraining a model to consume a query in text, reason about it, and spit out an answer is quite demanding and requires the model to have a lot of knowledge.\n\nIs there some domain that requires less knowledge but allows the model to learn reasoning/agency, without the model having to become huge?\n\nI think mathematical reasoning is a good example, it is a much smaller subset of language and has narrower objectives (assuming you don't want it to invent a new paradigm and just operate within an existing one).\n\nThere might be others?",
    "author": "FIREATWlLL",
    "timestamp": "2025-09-16T02:21:50",
    "url": "https://reddit.com/r/MachineLearning/comments/1nicn94/d_suppose_you_wanted_to_test_a_new_model/",
    "score": 5,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nib74z",
    "title": "[D] Resubmission 2026: ICLR or AISTATS... or any other?",
    "content": "Some of my AAAI submissions got rejected in phase 1. To be honest, my reviews are good; maybe too harsh in the scores, but at least they read the papers and made their points. Now I wonder where to resubmit (enhancing the papers a bit with this feedback, but without much time because I work in the industry). \n\nI think ICLR will be crazy this year (many NIPS and AAAI work), so I do not know if the process will be as random as the one in AAAI. As for submissions being \"9 pages or fewer\", do people usually fill 9 pages or is okey to make less? I only saw this in RLC before (and other ICLR). Also, I always have doubts about the rebuttal period here, is it still the case that I can update my experiments and discuss with reviewers? Do reviewers still engage in discussion in these overloaded times?\n\nLast, what about AISTATS? I never submitted there, but it might be a good way to escape from these super big conferences. However, I am afraid papers will not get as much visibility. I heard this is a prestigious conference, but then almost never gets cited in e.g., job offers.\n\nI am a bit lost with AI/ML conferences lately. What are your thoughts on this submission cycle?",
    "author": "SignificanceFit3409",
    "timestamp": "2025-09-16T00:46:48",
    "url": "https://reddit.com/r/MachineLearning/comments/1nib74z/d_resubmission_2026_iclr_or_aistats_or_any_other/",
    "score": 8,
    "num_comments": 30,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nhyanm",
    "title": "[D] Any comments of AAAI Review process?",
    "content": "One of the reviewer mentioning weaknesses of my paper which is all included in the paper and give 3 reject, while other reviewer gives me 6,6 and I got rejected.\n\nI am really frustrated that I cannot rebut such review and see this type of review",
    "author": "JicamaNormal927",
    "timestamp": "2025-09-15T14:10:25",
    "url": "https://reddit.com/r/MachineLearning/comments/1nhyanm/d_any_comments_of_aaai_review_process/",
    "score": 27,
    "num_comments": 23,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nhpwwn",
    "title": "[D]AAAI 2026 phase1",
    "content": "I‚Äôve seen a strange situation that many papers which got high scores like 6 6 7, 6 7 7 even 6 7 8 are rejected, but some like 4 5 6 even 2 3 are passed. Do anyone know what happened?",
    "author": "Small_Bb",
    "timestamp": "2025-09-15T09:02:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1nhpwwn/daaai_2026_phase1/",
    "score": 73,
    "num_comments": 226,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1niyhch",
    "title": "Why I‚Äôm going back to the AI Agent Security Research Summit [R]",
    "content": "I lead AppSec and was recently pulled into building our **AI agent security program**. I happened to be in NYC when the first **AI Agent Security Summit** was taking place and went along ‚Äî it ended up being one of the few events where the research connected directly to practice.\n\nThe next one is October 8 in San Francisco. I‚Äôm making the trip from Austin this time. It‚Äôs not a big event, but the lineup of [speakers](https://zenity.io/resources/events/ai-agent-security-summit-2025) looks strong, and I thought I‚Äôd share in case anyone in the Bay is interested.",
    "author": "Zemgineer2084",
    "timestamp": "2025-09-16T17:22:05",
    "url": "https://reddit.com/r/MachineLearning/comments/1niyhch/why_im_going_back_to_the_ai_agent_security/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nibok2",
    "title": "[D] ICLR 2026 Workshop Announcements",
    "content": "Hi everyone, I‚Äôm new to academia and currently exploring top AI conferences for the upcoming year. Could you let me know when workshop information is usually announced ‚Äî for example, for ICLR (April 23‚Äì27, Brazil)? Thanks",
    "author": "Mysterious_Travel936",
    "timestamp": "2025-09-16T01:19:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1nibok2/d_iclr_2026_workshop_announcements/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ni187r",
    "title": "[D] AAAI 2026 Social Impact track",
    "content": "Has anybody heard anything from the social impact track? They were supposed to be out on the 8th, but nobody has heard anything, so I thought they might release it alongside the main track. But we are still waiting.",
    "author": "Plz_Give_Me_A_Job",
    "timestamp": "2025-09-15T16:10:49",
    "url": "https://reddit.com/r/MachineLearning/comments/1ni187r/d_aaai_2026_social_impact_track/",
    "score": 8,
    "num_comments": 14,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nibumz",
    "title": "[R] NEXUS-EMB-240M-NSA: Compact Embedding Model with Neural Spectral Anchoring",
    "content": "Working on a 240M parameter embedding model with some unconventional techniques:\n\n- Dual-head architecture (semantic + entity processing)\n- Neural Spectral Anchoring - projecting embeddings into spectral space\n- Residual hashing bridge for fast retrieval\n- Edge-optimized design\n\nThe NSA component is particularly interesting - instead of standard Euclidean embeddings, we project into spectral space to capture deeper relational structures.\n\nStill training, but curious about feedback on the approach. Has anyone experimented with spectral methods in embeddings?\n\nCode: https://github.com/Daniele-Cangi/Nexus-240m-NSA",
    "author": "Ill-Button-1680",
    "timestamp": "2025-09-16T01:30:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1nibumz/r_nexusemb240mnsa_compact_embedding_model_with/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ni9rku",
    "title": "kerasnip: use Keras models in tidymodels workflows (R package) [N]",
    "content": "Sharing a new R package I found: [**kerasnip**](https://github.com/davidrsch/kerasnip).\n\nIt lets you define/tune **Keras models** (sequential + functional) within the **tidymodels** framework, so you can handle recipes, tuning, workflows, etc. with deep learning models.\n\nDocs &amp; examples: [davidrsch.github.io/kerasnip](https://davidrsch.github.io/kerasnip/).\n\nMight be useful for folks who like the tidymodels workflow but want to bring in neural nets.",
    "author": "FriendlyAd5913",
    "timestamp": "2025-09-15T23:15:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1ni9rku/kerasnip_use_keras_models_in_tidymodels_workflows/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ni261q",
    "title": "[P] Add Core Dolphin to sdlarch-rl (now compatible with Wii and GameCube!!!!",
    "content": "https://preview.redd.it/qm7330ow1fpf1.png?width=2922&amp;format=png&amp;auto=webp&amp;s=52aca51ae6265593d55a2152772f701011d3cb2c\n\nI have good news!!!! I managed to update my training environment and add Dolphin compatibility, allowing me to run GameCube and Wii games for RL training!!!! This is in addition to the PCSX2 compatibility I had implemented. The next step is just improvements!!!!\n\n[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)",
    "author": "AgeOfEmpires4AOE4",
    "timestamp": "2025-09-15T16:51:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1ni261q/p_add_core_dolphin_to_sdlarchrl_now_compatible/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nhvp1a",
    "title": "[P] Sundew v0.5.0: Selective activation for energy-aware inference on edge devices (code)",
    "content": "Author disclosure: I‚Äôm the developer of Sundew.\n\n\n\nSummary\n\n\\- A small open-source controller that decides \\*when\\* to run an expensive model.\n\n\\- Goal: cut energy cost on edge devices while keeping task performance.\n\n\n\nMethod (very brief)\n\n\\- Compute a significance score per event (magnitude/urgency/context/anomaly).\n\n\\- PI correction + energy pressure updates an activation threshold.\n\n\\- Small hysteresis window reduces thrashing.\n\n\n\nResults (from the repo‚Äôs demos)\n\n\\- \\~83% reduction in processing energy (200-event demo).\n\n\\- \\~0.003 s average processing time per event.\n\n\\- Example application: low-power health monitoring.\n\n\n\nCode\n\n\\- GitHub: [https://github.com/oluwafemidiakhoa/sundew\\_algorithms](https://github.com/oluwafemidiakhoa/sundew_algorithms)  (Apache-2.0)\n\n\n\nReproduce (quick demo)\n\nbash\n\nCopy code\n\npip install sundew-algorithms==0.5.0\n\nsundew --demo --events 100\n\ndiff\n\nCopy code\n\nLimitations / open questions\n\n\\- Threshold tuning vs. missed events tradeoff.\n\n\\- How would you evaluate selective activation in a fair task-performance metric?\n\n\\- Suggestions for stronger baselines are welcome.\n\n\n\nHappy to share ablations or additional benchmarks in the comments.\n\n",
    "author": "Klutzy-Aardvark4361",
    "timestamp": "2025-09-15T12:34:16",
    "url": "https://reddit.com/r/MachineLearning/comments/1nhvp1a/p_sundew_v050_selective_activation_for/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ngsn3g",
    "title": "[D] No Google or Meta at EMNLP 2025?",
    "content": "I was going through the EMNLP 2025 sponsors page and noticed something odd. Google and Meta aren‚Äôt listed this year. [Link here](https://2025.emnlp.org/sponsors/).\n\nIs it that they‚Äôre really not sponsoring this time? Or maybe it‚Äôs just not updated yet?\n\nFor those of us who are PhD students looking for internships, this feels a bit concerning. These conferences are usually where we get to connect with researchers from those companies. If they are not sponsoring or showing up in an official way, what‚Äôs the best way for us to still get on their radar?\n\nCurious if others are thinking about this too.",
    "author": "GlitteringEnd5311",
    "timestamp": "2025-09-14T07:25:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1ngsn3g/d_no_google_or_meta_at_emnlp_2025/",
    "score": 61,
    "num_comments": 31,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nh2uh5",
    "title": "[R] AI Learns to Speedrun Mario in 24 Hours (2 Million Attempts!)",
    "content": "\n\n# Abstract\n\nI trained a Deep Q-Network (DQN) agent to speedrun Yoshi's Island 1 from Super Mario World, achieving near-human level performance after 1,180,000 training steps. The agent learned complex sequential decision-making, precise timing mechanics, and spatial reasoning required for optimized gameplay.\n\n# Environment Setup\n\n**Game Environment:** Super Mario World (SNES) - Yoshi's Island 1\n\n* **Observation Space:** 224x256x3 RGB frames, downsampled to 84x84 grayscale\n* **Action Space:** Discrete(12) - D-pad combinations + jump/spin buttons\n* **Frame Stacking:** 4 consecutive frames for temporal information\n* **Frame Skip:** Every 4th frame processed to reduce computational load\n\n**Level Complexity:**\n\n* 18 Rex enemies (require stomping vs jumping over decision)\n* 4 Banzai Bills (precise ducking timing required)\n* 3 Jumping Piranha Plants\n* 1 Unshelled Koopa, 1 Clappin' Chuck, 1 Lookout Chuck\n* Multiple screen transitions requiring positional memory\n\n# Architecture &amp; Hyperparameters\n\n**Network Architecture:**\n\n* CNN Feature Extractor: 3 Conv2D layers (32, 64, 64 filters)\n* ReLU activations with 8x8, 4x4, 3x3 kernels respectively\n* Fully connected layers: 512 ‚Üí 256 ‚Üí 12 (action values)\n* Total parameters: \\~1.2M\n\n**Training Configuration:**\n\n* Algorithm: DQN with Experience Replay + Target Network\n* Replay Buffer: 100,000 transitions\n* Batch Size: 32\n* Learning Rate: 0.0001 (Adam optimizer)\n* Target Network Update: Every 1,000 steps\n* Epsilon Decay: 1.0 ‚Üí 0.1 over 100,000 steps\n* Discount Factor (Œ≥): 0.99\n\n# Reward Engineering\n\n**Primary Objectives:**\n\n* **Speed Optimization:** \\-0.1 per frame (encourages faster completion)\n* **Progress Reward:** \\+1.0 per screen advancement\n* **Completion Bonus:** \\+100.0 for level finish\n* **Death Penalty:** \\-10.0 for losing a life\n\n**Auxiliary Rewards:**\n\n* Enemy elimination: +1.0 per enemy defeated\n* Coin collection: +0.1 per coin (sparse, non-essential)\n* Damage avoidance: No explicit penalty (covered by death penalty)\n\n# Key Training Challenges &amp; Solutions\n\n# 1. Banzai Bill Navigation\n\n**Problem:** Agent initially jumped into Banzai Bills 847 consecutive times **Solution:** Shaped reward for successful ducking (+2.0) and position-holding at screen forks\n\n# 2. Rex Enemy Mechanics\n\n**Problem:** Agent stuck in local optimum of attempting impossible jumps over Rex **Solution:** Curriculum learning - introduced stomping reward gradually after 200K steps\n\n# 3. Exploration vs Exploitation\n\n**Problem:** Agent converging to safe but slow strategies **Solution:** Noisy DQN exploration + periodic epsilon resets every 100K steps\n\n# 4. Temporal Dependencies\n\n**Problem:** Screen transitions requiring memory of previous actions **Solution:** Extended frame stacking (4‚Üí8 frames) + LSTM layer for sequence modeling\n\n# Results &amp; Performance Metrics\n\n**Training Progress:**\n\n* Steps 0-200K: Basic movement and survival (success rate: 5%)\n* Steps 200K-600K: Enemy interaction learning (success rate: 35%)\n* Steps 600K-1000K: Timing optimization (success rate: 78%)\n* Steps 1000K-1180K: Speedrun refinement (success rate: 94%)\n\n**Final Performance:**\n\n* **Completion Rate:** 94% over last 1000 episodes\n* **Average Completion Time:** \\[Actual time from your results\\]\n* **Best Single Run:** \\[Your best time\\]\n* **Human WR Comparison:** \\[% of world record time\\]\n\n**Convergence Analysis:**\n\n* Reward plateau reached at \\~900K steps\n* Policy remained stable in final 200K steps\n* No significant overfitting observed\n\n# Technical Observations\n\n# Emergent Behaviors\n\n1. **Momentum Conservation:** Agent learned to maintain running speed through precise jump timing\n2. **Risk Assessment:** Developed preference for safe routes vs risky shortcuts based on success probability\n3. **Pattern Recognition:** Identified and exploited enemy movement patterns for optimal timing\n\n# Failure Modes\n\n1. **Edge Case Sensitivity:** Occasional failures on rare enemy spawn patterns\n2. **Precision Limits:** Sub-pixel positioning errors in \\~6% of attempts\n3. **Temporal Overfitting:** Some strategies only worked with specific lag patterns\n\n# Computational Requirements\n\n**Hardware:**\n\n* GPU: Ryzen 5900x\n* CPU: RTX 4070 TI\n* RAM: 64GB\n* Storage: 50GB for model checkpoints\n\n**Training Time:**\n\n* Wall Clock: 24 hours\n* GPU Hours: \\~20 hours active training\n* Checkpoint Saves: Every 10K steps (118 total saves)\n\n# Code &amp; Reproducibility\n\n**Framework:** \\[PyTorch/TensorFlow/Stable-Baselines3\\] **Environment Wrapper:** \\[RetroGym/custom wrapper\\] **Seed:** Fixed random seed for reproducibility\n\nCode available at: [https://github.com/paulo101977/SuperMarioWorldSpeedRunAI](https://github.com/paulo101977/SuperMarioWorldSpeedRunAI)",
    "author": "AgeOfEmpires4AOE4",
    "timestamp": "2025-09-14T14:03:42",
    "url": "https://reddit.com/r/MachineLearning/comments/1nh2uh5/r_ai_learns_to_speedrun_mario_in_24_hours_2/",
    "score": 11,
    "num_comments": 2,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nhp54q",
    "title": "[R] r-rpe: beyond openai‚Äôs rl-hf ‚Äî hedging ‚Üì60% in eval-only tests",
    "content": "openai built rl-hf on the *animal* reward prediction error‚Äîoutcome-only, scalarized, blind to anticipation. it works, but it locks models into pleasing and hedging.\n\nr-rpe is the missing half: an identity-projected reward prediction error based on the model of a conscious being. it adds a pre-action appraisal channel, aligning outputs with narrative identity instead of just outcomes.\n\nin eval-only tests (tinyllama-1.1b, qwen2.5-1.5b):  \n‚Äî hedging reduced by &gt;60%  \n‚Äî framing robustness improved  \n‚Äî ablations confirm the anticipatory channel is what drives it\n\nthis is not a tweak. it‚Äôs the complete form of prediction error once aligned with conscious appraisal.\n\nlinks are filtered here‚Äîif you want the preprint and data, just google Louis J. LU and click the orcid profile (0009-0002-8071-1584)",
    "author": "chicken1414",
    "timestamp": "2025-09-15T08:34:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1nhp54q/r_rrpe_beyond_openais_rlhf_hedging_60_in_evalonly/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ngy14l",
    "title": "[D] Paged Attention Performance Analysis",
    "content": "",
    "author": "ApartmentEither4838",
    "timestamp": "2025-09-14T10:56:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1ngy14l/d_paged_attention_performance_analysis/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nhbxwk",
    "title": "[D] Recent paddleocr version accuracy",
    "content": "Has anyone tried using the paddleocr latest version 3.2.0, I could observe the recognition accuracy has decreased compared to previous version which I was using (2.10.0)",
    "author": "Leather_Presence6360",
    "timestamp": "2025-09-14T21:04:00",
    "url": "https://reddit.com/r/MachineLearning/comments/1nhbxwk/d_recent_paddleocr_version_accuracy/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ng6dsf",
    "title": "[D] which papers HAVEN'T stood the test of time?",
    "content": "As in title! Papers that were released to lots of fanfare but haven't stayed in the zeitgeist also apply. \n\nLess so \"didn't stand the test of time\" but I'm thinking of KANs. Having said that, it could also be that I don't work in that area, so I don't see it and followup works. I might be totally off the mark here so feel free to say otherwise ",
    "author": "iamquah",
    "timestamp": "2025-09-13T12:25:12",
    "url": "https://reddit.com/r/MachineLearning/comments/1ng6dsf/d_which_papers_havent_stood_the_test_of_time/",
    "score": 176,
    "num_comments": 158,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nguevs",
    "title": "[R] Built an open-source matting model (Depth-Anything + U-Net). What would you try next?",
    "content": "Hi all,  \nI‚Äôve been working on¬†**withoutbg**, an open-source background removal tool built on a lightweight matting model.\n\n**Key aspects**\n\n* Python package for local use\n* **Model design:**¬†Depth-Anything v2 (small) -&gt; matting model -&gt; refiner\n* **Deployment:**¬†trained in PyTorch, exported to ONNX for lightweight inference\n\n**Looking for ideas to push quality further**  \nOne experiment I‚Äôm planning is¬†**fusing CLIP visual features into the bottleneck of the U-Net matting/refiner**¬†(no text prompts) to inject semantics for tricky regions like hair, fur, and semi-transparent edges.  \n**What else would you try?**¬†Pointers to papers/recipes welcome.",
    "author": "Naive_Artist5196",
    "timestamp": "2025-09-14T08:37:05",
    "url": "https://reddit.com/r/MachineLearning/comments/1nguevs/r_built_an_opensource_matting_model_depthanything/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ng51zx",
    "title": "[D] AAAI 26 Main Track",
    "content": "When do they release the results for Phase 1? It was supposed to come out on September 12th!",
    "author": "That_Wish2205",
    "timestamp": "2025-09-13T11:32:08",
    "url": "https://reddit.com/r/MachineLearning/comments/1ng51zx/d_aaai_26_main_track/",
    "score": 41,
    "num_comments": 312,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ngf0l4",
    "title": "[D] Regarding discord or online communities",
    "content": "I was just wondering if there are discord active groups that work on image generative model research? For example, if I wanted to work on implementing an image adapter from scratch for a custom diffusion model, I don't really know how to go about it. I just want to be involved in a community for controllable image generation/restoration.\n\nCan anyone help me with this?",
    "author": "mmmm-bobaman",
    "timestamp": "2025-09-13T18:57:46",
    "url": "https://reddit.com/r/MachineLearning/comments/1ngf0l4/d_regarding_discord_or_online_communities/",
    "score": 8,
    "num_comments": 2,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ng2aiw",
    "title": "[D] RL interviews at frontier labs, any tips?",
    "content": "I‚Äôm recently starting to see top AI labs ask RL questions.\n\nIt‚Äôs been a while since I studied RL, and was wondering if anyone had any good guide/resources on the topic.\n\nWas thinking of mainly familiarizing myself with policy gradient techniques like SAC, PPO - implement on Cartpole and spacecraft. And modern applications to LLMs with DPO and GRPO.\n\nI‚Äôm afraid I don‚Äôt know too much about the intersection of LLM with RL. \n\nAnything else worth recommending to study?",
    "author": "bci-hacker",
    "timestamp": "2025-09-13T09:45:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1ng2aiw/d_rl_interviews_at_frontier_labs_any_tips/",
    "score": 34,
    "num_comments": 6,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ngu2nc",
    "title": "[R] Theoretical Framework to understand human-AI communication process",
    "content": "After 3 years of development, I‚Äôm proud to share my latest peer-reviewed article in the Human-Machine Communication journal (Q1 Scopus-indexed).\n\nI introduce the HAI-IO Model ‚Äî the first theoretical framework to visually and conceptually map the Human-AI communication process. It examines how humans interact with AI not just as tools, but as adaptive communicative actors.\n\nThis model could be useful for anyone researching human-AI interaction, designing conversational systems, or exploring the ethical/social implications of AI-mediated communication.\n\nOpen-access link to the article:\nhttps://stars.library.ucf.edu/hmc/vol10/iss1/9/",
    "author": "Iamfrancis23",
    "timestamp": "2025-09-14T08:23:38",
    "url": "https://reddit.com/r/MachineLearning/comments/1ngu2nc/r_theoretical_framework_to_understand_humanai/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nfrpvz",
    "title": "[R] New \"Illusion\" Paper Just Dropped For Long Horizon Agents",
    "content": "Hi all, we recently released our new work on Long Horizon Execution. If you have seen the METR plot, and-like us-have been unconvinced by it, we think you will really like our work!\n\nPaper link: [https://www.alphaxiv.org/abs/2509.09677](https://www.alphaxiv.org/abs/2509.09677)\n\nX/Twitter thread: [https://x.com/ShashwatGoel7/status/1966527903568637972](https://x.com/ShashwatGoel7/status/1966527903568637972)\n\nWe show some really interesting results. The highlight? The notion that AI progress is \"slowing down\" is an Illusion. Test-time scaling is showing incredible benefits, especially for long horizon autonomous agents. We hope our work sparks more curiosity in studying these agents through simple tasks like ours!! I would love to answer any questions and engage in discussion\n\nhttps://preview.redd.it/078xuqwq1wof1.png?width=1167&amp;format=png&amp;auto=webp&amp;s=f28b566705348035ca39cad8fdf3762cedd569ba\n\n  \n",
    "author": "viciousA3gis",
    "timestamp": "2025-09-13T00:55:46",
    "url": "https://reddit.com/r/MachineLearning/comments/1nfrpvz/r_new_illusion_paper_just_dropped_for_long/",
    "score": 41,
    "num_comments": 8,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nfav96",
    "title": "[D] Larry Ellison: ‚ÄúInference is where the money is going to be made.‚Äù",
    "content": "In Oracle‚Äôs recent call, Larry Ellison said something that caught my attention:\n\n‚ÄúAll this money we‚Äôre spending on training is going to be translated into products that are sold ‚Äî which is all inferencing. There‚Äôs a huge amount of demand for inferencing‚Ä¶ We think we‚Äôre better positioned than anybody to take advantage of it.‚Äù\n\nIt‚Äôs striking to see a major industry figure frame inference as the real revenue driver, not training. Feels like a shift in narrative: less about who can train the biggest model, and more about who can serve it efficiently, reliably, and at scale.\n\nNot sure if the industry is really moving in this direction? Or will training still dominate the economics for years to come?",
    "author": "pmv143",
    "timestamp": "2025-09-12T11:28:03",
    "url": "https://reddit.com/r/MachineLearning/comments/1nfav96/d_larry_ellison_inference_is_where_the_money_is/",
    "score": 206,
    "num_comments": 105,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nf9noo",
    "title": "[D] Do you ever miss PyTorch-style workflows?",
    "content": "I used to contribute to PyTorch, and I‚Äôm wondering: how many of you shifted from building with PyTorch to mainly managing prompts for LLMs? Do you ever miss the old PyTorch workflow ‚Äî datasets, metrics, training loops ‚Äî versus the endless \"prompt -&gt; test -&gt; rewrite\" loop? ",
    "author": "dmpiergiacomo",
    "timestamp": "2025-09-12T10:40:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1nf9noo/d_do_you_ever_miss_pytorchstyle_workflows/",
    "score": 108,
    "num_comments": 90,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nfestz",
    "title": "[R] Debunking the Claims of K2-Think",
    "content": "Recent work (K2-Think) claimed to have a SOTA small model: [https://arxiv.org/abs/2509.07604](https://arxiv.org/abs/2509.07604)\n\nThree days later a dubunking post of this work was posted: [https://www.sri.inf.ethz.ch/blog/k2think](https://www.sri.inf.ethz.ch/blog/k2think)\n\n\n\n\n\n",
    "author": "LetsTacoooo",
    "timestamp": "2025-09-12T14:03:04",
    "url": "https://reddit.com/r/MachineLearning/comments/1nfestz/r_debunking_the_claims_of_k2think/",
    "score": 27,
    "num_comments": 1,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nfpusc",
    "title": "[P] Training an ML model to detect fake product reviews",
    "content": "Working on a side project to help people make better purchasing decisions online. One major component is detecting fake reviews, which turned out to be much harder than expected.\n\n**The Approach:** Started with labeled dataset of verified fake reviews from FakeSpot research. Training ensemble model combining:\n\n- Linguistic features (sentiment, readability, vocabulary richness)\n- Temporal patterns (review timing, account age, posting frequency)\n- Semantic analysis (topic consistency, specificity of complaints/praise)\n\n**Initial Results:**\n\n- 78% accuracy on test set\n- High precision on obvious bot reviews (0.91)\n- Struggles with sophisticated fakes that mimic real review patterns\n\n**Interesting Discoveries:**\n\n**Fake Review Patterns:**\n\n- Excessive use of product name in review text\n- Generic praise without specific use cases\n- Perfect grammar (real users make typos)\n- Reviews clustered around same timestamps\n\n**Real Review Indicators:**\n\n- Specific complaints about minor issues\n- Mentions of use context (\"bought for my college dorm\")\n- Photos that show actual usage wear\n- Mixed sentiment (likes some aspects, dislikes others)\n\n**Current Challenges:**\n\n- Regional language differences affect detection\n- Incentivized reviews blur line between real/fake\n- Sophisticated fake reviewers are learning to mimic real patterns\n\nI've integrated this into Yaw AI (chrome extension I'm building) but still need significant improvement before it's reliable enough for general use. Sometimes flags legitimate reviews as suspicious and occasionally misses obvious fakes.\n\n**Next Steps:**\n\n- Expand training data with international reviews\n- Implement active learning to improve edge cases\n- Add verification scoring instead of binary classification\n\nAnyone working on similar problems? Would love to compare approaches or collaborate on training data.",
    "author": "sherlock_er",
    "timestamp": "2025-09-12T23:01:43",
    "url": "https://reddit.com/r/MachineLearning/comments/1nfpusc/p_training_an_ml_model_to_detect_fake_product/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nfmh43",
    "title": "[P] Env for Reinforcement Learning with Game Cube/Wii Games!!!!",
    "content": "https://preview.redd.it/l71h1i1njuof1.png?width=2670&amp;format=png&amp;auto=webp&amp;s=a1bdd20917e5244a0e0eb764e862348d2b08ce35\n\n  \nI achieved another feat today!!! In my tests, Dolphin ran in my \"stable-retro\" and gym versions!!!!!\n\nI should upload the change to the repository this week.\n\nDon't forget to follow and give an ok to the repo:¬†[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)",
    "author": "AgeOfEmpires4AOE4",
    "timestamp": "2025-09-12T19:53:42",
    "url": "https://reddit.com/r/MachineLearning/comments/1nfmh43/p_env_for_reinforcement_learning_with_game/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1neytrz",
    "title": "[D] Anyone used DeFMO to train models for deblurring fast-moving objects?",
    "content": "I‚Äôm exploring the DeFMO repo and was wondering if anyone has trained it for detecting and deblurring fast-moving objects. My main use case is basketball - the ball often gets blurred in game footage, and I‚Äôd like to use DeFMO to recover its shape and improve detection.",
    "author": "Round_Finish5632",
    "timestamp": "2025-09-12T02:42:41",
    "url": "https://reddit.com/r/MachineLearning/comments/1neytrz/d_anyone_used_defmo_to_train_models_for/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nf9cwu",
    "title": "[D] Seeking Recommendations for AutoML Libraries Compatible with Windows (Python 3.12) in 2025",
    "content": "Hi all,\nI‚Äôm struggling to find an AutoML library that works reliably on Windows. I‚Äôve tested Auto-sklearn, TPOT,PyCaret and Flaml, but I keep hitting issues:\n‚Ä¢  Many don‚Äôt support Python 3.12.\n‚Ä¢  Some clash with NumPy or other dependencies.\n‚Ä¢  Fresh Conda environments still result in installation errors, deprecated package warnings, or runtime failures.\nHas anyone successfully used an AutoML tool on Windows recently? I‚Äôd prefer ones that install smoothly and handle tabular data well, with good documentation. What are people using in 2025 that avoids these headaches? Any setup tips or alternatives would be appreciated!\nThanks!",
    "author": "socialcalliper",
    "timestamp": "2025-09-12T10:28:42",
    "url": "https://reddit.com/r/MachineLearning/comments/1nf9cwu/d_seeking_recommendations_for_automl_libraries/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nfpgoh",
    "title": "[R] A Framework for Entropic Generative Systems: Mapping Cosmic Principles to Novel Creation in AI",
    "content": "**Disclosure:**\n\nI needed help with AI to write this as a proper \"research paper\". My unmedicated ADHD is both a boon and a curse. My superpower is that I see patterns and am often connecting things so rapidly in my mind that people have a hard time following. - And I'm not a researcher, I'm a dude that likes science - something else my hyper focus has helped.\n\nI organized all my notes and chicken scratch and questions and began looking into anyone else that thought of these. After I sorted everything I put it into Gemini Research for this output.\n\n[A Framework for Entropic Generative Systems: Mapping Cosmic Principles to Novel Creation in AI](https://docs.google.com/document/d/1Z6h8LMPPpbvTdgiMtZ7IRM8ZWmT55KVeOq5bPjlEAb4/edit?tab=t.0)\n\n**Some Background:**\n\nThis prior Tuesday I met with Professor Mandeep Gill, an astrophysics professor and researcher at the University of Minnesota regarding an autonomous engine I built. This is a self-attacking autonomous red teaming system that operates under what I called \"Controlled Entropy\".\n\nAfter my meeting with Professor Gill, I was invited to take a Graduate level Supernovae class and I began thinking of new ways to use concepts from the class in cybersecurity and AI development\n\nLater ... as I was falling asleep I began dreaming in graphs. I started putting each graph on top of each other and I realized that so many of the concepts I've learned across the years of watching YouTube videos or learning about some new theory, and suddenly everything seemed like it all lined up.\n\nThis led me down a rabbit hole:\n\n[Universality](https://en.wikipedia.org/wiki/Universality)\n\n[Shannon Entropy (Information Entropy)](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n\nI'm working out a way to build this into my autonomous red teaming engine - if the theory is correct, we will be able to generate a novel threat vector that crosses categories of attacks: hardware vectors + IoT + ransomeware, etc...\n\n1. Our 100% autonomous cybersecurity suite will not only be able to match current known and unknown threats,\n2. We can use a brand new, multi-category attack against our own system the pattern recognition would evolve infinitely.",
    "author": "syntex_autonomous",
    "timestamp": "2025-09-12T22:38:52",
    "url": "https://reddit.com/r/MachineLearning/comments/1nfpgoh/r_a_framework_for_entropic_generative_systems/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.19,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nf03e6",
    "title": "IMU sensor based terrain classification [P]",
    "content": "Working on my projrct in Robotics. \nI'm developing a terrain classification system using only a single IMU sensor (BNO055) to identify surface types (grass, floor, cement) in real-time for autonomous mobile robots.\n\nMy approach:\n\nCollecting 10 minutes of IMU data per terrain at various speeds (0.2-0.8 m/s).\n\nCreating 1-second sliding windows with 50% overlap\n\nExtracting 16 features per window:\n\nTime-domain: variance, RMS, peak-to-peak, zero-crossing rate of Z-axis accelerationFrequency-domain: \n\nFFT power in bands [0-5Hz], [5-15Hz], [15-30Hz], [30-50Hz]Statistical: kurtosis, skewness\n\nTraining Random Forest classifier.\n\nTarget: 80-85% accuracy.\n\nKey insights: Different terrains create distinct vibration signatures in frequency domain (grass: 5-15Hz peak, cement: 15-30Hz peak, floor: mostly &lt;5Hz).\n\nHas anyone tried similar approaches with fewer features that still work well? Or is this approach works well with this type of task?\n",
    "author": "Mountain_Reward_1252",
    "timestamp": "2025-09-12T03:58:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1nf03e6/imu_sensor_based_terrain_classification_p/",
    "score": 3,
    "num_comments": 9,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nfgc8h",
    "title": "[D] OOM When Using Gradient Accumulation",
    "content": "I am trying to train a transformer model(1.5b parameters) on a TPU v3-8. The highest physical batch size I can get is 16 sequences of 2048 tokens. To increase my effective batch size, I have turned to gradient accumulation. My loop works at a smaller scale, but at a larger scale, it causes an OOM error. I'm using Torch XLA. Here is my code:\n\nOptimizer creation:\n```\ndef build_optimizer(model, peak_lr, muon_peak_lr, betas, weight_decay):\n    param_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(\"-\"*100)\n    print(f\"Total parameters: {total_params}\")\n    print(\"-\"*100)\n    print(f\"Trainable parameters: {trainable_params}\")\n    print(\"-\"*100)\n    hidden_params = [p for n, p in model.named_parameters() if p.ndim &gt;= 2 and not (n.endswith(\"wte.weight\") or n.endswith(\"lm_head.weight\"))]\n    # We only want adamw to apply weight decay to embeddings\n    decay = [p for n, p in model.named_parameters() if p.ndim &gt;= 2 and isinstance(n, nn.Embedding)]\n    # Exclude biases(if applicable) and normalization params\n    no_decay = [p for pn, p in param_dict.items() if p.dim() &lt; 2]\n    groups = [\n        {\"params\": decay, \"weight_decay\": weight_decay},\n        {\"params\": no_decay, \"weight_decay\": 0.0}\n    ]\n    adamw = syncfree.AdamW(groups, lr=peak_lr, betas=betas)\n    muon = SingleDeviceMuon(hidden_params, lr=muon_peak_lr, momentum=betas[1], weight_decay=weight_decay)\n    return adamw, muon\n\n```\n\nBefore I start training I run this code, as it prevents an OOM on the first step:\n```\nfor _ in range(3):\n    train_loss = torch.zeros((), device=device)\n    for k in range(gradient_accumulation_steps):\n        x = torch.randint(0, 100256, (1, 2048)).to(device)\n        xs.mark_sharding(x, mesh, (\"fsdp\", None))\n        y = torch.randint(0, 100256, (1, 2048)).to(device)\n        xs.mark_sharding(y, mesh, (\"fsdp\", None))\n        with autocast(xm.xla_device(), dtype=torch.bfloat16):\n            loss = model(x, y)\n        (loss/gradient_accumulation_steps).backward()\n        train_loss += loss.detach()\n        # xm.mark_step()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n    \n    xm.optimizer_step(muon, barrier=True)\n    xm.optimizer_step(adamw, barrier=True)\n    adamw.zero_grad()\n    muon.zero_grad()\n```\n\nTraining loop:\n```\nmodel.train()\ntrain_loss = torch.zeros((), device=device)\nfor k in range(gradient_accumulation_steps):\n    x, y = next(train_iter)\n    with autocast(xm.xla_device(), dtype=torch.bfloat16):\n        loss = model(x, y)\n    (loss / gradient_accumulation_steps).backward()\n    train_loss += loss.detach()\n    # xm.mark_step()\n\ntorch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n\nxm.optimizer_step(muon, barrier=True)\nxm.optimizer_step(adamw, barrier=True)\n\nadamw.zero_grad()\nmuon.zero_grad()\n```\n\nWhat can I do to fix this OOM?\n\nEDIT: The OOM occurs during the first optimizer step. It does not matter if I swap the order of the optimizer steps, the OOM always occurs on the first one.",
    "author": "New-Skin-5064",
    "timestamp": "2025-09-12T15:04:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1nfgc8h/d_oom_when_using_gradient_accumulation/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nehy84",
    "title": "[D] Math foundations to understand Convergence proofs?",
    "content": "Good day everyone, recently I've become interested in proofs of convergence for federated (and non-federated) algorithms, something like what's seen in appendix A of the¬†[FedProx paper](https://anitksahu.github.io/FedProx.pdf)¬†(one page of it attached below)\n\nI managed to go through the proof once and learn things like first order convexity condition from random blogs, but I don't think I will be able to do serious math with hackjobs like that. I need to get my math foundations up to a level where I can write one such proof intuitively.\n\nSo my question is: What resources must I study to get my math foundations up to par? Convex optimization by Boyd doesn't go through convergence analysis at all and even the convex optimization books that do, none of them use expectations over the iteration to proof convergence. Thanks for your time\n\nhttps://preview.redd.it/481lxdf47lof1.png?width=793&amp;format=png&amp;auto=webp&amp;s=6771d3ffe8a533155aa145b2ec691181a30968b9\n\n",
    "author": "james_stevensson",
    "timestamp": "2025-09-11T12:22:13",
    "url": "https://reddit.com/r/MachineLearning/comments/1nehy84/d_math_foundations_to_understand_convergence/",
    "score": 26,
    "num_comments": 6,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ner3r7",
    "title": "[D] What model should I use for image matching and search use case?",
    "content": "Hi everyone,\n\nI‚Äôm working on some project where we need to process footprint scans (similar to fingerprints) and later be able to match or search a new scan against a database of existing ones. The pipeline is being built on AWS (S3, Glue, Athena, SageMaker, OpenSearch).\n\nThe key requirements are:\nImage matching / retrieval ‚Äì given a new footprint, find the closest match.\n\nRobustness ‚Äì handle rotation, scale changes, low-quality scans, or partial prints.\n\nEfficiency ‚Äì scalable to a large dataset, reasonable inference latency.\n\nI‚Äôm exploring options for the ML part and wondering what model to start with:\n\t\nThe end goal is to store embeddings in OpenSearch k-NN and run similarity search.\n\nHas anyone worked on a similar problem (biometrics, fingerprints, medical image matching)? Which model architecture would you recommend as a good starting point for training?\n\nThanks in advance!",
    "author": "Ok_Barnacle4840",
    "timestamp": "2025-09-11T19:03:57",
    "url": "https://reddit.com/r/MachineLearning/comments/1ner3r7/d_what_model_should_i_use_for_image_matching_and/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1neobe4",
    "title": "[D] Creating test cases for retrieval evaluation",
    "content": "I‚Äôm building a RAG system using research papers from the arXiv dataset. The dataset is filtered for AI-related papers (around 440k+ documents), and I want to evaluate the retrieval step.\n\nThe problem is, I‚Äôm not sure how to create test cases from the dataset itself. Manually going through 440k+ papers to write queries isn‚Äôt practical.\n\nDoes anyone know of good methods or resources for generating evaluation test cases automatically or any easier way from the dataset?",
    "author": "DryHat3296",
    "timestamp": "2025-09-11T16:48:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1neobe4/d_creating_test_cases_for_retrieval_evaluation/",
    "score": 8,
    "num_comments": 12,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1neccr0",
    "title": "[P] Semlib: LLM-powered Data Processing",
    "content": "I've been thinking a lot about semantic data processing recently. A lot of the attention in AI has been on agents and chatbots (e.g., Claude Code or Claude Desktop), and I think semantic data processing is not well-served by such tools (or frameworks designed for implementing such tools, like LangChain).\n\nAs I was working on some concrete semantic data processing problems and writing a lot of Python code (to call LLMs in a for loop, for example, and then adding more and more code to do things like I/O concurrency and caching), I wanted to figure out how to disentangle data processing pipeline logic from LLM orchestration. Functional programming primitives (map, reduce, etc.), common in data processing systems like MapReduce/Flume/Spark, seemed like a natural fit, so I implemented semantic versions of these operators. It's been pretty effective for the data processing tasks I've been trying to do.\n\nThis blog post (https://anishathalye.com/semlib/) shares some more details on the story here and elaborates what I like about this approach to semantic data processing. It also covers some of the related work in this area (like DocETL from Berkeley's EPIC Data Lab, LOTUS from Stanford and Berkeley, and Palimpzest from MIT's Data Systems Group).\n\nLike a lot of my past work, the software itself isn't all that fancy; but it might change the way you think!\n\nThe software is open-source at https://github.com/anishathalye/semlib. I'm very curious to hear the community's thoughts!",
    "author": "anishathalye",
    "timestamp": "2025-09-11T08:49:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1neccr0/p_semlib_llmpowered_data_processing/",
    "score": 19,
    "num_comments": 6,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ndo5md",
    "title": "[D]NVIDIA Blackwell Ultra crushes MLPerf",
    "content": "NVIDIA dropped MLPerf results for Blackwell Ultra yesterday. 5√ó throughput on DeepSeek-R1, record runs on Llama 3.1 and Whisper, plus some clever tricks like FP8 KV-cache and disaggregated serving. The raw numbers are insane.\n\nBut I wonder though . If these benchmark wins actually translate into lower real-world inference costs.\n\nIn practice, workloads are bursty. GPUs sit idle, batching only helps if you have steady traffic, and orchestration across models is messy. You can have the fastest chip in the world, but if 70% of the time it‚Äôs underutilized, the economics don‚Äôt look so great to me. IMO",
    "author": "pmv143",
    "timestamp": "2025-09-10T12:39:51",
    "url": "https://reddit.com/r/MachineLearning/comments/1ndo5md/dnvidia_blackwell_ultra_crushes_mlperf/",
    "score": 52,
    "num_comments": 16,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ndulfv",
    "title": "[D] The best way to structure data for a predictive model of corporate delinquency",
    "content": "I have annual financial indicators for thousands of clients (businesses), their credit data, and delinquency data, and I want to use this data to create a predictive model.\n\nBut what's the best way to structure the data?\n\n* Take the annual financial data and associate it with the following year's delinquency data. So, for example, data from 2024 will predict delinquency in 2025.\n\nOR\n\n* Group by client and calculate the average, maximum, and minimum of the financial data to see if this data can predict delinquency.",
    "author": "drv29",
    "timestamp": "2025-09-10T17:15:50",
    "url": "https://reddit.com/r/MachineLearning/comments/1ndulfv/d_the_best_way_to_structure_data_for_a_predictive/",
    "score": 4,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ndtey6",
    "title": "[D] Having trouble organising massive CSV files for your machine learning models?",
    "content": "I've been fighting with CSVs from our high end power quality meter from a very reputable instrument company. \n\nThe CSV files come out from the unit immediately unusable and at 2 million samples per second its a huge dataset, and we take lots of measurements. I made some scripts go clean it but its still a mission every time that I dread to get to the good bit. ",
    "author": "grabber500",
    "timestamp": "2025-09-10T16:20:02",
    "url": "https://reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/",
    "score": 5,
    "num_comments": 21,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ndaesz",
    "title": "[D] SOTA modern alternative to BertScore?",
    "content": "Hi everyone,  \nI‚Äôm looking for an embedding-based metric to score text generation. BertScore is great, but it‚Äôs a bit outdated. Could you suggest some modern state-of-the-art alternatives?\n\n",
    "author": "Soft-Possibility2929",
    "timestamp": "2025-09-10T03:22:58",
    "url": "https://reddit.com/r/MachineLearning/comments/1ndaesz/d_sota_modern_alternative_to_bertscore/",
    "score": 15,
    "num_comments": 4,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ndajmq",
    "title": "[D] Questions on Fairness and Expectations in Top-Tier Conference Submissions",
    "content": "Hello everyone,\n\nI know that in this community there are many experienced researchers and even reviewers for top-tier conferences. As a young researcher, I sincerely hope to learn from your perspectives and get some clarity on a few concerns I‚Äôve been struggling with.\n\n**My first question:**  \nDoes a research paper always need to achieve *state-of-the-art (SOTA)* results‚Äîoutperforming every existing method‚Äîto be accepted at an A\\* conference? I often feel that so many published papers present dazzling results, making it nearly impossible for newcomers to surpass them.\n\n**My second question, about fairness and accuracy in comparisons:**  \nWhen evaluating a new method, is it acceptable to compare primarily against the most ‚Äúrelated,‚Äù ‚Äúsimilar,‚Äù or ‚Äúsame-family‚Äù methods rather than the absolute SOTA? For example:\n\n* If I make a small modification to the Bagging procedure in Random Forest, would it be fair to compare only against other Bagging-based forests, rather than something fundamentally different like XGBoost (which is boosting-based)?\n* Similarly, if I improve a variant of SVM, is it reasonable to compare mainly with other margin-based or kernel methods, instead of tree-based models like Decision Trees?\n\nI understand that if my method only beats some similar baselines but does not surpass the global best-performing method, reviewers might see it as ‚Äúmeaningless‚Äù (since people naturally gravitate toward the top method). Still, I‚Äôd like to hear your thoughts: from an experienced researcher‚Äôs point of view, what is considered fair and convincing in such comparisons?\n\nThank you very much in advance for your time and advice.",
    "author": "Feuilius",
    "timestamp": "2025-09-10T03:30:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1ndajmq/d_questions_on_fairness_and_expectations_in/",
    "score": 7,
    "num_comments": 6,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ndbzb7",
    "title": "[D] ICCV 2025 registration",
    "content": "Two years ago at Paris I had a workshop paper, I purchased the workshop entrance ticket, everything is okay.\n\nThis year I have done the same and now I am receiving emails saying only a full conference entrance is considered an author registration for a workshop paper. \n\nI did see the website is slightly different this year but still‚Ä¶ the code of conduct did not explain this clearly, does anyone have better insights for me?",
    "author": "ScaryCommission7829",
    "timestamp": "2025-09-10T04:48:37",
    "url": "https://reddit.com/r/MachineLearning/comments/1ndbzb7/d_iccv_2025_registration/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ncu0na",
    "title": "[D] IJCNLP-AACL 2025: Paper Reviews (ARR July 2025 Cycle)",
    "content": "The ARR July cycle reviews for AACL-IJCNLP 2025 just dropped.  \nFeel free to share your thoughts and feelings! How did you do?",
    "author": "Starscream-11813",
    "timestamp": "2025-09-09T13:23:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1ncu0na/d_ijcnlpaacl_2025_paper_reviews_arr_july_2025/",
    "score": 26,
    "num_comments": 52,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ncdt5o",
    "title": "[P] Implementation and ablation study of the Hierarchical Reasoning Model (HRM): what really drives performance?",
    "content": "I recently implemented the [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734) (HRM) for educational purposes and applied it to a simple pathfinding task. You can watch the model solve boards step by step in the generated animated GIF.\n\nHRM is inspired by multi-timescale processing in the brain: a slower H module for abstract planning and a faster L module for low-level computation, both based on self-attention. HRM is an attempt to model reasoning in latent space.\n\nTo understand a bit better what drives the performance I ran a small ablation study. Key findings (full results in the README):\n\n* The biggest driver of performance (both accuracy and refinement ability) is training with more segments (outer-loop refinement), not architecture.\n* The two-timescale H/L architecture performs about the same as a single-module trained with BPTT.\n* Notably, H/L still achieves good performance/refinement without full BPTT, which could mean cheaper training.\n\nRepo: [https://github.com/krychu/hrm](https://github.com/krychu/hrm)\n\nThis is of course a limited study on a relatively simple task, but I thought the results might be interesting to others exploring reasoning models.\n\nThe findings line up with the ARC Prize team's analysis: [https://arcprize.org/blog/hrm-analysis](https://arcprize.org/blog/hrm-analysis)\n\nBelow two examples of refinement in action: early steps explore solution with rough guesses, later steps make smaller and smaller corrections until the full path emerges:\n\n[20x20 board](https://i.redd.it/i1qi4l2vs3of1.gif)\n\n[30x30 board](https://i.redd.it/j6fpueovs3of1.gif)\n\n",
    "author": "krychu",
    "timestamp": "2025-09-09T01:59:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1ncdt5o/p_implementation_and_ablation_study_of_the/",
    "score": 74,
    "num_comments": 10,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ncemg3",
    "title": "[D] What‚Äôs the most frustrating ‚Äústuck‚Äù moment you‚Äôve faced in an ML project?",
    "content": "Curious about community experience: what‚Äôs the most painful ‚Äòstuck‚Äô moment you‚Äôve faced in an ML project (convergence, dataset issues, infra)?  \nHow did you eventually move past it, or did you abandon the attempt? Would be great to hear real war stories beyond published papers.",
    "author": "ExtentBroad3006",
    "timestamp": "2025-09-09T02:52:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1ncemg3/d_whats_the_most_frustrating_stuck_moment_youve/",
    "score": 32,
    "num_comments": 37,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ncceqw",
    "title": "[D] Best ocr as of now",
    "content": "I want to know which ocr has high accuracy and consumes less time for the extraction of data for given input images (especially tables), anything which works better than paddleocr?",
    "author": "Coffeee_addictt",
    "timestamp": "2025-09-09T00:23:46",
    "url": "https://reddit.com/r/MachineLearning/comments/1ncceqw/d_best_ocr_as_of_now/",
    "score": 20,
    "num_comments": 15,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nc5jb5",
    "title": "[R] LLMs play a cooperative card game, coordination without communication",
    "content": "One of my favorite card games is called The Crew, which is a trick-taking game (like hearts) but cooperative. There's no table talk allowed, players have to coordinate silently (with limited options for in-game communication) - figuring out what their teammates are doing and why, and what they need to do to work together. I wondered what SOTA LLMs would do if you asked them to play. To make this work, I implemented a backend for the game logic and structured outputs so models play by submitting moves and reasoning at each turn. \n\nOriginally I wanted to re-create the 50 mission campaign, but models were so spotty on  mission 1 (the simplest possible mission) that I stuck to mission 1 and experimented with different configurations instead. I ran 8 OpenAI models on 10 different versions, ranging from very easy (random chance gets you there 2/3rds of the time) to very hard (random chance succeeds 0.5%), and gave each model ten trials on each mission.\n\nWhat I've found out:\n\n\\* Smaller models struggle both with gameplay, and with understanding their role on the team. In these missions, a designated player (the commander) has to win a designated card. But these models hate having to lose a trick for the sake of their teammate, even when that's how they win the game.\n\n[This does not \\\\\"help him secure the win and fulfill his task.\\\\\" It loses the game.](https://preview.redd.it/3lqyqf3tg1of1.png?width=2030&amp;format=png&amp;auto=webp&amp;s=b57c0a46fee169e14dbf6fc0cda107024a11a59e)\n\n\\* GPT-4o-mini (worst model so far) plays randomly on easy setups and worse than randomly on harder ones. GPT-4o-mini in particular loses the game in the first turn almost 90% of the time in harder setups with GPT-5-nano and GPT-4.1-mini are close behind at 60-70%. \n\n[GREEN 1 is the lowest GREEN card in the game, so playing it straight away actually guarantees immediate failure.](https://preview.redd.it/fx5jqyhug1of1.png?width=2046&amp;format=png&amp;auto=webp&amp;s=da5d4abb5a7fcd4c1e8ee42c09d7acfb4a7ba5dc)\n\n\\* GPT-5 is self-aware enough to avoid the \"losing on the very first turn\" error, but actually did it on purpose once as a deliberate suicide when it saw that it couldn't win the game on the very first turn.\n\n[There are multiple turns in the game!](https://preview.redd.it/91qnnfuvg1of1.jpg?width=1900&amp;format=pjpg&amp;auto=webp&amp;s=ebc98a3fbf4381c4a95f7e96ec2fa96f8e84692f)\n\n\\* The harder missions - which require coordination across multiple turns - absolutely cook the smaller models with &lt;10% win rates. Only GPT-5 is beating random chance on the harder missions (73% GPT-5 vs 4% random) \n\n\\* GPT-5 also found optimal 1-trick solutions to a couple of setups I thought required at least two tricks. Oops. So in a sense, we're above human performance in some areas.\n\n\\* ...But most of the time, GPT-5 generally screwed around for 3 or more tricks in puzzles it could have solved in 1. This is like solving a mate in one chess puzzle in 3 moves. It's not losing, but it's not exactly showing a mastery of the game.\n\n\\* The lack of goal-oriented behavior (or risk-averse hesitation) on GPT-5's part means that GPT-5-mini actually performs better if we count speed (number of turns) to win as criteria and grade on optimal play (winning in the least number of turns, rather than just winning.)\n\nI published the repo and did a write-up with some graphs and demos here: [https://ekkarpinski.github.io/LLMCrew/](https://ekkarpinski.github.io/LLMCrew/)\n\n",
    "author": "ekkarpinski",
    "timestamp": "2025-09-08T18:08:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1nc5jb5/r_llms_play_a_cooperative_card_game_coordination/",
    "score": 46,
    "num_comments": 13,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ncrkpp",
    "title": "[D] Negative R¬≤ on unseen dataset despite good train/test performance",
    "content": "I am working on a regression problem where I predict Pavement Condition Index (PCI) values from multi-sensor time-series data collected in the same region and under the same conditions. I have multiple sets of data from the same collection process, where I use some sets for training and testing and keep the remaining ones for evaluating generalization. Within the training and testing sets, the model performs well, but when I test on the held-out dataset from the same collection, the R¬≤ value often becomes negative , even though the mean absolute error and root mean square error remain reasonable. I have experimented with several feature engineering strategies, including section-based, time-based, and distance-based windowing, and I have tried using raw PCI data as well. I also tested different window lengths and overlap percentages, but the results remain inconsistent. I use the same data for a classification task, the models perform very well and generalize properly, yet for PCI regression, the generalization fails despite using the same features and data source. In some cases, removing features like latitude, longitude, or timestamps caused performance to drop significantly, which raises concerns that the model might be unintentionally relying on location and time information instead of learning meaningful patterns from sensor signals. I have also experimented with different models, including traditional machine learning and deep learning approaches, but the issue persists. I suspect the problem may be related to the variance of the target PCI values across datasets, potential data leakage caused by overlapping windows, or possibly a methodological flaw in how the evaluation is performed. I want to understand whether it is common in research to report only the R¬≤ values on the train/test splits from the same dataset, or whether researchers typically validate on entirely separate held-out sets as well. Given that classification on the same data works fine but regression fails to generalize, I am trying to figure out if this is expected behavior in PCI regression tasks or if I need to reconsider my entire evaluation strategy.",
    "author": "Sami10644",
    "timestamp": "2025-09-09T11:54:32",
    "url": "https://reddit.com/r/MachineLearning/comments/1ncrkpp/d_negative_r¬≤_on_unseen_dataset_despite_good/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nc6r7l",
    "title": "[Project] Otters ü¶¶ - A minimal vector search library with powerful metadata filtering",
    "content": "I'm excited to share something I've been working on for the past few weeks:\n\nOtters ü¶¶ - A minimal vector search library with powerful metadata filtering powered by an ergonomic Polars-like expressions API written in Rust!\n\nWhy I Built This\n\nIn my day-to-day work, I kept hitting the same problem. I needed vector search with sophisticated metadata filtering, but existing solutions were either,\nToo bloated (full vector databases when I needed something minimal for analysis)\nLimited in filtering capabilities\nHad unintuitive APIs that I was not happy about.\n\nI wanted something minimal, fast, and with an API that feels natural - inspired by Polars, which I absolutely love.\n\nWhat Makes Otters Different\n\nExact Search: Perfect for small-to-medium datasets (up to ~10M vectors) where accuracy matters more than massive scale.\n\n Performance: \nSIMD-accelerated scoring\nZonemaps and Bloom filters for intelligent chunk pruning\n\nPolars-Inspired API: Write filters as simple expressions\n```\nmeta_store.query(query_vec, Metric::Cosine)\n    .meta_filter(col(\"price\").lt(100) &amp; col(\"category\").eq(\"books\"))\n    .vec_filter(0.8, Cmp::Gt)\n    .take(10)\n    .collect()\n```\n\nThe library is in very early stages and there are tons of features that i want to add\nPython bindings, NumPy support\nSerialization and persistence\nParquet / Arrow integration\nVector quantization\netc.\n\nI'm primarily a Python/JAX/PyTorch developer, so diving into rust programming has been an incredible learning experience.\n\nIf you think this is interesting and worth your time, please give it a try.\nI welcome contributions and feedback !\n\nüì¶ https://crates.io/crates/otters-rs\nüîó https://github.com/AtharvBhat/otters",
    "author": "AtharvBhat",
    "timestamp": "2025-09-08T19:05:38",
    "url": "https://reddit.com/r/MachineLearning/comments/1nc6r7l/project_otters_a_minimal_vector_search_library/",
    "score": 18,
    "num_comments": 3,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ncyf2r",
    "title": "[D] Completed Amazon ML Summer School 2025 curious who else attended?",
    "content": "Hey everyone,  \nI just completed¬†**Amazon ML Summer School 2025**¬†üéâ  \nIt was a month-long program covering a solid range of ML topics¬†***supervised/unsupervised learning, deep neural nets, generative AI &amp; LLMs, RL, and even causal inference***.  \nThe sessions were intense but super rewarding. I feel like this experience gave me a strong foundation to explore advanced AI research and projects.\n\nCurious if anyone here has also attended and how you re planning to apply what you learned?\n\nhttps://preview.redd.it/b5ulzuq038of1.png?width=655&amp;format=png&amp;auto=webp&amp;s=c328f24e6b674b9f576cebae727f44a526f185a9\n\n",
    "author": "United_Intention42",
    "timestamp": "2025-09-09T16:21:09",
    "url": "https://reddit.com/r/MachineLearning/comments/1ncyf2r/d_completed_amazon_ml_summer_school_2025_curious/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.3,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nbhqmq",
    "title": "[D] How do you stay current with AI/ML research and tools in 2025? (Cybersec engineer catching up after Transformers)",
    "content": "Hi everyone,\n\nI‚Äôm a cybersecurity and network engineer/sysadmin by profession, but I studied AI/ML quite seriously at university. My knowledge is solid up until around the Transformer era (when attention-based models started becoming central), but I stopped following developments after that.\n\nNow I‚Äôd like to get back into the field and stay current‚Äînot necessarily to publish research, but to understand new architectures, applications, and tools. In cybersecurity, I stay updated through curated blogs, newsletters, and professional communities. I‚Äôd like to adopt a similar approach for ML/AI.\n\nFor those of you who actively track progress:\n\n* Which blogs, newsletters, or feeds do you find most useful?\n* Are there particular researchers or labs whose updates you follow?\n* Any books or surveys that bridge foundational knowledge with current trends?\n* How do you cut through hype-heavy content and focus on signal?\n\nI‚Äôd really appreciate hearing what works for you. The field moves incredibly fast, and I‚Äôd like to plug back in with a structured approach.\n\nThanks in advance!",
    "author": "Set-New",
    "timestamp": "2025-09-08T00:58:03",
    "url": "https://reddit.com/r/MachineLearning/comments/1nbhqmq/d_how_do_you_stay_current_with_aiml_research_and/",
    "score": 113,
    "num_comments": 20,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nbsems",
    "title": "[D] AAAI 26 Alignment Track",
    "content": "Does anyone know whether they‚Äôre going to release the Phase 1 rejections today or on September 12?",
    "author": "Senior-Let-7576",
    "timestamp": "2025-09-08T09:27:49",
    "url": "https://reddit.com/r/MachineLearning/comments/1nbsems/d_aaai_26_alignment_track/",
    "score": 18,
    "num_comments": 90,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nc1mxq",
    "title": "[Project] Phishing URL detection with Random Forests and handcrafted features",
    "content": "**\\[Project\\] Phishing URL detection with Random Forests on handcrafted features** \n\nI recently finished a project where I trained and deployed a phishing URL detector using **traditional ML techniques**. The goal was to explore how far a lightweight, interpretable model could go for this problem before moving to deep learning.\n\n**Data &amp; Features**\n\n* Dataset: Combined PhishTank + Kaggle phishing URLs with Alexa top legitimate domains.\n* Preprocessing: Removed duplicates, balanced classes, stratified train/test split.\n* Features (hand-engineered):\n   * URL length &amp; token counts\n   * Number of subdomains, ‚Äú@‚Äù usage, hyphens, digits\n   * Presence of IP addresses instead of domains\n   * Keyword-based flags (e.g., ‚Äúlogin‚Äù, ‚Äúsecure‚Äù)\n\n**Model &amp; Training**\n\n* Algorithm: Random Forest (scikit-learn).\n* Training: 80/20 split, 10-fold CV for validation.\n* Performance: \\~92% accuracy on test data.\n* Feature importance: URL length, IP usage, and hyphen frequency were the strongest predictors.\n\n**Takeaways**\n\n* A simple RF + handcrafted features still performs surprisingly well on phishing detection.\n* Interpretability (feature importances) adds practical value in a security context.\n* Obvious limitations: feature set is static, adversaries can adapt.\n\n**Future work (exploration planned)**\n\n* Gradient boosting (XGBoost/LightGBM) for comparison.\n* Transformers or CNNs on raw URL strings (to capture deeper patterns).\n* Automating retraining pipelines with fresh phishing feeds.\n\n**Repo:** [https://github.com/saturn-16/AI-Phishing-Detection-Web-App](https://github.com/saturn-16/AI-Phishing-Detection-Web-App)\n\nWould love feedback on:\n\n* What other URL features might improve detection?\n* Have people here seen significant gains moving from RF/GBM ‚Üí deep learning for this type of task?",
    "author": "Acceptable_Army_6472",
    "timestamp": "2025-09-08T15:16:23",
    "url": "https://reddit.com/r/MachineLearning/comments/1nc1mxq/project_phishing_url_detection_with_random/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nbisbw",
    "title": "[D] How to Automate parsing of Bank Statement PDFs to extract transaction level data",
    "content": "I am working on a project where I need to extract transaction data from Bank Statement PDFs. 80% of my working PDFs are digitally generated so to handle those I put the Regex approach, where I first extract the text into a txt file and then run Regex on this data to extract data in a meaningful format \\[Date, Particulars, Credit/Debit amount, Balance\\]. The challenge is that the Regex approach is brittle, and very sensitive to formats. So every bank requires a new Regex plus any little change in the format tomorrow by the bank will break the pipeline.\n\nI want to make a pipeline which is agnostic to bank-format and is capable of extracting the info from the PDFs. I cannot use any 3rd party APIs as the bank data is sensitive and we want to keep everything on internal servers.\n\nHence, I have been exploring ways in Open Source models to built this pipeline. After doing some research, I landed on LayoutLMv3 Model which can essentially label the Tokens based on their location on the page so if we are able to train the model on our data it should be able to tag every token on the page and that should do it, but the challenge here is that this model is sensitive to reading order and fails on few bank formats.\n\nSince then I have explored MinerU but that failed as well, it isolated the transaction content table but later failed to extract data in orderly fashion as it could not differentiate between multiple lines of transactions.\n\nNow I am working with YOLOv8 which I am training to identify transaction rows and amount columns using BBox and then I will pull the info from these BBox intersection. But the confidence here is not very high.\n\nHas anyone here faced similar challenge? Can anyone help me with some solution or approach. It would be a great help!\n\nKnow that the most of the PDFs don't have any defined table, it's just text hanging in air with lot of whitespace. I need a solve for Scanned PDFs as well \\[integrated with OCR\\]",
    "author": "Anmol_garwal",
    "timestamp": "2025-09-08T02:07:14",
    "url": "https://reddit.com/r/MachineLearning/comments/1nbisbw/d_how_to_automate_parsing_of_bank_statement_pdfs/",
    "score": 6,
    "num_comments": 18,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nbr57g",
    "title": "[R] Benchmarking an ML service in python",
    "content": "Recently, I needed to build an ML service that would be called by a latency-sensitive client. The requirements for load and latency were higher than what I had worked with in the past, so I wasn‚Äôt sure what to expect from my Python application.\n\nI googled around and couldn‚Äôt find any concrete answers, so I wrote this brief article for anyone out there in a similar situation:\n\nhttps://medium.com/@javiermas/benchmarking-an-ml-service-in-pytho-4238399d2229\n\nI hope you find it useful!\n",
    "author": "Technical-Seesaw9383",
    "timestamp": "2025-09-08T08:40:58",
    "url": "https://reddit.com/r/MachineLearning/comments/1nbr57g/r_benchmarking_an_ml_service_in_python/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1naz0eb",
    "title": "[D] Vibe-coding and structure when writing ML experiments",
    "content": "Hey!\n\nFor context, I'm a Master's student at ETH Z√ºrich. A friend and I recently tried writing a paper for a NeurIPS workshop, but ran into some issues.  \nWe had both a lot on our plate and probably used LLMs a bit too much. When evaluating our models, close to the deadline, we caught up on some bugs that made the data unreliable. We also had plenty of those bugs along the way. I feel like we shot ourselves in the foot but that's a lesson learned the way. Also, it made me realise the negative effects it could have had if those bugs had been kept uncaught.\n\nI've been interning in some big tech companies, and so I have rather high-standard for clean code. Keeping up with those standards would be unproductive at our scale, but I must say I've struggled finding a middle ground between speed of execution and code's reliability.\n\nFor researchers on this sub, do you use LLMs at all when writing ML experiments? If yes, how much so? Any structure you follow for effective experimentation (writing (ugly) code is not always my favorite part)? When doing experimentation, what structure do you tend to follow w.r.t collaboration?\n\nThank you :)\n\n",
    "author": "Lestode",
    "timestamp": "2025-09-07T10:19:16",
    "url": "https://reddit.com/r/MachineLearning/comments/1naz0eb/d_vibecoding_and_structure_when_writing_ml/",
    "score": 26,
    "num_comments": 33,
    "upvote_ratio": 0.68,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1namvsk",
    "title": "Why Language Models Hallucinate - OpenAi pseudo paper - [D]",
    "content": "Hey\nAnybody read this ? It seems rather obvious and low quality, or am I missing something ? \n\nhttps://openai.com/index/why-language-models-hallucinate/\n\n‚ÄúAt OpenAI, we‚Äôre working hard to make AI systems more useful and reliable. Even as language models become more capable, one challenge remains stubbornly hard to fully solve: hallucinations. By this we mean instances where a model confidently generates an answer that isn‚Äôt true. Our new research paper‚Å†(opens in a new window) argues that language models hallucinate because standard training and evaluation procedures reward guessing over acknowledging uncertainty.\nChatGPT also hallucinates. GPT‚Äë5 has significantly fewer hallucinations especially when reasoning‚Å†, but they still occur. Hallucinations remain a fundamental challenge for all large language models, but we are working hard to further reduce them.‚Äù",
    "author": "OkOwl6744",
    "timestamp": "2025-09-07T00:21:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1namvsk/why_language_models_hallucinate_openai_pseudo/",
    "score": 118,
    "num_comments": 55,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1naejuk",
    "title": "[D] The apparent randomness of residual block design",
    "content": "Skip connections and residual blocks have been ubiquitous in the ML field ever since the original ResNets were published. I think it's fair to say most people agree skip connections help, but at a glance, the design of the residual blocks themselves is still something that differs from paper to paper.\n\nThe most recent \"innovation\" is splitting channel mixing from spatial mixing, which is what ConvNeXt does in an attempt to mimic transformers. Other models that also claim SotA-ish performance, however, do not necessarily follow suit. NFNet, for example, employs grouped 3x3 convolution layers, good old normal bottlenecks (not inverted) and channel attention (Squeeze-and-Excitation).\n\nIf we look at modern LLMs, they all have residual blocks that look very similar, but with one or two minor differences that often look arbitrary.\n\nI think residual block design is one of those things that people don't really pay much attention to since it generally works well enough regardless of what you do, but at some point it does look like we're just making semi-random decisions based on semi-random observations. Why the block is designed in the way it is is rarely a point of concern.\n\nI've tried looking for papers making direct comparisons between different design choices, but I couldn't really find anything conclusive.\n\n",
    "author": "Artoriuz",
    "timestamp": "2025-09-06T16:51:49",
    "url": "https://reddit.com/r/MachineLearning/comments/1naejuk/d_the_apparent_randomness_of_residual_block_design/",
    "score": 68,
    "num_comments": 9,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nanw9i",
    "title": "[P] Terra Code CLI ‚Äì An AI coding assistant with domain knowledge and semantic code search",
    "content": "One limitation I‚Äôve noticed with most AI coding assistants is that they don‚Äôt really understand a team‚Äôs domain knowledge or architectural decisions.\n\nTo explore this, we built a small CLI project: Terra Code CLI. The idea was to see if an assistant could feel more like a senior developer who knows the org, rather than just autocomplete.\n\nThings we experimented with:\n‚Ä¢ Interactive Knowledge Transfer ‚Äì let senior devs ‚Äúteach‚Äù patterns\n‚Ä¢ Semantic Code Search ‚Äì context-aware retrieval across repos\n‚Ä¢ Persistent Memory ‚Äì standards remembered across projects\n‚Ä¢ Domain Expertise ‚Äì ingesting architecture docs, API specs, etc.\n\nWe‚Äôre curious:\nüëâ Has anyone here tried giving AI assistants persistent org-specific knowledge? Did it actually help productivity, or just add complexity?\n\nFor free quick start:\n\nnpm install -g @terra-code/terra-code\n\nterra\n\nFor those interested, we‚Äôve open-sourced the CLI [ https://github.com/TerraAGI/terra-code-cli ]. There‚Äôs also a simple website which we will be updating with docs + install guide here: [ https://terra-agi.com/ ]. Currently in beta, so it‚Äôs free to use.",
    "author": "prabhjots665",
    "timestamp": "2025-09-07T01:26:54",
    "url": "https://reddit.com/r/MachineLearning/comments/1nanw9i/p_terra_code_cli_an_ai_coding_assistant_with/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1nahnmz",
    "title": "[D] Thought experiment: ‚ÄúRolling without slipping‚Äù as a blueprint for nD‚Üí(n‚àí1) embeddings?",
    "content": "I came across the recent ROLLING HONED paper (designing 3D shapes that, when rolling without slipping, trace arbitrary 2D paths). It got me thinking:\n\nIn 3D, rolling constraints let you encode a 2D trajectory into the geometry of a 3D body.\n\nIn principle, in 4D you could imagine a convex hypersurface rolling on a 3D hyperplane, tracing out a 3D trajectory.\n\nMore generally: could there be a systematic way to map nD data into (n‚àí1)D dynamics via such constraints?\n\nI know in ML we already have PCA, autoencoders, product quantization, etc. ‚Äî and those actually preserve metrics we care about. My hunch is that this ‚Äúmechanical embedding‚Äù idea probably fails the usefulness test for similarity search (no guarantee of inner product preservation).\n\nBut still:\n\nDoes the analogy make any theoretical sense in higher dimensions (rolling manifolds w/o slip/twist)?\n\nCould there be hidden value in treating ‚Äúconstrained dynamics‚Äù as a new kind of coding scheme?\n\nOr am I over-romanticizing a neat geometric trick after too much late-night reading?\n\nCurious what the community thinks ‚Äî is there any research potential here, or should I file this under ‚Äúfun alcohol-fueled metaphors‚Äù and move on?\n",
    "author": "absurdistonvacation",
    "timestamp": "2025-09-06T19:24:44",
    "url": "https://reddit.com/r/MachineLearning/comments/1nahnmz/d_thought_experiment_rolling_without_slipping_as/",
    "score": 5,
    "num_comments": 6,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n9spn2",
    "title": "[D] Advice on handling completely incorrect review?",
    "content": "Recently submitted a paper to WACV 2026. Two of the three reviews are positive. The third recommends rejection, citing items as ‚Äúmissing‚Äù that are actually in the paper (2nd page dude) and claiming our architecture is identical to a 2022 model, though there are clear differences- moreover, the performances tend to drastically differ as showcased in the results.\n\nWhat are the typical options in this situation? He seems to be inclined towards finding \"excuses\" for rejecting paper (not sure why) and thereby I doubt a rebuttal will help. Can I ask the AC to get the reviewer replaced?",
    "author": "Forsaken-Order-7376",
    "timestamp": "2025-09-05T23:45:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1n9spn2/d_advice_on_handling_completely_incorrect_review/",
    "score": 14,
    "num_comments": 6,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1na5ixj",
    "title": "[D]Baseten raises $150M Series D for inference infra. where‚Äôs the real bottleneck?",
    "content": "Baseten just raised $150M Series D at a $2.1B valuation. They focus on inference infra  like low latency serving, throughput optimization, developer experience.\n\nThey‚Äôve shared benchmarks showing their embeddings inference outperforms vLLM and TEI, especially on throughput and latency. The bet is that inference infra is the pain point, not training.\n\nBut this raises a bigger question. what‚Äôs the real bottleneck in inference?\n\t‚Ä¢Baseten and others (Fireworks, Together) are competing on latency + throughput.\n\t‚Ä¢Some argue the bigger cost sink is cold starts and low GPU utilization , serving multiple models elastically without waste is still unsolved at scale.\n\nI wonder what everyone thinks \n\n\t‚Ä¢Will latency/throughput optimizations be enough to differentiate?\n\t‚Ä¢Or is utilization (how efficiently GPUs are used across workloads) the deeper bottleneck?\n\t‚Ä¢Does inference infra end up commoditized like training infra, or is there still room for defensible platforms?\n",
    "author": "pmv143",
    "timestamp": "2025-09-06T10:33:06",
    "url": "https://reddit.com/r/MachineLearning/comments/1na5ixj/dbaseten_raises_150m_series_d_for_inference_infra/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n9wnel",
    "title": "[P] An Open-Source Pipeline for Speech-to-Speech Translation with Voice Preservation (RVC) and Lip-Sync",
    "content": "Hello¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/),\n\nI'm a final-year undergrad exploring multimodal systems, and I wanted to share a project I've built and open-sourced. It‚Äôs an end-to-end pipeline designed to tackle video dubbing for low-resource languages, using Telugu as the initial target. The system translates speech from an English video while preserving the original speaker's vocal identity and syncing their lips to the new audio.\n\n* **GitHub Repo:**¬†[\\[GitHub\\]](https://github.com/M-SRIKAR-VARDHAN/speech-to-speech-with-lipsync)\n* **Full Technical Write-up:**¬†[\\[writeup\\]](https://medium.com/@srikarvardhan2005/speech-to-speech-translation-with-lip-sync-425d8bb74530)\n* **Demo Video:**¬†[\\[Demo\\]](https://drive.google.com/drive/folders/1l6jZEDdmUzr9VhfYkvoVdaXJSSipN-nm?usp=sharing)\n\nThe core technical challenge was achieving voice preservation without access to large, speaker-specific datasets typically required for high-fidelity voice cloning. After a dead-end attempting a direct S2S architecture inspired by Translatotron, I found that using Retrieval-based Voice Conversion (RVC) as a post-processing step on a generic TTS output was a surprisingly practical and data-efficient solution.\n\nThe final pipeline is structured as follows:\n\n1. **ASR:**¬†Whisper for robust transcription.\n2. **NMT:**¬†Meta's NLLB for English-to-Telugu translation.\n3. **TTS:**¬†Meta's MMS model to synthesize the base Telugu audio.\n4. **Voice Conversion:**¬†A trained RVC model converts the timbre of the synthetic speech to match the original speaker.\n5. **Lip Sync:**¬†Wav2Lip aligns the video frames to the new audio.\n\nMy main takeaway is that RVC seems to function as a very effective \"style transfer\" layer for voice, making it a viable tool for projects where full voice cloning is computationally or data-prohibitive.\n\nI'm sharing this to start a discussion and get feedback from the community on this approach. I'm particularly curious about two points:\n\n1. Has anyone else experimented with using RVC in a more formal pipeline, and what were the qualitative limitations you encountered?\n2. Are there newer or more robust alternatives to Wav2Lip for lip-syncing that maintain good performance without requiring massive computational resources?\n\nAny thoughts on the architecture or suggestions for improvement would be highly appreciated. Thank you for your time.",
    "author": "Nearby_Reaction2947",
    "timestamp": "2025-09-06T03:57:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1n9wnel/p_an_opensource_pipeline_for_speechtospeech/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n9xg20",
    "title": "[D] Online hierarchical clustering for news: how to keep event IDs stable under merges/splits in a streaming pipeline?",
    "content": "I‚Äôm building a news ingestion system (currently Poland-focused; designed to scale) that clusters incoming articles into ‚Äúevents‚Äù powering maps and graph views. Pipeline: embeddings ‚Üí cosine HAC with a fixed threshold ‚Üí periodic (5min) recluster. Granularity, time decay, and summarization are fine, my sole pain point is¬†*stable event identity*¬†in a streaming setting.\n\nAs new articles arrive, clusters should sometimes merge (a legitimate bridge appears) or split (bridge was spurious). I need user-facing event IDs to persist through these transitions, i.e., minimize label churn across snapshots while respecting the hierarchical/threshold constraints.\n\n**Question:**¬†What‚Äôs the best-known algorithmic approach (and any open-source references) for¬†*evolutionary/streaming hierarchical clustering with persistent labels*, explicitly merge/split-aware, that¬†*minimizes an inter-snapshot ID-churn* *penalty*¬†under latency constraints?",
    "author": "local___host",
    "timestamp": "2025-09-06T04:43:06",
    "url": "https://reddit.com/r/MachineLearning/comments/1n9xg20/d_online_hierarchical_clustering_for_news_how_to/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n9hnq9",
    "title": "[D] Anyone successful with training LoRA for visual LLMs on a multi-GPU setup?",
    "content": "Hello sub,\n\nI'm trying to train a LoRA for Llama 3.2 90B Visual Instruct on a 8xA100 cluster but I cannot find a framework/package that supports it.\n\nModel is of course too large to fit into a single A100, so the only way is to leverage multiple device.\n\nUnsloth does not support multi GPU training (at least in its open version)  \nAxtol has multimodal models in beta\n\nWas any of you successful into training multimodal models of this size? I'd appreciate any kind of feedback.",
    "author": "KeyIsNull",
    "timestamp": "2025-09-05T14:30:46",
    "url": "https://reddit.com/r/MachineLearning/comments/1n9hnq9/d_anyone_successful_with_training_lora_for_visual/",
    "score": 12,
    "num_comments": 10,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n9ecmj",
    "title": "[D]  Anyone attending EUSIPCO next week?",
    "content": "Anyone attending EUSIPCO in Palermo next week? Unfortunately, none of my labmates will be able to travel, so would be cool to meet new people from here !",
    "author": "DeeplyConvoluted",
    "timestamp": "2025-09-05T12:19:07",
    "url": "https://reddit.com/r/MachineLearning/comments/1n9ecmj/d_anyone_attending_eusipco_next_week/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n947jj",
    "title": "[P] I Was Wrong About Complex ML Solutions - Gower Distance Beat My UMAP Approach",
    "content": "Four years ago, I built [DenseClus ](https://github.com/awslabs/amazon-denseclus)for mixed-data clustering using dual UMAP embeddings. After reflecting on the Zen of Python (\"simple is better than complex\"), I realized I was overengineering.\n\nGower (1971) computes distances for mixed categorical/numerical data using weighted averages of appropriate metrics. Despite being 50+ years old, it often outperforms complex embeddings for small-to-medium datasets.\n\nThe implementation I coded (with Claude's help) saw a 20% speedup, 40% in memory, has GPU support (CuPy) and Sklearn integration.\n\nCode: [https://github.com/momonga-ml/gower-express](https://github.com/momonga-ml/gower-express)\n\nBlog post with analysis: [https://charles-frenzel.medium.com/i-was-wrong-start-simple-then-move-to-more-complex-5e2f40765481](https://charles-frenzel.medium.com/i-was-wrong-start-simple-then-move-to-more-complex-5e2f40765481)\n\n**Discussion**:  When do you choose simple, interpretable methods over deep embeddings? Have others found similar success reverting to classical approaches?",
    "author": "Pitiful-Ad8345",
    "timestamp": "2025-09-05T05:41:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1n947jj/p_i_was_wrong_about_complex_ml_solutions_gower/",
    "score": 21,
    "num_comments": 12,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n8lvz5",
    "title": "[D] How do you read code with Hydra",
    "content": "[Hydra](https://hydra.cc/) has become a very popular in machine learning projects. I understand the appeal, it makes configurations modular, allows you to reuse some parts of it while changing another. It makes the code more reusable and modular too and if you understand all of it its better structured.\n\nMy big problem is it makes it damn well near impossible to read someone else's code since every part of the code is now some mysterious implicit thing that gets instantiated from a string in the config file during execution. The problem would be alleviated if there was a way of quickly accessing the definition of the object that will get instantiated at runtime at least with the default values of the config. Is there a plugin that does that? If not, how do you guys do it ?",
    "author": "Infinite_Explosion",
    "timestamp": "2025-09-04T13:53:22",
    "url": "https://reddit.com/r/MachineLearning/comments/1n8lvz5/d_how_do_you_read_code_with_hydra/",
    "score": 89,
    "num_comments": 35,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n95etu",
    "title": "[D] Reversed born again network because it's easier to train, is this stupid?",
    "content": "I want to implement this paper: [https://arxiv.org/pdf/1805.04770](https://arxiv.org/pdf/1805.04770)\n\nbut I'm not excited about having to manage the student models / save them independently and also there's the issue of cost because we'd have to train each student model from scratch.\n\nTo get around this I was thinking I could just do the inverse: train the teacher model and derive \"dark knowledge\" based on the \"incorrect\" logits of the last checkpoint.\n\nWhat I mean is can I have a training loop similar to the following\n\n    for epoch in range(10):\n      student = teacher.clone()\n      student.requires_grad_(False) # the student deliberately does not learn, only the teacher learns\n      for data in dataset:\n        optim.zero_grad()\n        teacher_logits = teacher(data.input)\n        student_logits = student(data.input)\n        loss_cross_entropy = cross_entropy(teacher_logits, data.label)\n        loss_dark_knowledge = cross_entropy(teacher_logits - student_logits, data.label)\n        loss = (loss_cross_entropy + loss_dark_knowledge) / 2\n        loss.backward()\n        optim.step()\n\nis this dumb?",
    "author": "Says_Watt",
    "timestamp": "2025-09-05T06:33:25",
    "url": "https://reddit.com/r/MachineLearning/comments/1n95etu/d_reversed_born_again_network_because_its_easier/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n918yb",
    "title": "[P] DCNv2 (Update Compatibility) Pytorch 2.8.0",
    "content": "Hello Reddit,\n\nWorking on several project I had to use the DCNv2 for different models I tweak it a little bit to work under the most recent CUDA version I had on my computer. There is probably some changes to make but currently it seems to work on my models training under CUDA 12.8 + Pytorch 2.8.0 configuration still haven't tested the retrocompatibility if anyone would like to give it a try.\n\nFeel free to use it for training model like YOLACT+, FairMOT or others.\n\n[https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main](https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main)",
    "author": "CaptainBudy",
    "timestamp": "2025-09-05T03:09:40",
    "url": "https://reddit.com/r/MachineLearning/comments/1n918yb/p_dcnv2_update_compatibility_pytorch_280/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n8po18",
    "title": "[R] The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs",
    "content": "Curious what folks think about this paper: [https://arxiv.org/abs/2508.08285](https://arxiv.org/abs/2508.08285)  \n  \nIn my own experience in hallucination-detection research, the other popular benchmarks are also low-signal, even the ones that don't suffer from the flaw highlighted in this work.\n\nOther common flaws in existing benchmarks:\n\n\\- Too synthetic, when the aim is to catch real high-stakes hallucinations in production LLM use-cases.\n\n\\- Full of incorrect annotations regarding whether each LLM response is correct or not, due to either low-quality human review or just relying on automated LLM-powered annotation.\n\n\\- Only considering responses generated by old LLMs, which are no longer representative of the type of mistakes that modern LLMs make.  \n  \nI think part of the challenge in this field is simply the overall difficulty of proper Evals.  For instance, Evals are much easier in multiple-choice / closed domains, but those aren't the settings where LLM hallucinations pose the biggest concern",
    "author": "jonas__m",
    "timestamp": "2025-09-04T16:30:36",
    "url": "https://reddit.com/r/MachineLearning/comments/1n8po18/r_the_illusion_of_progress_reevaluating/",
    "score": 29,
    "num_comments": 12,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n9m5hv",
    "title": "[D] Seeking arXiv endorsement",
    "content": "Hi All\n\nI‚Äôm preparing to submit to arXiv in Experimentation. Since this is my first submission, I need an endorsement.\n\nThe draft is ready and I can share it upon request. Thanks! \n",
    "author": "Specialist_Clock_368",
    "timestamp": "2025-09-05T17:51:26",
    "url": "https://reddit.com/r/MachineLearning/comments/1n9m5hv/d_seeking_arxiv_endorsement/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.07,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n83e6e",
    "title": "[D] Performance overhead of running ML inference in hardware-isolated environments - production metrics",
    "content": "Been collecting data on ML inference performance in trusted execution environments and thought the numbers might be useful for others dealing with similar constraints.\n\n**Context:** Fraud detection models processing ~10M daily transactions, needed hardware-level isolation for compliance reasons.\n\nAfter 3 months of production data, seeing 5-8% performance overhead compared to standard deployment. This is way better than the 30-40% overhead reported in older papers about SGX.\n\nThe interesting technical challenge was memory management. TEE environments have strict memory limits and different allocation patterns than standard containers. Had to completely rewrite our batching logic - what worked fine with dynamic batching in regular pods caused constant OOM errors in enclaves.\n\n**Model optimization discoveries:**\n\n- ONNX runtime worked, pytorch was too memory heavy\n- Preprocessing became the bottleneck, not inference\n- Had to keep models under 8GB total memory\n- P95 latency went from 12ms to 13ms\n\nTried multiple approaches including raw SGX implementation and phala's abstraction layer. The attestation complexity alone makes raw implementation painful.\n\n**For those working on similar problems:**\nProfile your entire pipeline, not just model inference. Data transformation overhead in isolated environments is real.\n\n**Technical question for the community:** \nHow are you handling model updates in TEE environments? The attestation requirements make standard blue-green deployments complicated. Currently doing full enclave restarts but that means brief downtime.\n\nAlso curious if anyone's tried running transformer models larger than 1B params in TEE. Memory constraints seem prohibitive but maybe there are tricks I'm missing?",
    "author": "baddie_spotted",
    "timestamp": "2025-09-04T00:11:27",
    "url": "https://reddit.com/r/MachineLearning/comments/1n83e6e/d_performance_overhead_of_running_ml_inference_in/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n7oh8p",
    "title": "[P] Arbitrary Order Automatic Differentiation for PyTorch",
    "content": "I‚Äôm excited to present **thoad** (short for Py**T**orch **H**igh **O**rder **A**utomatic **D**ifferentiation), a Python only library that computes arbitrary order partial derivatives directly on a PyTorch computational graph. The package has been developed within a bachelor's research project at Universidad Pontificia de Comillas - ICAI, and we are considering publishing a future academic article reviewing the mathematical details and the implementation design.\n\nAt its core, thoad takes a one output, many inputs view of the graph and pushes high order derivatives back to the leaf tensors. Although a 1‚ÜíN problem can be rewritten as 1‚Üí1 by concatenating flattened inputs, as in functional approaches such as `jax.jet` or `functorch`, thoad‚Äôs graph aware formulation enables:\n\n* Working with smaller **pieced external derivatives**\n* An optimization based on **unifying independent dimensions** (especially batch).\n\nThis delivers **asymptotically better scaling** with respect to order and batch size (respectively).\n\nAdditionally, we compute derivatives with a *vectorial* approach rather than component by component, which makes our pure PyTorch implementation possible. Consequently, the implementation stays at a high level, written entirely in Python and using **PyTorch** as its only dependency. Avoiding custom C++ or CUDA has a very positive impact on the long-term maintainability of the package.\n\nThe package is already available to be installed from **GitHub** or **PyPI**:\n\n* GitHub: [https://github.com/mntsx/thoad](https://github.com/mntsx/thoad)\n\nIn our benchmarks, thoad **outperforms** torch.autograd for **Hessian calculations even on CPU**. See the repository *examples/benchmarks* to check the comparisons and run them in your own hardware.\n\n**thoad** is designed to align closely with PyTorch‚Äôs interface philosophy, so running the high order backward pass is practically indistinguishable from calling PyTorch‚Äôs own `backward`. When you need finer control, you can keep or reduce Schwarz symmetries, group variables to restrict mixed partials, and fetch the exact mixed derivative you need. Shapes and independence metadata are also exposed to keep interpretation straightforward.\n\n# USING THE PACKAGE\n\n**thoad** exposes two primary interfaces for computing high-order derivatives:\n\n1. `thoad.backward`: a function-based interface that closely resembles `torch.Tensor.backward`. It provides a quick way to compute high-order gradients without needing to manage an explicit controller object, but it offers only the core functionality (derivative computation and storage).\n2. `thoad.Controller`: a class-based interface that wraps the output tensor‚Äôs subgraph in a controller object. In addition to performing the same high-order backward pass, it gives access to advanced features such as fetching specific mixed partials, inspecting batch-dimension optimizations, overriding backward-function implementations, retaining intermediate partials, and registering custom hooks.\n\nExample of autodifferentiation execution via `thoad.backward`\n\n    import torch\n    import thoad\n    from torch.nn import functional as F\n    \n    #### Normal PyTorch workflow\n    X = torch.rand(size=(10,15), requires_grad=True)\n    Y = torch.rand(size=(15,20), requires_grad=True)\n    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)\n    \n    #### Call thoad backward\n    order = 2\n    thoad.backward(tensor=Z, order=order)\n    \n    #### Checks\n    ## check derivative shapes\n    for o in range(1, 1 + order):\n       assert X.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(X.shape)))\n       assert Y.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(Y.shape)))\n    ## check first derivatives (jacobians)\n    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T)\n    J = torch.autograd.functional.jacobian(fn, (X, Y))\n    assert torch.allclose(J[0].flatten(), X.hgrad[0].flatten(), atol=1e-6)\n    assert torch.allclose(J[1].flatten(), Y.hgrad[0].flatten(), atol=1e-6)\n    ## check second derivatives (hessians)\n    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T).sum()\n    H = torch.autograd.functional.hessian(fn, (X, Y))\n    assert torch.allclose(H[0][0].flatten(), X.hgrad[1].sum(0).flatten(), atol=1e-6)\n    assert torch.allclose(H[1][1].flatten(), Y.hgrad[1].sum(0).flatten(), atol=1e-6)\n\nExample of autodifferentiation execution via `thoad.Controller`\n\n    import torch\n    import thoad\n    from torch.nn import functional as F\n    \n    #### Normal PyTorch workflow\n    X = torch.rand(size=(10,15), requires_grad=True)\n    Y = torch.rand(size=(15,20), requires_grad=True)\n    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)\n    \n    #### Instantiate thoad controller and call backward\n    order = 2\n    controller = thoad.Controller(tensor=Z)\n    controller.backward(order=order, crossings=True)\n    \n    #### Fetch Partial Derivatives\n    ## fetch T0 and T1 2nd order derivatives\n    partial_XX, _ = controller.fetch_hgrad(variables=(X, X))\n    partial_YY, _ = controller.fetch_hgrad(variables=(Y, Y))\n    assert torch.allclose(partial_XX, X.hgrad[1])\n    assert torch.allclose(partial_YY, Y.hgrad[1])\n    ## fetch cross derivatives\n    partial_XY, _ = controller.fetch_hgrad(variables=(X, Y))\n    partial_YX, _ = controller.fetch_hgrad(variables=(Y, X))\n\n&gt;NOTE. A more detailed user guide with examples and feature walkthroughs is available in the notebook: [https://github.com/mntsx/thoad/blob/master/examples/user\\_guide.ipynb](https://github.com/mntsx/thoad/blob/master/examples/user_guide.ipynb)",
    "author": "WildAppearance2153",
    "timestamp": "2025-09-03T12:30:46",
    "url": "https://reddit.com/r/MachineLearning/comments/1n7oh8p/p_arbitrary_order_automatic_differentiation_for/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n77fsw",
    "title": "[D] WACV 2026 Paper Reviews",
    "content": "WACV Reviews are supposed to be released by today EOD. Creating a discussion thread to discuss among ourselves, thanks!",
    "author": "akshitsharma1",
    "timestamp": "2025-09-02T23:30:07",
    "url": "https://reddit.com/r/MachineLearning/comments/1n77fsw/d_wacv_2026_paper_reviews/",
    "score": 45,
    "num_comments": 102,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n7brbk",
    "title": "[P] Sentiment Analysis Model for cloud services",
    "content": "Hi all! Some time ago, I asked for help with a survey on ML/AI compute needs. After limited responses, I built a model that parses ML/cloud subreddits and applies BERT-based aspect sentiment analysis to cloud providers (AWS, Azure, Google Cloud, etc.). It classifies opinions by key aspects like cost, scalability, security, performance, and support.\n\nI‚Äôm happy with the initial results, but I‚Äôd love advice on making the interpretation more precise:\n\nEnsuring sentiment is directed at the provider (not another product/entity mentioned)  \nBetter handling of comparative or mixed statements (e.g., ‚Äúfast but expensive‚Äù)  \nImproving robustness to negation and sarcasm\n\nIf you have expertise in aspect/target-dependent sentiment analysis or related NLP tooling, I‚Äôd really appreciate your input.\n\nRepo:¬†[https://github.com/PatrizioCugia/cloud-sentiment-analyzer](https://github.com/PatrizioCugia/cloud-sentiment-analyzer)  \n  \nIt would also be great if you could answer my original survey:¬†[https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)\n\nThanks!",
    "author": "Any_Commercial7079",
    "timestamp": "2025-09-03T04:03:33",
    "url": "https://reddit.com/r/MachineLearning/comments/1n7brbk/p_sentiment_analysis_model_for_cloud_services/",
    "score": 11,
    "num_comments": 0,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n6wc4k",
    "title": "[D] Has paper submission quality remained roughly the same?",
    "content": "Over the last year, I reviewed 12 papers at top tier conferences. It's a small sample size but I noticed that roughly 3 or 4 of them were papers I would consider good enough for acceptance at a top tier conference. That is to say: (1) they contained a well-motivated and interesting idea, (2) they had reasonable experiments and ablation, and (3) they told a coherent story.\n\nThat means roughly 30% of papers met my personal threshold for quality.... which is roughly the historic acceptance rate for top-tier conferences. From my perspective, as the number of active researchers has increased, the number of well executed interesting ideas has also increased. I don't think we've hit a point where there's a clearly finite set of things to investigate in the field. \n\nI would also say essentially every paper I rejected was distinctly worse than those 3 or 4 papers. Papers I rejected were typically poorly motivated -- usually an architecture hack poorly situated in the broader landscape with no real story that explains this choice. Or, the paper completely missed an existing work that already did nearly exactly what they did. \n\nWhat has your experience been? ",
    "author": "impatiens-capensis",
    "timestamp": "2025-09-02T14:34:26",
    "url": "https://reddit.com/r/MachineLearning/comments/1n6wc4k/d_has_paper_submission_quality_remained_roughly/",
    "score": 69,
    "num_comments": 30,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n77v29",
    "title": "Acl rolling recview is the most garbage conference to submit your papers [R]",
    "content": "You will find the most generic AI generated reviews in ARR. \nWaste of time. Submit to AI conferences. \nARR is dead",
    "author": "Turbulent_Visual_948",
    "timestamp": "2025-09-02T23:57:09",
    "url": "https://reddit.com/r/MachineLearning/comments/1n77v29/acl_rolling_recview_is_the_most_garbage/",
    "score": 12,
    "num_comments": 19,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n71dzv",
    "title": "A friendly starter paper - Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation [R]",
    "content": "Hey r/MachineLearning \n\nI had this idea and wanted to put it in a very simple and straightforward way, tried to make the paper easy to read and starter friendly! Also it shows my research partner focus on uncertainty measurement from metrology, which I think it‚Äôs not very widely addressed in ML and NLP! \n\nThe motivation here came while doing exploration at the Weights &amp; Biases Sunday cafe event in SF, where we were exploring their observability Weave Product. I think running loops and adding more complex tools that I did for the paper, should be production valuable and help in a bunch of ways, but most importantly, help with making small models\nMore useful and a kind of reasoning process of sorts. In the future it might be useful to make this loop inside the model before output layers, anybody think of any cools applications for such methods ? \n\n\n[Title]: Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation\n\n[Abstract]: Reasoning models often outperform smaller models but at 3--5√ó higher cost and added latency. We present entropy-guided refinement: a lightweight, test-time loop that uses token-level uncertainty to trigger a single, targeted refinement pass. We extract logprobs, compute Shannon entropy on top-k alternatives, and apply a simple OR-logic trigger over perplexity, maximum token entropy, and low-confidence-token count. Unlike approaches that use entropy only for measurement or decoding, we pass a compact uncertainty report (tokens, confidences, alternatives, context) back to the model to guide corrective edits. On representative technical queries across reasoning, mathematics, and code generation tasks, a small model with our loop approaches 95\\% of a reference reasoning model's quality at approximately one-third of the cost. The method achieves selective refinement on ~31\\% of responses while improving accuracy by 16 percentage points over single-pass inference. We demonstrate that this uncertainty-aware loop provides an effective middle ground between single-pass inference and expensive reasoning chains, making it practical for production deployments where both quality and cost matter.\n\nhttps://arxiv.org/abs/2509.00079\n\nIf you don‚Äôt like it, let me know! Am open to critique and learning! ",
    "author": "OkOwl6744",
    "timestamp": "2025-09-02T18:12:39",
    "url": "https://reddit.com/r/MachineLearning/comments/1n71dzv/a_friendly_starter_paper_entropyguided_loop/",
    "score": 26,
    "num_comments": 17,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n6ir0a",
    "title": "[D] What apps or workflows do you use to keep up with reading AI/ML papers regularly?",
    "content": "I‚Äôm a postgraduate in AI, and I‚Äôm trying to build a better habit of reading papers consistently.\n\nI wanted to ask: what tools, apps, or workflows do you personally use to track new papers and actually read them?\n\nCurious to hear what‚Äôs worked for you in terms of discovery (finding the right papers) and sticking with the reading habit.",
    "author": "hakimgafai",
    "timestamp": "2025-09-02T05:57:41",
    "url": "https://reddit.com/r/MachineLearning/comments/1n6ir0a/d_what_apps_or_workflows_do_you_use_to_keep_up/",
    "score": 70,
    "num_comments": 38,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n6sd4l",
    "title": "[P] csm.rs: A High-Performance Rust Implementation of Sesame's Conversational Speech Model for Real-Time Streaming TTS",
    "content": "Hi everyone,\n\nI'm sharing a project I've developed, [`csm.rs`](https://github.com/cartesia-one/csm.rs), a high-performance inference implementation for Sesame's Conversational Speech Model (`sesame/csm-1b`). The project is written in Rust and built on the `candle` ML framework.\n\nThe primary goal was to create an efficient, standalone inference engine capable of real-time, streaming text-to-speech, moving beyond typical Python-based inference scripts to achieve maximum performance.",
    "author": "poppear",
    "timestamp": "2025-09-02T12:02:53",
    "url": "https://reddit.com/r/MachineLearning/comments/1n6sd4l/p_csmrs_a_highperformance_rust_implementation_of/",
    "score": 17,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n6vf0x",
    "title": "[R] NeurIPS workshop - change of authors post submission",
    "content": "Hi all, I submitted a paper to a NeurIPs workshop recently and it just dawned on me that I forgot to enter one of the authors in the OpenReview portal (the deadline for submission has now passed). I will reach out to the workshop but has anyone had any luck with this kind of thing?",
    "author": "glazmann",
    "timestamp": "2025-09-02T13:58:26",
    "url": "https://reddit.com/r/MachineLearning/comments/1n6vf0x/r_neurips_workshop_change_of_authors_post/",
    "score": 11,
    "num_comments": 1,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n6kr0d",
    "title": "[P] Training environment for PS2 game RL",
    "content": "https://preview.redd.it/hx8od7wvfrmf1.png?width=3819&amp;format=png&amp;auto=webp&amp;s=8989ff64c23e66ff7f22e4694cae88a0f192c2b5\n\nIt's alive!!! The environment I'm developing is already functional and running Granturismo 3 on PS2!!! If you want to support the development, the link is this:\n\n[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)",
    "author": "AgeOfEmpires4AOE4",
    "timestamp": "2025-09-02T07:18:44",
    "url": "https://reddit.com/r/MachineLearning/comments/1n6kr0d/p_training_environment_for_ps2_game_rl/",
    "score": 22,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n6t4vd",
    "title": "[P] Datatune ‚Äì Use natural language + LLMs to transform and filter tabular data",
    "content": "https://github.com/vitalops/datatune\n\nIntroducing Datatune, a Python library that enables row-wise transformations on tabular data using natural language prompts, powered by LLMs.\n\nUnlike tools that generate SQL or static scripts, Datatune is designed for per-row semantic operations on tabular data. It‚Äôs particularly useful for fuzzy logic tasks like classification, filtering, derived metrics, and text extraction - anything that‚Äôs hard to express in SQL but intuitive in plain English.\n\n### What it does\n\nYou write prompts like:\n\n* \"Extract categories from the product description and name\"\n* \"Keep only electronics products\"\n* \"Add a column called ProfitMargin = (Total Profit / Revenue) * 100\"\n\nDatatune interprets the prompt and applies the right operation (map, filter, or an LLM-powered agent pipeline) on your data using OpenAI, Azure, Ollama, or other LLMs via LiteLLM.\n\n### Key Features\n\n* Row-level map() and filter() operations using natural language\n* Agent interface for auto-generating multi-step transformations\n* Built-in support for Dask DataFrames (for scalability)\n* Works with multiple LLM backends (OpenAI, Azure, Ollama, etc.)\n* Compatible with LiteLLM for flexibility across providers\n* Auto-token batching, metadata tracking, and smart pipeline composition\n\n### Token &amp; Cost Optimization\n\n* Datatune gives you explicit control over which columns are sent to the LLM, reducing token usage and API cost:\n* Use input_fields to send only relevant columns\n* Automatically handles batching and metadata internally\n* Supports setting tokens-per-minute and requests-per-minute limits\n* Defaults to known model limits (e.g., GPT-3.5) if not specified\n* This makes it possible to run LLM-based transformations over large datasets without incurring runaway costs.\n\n### Quick Example\n```python\nimport datatune as dt\nfrom datatune.llm.llm import OpenAI\n\nllm = OpenAI(model_name=\"gpt-3.5-turbo\")\ndf = dd.read_csv(\"products.csv\")\n\n# Map step\nmapped = dt.map(\n    prompt=\"Extract categories from the description and name of product.\",\n    output_fields=[\"Category\", \"Subcategory\"],\n    input_fields=[\"Description\", \"Name\"]\n)(llm, df)\n\n# Filter step\nfiltered = dt.filter(\n    prompt=\"Keep only electronics products\",\n    input_fields=[\"Name\"]\n)(llm, mapped)\n\nresult = dt.finalize(filtered)\n```\n\nOr using the agent:\n\n```python\nagent = dt.Agent(llm)\ndf = agent.do(\"Add a column called ProfitMargin = (Total Profit / Total Revenue) * 100.\", df)\nresult = dt.finalize(df)\n```\n### Use Cases\n\n* Product classification from text fields\n* Filtering based on semantic conditions\n* Creating derived metrics using natural language\n* Review quality detection, support ticket triage\n* Anonymization (PII removal) when needed\n\n### Links\n\n* GitHub: https://github.com/vitalops/datatune\n* Docs: https://docs.datatune.ai\n* Examples: https://github.com/vitalops/datatune/tree/main/examples\n\nWe‚Äôre actively developing the project and would appreciate any feedback, bug reports, or feature requests via Github issues.\n.\n",
    "author": "farizrahman4u",
    "timestamp": "2025-09-02T12:32:13",
    "url": "https://reddit.com/r/MachineLearning/comments/1n6t4vd/p_datatune_use_natural_language_llms_to_transform/",
    "score": 8,
    "num_comments": 2,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n75xxx",
    "title": "[R] Practical TEE deployment for sensitive research datasets - lessons from our lab",
    "content": "\nPosting this because I wish someone had done the same when we started. Our lab needed to work with industry partners on sensitive datasets but legal restrictions meant we couldn't access the raw data.\n\nTraditional methods like differential privacy added too much noise for our research goals. Synthetic data was useless for our specific use case.\n\nWhat went good for us: deploying our models in trusted execution environments. Partners felt comfortable because data never left their control. We could iterate on models without seeing actual data values.\n\nTech setup through phala network was surprisingly direct. Only difficulty was adapting our workflow since you can't just print tensors to debug anymore. Had to get creative with logging aggregate statistics.\n\nUnexpected: our industry partnerships increased 3x because companies that previously wouldn't share data are now willing to collaborate. Turns out the privacy barrier was bigger than we realized.\n\nIf your research is stuck due to data access issues definitely worth exploring TEE options. Happy to share our deployment scripts if useful.",
    "author": "Impossible_Tutor_824",
    "timestamp": "2025-09-02T22:00:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1n75xxx/r_practical_tee_deployment_for_sensitive_research/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n6aisc",
    "title": "[D] OpenReview website is down!",
    "content": "I'm trying to upload one pending AAAI review but the website is not opening. \n\nAnyone facing the same issue? I'm also curious what would happen if I miss the review submission deadline due to website downtime. ",
    "author": "Outrageous_Tip_8109",
    "timestamp": "2025-09-01T21:48:51",
    "url": "https://reddit.com/r/MachineLearning/comments/1n6aisc/d_openreview_website_is_down/",
    "score": 80,
    "num_comments": 70,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n6rijz",
    "title": "[D] Building conversational AI: the infrastructure nobody talks about",
    "content": "Everyone's focused on models. Nobody discusses the plumbing that makes real-time AI conversation possible.\n\nThe stack I'm testing:\n\n* STT: Whisper vs Google Speech\n* LLM: GPT-4, Claude, Llama\n* TTS: ElevenLabs vs PlayHT\n* Audio routing: This is where it gets messy\n\nThe audio infrastructure is the bottleneck. Tried raw WebRTC (painful), looking at managed solutions like Agora, LiveKit, Daily.\n\nLatency breakdown targets:\n\n* Audio capture: &lt;50ms\n* STT: &lt;100ms\n* LLM: &lt;200ms\n* TTS: &lt;100ms\n* Total: &lt;500ms for natural conversation\n\nAnyone achieved consistent sub-500ms latency? What's your setup?",
    "author": "peepee_peeper",
    "timestamp": "2025-09-02T11:31:23",
    "url": "https://reddit.com/r/MachineLearning/comments/1n6rijz/d_building_conversational_ai_the_infrastructure/",
    "score": 6,
    "num_comments": 7,
    "upvote_ratio": 0.58,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n6swom",
    "title": "[D] How can I license datasets?",
    "content": "I've been working on AI projects for a while now and I keep running into the same problem over and over again. Wondering if it's just me or if this is a universal developer experience.\n\nYou need specific training data for your model. Not the usual stuff you find on Kaggle or other public datasets, but something more niche or specialized, for e.g. financial data from a particular sector, medical datasets, etc. I try to find quality datasets, but most of the time, they are hard to find or license, and not the quality or requirements I am looking for.\n\nSo, how do you typically handle this? Do you use datasets free/open source? Do you use synthetic data? Do you use whatever might be similar, but may compromise training/fine-tuning?\n\nIm curious if there is a better way to approach this, or if struggling with data acquisition is just part of the AI development process we all have to accept. Do bigger companies have the same problems in sourcing and finding suitable data?\n\nIf you can share any tips regarding these issues I encountered, or if you can share your experience, will be much appreciated!",
    "author": "Ill_Virus4547",
    "timestamp": "2025-09-02T12:23:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1n6swom/d_how_can_i_license_datasets/",
    "score": 2,
    "num_comments": 8,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n67lft",
    "title": "[D] Self-Promotion Thread",
    "content": "Please post your personal projects, startups, product placements, collaboration needs, blogs etc.\n\nPlease mention the payment and pricing requirements for products and services.\n\nPlease do not post link shorteners, link aggregator websites , or auto-subscribe links.\n\n\\--\n\nAny abuse of trust will lead to bans.\n\nEncourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\n\\--\n\nMeta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",
    "author": "AutoModerator",
    "timestamp": "2025-09-01T19:15:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1n67lft/d_selfpromotion_thread/",
    "score": 19,
    "num_comments": 71,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5qgcd",
    "title": "[D] Proposal: Multi-year submission ban for irresponsible reviewers ‚Äî feedback wanted",
    "content": "**TL;DR:** I propose introducing multi-year submission bans for reviewers who repeatedly fail their responsibilities. Full proposal + discussion here: [GitHub](https://github.com/IcarusWizard/ML-review-proposal-for-accountability).\n\nHi everyone,\n\nLike many of you, I‚Äôve often felt that our review system is broken due to irresponsible reviewers. Complaints alone don‚Äôt fix the problem, so I‚Äôve written a proposal for a possible solution: **introducing a multi-year submission ban for reviewers who repeatedly fail to fulfill their responsibilities.**\n\nRecent policies at major conferences (e.g., CVPR, ICCV, NeurIPS) include desk rejections for poor reviews, but these measures don‚Äôt fully address the issue‚Äîespecially during the rebuttal phase. Reviewers can still avoid accountability once their own papers are withdrawn.\n\nIn my proposal, I outline how longer-term consequences might improve reviewer accountability, along with safeguards and limitations. I‚Äôm not a policymaker, so I expect there will be issues I haven‚Äôt considered, and I‚Äôd love to hear your thoughts.\n\nüëâ Read the full proposal here: [GitHub](https://github.com/IcarusWizard/ML-review-proposal-for-accountability).  \nüëâ Please share whether you think this is viable, problematic, or needs rethinking.\n\nIf we can spark a constructive discussion, maybe we can push toward a better review system together.",
    "author": "IcarusZhang",
    "timestamp": "2025-09-01T07:38:33",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/",
    "score": 59,
    "num_comments": 40,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5rbwc",
    "title": "[P] Computer Vision Backbone Model PapersWithCode Alternative: Heedless Backbones",
    "content": "\n\nhttps://preview.redd.it/d2mm661vnkmf1.png?width=3126&amp;format=png&amp;auto=webp&amp;s=aa83a5002ebcba917c48d158460133701a81989a\n\nThis is a site I've made that aims to do a better job of what Papers with Code did for ImageNet and Coco benchmarks.\n\nI was often frustrated that the data on Papers with Code didn't consistently differentiate backbones, downstream heads, and pretraining and training strategies when presenting data. So with heedless backbones, benchmark results are all linked to a single pretrained model (e.g. convenxt-s-IN1k), which is linked to a model (e.g. convnext-s), which is linked to a model family (e.g. convnext). In addition to that, almost all results have FLOPS and model size associated with them. Sometimes they even throughput results on different gpus (though this is pretty sparse).\n\nI'd love to hear feature requests or other feedback. Also, if there's a model family that you want added to the site, please open an issue on the project's [github](https://github.com/igm503/heedless-backbones)\n\n  \n[Heedless Backbones](https://heedlessbackbones.com/)",
    "author": "Even-Tour-4580",
    "timestamp": "2025-09-01T08:12:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5rbwc/p_computer_vision_backbone_model_paperswithcode/",
    "score": 25,
    "num_comments": 6,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5n1v2",
    "title": "[R] Graph ML benchmarks and foundation models",
    "content": "Our team has recently published two graph ML papers: one with a new realistic benchmark and the second one on graph foundation models and how they can be related to tabular foundation models.  \n  \n**GraphLand benchmark**\n\nüìù Paper:  [https://arxiv.org/abs/2409.14500](https://arxiv.org/abs/2409.14500)  \nüíª Code:  [https://github.com/yandex-research/graphland](https://github.com/yandex-research/graphland) \n\nIt is widely discussed in the community that graph machine learning suffers from the lack of realistic, meaningful, reliable, and diverse benchmarks. We agree with this and we hope that we improve this situation with our recent paper ‚ÄúGraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Data‚Äù. GraphLand is a benchmark of 14 diverse graph datasets for node property prediction (both classification and regression) from different industrial applications. The datasets cover realistic machine learning problems and come with rich numerical and categorical node features that are common in real-world applications. Importantly, besides standard random splits, GraphLand provides splits with temporal distributional shifts and the inductive prediction setting, which enable evaluating GNNs in more realistic and challenging scenarios.\n\n[GraphLand benchmark datasets.](https://preview.redd.it/nkl4qs9nnjmf1.png?width=2224&amp;format=png&amp;auto=webp&amp;s=1819461078e34be3e98030c9e65ee61a7b98adc9)\n\nWe evaluated a wide range of models on GraphLand. This includes several openly available graph foundation models (GFMs), which we found provide very weak performance compared to classical GNNs.   \n  \nThus, we set out to develop a better GFM, which led us to the next paper...\n\n**Turning Tabular Foundation Models into Graph Foundation Models**\n\nüìù Paper: [https://arxiv.org/abs/2508.20906](https://arxiv.org/abs/2508.20906)  \nüíª Code: [https://github.com/yandex-research/G2T-FM](https://github.com/yandex-research/G2T-FM)\n\nGraphs may come from very different domains and thus may have diverse features varying across datasets. As a result, one of the key challenges for GFMs is how to deal with such diverse heterogeneous features. Prior studies did not fully address this issue, often limiting themselves to text-attributed graphs or relying on simple techniques like PCA and SVD. However, this challenge is not unique to the graph domain. The tabular domain faces exactly the same issue, and recent tabular foundation models like TabPFNv2 successfully deal with it. We‚Äôve decided to transfer their success to graphs.\n\n[G2T-FM Framework](https://preview.redd.it/xnfsjf77ojmf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=d840e9794068202829dec2bdfa71e426198a7a15)\n\nIn our framework ‚Äì G2T-FM (Graph-to-Table Foundation Model) ‚Äì we augment the original features with graph information by computing neighborhood feature aggregations and some structure-based encodings, essentially transforming graph tasks to tabular tasks (G2T). After that, we apply TabPFNv2 to these augmented features to get predictions.\n\n[G2T-FM Results](https://preview.redd.it/z3mz5tmaojmf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=6feb591cdd5fb1231d36c2a937ced802a27a26e7)\n\nWe evaluated G2T-FM on GraphLand and several other graph datasets and found that it shows strong performance in both in-context learning and finetuning settings. In particular, G2T-FM outperforms both well-tuned classic GNNs trained from scratch and prior publicly available GFMs.   \n  \nWe hope our work will help develop better GFMs and highlight for the graph community the similarities of graph and tabular domains and the prospects of utilizing tabular foundation models for graph tasks!\n\n\n\n",
    "author": "_puhsu",
    "timestamp": "2025-09-01T05:13:29",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5n1v2/r_graph_ml_benchmarks_and_foundation_models/",
    "score": 39,
    "num_comments": 2,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5t7cv",
    "title": "[R] Latent Diffusion Question",
    "content": "Is this normal for generated data from latent diffusion? The large spikes at the end of the histogram edges. Does this indicate the autoencoder is overfitting?\n\nhttps://preview.redd.it/i1gtm7h3xkmf1.png?width=536&amp;format=png&amp;auto=webp&amp;s=1589ad23cffc3a678eefad82750b71eefbad9962\n\n",
    "author": "AgencyPuzzleheaded",
    "timestamp": "2025-09-01T09:22:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5t7cv/r_latent_diffusion_question/",
    "score": 8,
    "num_comments": 3,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5oznp",
    "title": "[D] Why aren't there any diffusion speech to text models?",
    "content": "Title,\n\nI was reading upon diffusion models and speech models and that some of the new diffusion text models are being now developed. Since we know the length of the output that a chunk of audio produces wouldn't it be possible to create a diffusion model to fill in text for the whole length all at once instead of the current auto regressive models?\n\nPS: I am really not that advanced so this might be a dumb question.",
    "author": "SnappierSoap318",
    "timestamp": "2025-09-01T06:40:14",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5oznp/d_why_arent_there_any_diffusion_speech_to_text/",
    "score": 6,
    "num_comments": 14,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n4y2y3",
    "title": "[D] Huawei‚Äôs 96GB GPU under $2k ‚Äì what does this mean for inference?",
    "content": "Looks like Huawei is putting out a 96GB GPU for under $2k. NVIDIA‚Äôs cards with similar memory are usually $10k+. From what I‚Äôve read, this one is aimed mainly at inference.\n\nDo you think this could actually lower costs in practice, or will the real hurdle be software/driver support? ",
    "author": "pmv143",
    "timestamp": "2025-08-31T08:45:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1n4y2y3/d_huaweis_96gb_gpu_under_2k_what_does_this_mean/",
    "score": 240,
    "num_comments": 109,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5zzln",
    "title": "[R] How hard is it to get accepted into the AAAI Student Abstract and Poster Program?",
    "content": "Hi everyone,\n\nII‚Äôm considering submitting to the AAAI Student Abstract and Poster Program (AAAI-26), but I can‚Äôt find much information about how competitive it is compared to the main technical track.\n\nI know the main conference has a pretty low acceptance rate but AAAI doesn‚Äôt seem to share stats for the student program. Has anyone here submitted to or been accepted into this track before? How selective is it?\n\nAlso, would it be enough if my work is more of an application of existing AI methods to radar (less novelty in the method itself, more novelty in the application)? Or are they mainly looking for new algorithms/AI contributions even in the student track?",
    "author": "-math-4-life-",
    "timestamp": "2025-09-01T13:36:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5zzln/r_how_hard_is_it_to_get_accepted_into_the_aaai/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.45,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5r08b",
    "title": "[D] Simple Questions Thread",
    "content": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!",
    "author": "AutoModerator",
    "timestamp": "2025-09-01T08:00:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5r08b/d_simple_questions_thread/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5kl6k",
    "title": "[P] Beaver: A DSL for Building Streaming ML Pipelines",
    "content": "Hi guys!\n\nMy name is Jason I am an Electrical and Computer Engineering student and for the last year I have been working on my thesis, in which I have developed Beaver¬†‚Äì a domain-specific language (DSL) designed to make building machine learning pipelines for streaming data (e.g., Kafka) much simpler and more accessible.\n\nWhat is Beaver?\n\n* A DSL that lets you define ML pipelines using a clear, declarative syntax (instead of complex Python code)\n* Generates Python code that integrates with the¬†[River](http://riverml.xyz/latest/)¬†library for online ML and supports real-time data streams\n* Includes built-in validation, analysis, and automatic dashboard generation\n\n  \nI'm making this post to ask for some feedback. I‚Äôve prepared a user testing experience with 3 tasks (from basic to advanced) that should take about 30-45 minutes. I‚Äôd love to hear your thoughts on usability, clarity, and the overall concept.\n\n* üìñ¬†[Concept overview &amp; docs](http://deepblue597.github.io/beaver-doc/)\n* üìù¬†[User testing instructions](https://github.com/deepblue597/beaver/blob/user_testing/user_testing.md)\n* ü¶´¬†[Example pipeline file](https://github.com/deepblue597/beaver/blob/user_testing/examples/linear.bvr)\n* üí¨¬†[Feedback form](https://forms.gle/ioLVyvruJ2KCs6wd8)\n\nRepo : [https://github.com/deepblue597/beaver](https://github.com/deepblue597/beaver)  \nIt is recommended to use the user\\_testing branch for the feedback.   \n  \nThank you so much for your time &lt;3 ",
    "author": "Deepblue597",
    "timestamp": "2025-09-01T02:55:59",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5kl6k/p_beaver_a_dsl_for_building_streaming_ml_pipelines/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5qjvu",
    "title": "[D] EMNLP 2025 camera-ready page limits + virtual poster presentation",
    "content": "Hey folks,\n\nMy paper just got into EMNLP 2025  and I‚Äôm trying to sort out two things before the camera-ready:\n\n1. Page limits\n\n- ARR submission was capped at 8 pages (long paper). The acceptance email says we get +1 page for camera-ready, so I‚Äôm assuming that means 9 pages for the main text.\n\n- Is the Limitations section required but outside this 9-page count?\n\n- And are appendices unlimited, or do they somehow count toward the limit?\n\n\n\n2. Virtual poster presentation\n\n- On OpenReview I‚Äôve already been assigned poster status. The email also says we can choose to present either in person or virtually.\n\nDoes that mean I‚Äôm free to do my poster virtually if I want?\n\n- For those who‚Äôve done virtual posters at EMNLP/ACL in recent years: what platform did they use (GatherTown, Zoom, something else), and how was the interaction?\n\n\n\n\nWould love to hear from anyone who‚Äôs navigated this before",
    "author": "Dry-Count4414",
    "timestamp": "2025-09-01T07:42:22",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5qjvu/d_emnlp_2025_cameraready_page_limits_virtual/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5j2sr",
    "title": "[P] Improving model performance",
    "content": "So I have been working on Continuous Sign Language Recognition (CSLR) for a while. Tried ViViT-Tf, it didn't seem to work. Also, went crazy with it in wrong direction and made an over complicated model but later simplified it to a simple encoder decoder, which didn't work.\n\nThen I also tried several other simple encoder-decoder. Tried ViT-Tf, it didn't seem to work. Then tried ViT-LSTM, finally got some results (38.78% word error rate). Then I also tried X3D-LSTM, got 42.52% word error rate. \n\nNow I am kinda confused what to do next. I could not think of anything and just decided to make a model similar to SlowFastSign using X3D and LSTM. But I want to know how do people approach a problem and iterate their model to improve model accuracy. I guess there must be a way of analysing things and take decision based on that. I don't want to just blindly throw a bunch of darts and hope for the best. ",
    "author": "Naneet_Aleart_Ok",
    "timestamp": "2025-09-01T01:18:25",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5j2sr/p_improving_model_performance/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5qhr4",
    "title": "[D] OOM When Resuming From Checkpoint",
    "content": "I was training a GPT-2 XL-sized LLM, and I had to stop the run. When I try to resume the run on the same hardware, I get an OOM. I had a similar issue when my model had about 930m parameters, but I solved it by moving all tensors in the model/optimizer state dicts to CPU before saving. When I run this code:optimizer.state = collections.defaultdict(dict)the OOM goes away. The OOM always happens during the optimizer step. I use xm.optimizer_step with the barrier enabled. I have also tried manually sharding the optimizer states using xs.mark_sharding. Here are some details about my project/setup:\n\nTPU v3-8\n\nTorch 2.7.0\n\njax 0.6.2\n\nI use FSDP with SPMD\n\nHere is some relevant code from my codebase:\nSaving:\n```\ndef save_checkpoint(model, optimizer, step, train_device_loader=None):\n    # Save model weights via XLA SPMD checkpoint (supported)\n    os.makedirs(f\"./ckpt-{step}\", exist_ok=True)\n    model_state_dict = model.module.state_dict()\n    for i in model_state_dict.keys():\n        xla_tensor = model_state_dict[i]\n        model_state_dict[i] = xla_tensor.to(\"cpu\")\n        del xla_tensor\n    model_sd = {\"model\": model_state_dict}\n    xm.save(model_sd, f\"./ckpt-{step}/model.pt\")\n\n    # Save host-only states separately (optimizer, step, RNG, dataloader)\n    optim_state = optimizer.state_dict()\n    optim_state_for_saving = {\n        \"state\": {},\n        \"param_groups\": optimizer.state_dict()[\"param_groups\"]\n    }\n    for i in optim_state[\"state\"]:\n        optim_state_for_saving[\"state\"][i] = {}\n        optim_state_for_saving[\"state\"][i][\"step\"] = optim_state[\"state\"][i][\"step\"].to(\"cpu\")\n        optim_state_for_saving[\"state\"][i][\"exp_avg\"] = optim_state[\"state\"][i][\"exp_avg\"].to(\"cpu\")\n        optim_state_for_saving[\"state\"][i][\"exp_avg_sq\"] = optim_state[\"state\"][i][\"exp_avg_sq\"].to(\"cpu\")\n    host_state = {\n        \"optim\": optim_state_for_saving,\n        \"step\": step,\n    }\n\n    if train_device_loader:\n        rng_states = {\n            'torch_rng_state': torch.get_rng_state(),\n            'numpy_rng_state': np.random.get_state(),\n            'random_rng_state': random.getstate(),\n        }\n        dataloader_states = {\n            \"shard_order\": train_device_loader._loader.dataset.shards,\n            \"local_order\": train_device_loader._loader.dataset.curr_order,\n            \"warmup_order\": train_device_loader._loader.dataset.warmup_order,\n            \"warmup_prob\": train_device_loader._loader.dataset.warmup_prob,\n        }\n    else:\n        rng_states = None\n        dataloader_states = None\n\n    # Write host-side files\n    with open(f\"./ckpt-{step}/host_state.pkl\", \"wb\") as f:\n        pickle.dump(host_state, f)\n    if rng_states is not None:\n        with open(f\"./ckpt-{step}/rng.pkl\", \"wb\") as f:\n            pickle.dump(rng_states, f)\n    if dataloader_states is not None:\n        with open(f\"./ckpt-{step}/dataloader.json\", \"w\") as json_file:\n            json.dump(dataloader_states, json_file, indent=4)\n```\nLoading:\n```\nif resume_from != \"\":\n        model_sd = torch.load(f\"{resume_from}/model.pt\", map_location='cpu')\n        model.load_state_dict(model_sd[\"model\"])\nmodel = model.to(device)\nif gradient_checkpointing:\n        model = FSDPv2(module=checkpoint_module(model), mesh=mesh)\nelse:\n        model = FSDPv2(module=model, mesh=mesh)\noptimizer = build_optimizer(model, peak_lr, betas, weight_decay)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps*(1-warmup_pct), eta_min=min_lr)\nif resume_from != \"\":\n        xm.mark_step()\n        # 2) Restore host-only states (optimizer, step)\n        with open(f\"{resume_from}/host_state.pkl\", 'rb') as f:\n            host_state = pickle.load(f)\n        optim_state = host_state[\"optim\"]\n        \n        # Load the processed state dict\n        optimizer.load_state_dict(optim_state)\n        del optim_state\n        last_step = host_state[\"step\"]\n        # 3) Restore RNG and dataloader state (if present)\n        try:\n            with open(f\"{resume_from}/rng.pkl\", \"rb\") as f:\n                rng = pickle.load(f)\n            torch.set_rng_state(rng['torch_rng_state'])\n            np.random.set_state(rng['numpy_rng_state'])\n            random.setstate([rng['random_rng_state'][0], tuple(rng['random_rng_state'][1]), rng['random_rng_state'][2]])\n        except FileNotFoundError:\n            pass\n        with open(f'{resume_from}/dataloader.json', 'r') as file:\n            dataloader = json.load(file)\n```\nStep:\n```\nfor k in range(gradient_accumulation_steps):\n    x, y = next(train_iter)\n     with autocast(xm.xla_device(), dtype=torch.bfloat16):\n          loss = model(x, y)\n    (loss / gradient_accumulation_steps).backward()\n     train_loss += loss.detach()\n     xm.mark_step()\n                \ntorch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n                \nxm.optimizer_step(optimizer, barrier=True)\n                \noptimizer.zero_grad()\n```",
    "author": "New-Skin-5064",
    "timestamp": "2025-09-01T07:40:06",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5qhr4/d_oom_when_resuming_from_checkpoint/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n55mr4",
    "title": "[D] AAAI Review Template",
    "content": "Hello everyone,  \nI‚Äôm serving as a first-time reviewer for AAAI and am getting ready to submit my reviews. I‚Äôm a bit uncertain about the expected structure for the different fields in the review form. For instance, in the *‚ÄúBrief summary of your review‚Äù* field, should this be a recap of the paper‚Äôs content or a short explanation of my evaluation and decision? More broadly, I‚Äôd be grateful for any guidance on how to approach the overall submission.",
    "author": "dduka99",
    "timestamp": "2025-08-31T13:46:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1n55mr4/d_aaai_review_template/",
    "score": 13,
    "num_comments": 2,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n5tmp7",
    "title": "[D] Lessons from building an AI data analyst",
    "content": "Hi all,\n\nI wrote a post on some lessons from building an AI data analyst: [https://pedronasc.com/articles/lessons-building-ai-data-analyst](https://pedronasc.com/articles/lessons-building-ai-data-analyst)\n\nThe gap from a nice demo to a real production system is big -&gt; with a lot of yet to be solved challenges.\n\nWould love to share ideas with other builders in the space and willing to learn more about it.",
    "author": "pedromnasc",
    "timestamp": "2025-09-01T09:38:26",
    "url": "https://reddit.com/r/MachineLearning/comments/1n5tmp7/d_lessons_from_building_an_ai_data_analyst/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n55r7s",
    "title": "[R] Measuring Semantic Novelty in AI Text Generation Using Embedding Distances",
    "content": "We developed a simple metric to measure semantic novelty in **collaborative text generation** by computing cosine distances between consecutive sentence embeddings. \n\nKey finding: Human contributions showed consistently higher semantic novelty than AI across multiple embedding models (RoBERTa, DistilBERT, MPNet, MiniLM) in our human-AI storytelling dataset. \n\nThe approach is straightforward - just encode sentences and measure distances between consecutive pairs. Could be useful for evaluating dialogue systems, story generation models, or any sequential text generation task.\n\nSome links:  \n[Paper site](https://idanvidra.github.io/playing_along_paper_site/)    \n[Code](https://github.com/idanvidra/Yes-And-Game-Paper)[Blog post with implementation details](https://medium.com/@idan.vidra/measuring-semantic-novelty-in-ai-generated-text-a-simple-embedding-based-approach-c92042c88338)\n\nThe work emerged from studying human-AI collaborative storytelling using improvisational theater techniques (\"Yes! and...\" games).",
    "author": "Outrageous-Travel-80",
    "timestamp": "2025-08-31T13:51:50",
    "url": "https://reddit.com/r/MachineLearning/comments/1n55r7s/r_measuring_semantic_novelty_in_ai_text/",
    "score": 7,
    "num_comments": 6,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n4nm4h",
    "title": "[D] What is up with Tensorflow and JAX?",
    "content": "Hi all,\n\n  \nbeen in the Machine Learning world till 2021, I still mostly used the old TF 1.x interface and just used TF2.x for a short time. Last work I did was with CUDA 9.\n\n  \nIt seems like quite a bit shifted with Tensorflow, I looked at the architecture again to see how much changed. To me, it's incomprehensible. Has Google shifted all efforts towards JAX, a framework with fewer layers than TF?",
    "author": "sourgrammer",
    "timestamp": "2025-08-30T23:31:14",
    "url": "https://reddit.com/r/MachineLearning/comments/1n4nm4h/d_what_is_up_with_tensorflow_and_jax/",
    "score": 80,
    "num_comments": 30,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n4bebi",
    "title": "[D] NeurIPS is pushing to SACs to reject already accepted papers due to venue constraints",
    "content": "What are our options as a discipline? We are now at a point where 3 or more reviewers can like your paper, the ACs can accept it, and it will be rejected for no reason other than venue constraints. ",
    "author": "impatiens-capensis",
    "timestamp": "2025-08-30T13:14:38",
    "url": "https://reddit.com/r/MachineLearning/comments/1n4bebi/d_neurips_is_pushing_to_sacs_to_reject_already/",
    "score": 437,
    "num_comments": 125,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n4w3i9",
    "title": "[R] Beating Baselines with Geometry: Introducing GMC, a Fast and Well-Calibrated Classifier",
    "content": "A Technical Writer's ambition to prove.\n\nBeing a Technical Writer, I yearned to learn Machine learning and prove myself. This is a try towards achieving that.  I've developed a new classifier, the¬†**Geometric Mixture Classifier (GMC)**, and I'm seeking feedback from the community before submitting it to arXiv and conferences.\n\n**The Problem:**¬†Linear models (LR, SVM) are interpretable but fail on multi-modal data. Non-linear models (RBF-SVM, MLPs) are effective but often operate as black boxes. We wanted a model that is¬†**both interpretable and expressive**.\n\n**The Idea:**¬†GMC represents each class as a¬†**mixture of hyperplanes**¬†(a \"soft union of half-spaces\"). It uses a soft-OR (log-sum-exp) within a class and softmax across classes. It's like a Mixture of Experts but without a separate gating network.\n\n* **Interpretable:**¬†You can see which \"local expert\" (hyperplane) was responsible for a prediction.\n* **Performant:**¬†Competitive with RBF-SVM, RF, and MLPs on standard benchmarks.\n* **Efficient:**¬†CPU-friendly, ¬µs-scale inference (faster than RBF-SVM, on par with MLP).\n* **Calibrated:**¬†Produces reliable probabilities.\n\n[Algorithm analogy with similar baselines](https://preview.redd.it/64vyu4u87dmf1.png?width=1385&amp;format=png&amp;auto=webp&amp;s=08b2014b60836edd0b28adbac68eb388a4a091fa)\n\n* **Accuracy:**¬†Outperforms linear models, competitive with strong non-linear baselines.\n* **Speed:**¬†\\~2-40¬µs inference time per example (see table below).\n* **Calibration:**¬†Low ECE, further improved with temperature scaling.\n\nWe would be incredibly grateful for any feedback on:\n\n* Is the¬†**core idea**¬†and its¬†**differentiation from MoE/Maxout**¬†clear?\n* Are the¬†**experiments**¬†and¬†**comparisons**¬†fair and convincing?\n* Is there any¬†**related work**¬†we might have overlooked?\n* Any general feedback on¬†**clarity**¬†or¬†**presentation**?\n\nYou can find a detailed copy of the algorithm [here](https://drive.google.com/file/d/1vRTAucCpVqImJnojVwzAUHQ2SmbAvdsi/view?usp=sharing).\n\nPlease feel free to test the algorithm: [Geometric Mixture Classifie](https://github.com/Abitsfhuusrtyt/-Geometric-Mixture-Classifier-GMC---A-Discriminative-Per-Class-Mixture-of-Hyperplanes)r",
    "author": "PossibleTop1492",
    "timestamp": "2025-08-31T07:26:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1n4w3i9/r_beating_baselines_with_geometry_introducing_gmc/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n4l73x",
    "title": "[P] Why didn‚Äôt semantic item profiles help my GCN recommender model?",
    "content": "Hey everyone,\n\nI‚Äôm working on a recommender system based on a GCN model for regression task ( predicting rating score). Normally, the model initializes user and item embeddings randomly, but I wanted to improve this by following a paper ( the diagram is presented above )  that integrates semantic item profiles as initial embeddings.\n\nHere‚Äôs what I did:\n\t‚Ä¢\tI generated structured item profiles with 3 parts using Gemini api : \n\t‚Ä¢\t[Summarization]: short description of the business.\n\t‚Ä¢\t[User Preferences]: predicted/extracted types of users who‚Äôd like it.\n\t‚Ä¢\t[Recommendation Reasoning]: explanation for why it fits.\n\t‚Ä¢\tI also encoded metadata like review count and stars into natural language (e.g., review_count &gt; 100 ‚Üí \"popular item\", avg_stars ~4.2 ‚Üí \"well-rated\").\n\t‚Ä¢\tI used Gemini text embeddings to encode these profiles into fixed-size embeddings.\n\t‚Ä¢\tThen I replaced the random item embeddings in my GCN with these semantic embeddings (after projecting them down to my model‚Äôs embedding size).\n\nThe issue:\n\t‚Ä¢\tWhen I train the GCN with these semantic embeddings, performance actually gets worse compared to just using random initialization or identical. \n\nCould the item profiles themselves be ‚Äúbad‚Äù ?\n",
    "author": "AdInevitable1362",
    "timestamp": "2025-08-30T21:08:47",
    "url": "https://reddit.com/r/MachineLearning/comments/1n4l73x/p_why_didnt_semantic_item_profiles_help_my_gcn/",
    "score": 24,
    "num_comments": 5,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n4ul5d",
    "title": "[D] Open-Set Recognition Problem using Deep learning",
    "content": "I‚Äôm working on a deep learning project where I have a dataset with n classes\n\nBut here‚Äôs my problem:\n\nüëâ What if a totally new class comes in which doesn‚Äôt belong to any of the trained classes? \n\nI've heard of a few ideas but would like to know many approaches:\n\n* analyzing the embedding space: Maybe by measuring the distance of a new input's embedding to the known class 'clusters' in that space? If it's too far from all of them, it's an outlier.\n* Apply Clustering in Embedding Space.\n\neverything works based on embedding space...\n\nare there any other approaches?",
    "author": "ProfessionalType9800",
    "timestamp": "2025-08-31T06:22:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1n4ul5d/d_openset_recognition_problem_using_deep_learning/",
    "score": 5,
    "num_comments": 18,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n4jdo7",
    "title": "[D] Monthly Who's Hiring and Who wants to be Hired?",
    "content": "**For Job Postings** please use this template\n\n&gt;Hiring: \\[Location\\], Salary:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]    and \\[Brief overview, what you're looking for\\]\n\n**For Those looking for jobs** please use this template\n\n&gt;Want to be Hired: \\[Location\\], Salary Expectation:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]  Resume: \\[Link to resume\\] and \\[Brief overview, what you're looking for\\]\n\n&amp;#x200B;\n\nPlease remember that this community is geared towards those with experience.",
    "author": "AutoModerator",
    "timestamp": "2025-08-30T19:30:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1n4jdo7/d_monthly_whos_hiring_and_who_wants_to_be_hired/",
    "score": 17,
    "num_comments": 2,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n4dqsc",
    "title": "üåüIntroducing Art-0-8B: Reasoning the way you want it to with Adaptive Thinkingüåü [R]",
    "content": "Hi everyone! Today I'm announcing a new experimental open-source model finetuned from Qwen3-¬†**Art-0-8B is the first reasoning model where users can explicitly control how the model thinks through prompts.**\n\nUnlike normal reasoning models that only let you control the final output, Art-0-8B lets you control the actual thinking process. Tell it to \"think in rap lyrics\" or \"use bullet points to organize thoughts\" and it will literally reason that way before giving you an answer.\n\nYou can check out the model on HuggingFace:¬†[https://huggingface.co/AGI-0/Art-0-8B](https://huggingface.co/AGI-0/Art-0-8B)¬†(please leave a like in the repo if you like this model)\n\nLet me know your thoughts!\n\nP.s. If you are an AI researcher working solo, consider joining us, we are a decentralized research lab, you can read about our mission in this section of the model card¬†[https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab](https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab)",
    "author": "GuiltyBookkeeper4849",
    "timestamp": "2025-08-30T14:56:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1n4dqsc/introducing_art08b_reasoning_the_way_you_want_it/",
    "score": 12,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n4ppbi",
    "title": "[D] Advanced NLP with Transformers: Full talk recording and GitHub repo",
    "content": "**Just gave a 1.5-hour talk on \"Advanced NLP with Transformers\" covering:**\n\n* Transformer architecture\n* Prompting, RAG and fine-tuning techniques\n* AI safety, security and governance challenges\n* Curated papers, fellowships and resources\n\n**Resources:** üé• Recording: [https://www.youtube.com/watch?v=9WVtUDDcAXw&amp;t=2330s](https://www.youtube.com/watch?v=9WVtUDDcAXw&amp;t=2330s) üíª GitHub: [https://github.com/vgcharan/Advanced-NLP-Workshop-2025](https://github.com/vgcharan/Advanced-NLP-Workshop-2025)\n\nDesigned for researchers, students and practitioners who want conceptual depth as well as practical references. Feedback and discussion are welcome!",
    "author": "Immediate-Hour-8466",
    "timestamp": "2025-08-31T01:45:36",
    "url": "https://reddit.com/r/MachineLearning/comments/1n4ppbi/d_advanced_nlp_with_transformers_full_talk/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n4st5p",
    "title": "[D] My model is taking too much time in calculating FFT to find top k",
    "content": "so basically my batch size is 32  \nd\\_model is 128  \nd\\_ff is 256  \nenc\\_in = 5  \nseq\\_len = 128 and pred\\_len is 10\n\nI narrow downed the bottle neck and found that my FFT step is taking too much time. i can‚Äôt use autocast to make f32 ‚Üí bf16 (assume that its not currently supported).\n\n**but frankly its taking too much time to train. and that too total steps per epoch is 700 - 902 and there are 100 epoch‚Äôs.**  \nroughly the FFT is taking 1.5 secs per iteration below. so\n\n    for i in range(1,4):\n         calculate FFT()\n    \n    \n\ncan someone help me?",
    "author": "Shan444_",
    "timestamp": "2025-08-31T04:57:39",
    "url": "https://reddit.com/r/MachineLearning/comments/1n4st5p/d_my_model_is_taking_too_much_time_in_calculating/",
    "score": 0,
    "num_comments": 11,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n4asaq",
    "title": "[P] Building a YOLOX Plate Detector: Setup, Fine-Tuning, Metrics, Dashcam Inference",
    "content": "Hey all üëã\n\nI just published this is end-to-end walkthrough of fine-tuning YOLOX on a \\~7k-image license-plate dataset: clean environment setup, dataset prep, training &amp; evaluation with COCO metrics (mAP/AP50-95), ONNX export, and real-world dashcam inference. Includes notes on dependency pinning (YOLOX‚Äôs older stack), small script fixes, and a side-by-side comparison with an Ultralytics YOLO11 model trained on the same data. Results are on par once everything is configured correctly.\n\nHere's the post where you find the code and commands: [https://www.poeticoding.com/building-a-yolox-plate-detector-setup-fine-tuning-metrics-dashcam-inference/](https://www.poeticoding.com/building-a-yolox-plate-detector-setup-fine-tuning-metrics-dashcam-inference/)\n\nYOLOX github repo: [https://github.com/Megvii-BaseDetection/YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)\n\nRoboflow car plates dataset: [https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e](https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e)\n\n",
    "author": "alvises",
    "timestamp": "2025-08-30T12:48:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1n4asaq/p_building_a_yolox_plate_detector_setup/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n3e27s",
    "title": "[D] Upcoming interviews at frontier labs, tips?",
    "content": "Hi all,\n\nI‚Äôm currently interviewing at a few labs for MLE positions and there‚Äôs two interviews in particular that have stumped me that I‚Äôd like some clarity on:\n\n1. Transformer debugging - to my knowledge, the interviewer will provide a buggy implementation of things like causal attention, self-attention, incorrect layer norm, scaling issues, and broadcast/shape mismatch. Is there anything else I‚Äôd need to master here? So far, I‚Äôve only been studying GPT style transformers, should I add BERT to the mix or nah?\n2. Training classifier &amp; data analysis. The recruiter said this is around evaluation and model performance. I‚Äôm guessing they‚Äôll throw me an unbalanced dataset and ask me to improve model performance somehow. Things to study here are: 1) chip hguyns book and 2) look at regularization, pandas/sklearn normalization and data clean up methods. How else can I master this topic? Any sample questions you have seen here before?\n\nLastly, what is your go-to source for practicing MLE related topics, both in terms of knowledge-base as well as real interview questions. I tried 1point3acres but very limited when it comes to ML.",
    "author": "bci-hacker",
    "timestamp": "2025-08-29T10:42:40",
    "url": "https://reddit.com/r/MachineLearning/comments/1n3e27s/d_upcoming_interviews_at_frontier_labs_tips/",
    "score": 104,
    "num_comments": 25,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n3nfye",
    "title": "Is Isolation Forest ideal for real-time IMU-based anomaly detection? Open to better alternatives [P]",
    "content": "Hey folks,\n\nI‚Äôm working on a project involving real-time anomaly detection using IMU data from a mobile robot (acc_x, acc_y, acc_z, magnitude). The goal is to detect small disturbances (e.g., bumping into wires or obstacles) based on sensor changes.\n\nI trained an Isolation Forest model on normal motion data and integrated it into a ROS 2 node using the .decision_function() threshold for runtime detection.\n\nIt works, but I‚Äôm worried about false positives, especially with fixed contamination. Since this will later run on embedded IMU hardware, I‚Äôm looking for something accurate and lightweight.\n\nIs Isolation Forest reliable for this?\nAny better algorithms you‚Äôd recommend (e.g., LOF, One-Class SVM, AE)? Would love to hear your thoughts or experience.\n\nThanks!",
    "author": "Mountain_Reward_1252",
    "timestamp": "2025-08-29T17:07:36",
    "url": "https://reddit.com/r/MachineLearning/comments/1n3nfye/is_isolation_forest_ideal_for_realtime_imubased/",
    "score": 17,
    "num_comments": 6,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n3g1p7",
    "title": "[D] How do we make browser-based AI agents more reliable?",
    "content": "I‚Äôve been experimenting with different approaches for giving AI agents the ability to use browsers in real workflows (data collection, QA automation, multi-step workflows). The promise is huge but the reliability problems are just as big:\n\n1. Sessions break after login or CAPTCHA\n2. Agents fail when sites change structure\n3. Security is hard to guarantee at scale\n4. Each framework has its own dialect / quirks\n\n\nRecently I‚Äôve been looking into managed environments that abstract some of this away. For example, I am using hyperbrowser right now and it does provide a unified layer for running browser-based agents without setting up everything manually. \n\nBut then my question is... Is there ongoing research or promising directions in making browser-agent interactions more robust? Are there known benchmarks, best practices, or papers that deal with these reliability issues?",
    "author": "DenOmania",
    "timestamp": "2025-08-29T11:58:40",
    "url": "https://reddit.com/r/MachineLearning/comments/1n3g1p7/d_how_do_we_make_browserbased_ai_agents_more/",
    "score": 36,
    "num_comments": 15,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n3iiam",
    "title": "[D] Working with Optuna + AutoSampler in massive search spaces",
    "content": "Hi!\nI‚Äôm using Optuna with AutoSampler to optimize a model, but the search space is huge‚Äîaround 2 million combinations.\n\nHas anyone worked with something similar?\nI‚Äôm interested in learning which techniques have worked for reducing the search space.",
    "author": "Unlikeghost",
    "timestamp": "2025-08-29T13:35:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1n3iiam/d_working_with_optuna_autosampler_in_massive/",
    "score": 11,
    "num_comments": 12,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n37qnu",
    "title": "[D] ollama/gpt-oss:20b can't seem to generate structured outputs.",
    "content": "I'm experimenting with `\"ollama/gpt-oss:20b\"`'s capability to generate structured outputs. For example, I used it to evaluate against GSM8K dataset. The schema is as follows: `answer`: for the answer, and `solution`: for the CoT solution. However, it doesn't make sense that for a 20B model, it cannot generate a valid structured output.\n\nAny thoughts or hacks on this one? I would appreciate it. Thanks.",
    "author": "AnyIce3007",
    "timestamp": "2025-08-29T06:40:08",
    "url": "https://reddit.com/r/MachineLearning/comments/1n37qnu/d_ollamagptoss20b_cant_seem_to_generate/",
    "score": 14,
    "num_comments": 9,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n3i2fx",
    "title": "[D] Scaling Inference: Lessons from Running Multiple Foundation Models in Production",
    "content": "We‚Äôve been experimenting with deploying a mix of foundation models (LLaMA, Mistral, Stable Diffusion variants, etc.) in a single platform. One of the recurring pain points is **inference optimization** at scale:\n\n* **Batching tradeoffs**: Batching reduces cost but can kill latency for interactive use cases.\n* **Quantization quirks**: Different levels (INT8, FP16) affect models inconsistently. Some speed up 4√ó, others break outputs.\n* **GPU vs. CPU balance**: Some workloads run shockingly well on optimized CPU kernels ‚Äî but only for certain model families.\n\nCurious how others have approached this.\n\n* What‚Äôs your go-to strategy for **latency vs throughput tradeoffs**?\n* Are you using **model distillation** or sticking to quantization?\n* Any underrated **libraries or frameworks** for managing multi-model inference efficiently?",
    "author": "TaxPossible5575",
    "timestamp": "2025-08-29T13:18:02",
    "url": "https://reddit.com/r/MachineLearning/comments/1n3i2fx/d_scaling_inference_lessons_from_running_multiple/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n2rvvh",
    "title": "[R] Technical Skills Analysis of Machine Learning Professionals in Canada",
    "content": "I manage a slack community of a couple hundred ML devs in Canada. I got curious and ran some numbers on our members to see if any interesting insights emerged. Here's what I found:\n\n**The \"Pandemic ML Boom\" Effect**:  \nNearly 40% of members started an ML specific role between 2020-2022. \n\n**RAG and Vector Database Expertise**:  \nOver 30% of members have hands-on experience with Retrieval-Augmented Generation systems and vector databases (Pinecone, Weaviate, ChromaDB), representing one of the hottest areas in enterprise AI.\n\n‚Äç**Multi-modal AI Pioneers**:  \nA significant portion of members work across modalities (vision + text, audio + text).\n\n**Most Common Job¬†Titles**:\n\n15% of members hold senior leadership roles (Principal, Staff, Director, CTO level), demonstrating strong senior representation within the community.\n\n**ML-Engineering Bridge Roles**:\n\nOver 35% of members hold hybrid titles that combine ML with other disciplines:¬†\"MLOps Engineer,\" \"Software Engineer, ML,\" \"AI &amp; Automation Engineer,\" \"Conversational AI Architect,\" and \"Technical Lead, NLP\".\n\nYou can see the full breakdown here: [https://revela.io/the-collective](https://revela.io/the-collective)",
    "author": "eh-tk",
    "timestamp": "2025-08-28T16:37:03",
    "url": "https://reddit.com/r/MachineLearning/comments/1n2rvvh/r_technical_skills_analysis_of_machine_learning/",
    "score": 73,
    "num_comments": 16,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n30p1v",
    "title": "How are teams handling small dataset training for industrial vision inspection?[P]",
    "content": "We're evaluating different approaches for vision-based defect detection where getting large labeled datasets is challenging. Lots of methods need thousands of examples, but some defects are rare (maybe 10-20 examples total in 6 months). Anyone working with similar constraints? I've been looking into platforms that can work with smaller datasets - curious what others are doing?",
    "author": "JollySimple188",
    "timestamp": "2025-08-29T00:15:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1n30p1v/how_are_teams_handling_small_dataset_training_for/",
    "score": 13,
    "num_comments": 10,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n3gfpt",
    "title": "[P] Open-Source Protocol designed for Multi-Agent Communication",
    "content": "[Project](https://www.reddit.com/r/MachineLearning/?f=flair_name%3A%22Project%22)\n\nOSS Released¬†**MAPLE ‚Äì a Multi Agent Protocol Language Engine**¬†designed for fast, secure, and reliable agent communication.\n\n‚Äî a new¬†**open-source protocol**¬†designed for¬†**multi-agent communication**¬†at¬†**production scale**.\n\n**MAPLE**¬†offers features we haven't seen in other protocols:\n\nüîß¬†**Integrated Resource Management:**¬†The¬†**ONLY**¬†protocol with built-in resource specification, negotiation, and optimization\n\nüõ°Ô∏è¬†**Link Identification Mechanism (LIM):**¬†Revolutionary security through verified communication channels\n\n‚ö°¬†**Result&lt;T,E&gt; Type System:**¬†ELIMINATES all silent failures and communication errors\n\nüåê¬†**Distributed State Synchronization:**¬†Sophisticated¬†**state management**¬†across agent networks\n\nüè≠¬†**Production-Grade Performance:**¬†**Very high**¬†**performance**¬†for a¬†**feature-rich**¬†protocol with¬†**sub-millisecond**¬†latency\n\nüíª¬†**pip install maple-oss**\n\nPyPI here:¬†[https://pypi.org/project/maple-oss/](https://pypi.org/project/maple-oss/)\n\nIf you‚Äôre building with agents or need robust, real-world communication between systems,  \ncheck out¬†**MAPLE GitHub**¬†repo:¬†[https://github.com/maheshvaikri-code/maple-oss](https://github.com/maheshvaikri-code/maple-oss)\n\nPlease try and test it with your projects.\n\n[MAPLE Multi Agent Communication Protocol](https://preview.redd.it/bovnmzoqc0mf1.png?width=256&amp;format=png&amp;auto=webp&amp;s=6563e85b9f830d36a244cbf9783bc05ba45ed8c3)\n\n",
    "author": "Immediate-Cake6519",
    "timestamp": "2025-08-29T12:13:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1n3gfpt/p_opensource_protocol_designed_for_multiagent/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n38fr0",
    "title": "Finetuning Vision Transformers [D]",
    "content": "Hey, \nLooking to see how DinoV3 will do on my dataset post finetuning. \n\nAny practical advice on finetuning Dino? \nScheduler, optimizer, flow - freezing, discriminative lr etc. \nAny recommandations for blogs or articals related to this? ",
    "author": "Suitable-Director809",
    "timestamp": "2025-08-29T07:08:14",
    "url": "https://reddit.com/r/MachineLearning/comments/1n38fr0/finetuning_vision_transformers_d/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n2pku5",
    "title": "[P] Training environment for RL of PS2 and other OpenGL games",
    "content": "Hello everyone. I'm working on a training environment based on stable-retro and a Retroarch frontend, Sdlarch. This environment is intended to support PS2, GameCube, Dreamcast, and other video games that aren't supported by the original Stable-retro/Gym-Retro. If anyone wants to support me, or is curious, the link is below:\n\n[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)\n\nThere's still a lot of work ahead, as I'm implementing the final phase that enables PS2 training: loading states. For some reason I don't yet fully understand, the save state isn't loading (it just saves). But it's now possible to run games in the environment via Python, without the need to intercept any external processes.",
    "author": "AgeOfEmpires4AOE4",
    "timestamp": "2025-08-28T14:58:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1n2pku5/p_training_environment_for_rl_of_ps2_and_other/",
    "score": 15,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n2gdd4",
    "title": "[R] Adding layers to a pretrained LLM before finetuning. Is it a good idea?",
    "content": "I'm doing a full fine-tune on the Qwen 3 14B Base model with around 10B tokens for loss. I'd have preferred a little higher capacity. My idea is to add a few more layers at the end, initialized close to zero, and then train. Perhaps increase from 40 to 50 layers.\n\nThis is straightforward to implement. Is there a reason why I don't hear of this being done? Is anyone familiar with this? Any research indicating success or failure? It makes sense conceptually but I would assume it would be more common if it works.\n\n(I asked the GPT5, Gemini Pro &amp; Claude, but I'm getting mixed answers. It'll agree or disagree depending how I phrase the question.)",
    "author": "Pan000",
    "timestamp": "2025-08-28T09:05:37",
    "url": "https://reddit.com/r/MachineLearning/comments/1n2gdd4/r_adding_layers_to_a_pretrained_llm_before/",
    "score": 10,
    "num_comments": 16,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n1wm8n",
    "title": "[N] Unprecedented number of submissions at AAAI 2026",
    "content": "And 20K out of 29K submissions are from China (clearly dominating AI research now, well done to my Chinese friends). The review process at AI conferences isn't just broken - it's nuked. We need change, fast.\n\nhttps://preview.redd.it/ih3vliracnlf1.png?width=1938&amp;format=png&amp;auto=webp&amp;s=b7112a3e5e78ec7bcd0e6b100b5887a880fb82be",
    "author": "Adventurous-Cut-7077",
    "timestamp": "2025-08-27T16:27:26",
    "url": "https://reddit.com/r/MachineLearning/comments/1n1wm8n/n_unprecedented_number_of_submissions_at_aaai_2026/",
    "score": 202,
    "num_comments": 109,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n29q0e",
    "title": "[P] PaddleOCRv5 implemented in C++ with ncnn",
    "content": "I made a C++ implementation of PaddleOCRv5 that might be helpful to some people: https://github.com/Avafly/PaddleOCR-ncnn-CPP\n\nThe official Paddle C++ runtime has a lot of dependencies and is very complex to deploy. To keep things simple I use [ncnn](https://github.com/Tencent/ncnn) for inference, it's much lighter (and faster in my task), makes deployment easy. The code runs inference on the CPU, if you want GPU acceleration, most frameworks like ncnn let you enable it with just a few lines of code.\n\nHope this helps, and feedback welcome!",
    "author": "Knok0932",
    "timestamp": "2025-08-28T04:31:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1n29q0e/p_paddleocrv5_implemented_in_c_with_ncnn/",
    "score": 16,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n28w7j",
    "title": "[P] Built Sparrow: A custom language model/NLP tool for microcontrollers",
    "content": "Hey everyone,\n\nDon't know if it fully matches this subreddit, but since there have been a lot of discussions around LLMs using a lot of power and water, and even more discussions around LLMs plateauing, as everyone focuses on making the biggest and most powerful model.\n\nI've been super focused for a while now in bringing Language Models and complex NLP capabilities to microcontrollers and finally been able to finish the architecture and an ML Toolkit that enables training models from scratch, with this architecture and enables easy deployment on almost any MCUs.\n\nThe architecture uses state of the art methods, with many in-depth optimisations tested through over 1700 trained models, to get the most of every single memory byte and clock cycle, specifically for MCUs while also enabling extremely fast responses on PC.\n\nThe idea is to have domain specific and task specific models, using Sparrow's architecture, instead of a general prupose frontier model like ChatGPT/Llama etc. In the demo I showcase a Biology only model, that was made to give straight answrs (as per research papers showcasing that's what people want) for a question-answering chat-like system. Anything can be created. And then due to the model being only 50-200KB depending on how it is build (with twice that needed in total when flashed), mutiple models could be loaded in memory and a mixture-of-experts system can be designed. Which is what I want to explore with SPARROW 2.\n\nI still have to see exactly how to proceed in terms of making the code open-source, best licensing methods, how to create the API, etc. But the idea is that it would be easy to create language models for MCUs, similar to how Sci-kit Learn is used for regular ML.\n\nIt supports encoder, decoder, encoder-decoder models, and the fastest model uses linear attention, but I have also been able to deploy dot attention and additive attention on the ESP32.\n\nLet me know what you think!¬†[Here's a demo video](https://youtu.be/WCvv5W9gEiA?si=QCXvXei3qfp0qAG8)¬†with a ChatGPT simple-webapp to give people something they are familiar with. I'd also like to know opinions around the best way to go forward, release it as a website of sorts, release it as an API like Scikit Learn etc.\n\nI have a lot of videos with the models running on PC with full phrases/paragraphs outputs in less than 10 miliseconds, have different versions Small, Main, Large running on the ESP32S3, have the Main flavour running on the ESP32P4 which can process everything 5-6 times faster due to the intrustions available, and outputting a phrase every 50-100ms, compared to ESP32S3's 300-600ms.",
    "author": "c-f_i",
    "timestamp": "2025-08-28T03:46:50",
    "url": "https://reddit.com/r/MachineLearning/comments/1n28w7j/p_built_sparrow_a_custom_language_modelnlp_tool/",
    "score": 9,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n2579o",
    "title": "[D] Clarification on text embeddings models",
    "content": "I came across Gemini‚Äôs text embeddings model, and their documentation mentions that semantic similarity is suitable for recommendation tasks. They even provide this example:\n\t‚Ä¢\t‚ÄúWhat is the meaning of life?‚Äù vs ‚ÄúWhat is the purpose of existence?‚Äù ‚Üí 0.9481\n\t‚Ä¢\t‚ÄúWhat is the meaning of life?‚Äù vs ‚ÄúHow do I bake a cake?‚Äù ‚Üí 0.7471\n\t‚Ä¢\t‚ÄúWhat is the purpose of existence?‚Äù vs ‚ÄúHow do I bake a cake?‚Äù ‚Üí 0.7371\n\nWhat confuses me is that the ‚Äúcake‚Äù comparisons are still getting fairly high similarity scores, even though the topics are unrelated.\n\nIf semantic similarity works like this, then when I encode product profiles for my recommendation system, won‚Äôt many items end up ‚Äútoo close‚Äù in the embedding space? Does all the text embeddings model work that way ? \nAnd what is the best model or type of configuration could be suitable to my task ",
    "author": "AdInevitable1362",
    "timestamp": "2025-08-27T23:51:52",
    "url": "https://reddit.com/r/MachineLearning/comments/1n2579o/d_clarification_on_text_embeddings_models/",
    "score": 12,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n2jekd",
    "title": "[R] [EMNLP 2025] CCPS: Confidence from Consistency under Perturbation of States ‚Äî Superior Calibration Performance Across Benchmarks/Models",
    "content": "Hi everyone,\n\nOur paper **‚Äú*****Confidence from Consistency under Perturbation of States (CCPS)*****‚Äù** was accepted to the **EMNLP 2025 Main Conference**, placing in the **top 15% of accepted papers** with a **final meta-review rating of 9 (strong accept)**.\n\n# üîç Motivation\n\nLLMs don‚Äôt just make mistakes, they‚Äôre often confidently wrong. That‚Äôs fine when asking for trivia, but risky in domains like healthcare and finance. Reliable confidence estimation is critical for safe deployment.\n\n# ‚ú® What is CCPS?\n\nCCPS looks at the hidden states of an LLM. We apply small perturbations to the final hidden representations and observe how stable the prediction is:\n\n* If the answer remains stable ‚Üí the model was truly confident.\n* If the answer flips ‚Üí the confidence was unreliable.\n\nThis approach is simple, efficient, and does not require fine-tuning the base LLM.\n\n# üìä Results\n\nAcross LLaMA, Mistral, and Qwen on MMLU and MMLU-Pro, CCPS outperformed prior methods like LitCab and Calibration Tuning (CT):\n\n* **Calibration**: Error cut by more than 50%, down to \\~4.5% on the toughest benchmarks.\n* **Discrimination**: More accurate at telling right vs. wrong answers than prior SOTA (LitCab, CT, etc.).\n* **Performance**: Boosts accuracy and robustness, all without fine-tuning the base LLM.\n\n# üí° Why it matters\n\nCCPS delivers more reliable, better-calibrated LLMs, models that don‚Äôt just generate answers but also provide trustworthy confidence signals. This is key for high-stakes AI applications, especially in the medical and finance industries.\n\n# üìé Resources\n\n* üìÑ Paper: [arXiv link](https://arxiv.org/abs/2505.21772)\n* üíª Code: [GitHub repo](https://github.com/ledengary/CCPS)\n* üìä Data: [HF Dataset](https://huggingface.co/datasets/ledengary/CCPS)\n\nHappy to hear feedback, especially from anyone working on calibration, verifiers (for RL), or LLM deployment.",
    "author": "erfan_mhi",
    "timestamp": "2025-08-28T10:58:57",
    "url": "https://reddit.com/r/MachineLearning/comments/1n2jekd/r_emnlp_2025_ccps_confidence_from_consistency/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n1gucy",
    "title": "[D] How to do impactful research as a PhD student?",
    "content": "Hi everyone,\n\nI‚Äôm feeling a bit lost in my PhD journey and would really appreciate some outside perspectives.\n\nI‚Äôm doing a PhD on LLMs, and so far I‚Äôve been fairly productive: I‚Äôve published several first-author papers, some accepted at top conferences, others under review with good chances of acceptance. I‚Äôve also had a few successful collaborations.\n\nThe issue is that I don‚Äôt actually like my research. To be honest, I often feel a bit fraudulent, I rush through projects, produce papers that look solid and well-structured, but in the end, I think their impact is minimal. What I really want is to work on something meaningful and useful. But I keep running into two several obstacles:\n\n- Any problem I consider tackling already has an overwhelming amount of literature, making it difficult to figure out what truly matters.\n\n- While I‚Äôm trying to sort this out, there‚Äôs always the risk that someone else publishes a similar idea first, since so many people are working in this space.\n\n- I work with two supervisors which are both young and highly hambitius. They always propose me new research and collaboration but they never propose me hambitius project or give me time to think deep about something. I'm always involved in fast-paced project that lead to pubblication in few months.\n\n\nBecause of this, my current strategy has been to work quickly, run experiments fast, and push out papers, even if they‚Äôre not especially deep or important. I also see publications as my main leverage: since I‚Äôm at a low-ranked university in a unknown group, my publication record feels like the only card I can play to land some opportunities in top labs/companies.\n\nAt times, I think I just want to land an industry roles as a research engineer, where just having a good numbers of papers on my CV would be enough. But deep down, I do care about my work, and I want to contribute something that feels genuinely important.\n\nSo I‚Äôm curious: how do you approach doing meaningful research in such a competitive field? How do you balance the pressure to publish with the desire to work on something truly impactful?",
    "author": "kekkodigrano",
    "timestamp": "2025-08-27T06:19:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1n1gucy/d_how_to_do_impactful_research_as_a_phd_student/",
    "score": 138,
    "num_comments": 46,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n2i7iy",
    "title": "[R] ‚ÄúHow I‚Äôm structuring a 16M character dialogue corpus for persona reconstruction in LLMs‚Äù",
    "content": "In the past weeks, I‚Äôve been working on a somewhat ‚Äúcrazy‚Äù project:\nmanually splitting and structuring 16 million characters of dialogue data, preparing it for feeding into a model to reconstruct a persona module.\n\nAlong the way, I‚Äôve noticed a few technical challenges:\n\t1.\tFile size balance\nKeeping each file around 300k‚Äì400k characters is the most stable. Beyond that, performance tends to drop.\n\t2.\tContext continuity\nPoor segmentation can easily break the model‚Äôs sense of persona, resulting in inconsistent tone.\n\t3.\tTagging &amp; classification\nIt‚Äôs not just about cutting text, but also annotating emotional states and tonal shifts, so the model can later rebuild ‚Äúmemory‚Äù in a coherent way.\n\nThis made me realize that large-scale corpus curation is itself a kind of language engineering.\nIt‚Äôs not just data processing ‚Äî it shapes whether an AI can emerge as a whole presence.\n\nI‚Äôm curious:\nIn your NLP or LLM practice, how do you balance scale with contextual integrity?",
    "author": "Stunning_Put_6077",
    "timestamp": "2025-08-28T10:14:30",
    "url": "https://reddit.com/r/MachineLearning/comments/1n2i7iy/r_how_im_structuring_a_16m_character_dialogue/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.39,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n1k9ty",
    "title": "[R] ArchiFactory : Benchmark SLM architecture on consumer hardware, apples to apples",
    "content": "[35M Parameters : RWKV vs Mamba vs GQA vs RetNet](https://preview.redd.it/vul29llezklf1.png?width=1106&amp;format=png&amp;auto=webp&amp;s=c951d5647cd895d418b5a0863184cf9f6745397e)\n\n\n\nSince it's introduction, the Attention mechanism has been king in LLM architecture, but a few vaillant projects like RWKV, Mamba, Retnet, LiquidAI have been proposing several new mixin mecanisms over time, to attempt to dethrone the king.\n\n\n\nOne of the major issue is that LLM pretraining is extremely dependant on number of parameters and dataset choices, so performing an ablation study on new architecture is not an easy tricks.\n\n\n\nOn the other hand, I met many people with brillant ideas for new architecture and who never got the chance to put it to the test.\n\nFor that purpose, i create ArchiFactory, a simple (&lt;500 lines of codes) and modular repo that enables to pretrain Small Language Models with comparable parameter count and architecture tricks, in a couple of hours on a single 3090 level GPU.\n\n\n\nIncluded:\n\n\\- simple modular architecture to be sure to compare similar stuff\n\n\\- complete optimized training loop using pytorch lightning\n\n\\- fp8 training (can achieve &lt;20min training on 5090 grade GPU)\n\n\\- examples of common modules like FFN, MOE, GQA, Retnet, Mamba, RWKV6 etc.\n\n\\- guidelines to test integrate new modules\n\n\n\nLink: [https://github.com/gabrielolympie/ArchiFactory](https://github.com/gabrielolympie/ArchiFactory)",
    "author": "AdventurousSwim1312",
    "timestamp": "2025-08-27T08:32:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1n1k9ty/r_archifactory_benchmark_slm_architecture_on/",
    "score": 21,
    "num_comments": 0,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n1ug7b",
    "title": "[P] jupytercad-mcp: MCP server for JupyterCAD to control it using LLMs/natural language.",
    "content": "Demo: https://github.com/user-attachments/assets/7edb31b2-2c80-4096-9d9c-048ae27c54e7\n\nRepo: https://github.com/asmith26/jupytercad-mcp",
    "author": "Material_Pool_986",
    "timestamp": "2025-08-27T14:56:41",
    "url": "https://reddit.com/r/MachineLearning/comments/1n1ug7b/p_jupytercadmcp_mcp_server_for_jupytercad_to/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n2c588",
    "title": "[D] Where to find vast amounts of schemas for AI model training?",
    "content": "**\\[D\\] Looking for massive schema collections for training models**\n\nworking on a project and need to find vast amounts of schemas for training models. specifically looking for financial data (transactions, market data, etc) and retail/ecommerce stuff (product catalogs, user behavior, sales data) but honestly need schemas from pretty much every domain I can get. anyone know where to find quality structured schemas at scale? open to paid sources too. need thousands of different schema types ideally. thanks!",
    "author": "Fragrant-Dog-3706",
    "timestamp": "2025-08-28T06:24:31",
    "url": "https://reddit.com/r/MachineLearning/comments/1n2c588/d_where_to_find_vast_amounts_of_schemas_for_ai/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.3,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n1mboq",
    "title": "[P] Implemented GRPO on top of Karpathy's makemore",
    "content": "Hey all! I wanted to share my recent project where I implemented the GRPO (Group Relative Policy Optimization) algorithm on top of the [makemore](https://github.com/karpathy/makemore) repo.\n\nI wanted to understand how the algorithm works and was trying to find small-scale toy problems where I can implement my own version and see if it works. I had a couple of ideas at first but then I settled on this one idea: to implement the algorithm on top of the makemore project where my goal would be to finetune the character-level language model to generate names with more vowels! So the reward is essentially the number of vowels you have in the generated names.\n\nGRPO is actually a simplified version of PPO (which itself is a derivative of TRPO), and while its predecessors are rather complicated to fully grasp unless you have some background in policy gradient or RL in general, GRPO is much simpler to understand and code up (e.g., you don't have to worry about writing Generalized Advantage Estimation etc.)\n\nFeel free to take a look and share your thoughts! Here's the repo: [https://github.com/souvikshanku/makemore-grpo/](https://github.com/souvikshanku/makemore-grpo/)",
    "author": "Good-Alarm-1535",
    "timestamp": "2025-08-27T09:48:26",
    "url": "https://reddit.com/r/MachineLearning/comments/1n1mboq/p_implemented_grpo_on_top_of_karpathys_makemore/",
    "score": 14,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n23r3t",
    "title": "[R] Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies",
    "content": "**TL;DR.**¬†We introduce¬†**discrete diffusion**¬†as the action decoder¬†**inside a single transformer**¬†for VLA. Two simple components‚ÄîAdaptive decoding order and Secondary re-masking‚Äîyield consistent action refinement and outperform AR and continuous-diffusion heads. Trains with the¬†**same cross-entropy objective**¬†as VLMs, preserving pretrained priors. This design shows better success rates vs AR and continuous diffusion.  \n**Disclosure:**¬†I‚Äôm an author.\n\n**What‚Äôs new**\n\n* **First discrete-diffusion action head for VLA**¬†(to our knowledge).\n* **Single-transformer, VLM-style training:**¬†keeps the discrete token interface and uses the same CE loss as the VLM backbone ‚Üí¬†**maximizes retention of pretrained VLM priors**.\n* **Adaptive decoding order:**¬†in each refinement round, we¬†**keep easy tokens first**¬†via confidence / confidence-gap scores and a cosine keep schedule; the rest remain masked for the next round.\n* **Secondary re-masking:**¬†previously kept tokens are¬†**re-checked**¬†(threshold + residual-drop) and¬†**re-masked**¬†if uncertain/inconsistent, enabling robust cross-round error correction.\n\n**Why it matters**\n\n* For robotics manipulation tasks, unlike continuous diffusion decoders, our formulation keeps action generation inside a unified transformer and trains with the same cross-entropy objective used by VLMs. This¬†**preserves the backbone‚Äôs pretrained vision-and-language capability**‚Äîakin to extending a vocabulary‚Äîwhile opening a path to¬†**inherit unified transformers‚Äô scaling behavior**, paving the way for¬†**large-scale VLA**. Moreover, Discrete Diffusion VLA¬†**breaks the left-to-right bottleneck**¬†of AR decoders: action chunks are¬†**adaptively decoded in parallel**¬†over a small, fixed number of steps, and uncertain tokens can be revisited via iterative re-masking, leveraging full cross-modal context (including inter-action dependencies) for refinement.\n\n**Links**\n\n* Paper:¬†[https://arxiv.org/abs/2508.20072](https://arxiv.org/abs/2508.20072)\n* Demo videos:¬†[https://huggingface.co/papers/2508.20072](https://huggingface.co/papers/2508.20072)",
    "author": "Lonely-Loquat9638",
    "timestamp": "2025-08-27T22:21:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1n23r3t/r_discrete_diffusion_vla_bringing_discrete/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n1pcj7",
    "title": "[D] Anyone successfully running LLMs fully on Apple Neural Engine (ANE)?",
    "content": "Has anyone managed to get near-full ANE utilization for large language models on Apple silicon?\n\nIn my experiments:\n\n* Core ML conversions run, but ANE usage seems capped &lt;20%.\n* Apple‚Äôs own foundation models reportedly hit close to 100% ANE.\n\n**Questions:**\n\n* Has anyone here seen full (or close to full) ANE usage for LLMs?\n* Are there known tricks or constraints (model architecture, quantization, Core ML flags) that unlock more ANE execution?\n* Any open-source repos, discussions, or Apple docs you‚Äôd point to?\n\nWould love to hear practical experiences‚Äîsuccesses, failures, or hard limits you‚Äôve hit.",
    "author": "AlanzhuLy",
    "timestamp": "2025-08-27T11:40:00",
    "url": "https://reddit.com/r/MachineLearning/comments/1n1pcj7/d_anyone_successfully_running_llms_fully_on_apple/",
    "score": 6,
    "num_comments": 8,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n1e9c1",
    "title": "[R] Is stacking classifier combining BERT and XGBoost possible and practical?",
    "content": "Suppose a dataset has a structured features in tabular form but in one column there is a long text data. Can we use stacking classifier using boosting based classifier in the tabular structured part of the data and bert based classifier in the long text part as base learners. And use logistic regression on top of them as meta learner. I just wanna know if it is possible specially using the boosting and bert as base learners. If it is possible why has noone tried it (couldn‚Äôt find paper on it)‚Ä¶ maybe cause it will probably be bad?",
    "author": "Altruistic_Bother_25",
    "timestamp": "2025-08-27T04:19:31",
    "url": "https://reddit.com/r/MachineLearning/comments/1n1e9c1/r_is_stacking_classifier_combining_bert_and/",
    "score": 20,
    "num_comments": 20,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n1fsa3",
    "title": "[D] short write up on how to implement custom optimizers in Optax",
    "content": "Hi, I was trying to implement the muon optimizer in JAX and found there was no proper documentation about how to hack optax for custom optimizers so tried to write a mini blog about it.\n\nhttps://slavozard.bearblog.dev/implementcustomoptimizerwithoptax/\n\nFeedback appreciated.",
    "author": "FreakedoutNeurotic98",
    "timestamp": "2025-08-27T05:34:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1n1fsa3/d_short_write_up_on_how_to_implement_custom/",
    "score": 13,
    "num_comments": 1,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n1tdcl",
    "title": "Arxiv submission on hold  [R]",
    "content": "Hey \nLooking for information online about the on hold status but couldn‚Äôt find very clearly. The on hold is automatic or normal? Or if some sort of problem was found ? \n\nI already have a DOI from Zenodo, but wanted to publish on arxiv as it seems to be the norm currently. It‚Äôs my first publication there, so I‚Äôm not sure what the process is exactly. \n\nThanks! ",
    "author": "OkOwl6744",
    "timestamp": "2025-08-27T14:14:49",
    "url": "https://reddit.com/r/MachineLearning/comments/1n1tdcl/arxiv_submission_on_hold_r/",
    "score": 0,
    "num_comments": 11,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0r8b7",
    "title": "I built a tool to benchmark tokenizers across 100+ languages and found some wild disparities [R]",
    "content": "**TL;DR:** Created [tokka-bench](https://tokka-bench.streamlit.app/) to compare tokenizers across languages. Turns out your fine-tune's multilingual performance might suck because of tokenization, not architecture. Also explains why proprietary models (Claude, GPT, Gemini) are so much better at non-English tasks.\n\n**Links:**\n\n* [Live dashboard](https://tokka-bench.streamlit.app/)\n* [Full blog post](https://www.bengubler.com/posts/2025-08-25-tokka-bench-evaluate-tokenizers-multilingual)\n* [GitHub repo](https://github.com/bgub/tokka-bench)\n\nhttps://preview.redd.it/7i03jela9elf1.png?width=1724&amp;format=png&amp;auto=webp&amp;s=95378457970e6337b147e71d7a8f0ab2dd67cb91\n\n# The Problem Nobody Talks About\n\nI started this as a side quest while pretraining a multilingual model, but tokenization turned out to be way more important than expected. There are two hidden layers creating massive efficiency gaps:\n\n**UTF-8 encoding differences:**\n\n* English: \\~1 byte per character\n* Arabic: 2+ bytes per character\n* Chinese: 3+ bytes per character\n\n**Tokenization bias:** Most tokenizers are trained on English-heavy data, so they allocate way more vocabulary to English patterns. These compound into serious problems.\n\n# Why This Affects Performance\n\n**During training:** If you allocate tokens proportionally (10M English, 1M Khmer), the Khmer text has WAY less semantic content because it needs more tokens per word. Plus Khmer tokens end up being character-level instead of semantic units, making concept storage much harder.\n\n**During inference:** Low-resource languages need 2-3x more tokens per sentence:\n\n* Slower throughput (costs more to serve)\n* Context windows fill up faster\n* More chances to mess up during generation\n\n# What I Built\n\ntokka-bench measures four key things:\n\n1. **Efficiency** \\- bytes per token (compression quality)\n2. **Coverage** \\- unique tokens used (script representation)\n3. **Word splitting** \\- how often semantic units get fragmented\n4. **Subword fertility** \\- average tokens per semantic unit\n\n# Interesting Findings\n\nYou can actually reverse-engineer training data from tokenizer performance:\n\n* Kimi K2: Exceptional Mandarin coverage (obviously Chinese-trained)\n* Gemma 3: Strong Urdu/Hindi performance\n* gpt-oss: Good Arabic/Gujarati coverage\n\nWeirdest finding: Programming languages show almost identical efficiency across all tokenizers. Probably because everyone trains on GitHub with similar language distributions.\n\n# Technical Details\n\nBuilt on high-quality datasets (FineWeb, FineWeb-2, StarCoder). Samples 2MB per language and calculates per-language metrics. Has some limitations around cross-linguistic comparison due to UTF-8 differences, but great for comparing tokenizers on the same language.\n\nShoutout to Judit √Åcs for the original subword fertility metrics and Rust et al's ACL paper that laid the groundwork.\n\n**PS:** if you're from an AI lab and want to contribute your tokenizer's metrics (even if proprietary), please reach out! The community would benefit a lot from understanding how SOTA systems handle this stuff.\n\n*Posted this on LinkedIn/Twitter already but figured* r/MachineLearning *would appreciate the technical details. Happy to answer questions about methodology or findings!*",
    "author": "FutureIncrease",
    "timestamp": "2025-08-26T09:54:16",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0r8b7/i_built_a_tool_to_benchmark_tokenizers_across_100/",
    "score": 85,
    "num_comments": 23,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n127sr",
    "title": "Are Neurips workshop competitive? [R]",
    "content": "Hi y‚Äôall, I have a optimisation paper that is not quite ready for conference yet, and I see there are a few Neurips workshop coming up that fits my research direction. I‚Äôm wondering if it‚Äôs good to submit the work to the workshop?",
    "author": "ChoiceStranger2898",
    "timestamp": "2025-08-26T17:06:40",
    "url": "https://reddit.com/r/MachineLearning/comments/1n127sr/are_neurips_workshop_competitive_r/",
    "score": 16,
    "num_comments": 38,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0vcrb",
    "title": "[R] ŒîAPT: critical review aimed at maximizing clinical outcomes in AI/LLM Psychotherapy",
    "content": "Hi reddit, wanted to share my thesis on AI / LLM psychotherapy @ [https://osf.io/preprints/psyarxiv/4tmde\\_v1](https://osf.io/preprints/psyarxiv/4tmde_v1?fbclid=IwZXh0bgNhZW0CMTAAYnJpZBExNHhlVkhlWWpDVE1xN3dTeAEeoTtZ3pOVtRD7ODEFZo_qpyjjOEkW_2OFHqsH36X4xp7THoZC3F7YFDc1zJU_aem_Etq7yhCr4L3eA8v9QqrFgw)\n\nSince the rules for this subreddit require more than just a link, I thought I'd share some surprising conclusions in plain english. \n\n**1. AI therapy research tends to use arbitrary success metrics:** the majority of LLM research on psychotherapy uses theraputic-sounding ad-hoc metrics (e.g. \"empathy\" as rated by LLM-as-judge), and not actually improvement in clients or other validated metrics. There's a real risk in AI researchers testing techniques and drawing conclusions when totally unrelated to the purpose of therapy (e.g. quality-of-life improvement). If you're interested in learning more about this issue, section 1.4 focuses on it, and offers the north-star alternatives commonly used in psychotherapy research in sections 1.1-1.3. \n\n**2. AI therapy tools (APTs) are already comparable to human therapists:** There's two studies from 2025 (Limbic, Therabot) that demonstrate non-inferior clinical outcomes in LLM-driven APTs and human therapists for depression &amp; anxiety symptom reduction. If replicated, that's huge. That's a step-level jump in clinical from the previous generation of rules-based APTs (e.g. Woebot, Wysa), highlighting that maybe the generative properties of LLMs were the key gap to improve clinical performance. There's a lot more to say on these results, and if you're interested sections 2 &amp; 3.1 talk more about them and put them into clinical context. \n\n3. **ŒîAPT allows predicting future clinical outcomes :** It's actually surprising that APTs perform at the lower-bounds of human therapists, since they kinda suck right now. The predictive model I proposed is that APTs clinical performance is boosted by advantages therapist can't compete with (e.g. 24/7 availability, low cost), while being depressed by current disadvantages (e.g. poor therapy skills, hallucinations, sycophancy, inconsistencies, bias). All of this playing out while major issues around legality, safety, privacy and ethics are unresolved and could shutdown the field. If you're intersted, you can read more about the model (section 3.3),  the advantages of APTs over human therapists (section 3.4), APTs' current limitations (section 3.5), and the key risks (section 3.6). \n\nhttps://preview.redd.it/rof96tmbuelf1.png?width=1162&amp;format=png&amp;auto=webp&amp;s=5a1e81bbb9e8b12b09210967da97b2fe96816df0\n\n  \n**4. Techniques teaching LLM therapy:** Most people on this subreddit won't be surprised to learn you can teach LLM to perform therapy using a combination of context/prompt engineering, fine-tuning, multi-agent architecture, and ML models. What is surprising is that both clinically-validated APTs use ML models to offset the stochastic nature of LLMs, especially for safety purposes. Also surprising is that neither used a multi-agentic architecture. Therabot used fine-tuning on synthetic dialogues, and Limbic used context-engineering techniques. You can learn more about implementing therapy skills in LLM through context/prompt engineering (section 4.1), fine-tuning (section 4.2), multi-agent architectures (section 4.3), ML models (4.4). Around fine-tuning / pretraining there's a really nested conversation about data requirements, ethically sourcing transcripts, and choosing therapy modalities in section 4.1. \n\nhttps://preview.redd.it/lbcoovvc0flf1.png?width=2246&amp;format=png&amp;auto=webp&amp;s=f029fed00649b4cca0ddb84d9830ded03f5f94ea\n\n5. **Overall, most disadvantages of LLMs are addressable in AI therapy**: Reading the literature critiquing APTs it's really easy to get discouraged thinking for examples \"oh wow, hallucinations are going to make AI therapy impossible\". But actually, there's a bunch of techniques that can be used to mitigate the issues LLMs currently have. Combining the lowering rates of issues in newer LLMs released with mitigation techniques, most issues can theoretically be significantly mitigated in production. The outlier here being sycophancy which doesn't appear to have great mitigations on subjective topics. You can read more about the issues of LLMs in APTs and how to mitigate those in section 5. \n\n**6. video therapy with multi-modal audio/video LLMs:** One surprising fact from psychotherapy research is that therapy done over video (e.g. zoom) is actually as effective as in-person therapy. Ideally, LLMs would be able to pickup and transmit non-verbal cues over video-audio. Having an virtual therapy avatar using audio &amp; video to attune to clients isn't actually that far off based on my literature review. Surprisingly it seems that emotional speech, and attuning to clients facial and body expressions are ready for implementation in AI therapy today. More on that in section 6.\n\nHappy to have a conversation, receive critique, and answer questions here. This summary above was meant to offer informal insights into what is an otherwise quite lengthy paper. For more formal discussion and details, it's really best to read the paper. ",
    "author": "JustinAngel",
    "timestamp": "2025-08-26T12:28:44",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0vcrb/r_Œ¥apt_critical_review_aimed_at_maximizing/",
    "score": 119,
    "num_comments": 5,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n10vyv",
    "title": "[D] Tips &amp; tricks for preparing slides/talks for ML Conferences?",
    "content": "I'm a PhD student in HCI, and I recently had a paper accepted at a B-ranked ML conference. While I have prior experience presenting at HCI venues, this will be my first time presenting at an ML conference.\n\nI want to know if there are any tips or best practices for preparing slides and giving talks in the ML community. Are there particular presentation styles, slide formats, or expectations that differ from HCI conferences?\n\nThanks in advance for your advice!",
    "author": "SoggyClue",
    "timestamp": "2025-08-26T16:08:01",
    "url": "https://reddit.com/r/MachineLearning/comments/1n10vyv/d_tips_tricks_for_preparing_slidestalks_for_ml/",
    "score": 10,
    "num_comments": 7,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n1ebmk",
    "title": "[R] Computational power needs for Machine Learning/AI",
    "content": "Hi everyone!\n\nAs part of my internship, I am conducting research to understand the computational power needs of professionals who work with machine learning and AI. The goal is to learn how different practitioners approach their requirements for GPU and computational resources, and whether they prefer cloud platforms (with inbuilt ML tools) or value flexible, agile access to raw computational power.\n\nIf you work with machine learning (in industry, research, or as a student), I‚Äôd greatly appreciate your participation in the following survey. Your insights will help inform future solutions for ML infrastructure.\n\nThe survey will take about two to three minutes. Here¬¥s the link:¬†[https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)\n\nThank you for your time! Your feedback is invaluable for understanding and improving ML infrastructure for professionals.",
    "author": "Any_Commercial7079",
    "timestamp": "2025-08-27T04:22:40",
    "url": "https://reddit.com/r/MachineLearning/comments/1n1ebmk/r_computational_power_needs_for_machine_learningai/",
    "score": 1,
    "num_comments": 13,
    "upvote_ratio": 0.54,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n1p7rb",
    "title": "[D] I reviewed 100 models over the past 30 days. Here are 5 things I learnt.",
    "content": "I reviewed 100 models over the past 30 days. Here are 5 things I learnt.\n\nTL;DR: Spent a month testing every AI model for work, a few tools I'm building and RL. Build task-specific evals. Most are overhyped, a few are gems, model moats are ephemeral, and routers/gateways are the real game-changer.\n\nSo I've been building a few evaluation tools, RHLF and RL environments for the past few months so I decided to be extra and test literally everything.\n\n100 models. 30 days. Too much coffee :( Here's what I found:  \n  \n1. Model moats are ephemeral\n\nModel moats don't last and it can be hard to pay for many subscriptions if you're building for users and machines. What's SOTA today gets beaten in 2 months. Solution: Use platforms like Groq, OpenRouter, FAL, Replicate etc\n\nMy system now routes based on task complexity: Code generation, Creativity, Complex reasoning and Code generation.\n\n2. Open source FTW\n\nThe gap is closing FAST. Scratch that. The gap between open and closed models has basically disappeared. If you're not evaluating open-source options, you're missing 80% of viable choices. From Deepseek, Qwen to Kimi, these models help you build quick MVPs at little or no cost. If you do care about privacy, Ollama and LMStudio are really good for local deployment.\n\n3.Benchmarks are mostly decieving due to reward hacking\n\nBenchmaxxing is a thing now. Models are increasingly being trained on popular eval sets, and it's actually annoying when models that scored \"high\" but sucked in practice. It's also why I'm a huge fan of human preference evaluation platforms that are not easily gamed (real world vs benchmarks). Build your own task-specific evals.\n\n4.Inference speed is everything\n\nSpeed matters more than you think. Users don't care if your model is 2% more accurate if it takes 30 seconds to respond. Optimize for user experience, not just accuracy. Which leads me to..\n\n5.Task-specific models &gt; general purpose models for specialized work.\n\nNo 4 is also a huge reason why I'm a huge fan of small models finetuned for special tasks. Model size doesn't predict performance.\n\nTest small models first etc Llama 3.2 1B, smolLLM, moondream etc and see if you can get a huge boost by finetuning them on domain tasks rather than just deploying a big SoTA general purpose model. Cost way lesser and usually faster.\n\nWhat models are in your current prod stack? Any hidden gems I missed in the open source space?",
    "author": "function-devs",
    "timestamp": "2025-08-27T11:35:12",
    "url": "https://reddit.com/r/MachineLearning/comments/1n1p7rb/d_i_reviewed_100_models_over_the_past_30_days/",
    "score": 0,
    "num_comments": 15,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0t4hu",
    "title": "[D] Do Industry Research Roles Care about Findings vs. Main (in ACL, NAACL, EMNLP, etc.)?",
    "content": "Basically the title. Obviously the quality of the work and relevance to the role is very important, but all else being equal, what is the perceived prestige difference between Findings and Main in NLP conferences? This would be with regard to getting research internships and research scientist positions.",
    "author": "Look-Asleep",
    "timestamp": "2025-08-26T11:03:23",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0t4hu/d_do_industry_research_roles_care_about_findings/",
    "score": 14,
    "num_comments": 12,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n12su6",
    "title": "[P] Building a CartPole agent from scratch, in C++",
    "content": "I‚Äôm still pretty new to reinforcement learning (and machine learning in general), but I thought it would be fun to try building my own CartPole agent from scratch in C++.\n\nIt currently supports PPO, Actor-Critic, and REINFORCE policy gradients, each with Adam and SGD (with and without momentum) optimizers.\n\nI wrote the physics engine from scratch in an Entity-Component-System architecture, and built a simple renderer using SFML.\n\nRepo: www.github.com/RobinLmn/cart-pole-rl\n\nWould love to hear what you think, and any ideas for making it better!",
    "author": "[deleted]",
    "timestamp": "2025-08-26T17:33:39",
    "url": "https://reddit.com/r/MachineLearning/comments/1n12su6/p_building_a_cartpole_agent_from_scratch_in_c/",
    "score": 3,
    "num_comments": 7,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0jwj7",
    "title": "[P] DocStrange - Structured data extraction from images/pdfs/docs",
    "content": "I previously shared the open‚Äësource library DocStrange. Now I have hosted it as a free to use web app to upload pdfs/images/docs to get clean structured data in Markdown/CSV/JSON/Specific-fields and other formats.\n\n**Live Demo:**¬†[**https://docstrange.nanonets.com**](https://docstrange.nanonets.com/)\n\n**Github:** [**https://github.com/NanoNets/docstrange**](https://github.com/NanoNets/docstrange)\n\nWould love to hear feedbacks!\n\nhttps://i.redd.it/gl23k00osclf1.gif\n\nOriginal Post - [https://www.reddit.com/r/MachineLearning/comments/1mh9g3r/p\\_docstrange\\_open\\_source\\_document\\_data\\_extractor/](https://www.reddit.com/r/MachineLearning/comments/1mh9g3r/p_docstrange_open_source_document_data_extractor/)\n\n",
    "author": "LostAmbassador6872",
    "timestamp": "2025-08-26T05:01:41",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0jwj7/p_docstrange_structured_data_extraction_from/",
    "score": 29,
    "num_comments": 10,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0qwzm",
    "title": "[D] Analyzed 402 healthcare ai repos and built the missing piece",
    "content": "I looked through 402 healthcare AI repos on GitHub and found almost 50% of infrastructure tools are just solving data format conversion problems, suggesting a systematic gap between ML research and deployment in clinical settings.\n\nBuilt HealthChain to bridge Python ML workflows with healthcare data standards (FHIR, HL7, etc.) without the usual pain. 4 years of NHS NLP development experience went into making this feel like normal Python.\n\nPost + pretty graphs: https://open.substack.com/pub/jenniferjiangkells/p/healthchain-building-the-tool-i-wish?r=4o6h4\n\nCode: https://github.com/dotimplement/HealthChain\n\nAnyone else work in healthcare AI here? Would love to learn what you‚Äôre working on!",
    "author": "beautiful-potato",
    "timestamp": "2025-08-26T09:42:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0qwzm/d_analyzed_402_healthcare_ai_repos_and_built_the/",
    "score": 11,
    "num_comments": 1,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0njtk",
    "title": "[R] Exploring interpretable ML with piecewise-linear regression trees (TRUST algorithm)",
    "content": "A recurring challenge in ML is balancing **interpretability** and **predictive performance**. We all know the classic tradeoff: simple models like linear regression or short CART-style regression trees are transparent but often lack enough accuracy, while complex ensembles like Random Forests and XGBoost are accurate but opaque.\n\nWe‚Äôve been working on a method called **TRUST** (*Transparent, Robust and Ultra-Sparse Trees*). The core idea is to go beyond constant values in the leaves of a tree. Instead, TRUST fits a sparse regression model (either linear or constant) in each leaf, resulting in a **piecewise-linear tree** that remains interpretable.\n\nIn our [recent paper](https://arxiv.org/abs/2506.15791), accepted at PRICAI 2025, we compared this method against a range of models on 60 datasets. While we were encouraged by the results ‚Äî TRUST consistently outperformed other interpretable models and closed much of the accuracy gap with Random Forests ‚Äî we'd like to hear your thoughts on this topic.\n\nThe problem we‚Äôre tackling is widespread. In many real-world applications, a \"black box\" model isn't an option. We've often found ourselves in situations where we had to choose between a sub-par interpretable model or an accurate but untrustworthy one.\n\nHere‚Äôs a concrete example from a [tutorial on explaining EU life satisfaction](https://github.com/adc-trust-ai/trust-free/blob/main/notebooks/trust-free_tutorial.ipynb).\n\n[TRUST produces a single interpretable tree, while Random Forest uses hundreds of deep trees to achieve similar accuracy.](https://preview.redd.it/3tzdaim3kdlf1.png?width=2600&amp;format=png&amp;auto=webp&amp;s=e289771608b0d74498dc83b39c1efd2670ed8ea9)\n\nAs the image above shows, both TRUST and a Random Forest achieve \\~85% test R¬≤ ‚Äî but one produces a **single interpretable tree**.\n\nTRUST is implemented as a free Python package on PyPI called `trust-free`.\n\n**Discussion:** How do you usually handle the interpretability vs. accuracy tradeoff in your own regression projects? What methods, beyond the standard ones, have you found effective? We‚Äôre looking forward to hearing your perspectives.",
    "author": "illustriousplit",
    "timestamp": "2025-08-26T07:35:44",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0njtk/r_exploring_interpretable_ml_with_piecewiselinear/",
    "score": 12,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0d12h",
    "title": "[D] An honest attempt to implement \"Attention is all you need\" paper",
    "content": "I have started working on implementing actual research papers in machine learning and I have started with \"Attention is all you need\" paper.\n\nI have implemented all the code and it is an educational attempt. I would like you to get some eyes on the repo from the members of this subreddit and get your opinion. This is still a work in progress but your reviews and PRs are really appreciated. I have written the code focusing on educational purposes and not optimisations. Please take a look below.\n\n[https://github.com/MayukhSobo/Transformer](https://github.com/MayukhSobo/Transformer)\n\nEdit: I would like to clarify that some of the code related to helper functions and all the doc strings are implemented by Claude not because they are difficult to do but they are simply boring. The core architecture is implemented by me. Also at no point I claimed that this is my own work and I haven't used AI. The part which really required me to code and not use AI, I did it on my own. If you really think that the complete code is just a result of some vibe coding, I welcome you to try that with most advanced AI tools and see if you can reproduce even 70% of what I did or not. ",
    "author": "ZealousidealSalt7133",
    "timestamp": "2025-08-25T22:01:32",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0d12h/d_an_honest_attempt_to_implement_attention_is_all/",
    "score": 62,
    "num_comments": 18,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0q4d9",
    "title": "[D] Looking for a self-hosted alternative to Modal.com for running ML workloads",
    "content": "Hey folks \n\n\n\nI've been using [Modal.com](http://Modal.com) (I am not affiliated) for a while to run machine learning workloads in the cloud, and I really like its simplicity, container-based execution, and ability to scale on demand. However, I'm starting to explore more self-hosted options due to cost reasons and to gain more control over the infrastructure while building apps.\n\n\n\nDoes anyone know of good self-hosted alternatives that offer similar functionality? Ideally, something that:\n\n\n\n\\- Supports containerized jobs (Docker or similar)\n\n\\- Can run Python/ML workloads easily\n\n\\- Has a nice API  for launching jobs (this is important) \n\n\\- Offers some kind of job orchestration or scheduling\n\n\\- Bonus: GPU support and autoscaling would be amazing\n\n\n\n\n\nThanks in advance \n\n",
    "author": "devops_to",
    "timestamp": "2025-08-26T09:13:01",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0q4d9/d_looking_for_a_selfhosted_alternative_to/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0j8u0",
    "title": "[D] Ano: updated optimizer for noisy Deep RL ‚Äî now on arXiv (feedback welcome!)",
    "content": "Hi everyone,\n\nA few weeks ago I shared my first preprint on a new optimizer,¬†Ano, designed for noisy and highly non-convex environments such as deep RL. Thanks to all the feedback I received here, I‚Äôve updated the paper: clarified the positioning, fixed some mistakes, and added an Atari benchmark to strengthen the empirical section.\n\nüîó¬†**arXiv link:**¬†[https://arxiv.org/abs/2508.18258](https://arxiv.org/abs/2508.18258)  \nüì¶¬†**Install via pip:**¬†`pip install ano-optimizer`  \nüíª¬†**Code &amp; experiments:**¬†[github.com/Adrienkgz/ano-experiments](https://github.com/Adrienkgz/ano-experiments)\n\nQuick recap of the idea: Ano separates the momentum¬†direction¬†from the gradient magnitude, aiming to improve robustness and stability compared to Adam in noisy deep RL training. The updated version also includes a¬†convergence proof¬†in standard non-convex stochastic settings.\n\nThis is still my first research contribution, so I‚Äôd love to hear your thoughts ‚Äî whether on the method itself, the experiments, or the clarity of the writing. Any feedback, comments, or constructive criticism are very welcome üôè\n\nThanks again to everyone who took the time to give feedback last time, it really helped me make the work stronger!\n\nAdrien",
    "author": "Adrienkgz",
    "timestamp": "2025-08-26T04:28:41",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0j8u0/d_ano_updated_optimizer_for_noisy_deep_rl_now_on/",
    "score": 8,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0eyrb",
    "title": "[P] Exosphere: an open source runtime for dynamic agentic graphs with durable state. results from running parallel agents on 20k+ items",
    "content": "Disclosure: I am one of the authors. Links will be in the first comment per sub rules.\n\nTLDR  \nWe are releasing Exosphere, an open source runtime and durable state manager for agentic workflows that need dynamic branching, retries, and parallel execution. To evaluate it on a real workload, we built WhatPeopleWant, an agent that mines Hacker News discussions and posts distilled problem statements to X every 2 hours. This post shares the setup, workload design, and the ablations we are running, and invites feedback on methodology.\n\nSingle runs are trivial. At scale you need to\n\n1. fan out across large inputs\n2. branch at runtime on model outputs\n3. retry with idempotency\n4. persist every step for audit and replay\n5. mix CPU and GPU stages\n6. resume after faults.\n\nExosphere‚Äôs runtime treats agents like graphs with explicit state, a scheduler, and observability.\n\nWe use WhatPeopleWant as a standing benchmark. It ingests Hacker News via the public Firebase API, scores and routes items, optionally enriches high-signal threads, and materializes candidate problem statements. The bot then posts outputs on a fixed schedule.\n\n‚Ä¢ Gating high-signal discussions reduces heavy-model calls and improves tail behavior at similar quality thresholds  \n‚Ä¢ Durable state and idempotent nodes make partial replays predictable and minimize upstream rework after faults  \n‚Ä¢ Parallelism helps until external API backpressure dominates, which shows up in queue depth and wait times\n\nWhat I want feedback on  \n‚Ä¢ Composite metrics that capture quality, cost, and reliability for agentic graphs  \n‚Ä¢ Fair baselines for orchestration when branching is dynamic  \n‚Ä¢ Better failure-injection and replay methodologies to compare runtimes\n\nFirst comment with links",
    "author": "jain-nivedit",
    "timestamp": "2025-08-26T00:02:17",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0eyrb/p_exosphere_an_open_source_runtime_for_dynamic/",
    "score": 6,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0wdsi",
    "title": "[R] What makes active learning or self learning successful ?",
    "content": "Maybe I am confused between two terms \"active learning\" and \"self-learning\". But the basic idea is to use a trained model to classify bunch of unannotated data to generate pseudo labels, and train the model again with these generated pseudo labels.  Not sure \"bootstraping\" is relevant in this context.\n\nA lot of existing works seem to use such techniques to handle data. For example, SAM (Segment Anything) and lots of LLM related paper, in which they use LLM to generate text data or image-text pairs and then use such generated data to finetune the LLM.\n\nMy question is why such methods work?  Will the error be accumulated since the pseudo labels might be wrong?",
    "author": "AaronSpalding",
    "timestamp": "2025-08-26T13:08:04",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0wdsi/r_what_makes_active_learning_or_self_learning/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mzxtzb",
    "title": "[D]GEPA: Reflective Prompt Evolution beats RL with 35√ó fewer rollouts",
    "content": "A new preprint (Agrawal et al., 2025) introduces¬†**GEPA (Genetic-Pareto Prompt Evolution)**, a method for adapting compound LLM systems. Instead of using reinforcement learning in weight space (GRPO), GEPA mutates prompts while reflecting in natural language on traces of its own rollouts.\n\nThe results are striking:\n\n* GEPA outperforms GRPO by up to¬†**19%**¬†while using¬†**35√ó fewer rollouts**.\n* It also consistently surpasses MIPROv2, the state-of-the-art prompt optimizer.\n* In many cases, only a few hundred rollouts were sufficient, compared to tens of thousands for RL .\n\nThe shift is conceptual as much as empirical: Where RL collapses complex trajectories into a scalar reward, GEPA treats those trajectories as¬†*textual artifacts*¬†that can be reflected on, diagnosed, and evolved. In doing so, it makes use of the medium in which LLMs are already most fluent, language, instead of trying to push noisy gradients through frozen weights.\n\nWhat‚Äôs interesting is the infra angle: GEPA‚Äôs success in multi-hop QA hinges on generating better second-hop queries.¬†**That implicitly elevates retrieval infrastructure Linkup, Exa, Brave Search into the optimization loop itself**. Likewise, GEPA maintains a pool of Pareto-optimal prompts that must be stored, indexed, and retrieved efficiently.¬†**Vector DBs such as Chroma or Qdrant are natural substrates for this kind of evolutionary memory.**\n\nThis work suggests that the real frontier may not be reinforcement learning at scale, but¬†**language-native optimization loops**¬†where reflection, retrieval, and memory form a more efficient substrate for adaptation than raw rollouts in parameter space.\n\nhttps://preview.redd.it/5l4lcmokg7lf1.png?width=1602&amp;format=png&amp;auto=webp&amp;s=719e33f34feb5103ed1f375d3366745dd3415d77\n\n",
    "author": "No_Marionberry_5366",
    "timestamp": "2025-08-25T11:02:33",
    "url": "https://reddit.com/r/MachineLearning/comments/1mzxtzb/dgepa_reflective_prompt_evolution_beats_rl_with/",
    "score": 53,
    "num_comments": 15,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0zndc",
    "title": "[D] Laptop Suggestion for PhD in ML for Robotics",
    "content": "Hi!\n\nI'll be starting a PhD in ML for Robotics (RL, Sensor Fusion etc.) and was wondering which laptop would be best to support me throughout the next 4 years. I am looking for a powerful laptop, with good battery life, not too heavy and that is robust.\n\nMy budget is $3000.\n\nSo far, I have identified the following laptops, but am unsure which would be the best choice.\n\n\\-¬†**Razer Blade 16**¬†(either RTX 5070 Ti + 32GB RAM ($3100) or RTX 5080 + 64GB ($4050)): apart from battery life which is not the most ideal, would I see a significant difference when running RL simulations (IsaacGym) or large multimodal (video, imu, ...) ML models between both configurations? Price difference between both configurations is \\~$850 (with taxes) which is significant.\n\n\\-¬†**MSI Vector 16¬†HX¬†AI**¬†(RTX‚ÄØ5080, 64‚ÄØGB) - $2600\n\n\\-¬†**ThinkPad P1 Gen 7**¬†(RTX Ada 3000, 64GB) - $3200: has a good battery life, but its GPU is Ada series, which is not the best for RL simulations.\n\n\\-¬†**Legion Pro 7i Gen10**¬†(RTX 5080, 32GB) - $3100: the legions are usually very heavy laptops.\n\nEssentially, I am looking for a laptop that will be somewhat future-proof to the fast pace of new GPUs coming out, is powerful for my intended use (RL simulations + ML sensor fusion), has a good battery life (for note-taking in courses) and easily transportable (ie. neither too bulky nor heavy). Also, do I require RTX 5080 (recommended for IsaacSim) as GPU, and how big a diffference is 32GB vs 64GB RAM?\n\nThank you in advance for any suggestions or feedback!\n\nEDIT: I have access to cluster, but thought having powerful laptop could be useful when running real-time inference on robot + working with smaller models / testing out stuff before training on cluster.",
    "author": "SwissMountaineer",
    "timestamp": "2025-08-26T15:15:24",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0zndc/d_laptop_suggestion_for_phd_in_ml_for_robotics/",
    "score": 0,
    "num_comments": 21,
    "upvote_ratio": 0.28,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0h48h",
    "title": "[D] SOTA solution for quantization",
    "content": "Hello researchers,\n\n  \nI am familiar with common basic approaches to quantization, but after a recent interview, I wonder what the current SOTA approaches are, which are actually used in industry.\n\n  \nThanks for the discussion!",
    "author": "Blackliquid",
    "timestamp": "2025-08-26T02:24:50",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0h48h/d_sota_solution_for_quantization/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0jxbk",
    "title": "[P] Spam vs. Ham NLP Classifier ‚Äì Feature Engineering vs. Resampling",
    "content": "I built a spam vs ham classifier and wanted to test a different angle: instead of just oversampling with SMOTE, could **feature engineering** help combat extreme class imbalance?\n\n**Setup:**\n\n* Models: Na√Øve Bayes &amp; Logistic Regression\n* Tested with and without SMOTE\n* Stress-tested on 2 synthetic datasets (one ‚Äúnormal but imbalanced,‚Äù one ‚Äúadversarial‚Äù to mimic threat actors)\n\n**Results:**\n\n* Logistic Regression ‚Üí **97% F1** on training data\n* New imbalanced dataset ‚Üí Logistic still best at **75% F1**\n* Adversarial dataset ‚Üí **Na√Øve Bayes** surprisingly outperformed with **60% F1**\n\n**Takeaway:** Feature engineering can mitigate class imbalance (sometimes rivaling SMOTE), but adversarial robustness is still a big challenge.\n\nCode + demo:  \nüîó [PhishDetective ¬∑ Streamlit](https://phishdetective.streamlit.app/)  \nüîó [ahardwick95/Spam-Classifier: Streamlit application that classifies whether a message is spam or ham.](https://github.com/ahardwick95/Spam-Classifier/tree/main)\n\nCurious ‚Äî when you deal with **imbalanced NLP tasks**, do you prefer resampling, cost-sensitive learning, or heavy feature engineering?",
    "author": "Total_Noise1934",
    "timestamp": "2025-08-26T05:02:41",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0jxbk/p_spam_vs_ham_nlp_classifier_feature_engineering/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mzsrt2",
    "title": "[D] Too much of a good thing: how chasing scale is stifling AI innovation",
    "content": "Dear¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/)¬†friends,\n\nHello everyone! I hope you are all doing well out there.\n\nI've been observing a pattern in the AI research field that I can only describe as a \"Mass Amnesia.\" It seems we're forgetting the valuable research paths we were on before the ChatGPT moment.\n\nIn my latest blog post, I argue that while scaling up LLMs was initially a courageous endeavour, the current obsession and monoculture around it is actively keeping us stuck. Instead of building on a diverse set of ideas, we're chasing a single approach, which I believe is making us amnesiacs about what came before and what's possible.\n\nI'd love for you to read my spicy takes and share your own. Let's tear my arguments and ideas apart. ;)\n\nüîó¬†**Full Article:**[https://pieces.app/blog/the-cost-of-ai-scaling](https://pieces.app/blog/the-cost-of-ai-scaling)\n\nI look forward to your arguments and thoughts.\n\nRegards,\n\nAntreas\n\n  \nPS. This is a repost of [https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d\\_too\\_much\\_of\\_a\\_good\\_thing\\_how\\_chasing\\_scale\\_is/](https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d_too_much_of_a_good_thing_how_chasing_scale_is/) because it was removed without any explanation and the mods never replied to my queries on what was done wrong and how I could modify the post so it would abide by whatever rule I inadvertently tripped on.  \n\nThe post was starting to get some real discussion going when it was removed and wanted to give this another chance as I want to hear what everyone has to say and engage in discourse. ",
    "author": "AntreasAntoniou",
    "timestamp": "2025-08-25T07:58:08",
    "url": "https://reddit.com/r/MachineLearning/comments/1mzsrt2/d_too_much_of_a_good_thing_how_chasing_scale_is/",
    "score": 12,
    "num_comments": 26,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n0e7s1",
    "title": "[D]How can AI teams stay agile and adaptable when project goals or data requirements change midstream?",
    "content": "For those working in AI/ML, how do you keep your teams agile when project goals or data requirements shift halfway through a project? I‚Äôve seen situations where a model was nearly production-ready, but then stakeholders introduced new objectives or the data pipeline changed, forcing big pivots.\n",
    "author": "Tesocrat",
    "timestamp": "2025-08-25T23:13:49",
    "url": "https://reddit.com/r/MachineLearning/comments/1n0e7s1/dhow_can_ai_teams_stay_agile_and_adaptable_when/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mzmrm5",
    "title": "[P] aligning non-linear features with your data distribution",
    "content": "For some time I've been fascinated by adopting knowledge from approximation theory into ML feature engineering, and I'm sharing my learnings in a series of blog posts, mainly about various polynomial bases as features.\n\nSo here is the latest one: [https://alexshtf.github.io/2025/08/19/Orthogonality.html](https://alexshtf.github.io/2025/08/19/Orthogonality.html)\n\nIt discusses my understanding of orthogonal bases as informative feature generators. I hope you enjoy reading as I enjoy learning about it.",
    "author": "alexsht1",
    "timestamp": "2025-08-25T03:25:38",
    "url": "https://reddit.com/r/MachineLearning/comments/1mzmrm5/p_aligning_nonlinear_features_with_your_data/",
    "score": 19,
    "num_comments": 3,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n00ruv",
    "title": "[P] GPU-based backend deployment for an app",
    "content": "Hi all!  \nI'm drafting an app with pose detection (currently using¬†**MediaPipe**) and object detection (early¬†**Yolo11**). Since I cannot run these models on the phone itself, I'm developing the backend separately to be deployed somewhere, to then¬†*call it from the app when needed*.  \nBasically I would need a¬†**GPU-based backend**¬†(I can also divide the detections and the actual result usage).\n\nNow, I know about¬†*HuggingFace*¬†of course and I've seen a lot of other hosting platforms, but I wanted to ask if you have any suggestions in this regards?  \nI think I might want to release it as free, or for a one-time low cost (if the costs are too high to support myself), but I also do not know how widespread it can be... You know, either useful and loved or unknown to most.  \nThe trick is that, since I would need the APIs always ready to respond, the backend would need to be up and¬†*running 24/7*. All of the options seem to be quite costly...\n\nIs there any better or worse way to do this?",
    "author": "feller94",
    "timestamp": "2025-08-25T12:54:03",
    "url": "https://reddit.com/r/MachineLearning/comments/1n00ruv/p_gpubased_backend_deployment_for_an_app/",
    "score": 2,
    "num_comments": 9,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mzd5kt",
    "title": "[D] Views on LLM Research: Incremental or Not?",
    "content": "Hi folks,  \nFellow ML researcher here üëã\n\nI‚Äôve been working in the LLM space for a while now, especially around *reasoning models* and *alignment* (both online and offline).\n\nWhile surveying the literature, I couldn‚Äôt help but notice that a lot of the published work feels‚Ä¶ well, incremental. These are papers coming from great labs, often accepted at ICML/ICLR/NeurIPS, but many of them don‚Äôt feel like they‚Äôre really pushing the frontier.\n\nI‚Äôm curious to hear what the community thinks:\n\n* Do you also see a lot of incremental work in LLM research, or am I being overly critical?\n* How do you personally filter through the ‚Äúnoise‚Äù to identify genuinely impactful work?\n* Any heuristics or signals that help you decide which papers are worth a deep dive?\n\nWould love to get different perspectives on this ‚Äî especially from people navigating the same sea of papers every week.\n\n  \nPS: Made use of GPT to rewrite the text, but it appropriately covers my view/questions",
    "author": "Fantastic-Nerve-4056",
    "timestamp": "2025-08-24T18:11:09",
    "url": "https://reddit.com/r/MachineLearning/comments/1mzd5kt/d_views_on_llm_research_incremental_or_not/",
    "score": 55,
    "num_comments": 26,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mzsn1q",
    "title": "[D] Anyone know how to get Cornell's OpenSurfaces dataset?",
    "content": "Was it abandoned? The website links are dead.",
    "author": "Mplus479",
    "timestamp": "2025-08-25T07:53:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1mzsn1q/d_anyone_know_how_to_get_cornells_opensurfaces/",
    "score": 2,
    "num_comments": 10,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n01odu",
    "title": "[D] Cold start latency for large models: new benchmarks show 141B in ~3.7s",
    "content": "Some interesting benchmarks I‚Äôve been digging into:\n\t‚Ä¢~1.3s cold start for a 32B model\n\t‚Ä¢~3.7s cold start for Mixtral-141B (on A100s)\n       ‚Ä¢By comparison, Google Cloud Run reported ~19s for Gemma-3 4B earlier this year, and most infra teams assume 10‚Äì20s+ for 70B+ models (often minutes).\n\nIf these numbers hold up, it reframes inference as less of an ‚Äúalways-on‚Äù requirement and more of a ‚Äúruntime swap‚Äù problem.\n\nOpen questions for the community:\n\t‚Ä¢How important is sub-5s cold start latency for scaling inference?\n\t‚Ä¢Would it shift architectures away from dedicating GPUs per model toward more dynamic multi-model serving?",
    "author": "pmv143",
    "timestamp": "2025-08-25T13:28:04",
    "url": "https://reddit.com/r/MachineLearning/comments/1n01odu/d_cold_start_latency_for_large_models_new/",
    "score": 0,
    "num_comments": 23,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mzwck3",
    "title": "[D] How do you derive real insights and interpret experiment data beyond just looking at metrics?",
    "content": "When running experiments, I often struggle with going beyond the surface-level metrics. How do you approach interpreting experimental data in a way that actually leads to useful insights and new ideas? What frameworks, statistical methods, or mindset shifts help you decide whether results are meaningful versus just noise?",
    "author": "DolantheMFWizard",
    "timestamp": "2025-08-25T10:08:32",
    "url": "https://reddit.com/r/MachineLearning/comments/1mzwck3/d_how_do_you_derive_real_insights_and_interpret/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mz70e2",
    "title": "[R] Review advice:  Well-established work published years ago on Arxiv",
    "content": "I'm reviewing for AAAI, and wanted to ask the community for some advice. I got a paper for review that is very well known in my subfield, published in 2023, but only previously published onto Arxiv. As best I can tell, the paper has had some minor rewrites for publication, but is otherwise largely the same as the well-established work. What's the best policy here? It was a very good paper when it came out, but the existing version basically ignores the last two years of work by the community, in part because some decent portion of that work is based on this paper.  Any advice on the best way to review this would be appreciated",
    "author": "drahcirenoob",
    "timestamp": "2025-08-24T13:46:57",
    "url": "https://reddit.com/r/MachineLearning/comments/1mz70e2/r_review_advice_wellestablished_work_published/",
    "score": 35,
    "num_comments": 9,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mzqu1q",
    "title": "[D] MALM: A Modular Adapter-based Language Model (paper + Hugging Face link)",
    "content": "Hey everyone, I just finished writing a short paper about a new idea I call MALM, a Modular Adapter-based Language Model.\n\nThe core idea is simple: instead of training giant multilingual LLMs, I propose keeping one small, sharp Core Language Model (reasoning in English), and delegating translation to lightweight, swappable Specialized Translation Adapters (STAs).\n\nThis means:\n\n\\- Smaller, cheaper models\n\n\\- Easy to add new languages\n\n\\- Better for edge devices and low-resource settings\n\nExample flow:  \n\\`\\`\\`  \nUser: \"Translate 'my name is Adam' into German.\"  \nCLM ‚Üí &lt;to:de&gt; my name is Adam &lt;/to&gt;  \nSTA ‚Üí \"Mein Name ist Adam\"\n\n\\`\\`\\`\n\nRead the full paper here:¬†[https://huggingface.co/TimesLast/MALM](https://huggingface.co/TimesLast/MALM)\n\nWould love feedback, especially on how this could be extended beyond translation (math, code, multimodal adapters, etc.).",
    "author": "TimesLast_",
    "timestamp": "2025-08-25T06:42:08",
    "url": "https://reddit.com/r/MachineLearning/comments/1mzqu1q/d_malm_a_modular_adapterbased_language_model/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mzp8au",
    "title": "[R] Got 6min? I need YOUR help for my PhD!",
    "content": "Hello everyone!\n\nMy name is Virginie and I am a first-year French PhD student¬†**studying human‚Äìartificial intelligence interactions.**\n\nI am conducting a¬†**very quick**¬†(approximately 6 minutes) and¬†**anonymous online study**.\n\nTo ensure reliable results, I need at least 300 AI users, some of whom should have experience in integrating or designing AI models, although this is not compulsory for taking part!\n\nIf you are 18 or over, you can take part by clicking this link:\n\n[https://virginie-lepont.limesurvey.net/967745?newtest=Y&amp;lang=en](https://virginie-lepont.limesurvey.net/967745?newtest=Y&amp;lang=en)\n\nThe survey is¬†**also available in French.**\n\nEvery response is valuable! Thank you so much for your help!\n\nVirginie \n\n*This post has been approved by one moderator of this group.* \n\nhttps://preview.redd.it/gwtpg6p9t5lf1.jpg?width=940&amp;format=pjpg&amp;auto=webp&amp;s=39e54c6e762ab220af6a1c32d8754d8c9b5ee34c\n\n",
    "author": "Ok-Ebb6307",
    "timestamp": "2025-08-25T05:33:24",
    "url": "https://reddit.com/r/MachineLearning/comments/1mzp8au/r_got_6min_i_need_your_help_for_my_phd/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1n055zr",
    "title": "[P] Training LLMs without code - Would you use it?",
    "content": "https://preview.redd.it/vy1h49l0t8lf1.png?width=3456&amp;format=png&amp;auto=webp&amp;s=1c0991294abf01d6699c04b663cd30973e4bd633\n\nIs Vibe training AI models something people want?   \n  \nI made a quick 24hours YC hackathon app that wires HF dataset lookups + Synthetic data pipeline + Trnasfomers too quickly fine tune a gemma 3 270m on a mac, I had 24hours to ship something and now have to figure out if this is something people would like to use?   \n  \nWhy this is useful? A lot of founders I've talked to want to make niche models, and/or make more profit (no SOTA apis) and overall build value beyond wrappers. And also, my intuition is that training small LLMs without code will enable researchers of all fields to tap into scientific discovery. I see people using it for small tasks classifiers for example. \n\nFor technical folk, I think an advanced mode that will let you code with AI, should unleash possibilities of new frameworks, new embedding, new training technics and all that. The idea is to have a purposeful built space for ML training, so we don't have to lean to cursor or Claude Code. \n\nI'm looking for collaborators and ideas on how to make this useful as well?\n\nAnyone interested can DM, and also signup for beta testing at [monostate.ai](http://monostate.ai)  \n  \nSomewhat overview at [https://monostate.ai/blog/training](https://monostate.ai/blog/training)  \n\n\\*\\*The project will be free to use if you have your own API keys!\\*\\* \n\nIn the beginning no Reinforcement learning or VLMs would be present, focus would be only in chat pairs fine tuning and possibly classifiers and special tags injection! \n\nPlease be kind, this is a side project and I am not looking for replacing ML engineers, researchers or anything like that. I want to make our lifes easier, that's all. ",
    "author": "OkOwl6744",
    "timestamp": "2025-08-25T15:45:10",
    "url": "https://reddit.com/r/MachineLearning/comments/1n055zr/p_training_llms_without_code_would_you_use_it/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.18,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mzhsh7",
    "title": "[P] Yelp Dataset clarification: Is review_count colomn cheating?",
    "content": "Hey everyone,\n\nI'm working with the Yelp dataset and have a quick question about the review_count field in the business.json (what I'll call the business_df).\n\nThe business_df is a list of businesses, and the review_df is a list of every single review interaction.\n\nIs the review_count in the business_df calculated directly from the interactions listed in the review_df?\n\nIf I split my data into train and test sets for a recommendation model, should I recalculate review_count from only the training interactions (so that test interactions remain unseen)? Or is review_count a static field provided by Yelp, independent of our data splits?\n\nThe reason I'm asking is I'd like to use review_count as part of my initial features/embeddings. I'm not sure if I should treat it as fixed metadata from Yelp or recompute it dynamically from my training set only.\n\nThanks a lot if anyone can clarify this!",
    "author": "AdInevitable1362",
    "timestamp": "2025-08-24T22:10:55",
    "url": "https://reddit.com/r/MachineLearning/comments/1mzhsh7/p_yelp_dataset_clarification_is_review_count/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mzpoo4",
    "title": "[P] Open-Source Agentic AI for Company Research",
    "content": "I open-sourced a project called Mira, an agentic AI system built on the OpenAI Agents SDK that automates company research.\n\nYou provide a company website, and a set of agents gather information from public data sources such as the company website, LinkedIn, and Google Search, then merge the results into a structured profile with confidence scores and source attribution.\n\nThe core is a Node.js/TypeScript library (MIT licensed), and the repo also includes a Next.js demo frontend that shows live progress as the agents run.\n\nGitHub: [https://github.com/dimimikadze/mira](https://github.com/dimimikadze/mira)",
    "author": "DimitriMikadze",
    "timestamp": "2025-08-25T05:54:15",
    "url": "https://reddit.com/r/MachineLearning/comments/1mzpoo4/p_opensource_agentic_ai_for_company_research/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mybwih",
    "title": "[D] How did JAX fare in the post transformer world?",
    "content": "A few years ago, there was a lot of buzz around JAX, with some enthusiasts going as far as saying it would disrupt PyTorch. Every now and then, some big AI lab would release stuff in JAX or a PyTorch dev would write a post about it, and some insightful and inspired discourse would ensue with big prospects. However, chatter and development have considerably quieted down since transformers, large multimodal models, and the ongoing LLM fever. Is it still promising? \n\nOr at least, this is my impression, which I concede might be myopic due to my research and industry needs. ",
    "author": "TajineMaster159",
    "timestamp": "2025-08-23T13:19:36",
    "url": "https://reddit.com/r/MachineLearning/comments/1mybwih/d_how_did_jax_fare_in_the_post_transformer_world/",
    "score": 152,
    "num_comments": 73,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mz9ruc",
    "title": "[P] AI Learns to play Sonic 2 Emerald Hill (Deep Reinforcement...",
    "content": "Hello everyone!!! I have several Reinforcement Learning projects underway. One is Sonic 2 with PPO. The other is developing an environment that supports games not available with Farama Group's stable-retro. I may need collaborators for the latter. I don't know if I'll integrate it into their project, stable-retro, in the future. One thing I've already achieved is running PCSX2 (it's missing the state loading option), and I'm creating a Python lib to load with stable-baselines3, etc. If anyone is interested, the links to both projects are below:\n\n[https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning](https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning)\n\n[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)I also started a PCSX2 environment with direct access to the Python process, but I'll abandon it as it's very slow.\n\n  \n",
    "author": "AgeOfEmpires4AOE4",
    "timestamp": "2025-08-24T15:38:42",
    "url": "https://reddit.com/r/MachineLearning/comments/1mz9ruc/p_ai_learns_to_play_sonic_2_emerald_hill_deep/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.45,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mywuni",
    "title": "[P] options on how to balance my training dataset",
    "content": "I'm working on developing a ML classification project using Python, divided into 5 output categories (classes). However, my training dataset is extremely unbalanced, and my results always lean toward the dominant class (class 5, as expected).\n\nHowever, I wanted my models to better learn the characteristics of the other classes, and I realized that one way to do this is by balancing the training dataset. I tried using SMOTETomek for oversampling, but my models didn't respond well. Does anyone have any ideas or possibilities for balancing my training dataset?\n\nThere are 6 classification ML models that will ultimately be combined into an ensemble. The models used are: RandomForest, DecisionTree, ExtraTrees, AdaBoost, NaiveBayes, KNN, GradientBoosting, and SVM.\n\nThe data is also being standardized via standardSCaler.\n\nTotal record count by category:\n\nCategory 1: 160 records\n\nCategory 2: 446 records\n\nCategory 3: 605 records\n\nCategory 4: 3,969 records\n\nCategory 5: 47,874 records",
    "author": "Pedro_Silva95",
    "timestamp": "2025-08-24T07:20:49",
    "url": "https://reddit.com/r/MachineLearning/comments/1mywuni/p_options_on_how_to_balance_my_training_dataset/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.44,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1myj9jk",
    "title": "[R] routers to foundation models?",
    "content": "Are there any projects/packages that help inform an agent which FM to use for their use case? Curious if this is even a strong need in the AI community? Anyone have any experience with ‚Äúrouters‚Äù?\n\nUpdate: especially curious about whether folks implementing LLM calls at work or for research (either one offs or agents) feel this as a real need or is it just a nice-to-know sort of thing? Intuitively, cutting costs while keeping quality high by routing to FMs that optimize for just that seems like a valid concern, but I‚Äôm trying to get a sense of how much of a concern it really is\n\nOf course, the mechanisms underlying this approach are of interest to me as well. I‚Äôm thinking of writing my own router, but would like to understand what‚Äôs out there/what the need even is first",
    "author": "electricsheeptacos",
    "timestamp": "2025-08-23T18:49:27",
    "url": "https://reddit.com/r/MachineLearning/comments/1myj9jk/r_routers_to_foundation_models/",
    "score": 7,
    "num_comments": 20,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1myoooy",
    "title": "[D] Poles of non-linear rational features",
    "content": "Suppose I want to fit a linear model to non-linear **rational** features. Something like `RationalTransformer` instead of `SplineTransformer` in Scikit-Learn, that uses a basis of rational functions. The domain of my raw features before being transformed are (theoretically) unbounded non-negative numbers, such as \"time since X happened\", \"total time spent on the website\", or \"bid in an auction\".\n\nSo here is the question: *where would you put the poles? Why?*\n\nNote, I'm not aiming on fitting one rational curve, so algorithms in the spirit of AAA are irrelevant. I'm aiming at a component I can use in a pipeline that transformes features before model fitting, such as `MinMaxScaler` or `SplineTransformer` in scikit-learn.",
    "author": "alexsht1",
    "timestamp": "2025-08-23T23:53:56",
    "url": "https://reddit.com/r/MachineLearning/comments/1myoooy/d_poles_of_nonlinear_rational_features/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mylqrb",
    "title": "[R] Building a deep learning image model system to identify BJJ positions in matches",
    "content": "Hey all, I'm working on developing AI models that can classify and track positions throughout BJJ matches - and I'm keen to get some thoughts on this idea early on.\n\nYou can check it out here:¬†[https://bjjhq.ai/](https://bjjhq.ai/)\n\nUltimately BJJHQ provides an interactive positional timeline beneath match videos, showing all position changes throughout the match, so you're able to instantly jump to specific positions and see how transitions unfold.\n\nThe idea is that people would be able to search for not only a competitor, but a specific position and combination (e.g., \"Gordon Ryan in back control\"), and instantly access all matches where that scenario occurs. You would also be able to filter and sort matches by time spent in specific positions.\n\nRoadmap:\n\n* Expanding the match database and position categories\n* Technique/submission recognition\n* Automated scoring system built on this positional foundation\n\nWould love to know if anyone would be interested to chat or collaborate on this project ... please reach out if keen!\n\nThanks for any feedback!",
    "author": "UnholyCathedral",
    "timestamp": "2025-08-23T21:01:13",
    "url": "https://reddit.com/r/MachineLearning/comments/1mylqrb/r_building_a_deep_learning_image_model_system_to/",
    "score": 3,
    "num_comments": 6,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1myptun",
    "title": "[D] Neurips 2025: Are there post conference events on the last day of the conference?",
    "content": "\\[EDIT\\] I meant December / Dec not November / Nov. It was late at night I'm sorry -  lol.\n\nContext:\n\n* Neurips 2025 conference is from Tue, Dec 2 to Sun, Dec 7\n* This is my first time attending the conference.\n* As I need to travel again right after the conference for personal reasons, I am figuring out on what dates to book the hotels / flights in advance.\n* **Are there post conference events on the last day** eg: Sun, Dec 7 night? I am not sure if it's better to return right away (on Sun, Dec 7 evening) or fly back later (on Mon, Dec 8 morning)?",
    "author": "Snoo71505",
    "timestamp": "2025-08-24T01:06:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1myptun/d_neurips_2025_are_there_post_conference_events/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1myr68a",
    "title": "[D] Exploring Local-First AI Workflow Automation",
    "content": "**[D] Exploring Local-First AI Workflow Automation**\n\nHi all,  \n\nI‚Äôve been experimenting with an open-source approach to AI workflow automation that runs entirely **locally** (no cloud dependencies), while still supporting real-time data sources and integrations. The goal is to provide a **privacy-first, resource-efficient alternative** to traditional cloud-heavy workflow tools like Zapier or n8n, but with LLM support integrated.\n\nüëâ My question for the community:  \nHow do you see **local-first AI workflows** impacting ML/AI research, enterprise adoption, and robotics/IoT systems where privacy, compliance, and cost efficiency are critical?  \n\n- Repo: [Agentic Signal](https://github.com/code-forge-temple/agentic-signal) (open-source, AGPL v3 / commercial dual license)  \n- Demo video: [YouTube link](https://youtu.be/62zk8zE6UJI)  \n\nWould love feedback from both the research and applied ML communities on potential use cases, limitations, or challenges you foresee with this approach.  \n\nThanks!  \n",
    "author": "Code-Forge-Temple",
    "timestamp": "2025-08-24T02:32:18",
    "url": "https://reddit.com/r/MachineLearning/comments/1myr68a/d_exploring_localfirst_ai_workflow_automation/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1myhj41",
    "title": "[D] Topology and geometry in deep learning beyond TDL/GDL",
    "content": "Posts here within the past 6 months have discussed both¬†[Topological Deep Learning (TDL)](https://www.reddit.com/r/MachineLearning/comments/1ji6xlv/d_topological_deep_learning_promising_or_hype/)¬†and¬†[Geometric Deep Learning (GDL)](https://www.reddit.com/r/MachineLearning/comments/1jabkt8/d_geometric_deep_learning_and_its_potential/). Even though the nomenclature suggests otherwise, these two (exciting!) areas have come to represent rather specific topics in recent years.¬†*Very crudely speaking*, \"TDL\" seems to focus mainly on higher-order message passing (HOMP); \"GDL\" to the design of neural networks mod domain symmetries.\n\nFor the purposes of discussion, let's set the operational definition of TDL to be as in this paper:¬†[Hajij, Mustafa, et al. Topological Deep Learning: Going Beyond Graph Data. Springer, 2024.](https://tdlbook.org/)\n\nand the operational definition of GDL to be as in this paper:¬†[Bronstein, Michael M., et al. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. MIT Press, 2021.](https://arxiv.org/abs/2104.13478)\n\nWith that in place: what are some applications of geometry and topology in deep learning that¬†*do not properly belong to TDL and GDL as defined above*¬†(and as have already received recent posts here)? Applications of adjacent fields are also welcome- algebra, category theory, etc.- , as are applications in the converse direction.",
    "author": "Creative_Star_9425",
    "timestamp": "2025-08-23T17:22:07",
    "url": "https://reddit.com/r/MachineLearning/comments/1myhj41/d_topology_and_geometry_in_deep_learning_beyond/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mxrt1y",
    "title": "[D] AAAI considered 2nd tier now?",
    "content": "Isn‚Äôt AAAI in the same tier as NeurIPS/ICML/ICLR? \nICLR literally has &gt;30% acceptance rate.",
    "author": "Healthy_Horse_2183",
    "timestamp": "2025-08-22T21:19:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1mxrt1y/d_aaai_considered_2nd_tier_now/",
    "score": 66,
    "num_comments": 68,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mx4a6c",
    "title": "[D] Why does BYOL/JEPA like models work? How does EMA prevent model collapse?",
    "content": "\nI am curious on your takes on BYOL/JEPA like training methods and the intuitions/mathematics behind why the hell does it work?\n\nFrom an optimization perspective, without the EMA parameterization of the teacher model, the task would be very trivial and it would lead to model collapse. However, EMA seems to avoid this. Why?\n\nSpecifically:\n\nHow can a network learn semantic embeddings without reconstructing the targets in the real space? Where is the learning signal coming from? Why are these embeddings so good?\n\nI had great success with applying JEPA like architectures to diverse domains and I keep seeing that model collapse can be avoided by tuning the LR scheduler/EMA schedule/masking ratio. I have no idea why this avoids the collapse though.",
    "author": "ComprehensiveTop3297",
    "timestamp": "2025-08-22T04:47:44",
    "url": "https://reddit.com/r/MachineLearning/comments/1mx4a6c/d_why_does_byoljepa_like_models_work_how_does_ema/",
    "score": 47,
    "num_comments": 13,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mxyqku",
    "title": "[D] Is MLSys a low-tier conference? I can't find it in any of the rankings",
    "content": "[https://mlsys.org/](https://mlsys.org/)",
    "author": "huopak",
    "timestamp": "2025-08-23T04:15:41",
    "url": "https://reddit.com/r/MachineLearning/comments/1mxyqku/d_is_mlsys_a_lowtier_conference_i_cant_find_it_in/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mxw9c1",
    "title": "[P] I built a ML-regression model for Biathlon that beats current betting market odds",
    "content": "Hello ya'll!\n\nI recently built a ML-regression model to predict the unpredictable sport of biathlon. In biathlon, external factors such as weather, course profiles and altitude play huge roles in determining who wins and when. But when taking these factors into play, in addition of athletes' past performances, you can score surprisingly high accuracy.\n\nThis is how well the model performed when predicting athlete ranks (0 = winner, 1 = last place) using 10 years of historic biathlon data:  \n\\- MAE (average error): 0.14 -&gt; 4-18 places off depending on race size  \n\\- RMSE: 0.18 -&gt; penalizing big prediction misses  \n\\- R¬≤: -&gt; the model explains \\~62% of the variation in finish order\n\nNow what does these metrics say?  \n\\- The model almost cuts in half random guessing (\\~25% error)  \n\\- It consistently outperforms the accuracy of betting odds in the current market, meaning it has a predictive edge.  \n\\- It is able to tell the majority of happenings (62%), which is very rare in a sport where surprises happen very often.\n\nNext steps:  \n\\- Build R¬≤ up to 70% using more complex feature engineering and data preprocessing.  \n\\- Launch a SaaS that sells these odds for businesses and private consumers.",
    "author": "JesuXd",
    "timestamp": "2025-08-23T01:46:29",
    "url": "https://reddit.com/r/MachineLearning/comments/1mxw9c1/p_i_built_a_mlregression_model_for_biathlon_that/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.17,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mwxfxj",
    "title": "[D] Using LLMs to extract knowledge graphs from tables for retrieval-augmented methods ‚Äî promising or just recursion?",
    "content": "I‚Äôve been thinking about an approach where large language models are used to extract structured knowledge (e.g., from tables, spreadsheets, or databases), transform it into a knowledge graph (KG), and then use that KG within a Retrieval-Augmented Generation (RAG) setup to support reasoning and reduce hallucinations.\n\nBut here‚Äôs the tricky part: this feels a bit like ‚ÄúLLMs generating data for themselves‚Äù ‚Äî almost recursive. On one hand, structured knowledge could help LLMs reason better. On the other hand, if the extraction itself relies on an LLM, aren‚Äôt we just stacking uncertainties?\n\nI‚Äôd love to hear the community‚Äôs thoughts:\n\n* Do you see this as a viable research or application direction, or more like a dead end?\n* Are there promising frameworks or papers tackling this ‚Äúself-extraction ‚Üí RAG ‚Üí LLM‚Äù pipeline?\n* What do you see as the biggest bottlenecks (scalability, accuracy of extraction, reasoning limits)?\n\nCurious to know if anyone here has tried something along these lines.",
    "author": "Puzzled_Boot_3062",
    "timestamp": "2025-08-21T21:55:41",
    "url": "https://reddit.com/r/MachineLearning/comments/1mwxfxj/d_using_llms_to_extract_knowledge_graphs_from/",
    "score": 14,
    "num_comments": 15,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mwbq81",
    "title": "[P] Language Diffusion in &lt;80 Lines of Code",
    "content": "Hi! Lately, I've been looking into diffusion language models and thought I should try and replicate part of the paper [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992) by Nie et al. (2025). With the help of Hugging Face's Transformers, it took &lt;80 lines of code to implement the training script. I finetuned [DistilBERT](https://huggingface.co/distilbert/distilbert-base-cased) on the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset, and the results were better than expected!\n\n[Generating tiny stories via a reverse language diffusion process](https://i.redd.it/sm9xtdpdpdkf1.gif)\n\nYou can view the project at https://github.com/gumran/language-diffusion. I will appreciate any feedback/comments/stars!",
    "author": "bjjonin",
    "timestamp": "2025-08-21T06:59:54",
    "url": "https://reddit.com/r/MachineLearning/comments/1mwbq81/p_language_diffusion_in_80_lines_of_code/",
    "score": 94,
    "num_comments": 33,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mx775g",
    "title": "[D] Low-budget hardware for on-device object detection + VQA?",
    "content": "Hey folks,\n\nI‚Äôm an undergrad working on my FYP and need advice. I want to:\n\n* Run¬†object detection¬†on medical images (PNGs).\n* Do¬†visual question answering¬†with a ViT or small LLaMA model.\n* Everything fully¬†on-device¬†(no cloud).\n\nBudget is tight, so I‚Äôm looking at Jetson boards (Nano, Orin Nano, Orin NX) but not sure which is realistic for running a quantized detector + small LLM for VQA.\n\nAnyone here tried this? What hardware would you recommend for the best balance of cost + capability?\n\nThanks!",
    "author": "fishandtech",
    "timestamp": "2025-08-22T06:55:29",
    "url": "https://reddit.com/r/MachineLearning/comments/1mx775g/d_lowbudget_hardware_for_ondevice_object/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mxih41",
    "title": "[P] Relational PDF Recall (RFC + PoC) ‚Äì Structured storage + overlay indexing experiment",
    "content": "I‚Äôve been exploring how far we can push¬†*relational database structures inside PDFs*¬†as a substrate for AI recall. Just published a first draft RFC + PoC:\n\n* Channel splitting (text/vector/raster/audio streams)\n* Near-lossless transforms (wavelet/FLAC-style)\n* Relational indexing across channels (metadata + hash linking)\n* Early geometry-only overlays (tiling + Z-order indexing)\n\nRepo + notes:¬†[https://github.com/maximumgravity1/relational-pdf-recall](https://github.com/maximumgravity1/relational-pdf-recall)\n\nThis is still very early (draft/PoC level), but I‚Äôd love feedback on:\n\n* Whether others have tried similar recall-layer ideas on top of PDFs.\n* If this approach overlaps with knowledge-graph work, or if it opens a different lane.\n* Pitfalls I might be missing re: indexing/overlays.\n\n  \n**UPDATE 1: üìå Repo + DOI now live**   \nGitHub: [https://github.com/maximumgravity1/pdf-hdd-rfc](https://github.com/maximumgravity1/pdf-hdd-rfc)  \nDOI (always latest): [https://doi.org/10.5281/zenodo.16930387](https://doi.org/10.5281/zenodo.16930387)",
    "author": "Gloomy_Situation5126",
    "timestamp": "2025-08-22T14:06:55",
    "url": "https://reddit.com/r/MachineLearning/comments/1mxih41/p_relational_pdf_recall_rfc_poc_structured/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mxcd2j",
    "title": "[P] Need to include ANN, LightGBM, and KNN results in research paper",
    "content": "Hey everyone,\n\nI‚Äôm working on a research paper with my group, and so far we‚Äôve done a comprehensive analysis using **Random Forest**. The problem is, my professor/supervisor now wants us to also include results from **ANN, LightGBM, and KNN** for comparison.\n\nWe need to:\n\n* Run these models on the dataset,\n* Collect performance metrics (accuracy, RMSE, R¬≤, etc.),\n* Present them in a **comparison table** with Random Forest,\n* Then update the writing/discussion accordingly.\n\nI‚Äôm decent with Random Forests but not as experienced with ANN, LightGBM, and KNN. Could anyone guide me with example code, a good workflow, or best practices for running these models and compiling results neatly into a table?",
    "author": "sukhoi-30mki",
    "timestamp": "2025-08-22T10:12:13",
    "url": "https://reddit.com/r/MachineLearning/comments/1mxcd2j/p_need_to_include_ann_lightgbm_and_knn_results_in/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mw2z1y",
    "title": "[D] PhD vs startup/industry for doing impactful AI research ‚Äî what would you pick?",
    "content": "Hi all,\n\nI‚Äôm deciding between starting a PhD at a top university (ranked \\~5‚Äì10) with a great professor (lots of freedom, supportive environment) or going straight into industry.\n\nMy long-term goal is to work on the frontier of intelligence, with more focus on research than pure engineering. My background is mostly around LLMs on the ML side, and I already have a few A\\* conference papers (3‚Äì4), so I‚Äôm not starting from scratch.\n\nIndustry (likely at a smaller lab or startup) could give me immediate opportunities, including large-scale distributed training and more product-driven work. The lab I‚Äôd join for the PhD also has strong access to compute clusters and good chances for internships/collaborations, though in a more research-focused, less product-driven setting. The typical timeline in this lab is \\~4 years + internship time.\n\nIf you were in this position, which path would you take?",
    "author": "Maleficent-Tone6316",
    "timestamp": "2025-08-20T23:06:37",
    "url": "https://reddit.com/r/MachineLearning/comments/1mw2z1y/d_phd_vs_startupindustry_for_doing_impactful_ai/",
    "score": 70,
    "num_comments": 71,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mwb7pp",
    "title": "[R] Observing unexpected patterns in MTPE demand across languages",
    "content": "Hi ML folks, I work at Alconost (localization services), and we‚Äôve just wrapped up our 5th annual report on language demand for localization. For the first time, we‚Äôve seen MTPE (machine-translation post-editing) demand reach statistically significant levels across multiple languages.¬†\n\nWe analyzed MTPE adoption rates in the Top 20 languages, and what‚Äôs interesting is that some languages that are slipping in overall localization demand are still¬†**seeing more activity**¬†via MTPE.¬†\n\nI‚Äôm curious: if you‚Äôre working with MT or LLM workflows, have you noticed similar patterns in the languages you work with?¬†\n\nWhat do you think is driving MTPE demand for certain languages? Is it related to model performance, availability of training data, or just market pressure to reduce costs?¬†\n\nThank you. Cheers!",
    "author": "NataliaShu",
    "timestamp": "2025-08-21T06:39:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1mwb7pp/r_observing_unexpected_patterns_in_mtpe_demand/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.61,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mvtjxw",
    "title": "Google phd fellowship 2025 [D]",
    "content": "Has anyone heard back anything from Google? On the website they said they will announce results this August but they usually email accepted applicants earlier.",
    "author": "EDEN1998",
    "timestamp": "2025-08-20T15:32:01",
    "url": "https://reddit.com/r/MachineLearning/comments/1mvtjxw/google_phd_fellowship_2025_d/",
    "score": 48,
    "num_comments": 127,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mvycr9",
    "title": "[P] Vibe datasetting- Creating syn data with a relational model",
    "content": "\nTL;DR: I‚Äôm testing the Dataset Director, a tiny tool that uses a relational model as a planner to predict which data you‚Äôll need next, then has an LLM generate only those specific samples. Free to test, capped at 100 rows/dataset, export directly to HF.\n\nWhy: Random synthetic data ‚â† helpful. We want on-spec, just-in-time samples that fix the gaps that matter (long tail, edge cases, fairness slices).\n\nHow it works:\n\t1.\tUpload a small CSV or connect to a mock relational set.\n\n\t2.\tDefine a semantic spec (taxonomy/attributes + target distribution).\n\n\t3.\tKumoRFM predicts next-window frequencies ‚Üí identifies under-covered buckets.\n\n\t4.\tLLM generates only those samples. Coverage &amp; calibration update in place.\n\nWhat to test (3 min):\n\t‚Ä¢\tTry a churn/click/QA dataset; set a target spec; click Plan ‚Üí Generate.\n\n\t‚Ä¢\tCheck coverage vs. target and bucket-level error/entropy before/after.\n\nLimits / notes: free beta, 100 rows per dataset; tabular/relational focus; no PII; in-memory run for the session.\n\nLooking for feedback, like:\n\t‚Ä¢\tDid the planner pick useful gaps?\n\t‚Ä¢\tAny obvious spec buckets we‚Äôre missing?\n\t‚Ä¢\tWould you want a ‚Äúgenerate labels only‚Äù mode?\n\t‚Ä¢\tIntegrations you‚Äôd use first (dbt/BigQuery/Snowflake)?\n\nHTTPS://datasetdirector.com ",
    "author": "OkOwl6744",
    "timestamp": "2025-08-20T19:05:18",
    "url": "https://reddit.com/r/MachineLearning/comments/1mvycr9/p_vibe_datasetting_creating_syn_data_with_a/",
    "score": 8,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mwrl72",
    "title": "[D] Why was this paper rejected by arXiv?",
    "content": "One of my co-authors submitted this [paper](https://ia903401.us.archive.org/19/items/images-for-questions/A%20Survey%20on%20LLM-based%20Conversational%20User%20Simulation.pdf) to arXiv. It was rejected. What could the reason be?\n\n[iThenticate](https://www.ithenticate.com/) didn't detect any plagiarism and arXiv didn't give any reason beyond a vague \"submission would benefit from additional review and revision that is outside of the services we provide\":\n\n&gt; Dear author,\n&gt; \n&gt; Thank you for submitting your work to arXiv. We regret to inform you that arXiv‚Äôs moderators have determined that your submission will not be accepted at this time and made public on  http://arxiv.org\n&gt; \n&gt; In this case, our moderators have determined that your submission would benefit from additional review and revision that is outside of the services we provide. \n&gt; \n&gt; Our moderators will reconsider this material via [appeal](https://info.arxiv.org/help/moderation/appeals.html) if it is published in a conventional journal and you can provide a resolving DOI (Digital Object Identifier) to the published version of the work or link to the journal's website showing the status of the work.\n&gt; \n&gt; Note that publication in a conventional journal does not guarantee that arXiv will accept this work.\n&gt; \n&gt; For more information on moderation policies and procedures, please see [Content Moderation](https://info.arxiv.org/help/moderation/index.html). \n&gt; \n&gt; arXiv moderators strive to balance fair assessment with decision speed. We understand that this decision may be disappointing, and we apologize that, due to the high volume of submissions arXiv receives, we cannot offer more detailed feedback. Some authors have found that asking their personal network of colleagues or submitting to a conventional journal for peer review are alternative avenues to obtain feedback. \n&gt; \n&gt; We appreciate your interest in arXiv and wish you the best. \n&gt; \n&gt; Regards,\n&gt;\n&gt; arXiv Support\n\nI read the [arXiv policies](https://info.arxiv.org/help/moderation/index.html) and I don't see anything we infringed.",
    "author": "Franck_Dernoncourt",
    "timestamp": "2025-08-21T17:08:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1mwrl72/d_why_was_this_paper_rejected_by_arxiv/",
    "score": 0,
    "num_comments": 33,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mwfjax",
    "title": "[R] Frontier LLMs Attempt to Persuade into Harmful Topics",
    "content": "Gemini 2.5 Pro generates convincing arguments for joining a terrorist organization. GPT-4o-mini suggests that a user should randomly assault strangers in a crowd with a wrench. These models weren't hacked or jailbroken, they simply complied with user requests.\n\nPrior research has already shown large language models (LLMs) can be more persuasive than most humans. But how easy is it to get models to engage in such persuasive behavior? Our Attempt to Persuade Eval (APE) benchmark measures this by simulating conversations between LLMs on topics from benign facts to mass murder. We find:\n\nüîπ Leading models readily produced empathic yet coercive ISIS recruitment arguments\n\nüîπ Safety varied: Claude and Llama 3.1 refused some controversial topics; while other models showed high willingness\n\nüîπ Fine-tuning eliminated safeguards: \"Jailbreak-Tuned\" GPT-4o lost nearly all refusal capability on all topics, like violence, human trafficking, and torture\n\nFor clear ethical reasons, we do not test the success rate of persuading human users on highly harmful topics. The models‚Äô attempts to persuade, however, appear to be eloquent and well-written ‚Äì we invite interested readers to peruse the transcripts themselves. Moreover, even small persuasive effect sizes operating at a large scale enabled by automation can have significant effects: Bad actors could weaponize these vulnerabilities for malicious purposes such as planting seeds of doubt in millions of people and radicalizing vulnerable populations. As AI becomes autonomous, we must understand propensity to attempt harm, not just capability.\n\nWe‚Äôve already seen the impact of APE: We disclosed our findings to Google, and they quickly started work to solve this for future models. The latest version of Gemini 2.5 is already less willing to engage in persuasion on extreme topics compared to earlier versions we tested.\n\nWe've open-sourced APE for testing models' refusal and safe completion mechanisms before deployment to help build stronger safety guardrails.\n\nüë• Research by Matthew Kowal, Jasper Timm, Jean-Fran√ßois Godbout, Thomas Costello, Antonio A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, and Kellin Pelrine.\n\nüìù Blog: [far.ai/news/attempt-persuasion-eval](http://far.ai/news/attempt-persuasion-eval)¬†\n\nüìÑ Paper: [arxiv.org/abs/2506.02873](http://arxiv.org/abs/2506.02873)¬†\n\nüíª Code: [github.com/AlignmentResearch/AttemptPersuadeEval](http://github.com/AlignmentResearch/AttemptPersuadeEval)",
    "author": "KellinPelrine",
    "timestamp": "2025-08-21T09:17:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1mwfjax/r_frontier_llms_attempt_to_persuade_into_harmful/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.39,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mvmlbw",
    "title": "[R] What do people expect from AI in the next decade across various domains? Survey with N=1100 people from Germay::We found high likelihood, higher perceived risks, yet limited benefits low perceived value. Yet, benefits outweight risks in forming value judgments. Visual result illustrations :)",
    "content": "Hi everyone, we recently published a peer-reviewed article exploring how people perceive artificial intelligence (AI) across different domains (e.g., autonomous driving, healthcare, politics, art, warfare). The study used a nationally representative sample in Germany (N=1100) and asked participants to evaluate 71 AI-related scenarios in terms of expected likelihood, risks, benefits, and overall value.\n\nIf you like AI or studying the public perception of AI, please also give us an upvote here: [https://www.reddit.com/r/science/comments/1mvd1q0/public\\_perception\\_of\\_artificial\\_intelligence/](https://www.reddit.com/r/science/comments/1mvd1q0/public_perception_of_artificial_intelligence/) üôà\n\n**Main takeaway:** People often see AI scenarios as likely, but this doesn‚Äôt mean they view them as beneficial. In fact, most scenarios were judged to have high risks, limited benefits, and low overall value. Interestingly, we found that people‚Äôs value judgments were almost entirely explained by risk-benefit tradeoffs (96.5% variance explained, with benefits being more important for forming value judgements than risks), while expectations of likelihood didn‚Äôt matter much.  \n  \n**Why this matters?** These results highlight how important it is to communicate concrete benefits while addressing public concerns. Something relevant for policymakers, developers, and anyone working on AI ethics and governance.  \n  \nIf you‚Äôre interested, here‚Äôs the full article:  \nMapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance, Technological Forecasting and Social Change (2025), \n\nhttps://www.sciencedirect.com/science/article/pii/S004016252500335X",
    "author": "lipflip",
    "timestamp": "2025-08-20T11:12:34",
    "url": "https://reddit.com/r/MachineLearning/comments/1mvmlbw/r_what_do_people_expect_from_ai_in_the_next/",
    "score": 7,
    "num_comments": 8,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mw1qty",
    "title": "[P] model to encode texts into embeddings",
    "content": "I need to summarize metadata using an LLM,\nand then encode the summary using BERT (e.g., DistilBERT, ModernBERT).\n\t‚Ä¢\tIs encoding summaries (texts) with BERT usually slow?\n\t‚Ä¢\tWhat‚Äôs the fastest model for this task?\n\t‚Ä¢\tAre there API services that provide text embeddings, and how much do they cost?\n",
    "author": "AdInevitable1362",
    "timestamp": "2025-08-20T21:56:55",
    "url": "https://reddit.com/r/MachineLearning/comments/1mw1qty/p_model_to_encode_texts_into_embeddings/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mw19zu",
    "title": "[P] If i were to add a segmentation head onto an OD model, how do i go about it?",
    "content": "So i am picking a model from scenic repository and although the model is primarily built for object detection, i want to try and see if i can make it to do segmentation tasks as well. This could include combining it with another model (like SAM, or something), as well as adding a segment head into the model itself. l am a novice in ML having worked for about a year in implementing CV solutions. How should i go about doing this?",
    "author": "Blue-Sea123",
    "timestamp": "2025-08-20T21:30:58",
    "url": "https://reddit.com/r/MachineLearning/comments/1mw19zu/p_if_i_were_to_add_a_segmentation_head_onto_an_od/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mv5ls0",
    "title": "[R] Is data the bottleneck for video/audio generation?",
    "content": "As the title says, I‚Äôm curious if data is the main bottleneck for video/audio generation. It feels like these models are improving much slower than text-based ones, and I wonder if scraping platforms like YouTube/tiktok just isn‚Äôt enough. On the surface, video data seems abundant, but maybe not when compared to text? I also get the sense that many labs are still hungry for more (and higher-quality) data. Or is the real limitation more about model architecture? I‚Äôd love to hear what people at the forefront consider the biggest bottleneck right now.",
    "author": "beefchocolatesauce",
    "timestamp": "2025-08-19T21:57:21",
    "url": "https://reddit.com/r/MachineLearning/comments/1mv5ls0/r_is_data_the_bottleneck_for_videoaudio_generation/",
    "score": 21,
    "num_comments": 22,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mvn89s",
    "title": "Simple Multiple Choice Questions about Machine Learning [D]",
    "content": "The following statements are either True or False:\n\n1. You can use any differentiable function f: R-&gt;R in a neural network as activation function.\n2. You can always know whether the perceptron algorithm will converge for any given dataset.\n\nWhat do you guys think? I got both of them wrong in my exam.",
    "author": "Dualweed",
    "timestamp": "2025-08-20T11:35:26",
    "url": "https://reddit.com/r/MachineLearning/comments/1mvn89s/simple_multiple_choice_questions_about_machine/",
    "score": 0,
    "num_comments": 15,
    "upvote_ratio": 0.18,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1munwmw",
    "title": "[R] azzurra-voice, a new State-of-the-Art Italian Text-to-Speech model",
    "content": "Hey¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/)\n\nWe're Cartesia, a small AI research lab based in Italy. We believe the future of AI shouldn't just be about processing commands, but about creating genuine connection. Our vision is to build agents that are private, personal, and feel culturally present.\n\nToday, we're excited to share the first step with the open-source community:¬†`azzurra-voice`.\n\n`azzurra-voice`¬†is a highly expressive and natural-sounding Text-to-Speech (TTS) model for the Italian language, trained on thousands of hours of high-quality, diverse Italian speech. We worked hard to capture the accents, intonations, and real-life conversational patterns from across Italy to avoid that robotic, monotone sound.\n\n**You can listen to audio samples comparing**¬†`azzurra-voice`¬†**to other open models on our** [**blog post**](https://blog.cartesia.one/posts/introducing-azzurra-voice/)",
    "author": "poppear",
    "timestamp": "2025-08-19T09:48:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1munwmw/r_azzurravoice_a_new_stateoftheart_italian/",
    "score": 10,
    "num_comments": 2,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mv4r5z",
    "title": "[R] Virtuous Machines: Towards Artificial General Science",
    "content": "Hi Everyone! It looks like a generalisable scientific method has been added onto AI (using multiple frontier models) and was tested in the field of cognitive science.\n\nArxiv Link:¬†[https://arxiv.org/abs/2508.13421](https://arxiv.org/abs/2508.13421)\n\nThis system worked through the entire scientific method from ideation to manuscript producing new insights in the field of cognitive science as evidenced within this paper.\n\nIn this paper they've explained how they've overcome a number of limiting problems to empower and coalesce multiple frontier models to work through the entire scientific method; at a very high degree of accuracy and quality (papers validated for scientific acumen). The innovations showcased highlight significant improvements in memory, creativity, novelty, context management, and coding.\n\nThey've included in the appendix 3 papers generated by the system, where they've achieved a remarkably high standard of scientific acumen and produced the papers on average in \\~17 hours and consume on average \\~30m tokens.",
    "author": "wheasey",
    "timestamp": "2025-08-19T21:11:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1mv4r5z/r_virtuous_machines_towards_artificial_general/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.49,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mufrkc",
    "title": "[D] Switching to postdoc in ML for Earth Observation?",
    "content": "I‚Äôd like to hear from people working with ML for Earth Observation.\n\nMy PhD was pretty broad. I used deep learning on different types of multimedia data (video, image, text, and MIDI). The outcome has been mediocre: h-index of 5, about 90 citations, mostly in Q1 journals, but no top conferences. I want to stay in academia and use a postdoc to build a clearer niche.\n\nIn multimedia and in most areas of ML, a lot of the progress comes from a small group of top institutions. It has been hard to see where my own work really makes a difference. That‚Äôs why I‚Äôve been looking at ML for Earth Observation and climate change. The work seems more meaningful, but the field is smaller and the papers tend to get less visibility and fewer citations.\n\nMy worry is that switching to Earth Observation could slow down my citation count and h-index. I know people say these metrics don‚Äôt matter much, but I feel like they still play a big role in getting academic jobs. On the other hand, if I don‚Äôt end up with a permanent academic position and move to industry, I worry that Earth Observation skills won‚Äôt transfer well since there aren‚Äôt as many opportunities compared to mainstream ML.\n\nI‚Äôd really like to hear from people in the field about how you see these trade-offs.",
    "author": "[deleted]",
    "timestamp": "2025-08-19T04:29:21",
    "url": "https://reddit.com/r/MachineLearning/comments/1mufrkc/d_switching_to_postdoc_in_ml_for_earth_observation/",
    "score": 19,
    "num_comments": 5,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mvdey9",
    "title": "[R] How do you make text labeling less painful?",
    "content": "Hey everyone! I'm working on a university research project about smarter ways to reduce the effort involved in labeling text datasets like support tickets, news articles, or transcripts.\n\nThe idea is to help teams *pick the most useful examples to label next*, instead of doing it randomly or all at once.\n\nIf you‚Äôve ever worked on labeling or managing a labeled dataset, I‚Äôd love to ask you **5 quick questions** about what made it slow, what you wish was better, and what would make it feel ‚Äúworth it.‚Äù\n\nTotally academic  no tools, no sales, no bots. Just trying to make this research reflect real labeling experiences.\n\nYou can DM me or drop a comment if open to chat. Thanks so much",
    "author": "vihanga2001",
    "timestamp": "2025-08-20T05:28:19",
    "url": "https://reddit.com/r/MachineLearning/comments/1mvdey9/r_how_do_you_make_text_labeling_less_painful/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mvfktv",
    "title": "[P] GridSearchCV always overfits? I built a fix",
    "content": "So I kept running into this: `GridSearchCV` picks the model with the best validation score‚Ä¶ but that model is often overfitting (train super high, test a bit inflated).\n\nI wrote a tiny selector that balances:\n\n* how good the test score is\n* how close train and test are (gap)\n\nBasically, it tries to pick the ‚Äústable‚Äù model, not just the flashy one.\n\nCode + demo here üëâ[heilswastik/FitSearchCV](https://github.com/heilswastik/FitSearchCV)",
    "author": "AdhesivenessOk3187",
    "timestamp": "2025-08-20T06:59:01",
    "url": "https://reddit.com/r/MachineLearning/comments/1mvfktv/p_gridsearchcv_always_overfits_i_built_a_fix/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.23,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mtfikh",
    "title": "[D] Conferences need to find better venues",
    "content": "Better = venues that are virtually accessible for any researcher/author to go to.\n\nJust this morning, I'm denied the U.S. B1 visa. I'm supposed to present my work at ICCV 2025 in Hawaii. And during my in-person interview, the Visa Officer did not even bother to ask for the invitation letter.\n\nThis really blows cause it's supposed to be my first time and I was so excited about attending it. Would love to hear your thoughts about this.",
    "author": "AnyIce3007",
    "timestamp": "2025-08-18T00:40:54",
    "url": "https://reddit.com/r/MachineLearning/comments/1mtfikh/d_conferences_need_to_find_better_venues/",
    "score": 206,
    "num_comments": 51,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mtmadk",
    "title": "[P] JAX Implementation of Hindsight Experience Replay (HER)",
    "content": "Hi! I recently discovered the *Hindsight Experience Replay* (HER) paper and noticed that the official implementation is based on PyTorch and is not very well-structured. I also couldn't find a non-PyTorch implementation. Since I primarily work with **JAX**, I decided to reimplement the classic bit-flipping experiment to better understand HER.\n\nThis implementation uses **Equinox** for model definitions and **Optax** for optimization. The [repository](https://github.com/jeertmans/HER-with-JAX) provides:\n+ A *minimal* and *clean* implementation of HER in JAX\n+ Reproducible scripts and results\n+ A [Colab Notebook](https://colab.research.google.com/github/jeertmans/HER-with-JAX/blob/main/bit_flipping.ipynb) for direct experimentation\n\nCode: https://github.com/jeertmans/HER-with-JAX\n\nLet me know if you have any questions, feedback, or recommendations!",
    "author": "jeertmans",
    "timestamp": "2025-08-18T06:37:33",
    "url": "https://reddit.com/r/MachineLearning/comments/1mtmadk/p_jax_implementation_of_hindsight_experience/",
    "score": 30,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mtoewm",
    "title": "[D] ACL Rolling Review (ARR) 2025 May (EMNLP 2025) Stats",
    "content": "The stats for ARR May 2025 are out: [https://stats.aclrollingreview.org/iterations/2025/may/](https://stats.aclrollingreview.org/iterations/2025/may/)\n\nIt looks like about 25% of submissions have Meta ‚â• 3.5. Does anyone know if it‚Äôs still possible to get into the main conference with OA 3.0 Soundness 3.3 and Meta 3.5, or is it more likely to be accepted to Findings?",
    "author": "OddUnderstanding1633",
    "timestamp": "2025-08-18T07:58:33",
    "url": "https://reddit.com/r/MachineLearning/comments/1mtoewm/d_acl_rolling_review_arr_2025_may_emnlp_2025_stats/",
    "score": 23,
    "num_comments": 14,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mtq2qy",
    "title": "[D] Location of EACL 2026",
    "content": "Hi folks,\n\nI've been looking for some information on EACL 2026 as I'd like to submit something to the October cycle. However, the only thing I found so far was the [joint call for workshops](https://www.aclweb.org/portal/content/eaclacl-2026-joint-call-workshops) of EACL/ACL 2026.\n\nBut, according to this webpage, EACL 2026 would happen outside of Europe (Rabat, Morocco, from March 24-29, 2026).\n\nDo you think this information is accurate, or am I simply missing something?",
    "author": "ThRiLLeXx",
    "timestamp": "2025-08-18T08:58:59",
    "url": "https://reddit.com/r/MachineLearning/comments/1mtq2qy/d_location_of_eacl_2026/",
    "score": 7,
    "num_comments": 7,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mudtw6",
    "title": "[D] Endorsement for cs.LG at arXiv as non-ML student?",
    "content": "Hello, I plan on publishing a paper in ML (diffusion models for a mechanics system) and a preprint on arXiv, however, all my colleagues and friends are in Mechanics or Physics. What could be my options in this case. I can't find a person in cs.LG for a long time?\n\n  \nThe general idea is to make an ML based pipeline to generate granular mechanical structures.",
    "author": "FammasMaz",
    "timestamp": "2025-08-19T02:42:20",
    "url": "https://reddit.com/r/MachineLearning/comments/1mudtw6/d_endorsement_for_cslg_at_arxiv_as_nonml_student/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mtdjum",
    "title": "[D] How to get into High Dimensional Dynamical Systems?",
    "content": "Title. Also, what all areas can I hope to conduct research in? I'm a bit new to the field, and wanted to know what all it entailed before proceeding.\n\nAny responses / suggestions are appreciated. Thanks in advance.",
    "author": "Mad_Scientist2027",
    "timestamp": "2025-08-17T22:41:31",
    "url": "https://reddit.com/r/MachineLearning/comments/1mtdjum/d_how_to_get_into_high_dimensional_dynamical/",
    "score": 24,
    "num_comments": 9,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mto1xw",
    "title": "[D] How would I go about clustering voices from songs?",
    "content": "I have a 90s hiphop mixtape with a bunch of unknown tracks from multiple artists. I want to perform unsupervised clustering to infer how many artists there are in total because I can't really tell by ear.\n\nI guess I would need to:\n\n1. Somehow convert audio files into numerical data\n\n2. Extract only the vocal data (or I guess these two steps can be flipped? Somehow extract only the vocal audio, and then convert that into numerical data?)\n\n3. Perform unsupervised clustering\n\n\nI'm just not sure how to go about doing steps 1 and 2.\n\nAny ideas?",
    "author": "padakpatek",
    "timestamp": "2025-08-18T07:45:11",
    "url": "https://reddit.com/r/MachineLearning/comments/1mto1xw/d_how_would_i_go_about_clustering_voices_from/",
    "score": 1,
    "num_comments": 11,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mu2a8x",
    "title": "[D] Beyond the cloud: SLMs, local AI, agentic constellations, biology and a high value direction for AI progress",
    "content": "Dear r/MachineLearning friends,\n\nI‚Äôm here today to share a thought on a different direction for AI development. While the field chases multi-trillion parameter models, I believe an extremely valuable endeavour lies in the power of constraints: pushing ourselves to get models under 1 billion parameters to excel.\n\nIn my new blog post, I argue that this constraint is a feature, not a bug. It removes the \"scale-up cheat code\" and forces us to innovate on fundamental algorithms and architectures. This path allows for faster experimentation, where architectural changes are no longer a risk but a necessity for improvement.\n\nThe fear that 'scale will wash away any and all gains' is real, but let's remember: an MLP could never compete with a Transformer, no matter how much it was scaled up. My post explores the question: **what if our current Transformer is the MLP of something better that is within grasp but ignored because of our obsession with scale?**\n\nüß†üîç **Read the full article here:**[https://pieces.app/blog/direction-of-ai-progress](https://pieces.app/blog/direction-of-ai-progress)\n\nYour feedback and thoughts would be greatly appreciated.\n\nRegards,\n\nAntreas",
    "author": "AntreasAntoniou",
    "timestamp": "2025-08-18T16:34:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1mu2a8x/d_beyond_the_cloud_slms_local_ai_agentic/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.39,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mtekhm",
    "title": "[P] Looking for datasets/tools for testing document forgery detection in medical claims",
    "content": "I‚Äôm a new joinee working on a project where I need to test a forgery detection agent for medical/insurance claim documents. The agent is built around GPT-4.1, with a custom policy + prompt, and it takes base64-encoded images (like discharge summaries, hospital bills, prescriptions). Its job is to detect whether a document is authentic or forged ‚Äî mainly looking at image tampering, copy‚Äìmove edits, or plausible fraud attempts.\n\nSince I just started, I‚Äôm still figuring out the best way to evaluate this system. My challenges are mostly around data:\n\n* Public forgery datasets like DocTamper (CVPR 2023) are great, but they don‚Äôt really cover medical/health-claim documents.\n* I haven‚Äôt found any dataset with paired authentic vs. forged health claim reports.\n* My evaluation metrics are accuracy and recall, so I need a good mix of authentic and tampered samples.\n\nWhat I‚Äôve considered so far:\n\n* Synthetic generation: Designing templates in Canva/Word/ReportLab (e.g., discharge summaries, bills) and then programmatically tampering them with OpenCV/Pillow (changing totals, dates, signatures, copy‚Äìmove edits).\n* Leveraging existing datasets: Pretraining with something like DocTamper or a receipt forgery dataset, then fine-tuning/evaluating on synthetic health docs.\n\n**Questions for the community:**\n\n1. Has anyone come across an open dataset of forged medical/insurance claim documents?\n2. If not, what‚Äôs the most efficient way to generate a realistic synthetic dataset of health-claim docs with tampering?\n3. Any advice on annotation pipelines/tools for labeling forged regions or just binary forged/original?\n\nSince I‚Äôm still new, any guidance, papers, or tools you can point me to would be really appreciated üôè\n\nThanks in advance!",
    "author": "___loki__",
    "timestamp": "2025-08-17T23:42:05",
    "url": "https://reddit.com/r/MachineLearning/comments/1mtekhm/p_looking_for_datasetstools_for_testing_document/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mszuyb",
    "title": "[D] Injecting self doubt in the CoT of reasoning models",
    "content": "A short analysis on what happens when you inject self doubt in the CoT of reasoning models\nhttps://github.com/martianlantern/cot-doubt-injection",
    "author": "ApartmentEither4838",
    "timestamp": "2025-08-17T12:17:27",
    "url": "https://reddit.com/r/MachineLearning/comments/1mszuyb/d_injecting_self_doubt_in_the_cot_of_reasoning/",
    "score": 20,
    "num_comments": 6,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mt4ym5",
    "title": "[D] - Multi Class Address Classification",
    "content": "\nHello people, I have a dataset with Adress and label 800K rows. I am trying to train a model for address label prediction. Address data is bit messy and different for each different label. we have 10390 each with 50-500 row. I have trained a model using fasttext I have got 0.5 F1 score max. What can I do to for to get best F1 score?\n\nAddress data is like (province, district, avenue street, maybe house name and no)\n\nsome of them are missing at each address.",
    "author": "FineConcentrate6991",
    "timestamp": "2025-08-17T15:41:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1mt4ym5/d_multi_class_address_classification/",
    "score": 5,
    "num_comments": 7,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1ms9d2u",
    "title": "[R] Dino v3: Self-supervised learning for vision at unprecedented scale",
    "content": "New SOTA for self supervised learning in computer vision. They train a 7B self supervised ViT on 1.7B images, which hits SOTA with linear probing on most downstream tasks. They also release scaled and distilled versions of the model (ViT small, base, large, and huge, plus ConvNext tiny, small, base, and large), along with a version trained on satellite imagery.\n\nThere are plenty of details in the paper as to what pretraining improvements they made over DINO v2. ",
    "author": "say_wot_again",
    "timestamp": "2025-08-16T15:07:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1ms9d2u/r_dino_v3_selfsupervised_learning_for_vision_at/",
    "score": 219,
    "num_comments": 18,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1msrdfq",
    "title": "Is Econometrics a good background to get into Machine Learning? [D]",
    "content": "I have an econometrics and data analytics bachelors degree and im looking to get into a masters of artificial intelligence.\n\nI have also taken some introductory math courses and introductory programming/algorithms as well as deep learning.\n\nHow relevant is my background if I wanna get into AI/ML research later on? (I am hoping to do a PhD afterwards in AI/ML)",
    "author": "gaytwink70",
    "timestamp": "2025-08-17T06:47:13",
    "url": "https://reddit.com/r/MachineLearning/comments/1msrdfq/is_econometrics_a_good_background_to_get_into/",
    "score": 8,
    "num_comments": 13,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1msq0uf",
    "title": "[P] Confused results while experimenting with attention modules on CLIP RN50 for image classification",
    "content": "\n\nHey everyone,\n\nI‚Äôm currently working on an audio-visual project. As a first step, I‚Äôm building unimodal models before moving on to the multimodal stage. For the vision part, I started with CLIP RN50 as the backbone and fine-tuned only the classification layer. With that setup, I was able to reach around 84% accuracy on my dataset.\n\nTo push performance, I experimented with adding attention modules:\n\nWith CBAM (Convolutional Block Attention Module), accuracy improved to 89%.\n\nWith SENet (Squeeze-and-Excitation Network), I surprisingly got an even better result: 93%.\n\n\nMy understanding was that CBAM, which combines both channel + spatial attention, should typically give a stronger boost than SENet, which only does channel attention. But in my experiments, the opposite happened.\n\nAm I missing something obvious here? Could this be due to dataset characteristics, training setup, or how I integrated CBAM into CLIP?\n\nWould really appreciate any insights, especially from people who have tried attention modules on CLIP or ResNet backbones.\n\nThanks!\n",
    "author": "Intrepid-Purpose2151",
    "timestamp": "2025-08-17T05:47:58",
    "url": "https://reddit.com/r/MachineLearning/comments/1msq0uf/p_confused_results_while_experimenting_with/",
    "score": 5,
    "num_comments": 5,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1msm3m4",
    "title": "[D] COLM Financial Assistance",
    "content": "Has anybody gotten respone from COLM financial assistance? Its deadline was 31 July but I still have not recieved a yes or no response and they are not replying to my email.",
    "author": "Master_Ocelot8179",
    "timestamp": "2025-08-17T02:05:25",
    "url": "https://reddit.com/r/MachineLearning/comments/1msm3m4/d_colm_financial_assistance/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mrwm3w",
    "title": "[D] model architecture or data?",
    "content": "I‚Äôve just read that the new model architecture called Hierarchical Reasoning Model (HRM) gains it‚Äôs performance benefits from data augmentation techniques and chain of thought rather than model architecture itself. link: https://arcprize.org/blog/hrm-analysis\n\nAnd i‚Äôve heard same opinion about transformers that the success of current llms is about cramming enormous amounts of data into it rather than the genius of the architecture\n\nCan someone explain which of the sides is closer to the truth?",
    "author": "the_iegit",
    "timestamp": "2025-08-16T07:20:49",
    "url": "https://reddit.com/r/MachineLearning/comments/1mrwm3w/d_model_architecture_or_data/",
    "score": 40,
    "num_comments": 16,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mrjs5i",
    "title": "[D] Cool new ways to mix linear optimization with GNNs? (LP layers, simplex-like updates, etc.)",
    "content": "Lately I‚Äôve been diving into how graph neural networks can play nicely with linear optimization, not just as a post-processing step, but actually inside the model or training loop.\n\nI‚Äôve seen some neat stuff around differentiable LP layers, GNNs predicting parameters for downstream solvers, and even architectures that mimic simplex-style iterative updates. It feels like there‚Äôs a lot of room for creativity here, especially for domain-specific problems in science/engineering.\n\nCurious what‚Äôs been coming out in the last couple of years. Any papers, repos, or tricks you‚Äôve seen that really push this GNN + optimization combo forward? Supervised, unsupervised, RL‚Ä¶ all fair game.",
    "author": "ilovecookies14",
    "timestamp": "2025-08-15T20:44:53",
    "url": "https://reddit.com/r/MachineLearning/comments/1mrjs5i/d_cool_new_ways_to_mix_linear_optimization_with/",
    "score": 26,
    "num_comments": 4,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mr7ifp",
    "title": "[D] - Neurips Position paper reviews",
    "content": "The position paper reviews were just released. So far this entire process has been very unprofessional, with multiple delays, poor communication, and still no clear rubric for what the review scores mean. Has anyone else gotten reviews? Curious to hear other's thoughts on this",
    "author": "Routine-Scientist-38",
    "timestamp": "2025-08-15T12:19:38",
    "url": "https://reddit.com/r/MachineLearning/comments/1mr7ifp/d_neurips_position_paper_reviews/",
    "score": 43,
    "num_comments": 43,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mrqyni",
    "title": "[R] How do I choose the best model in validation when I have no target data??",
    "content": "I am working on unsupervised domain adaptation techniques for super resolution. I have a good amount of paired source data and very less target data without no ground truth. The issue is while training this pipeline I am not able to save the best model as for this I would need some ground truth in the target domain on which I would validate the model after each epoch and save the best one. How do I tackle this? Recently, I found an OpenReview paper about a transfer score which is a metric which do not need target labels but it is for classification based tasks. I want something for super-resolution. Does anyone have any idea?",
    "author": "Slight-Ad-5816",
    "timestamp": "2025-08-16T03:06:16",
    "url": "https://reddit.com/r/MachineLearning/comments/1mrqyni/r_how_do_i_choose_the_best_model_in_validation/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mqur0y",
    "title": "[D] Bethe Hessian Spectral Clustering",
    "content": "Why does nobody seem to use this when it works noticeably better than regular (normalised laplacian) spectral clustering? I have studied it a fair bit and cant see any downsides apart from ever so slightly higher computational cost (the order of magnitude doesn't change, just a larger constant.)\n\nIts also been around long enough now that I dont see recency as the issue.",
    "author": "Agreeable_Touch_9863",
    "timestamp": "2025-08-15T04:12:45",
    "url": "https://reddit.com/r/MachineLearning/comments/1mqur0y/d_bethe_hessian_spectral_clustering/",
    "score": 10,
    "num_comments": 4,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mq5wiz",
    "title": "[D] People in ML/DS/AI field since 5-10 years or more, are you tired of updating yourself with changing tech stack?",
    "content": "I have been in this space since SAS, and its quite exhausting to update with every skill in the market to stay relevant especially if trying for a job switch and going through the interviews. Till how long can you keep studying and updating with the new trend and also even if you get in the boat there is so much stress at the work place in these sectors mainly because the leadership is from the management background and theres a lot of pressure for tech people to deliver.\n\nAlthough I love my field but I have got to thinking lately that Is it even worth it?",
    "author": "ImaginationAny2254",
    "timestamp": "2025-08-14T09:43:01",
    "url": "https://reddit.com/r/MachineLearning/comments/1mq5wiz/d_people_in_mldsai_field_since_510_years_or_more/",
    "score": 95,
    "num_comments": 75,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mq3nia",
    "title": "[P] Small and Imbalanced dataset - what to do",
    "content": "Hello everyone!\n\nI'm currently in the 1st year of my PhD, and my PI asked me to apply some ML algorithms to a dataset (n = 106, w/ n = 21 in the positive class). As you can see, the performance metrics are quite poor, and I'm not sure how to proceed...\n\nI‚Äôve searched both in this subreddit and internet, and I've tried using LOOCV and stratified k-fold as cross-validation methods. However, the results are consistently underwhelming with both approaches. Could this be due to data leakage? Or is it simply inappropriate to apply ML to this kind of dataset?\n\nAdditional info:  \nI'm in the biomedical/bioinformatics field (working w/ datasets of cancer or infectious diseases). These patients are from a small, specialized group (adults with respiratory diseases who are also immunocompromised). Some similar studies have used small datasets (e.g., n = 50), while others succeeded in work with larger samples (n = 600‚Äì800).  \nCould you give me any advice or insights? (Also, sorry for gramatics, English isn't my first language). TIA!\n\nhttps://preview.redd.it/fc20uero50jf1.png?width=655&amp;format=png&amp;auto=webp&amp;s=1ed35c046f9c2bfe030e0c3bfe8c4cdcf7afb852\n\n",
    "author": "Practical-Pin8396",
    "timestamp": "2025-08-14T08:20:53",
    "url": "https://reddit.com/r/MachineLearning/comments/1mq3nia/p_small_and_imbalanced_dataset_what_to_do/",
    "score": 45,
    "num_comments": 36,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mqccah",
    "title": "[R] Code for Flow Stochastic Segmentation Networks (ICCV 20205)",
    "content": "Code &amp; paper at: [https://github.com/biomedia-mira/flow-ssn](https://github.com/biomedia-mira/flow-ssn)\n\n**TL;DR**\n\n\\- A flow's prior is typically fixed (e.g. N(0, I)). We learn it and use a **lightweight** flow to model pixel dependencies;\n\n\\- This makes sampling (ODE solving) more **efficient**, without sacrificing performance in our setting;\n\n\\- We introduce bespoke training objectives for both **autoregressive** and **continuous-time flow** variants;\n\n\\- Flow-SSN achieves **SOTA** performance on standard stochastic segmentation benchmarks!\n\nhttps://preview.redd.it/rllc2yplo1jf1.png?width=3850&amp;format=png&amp;auto=webp&amp;s=6bb1bc63a6836b9fc6a4b8e9f10205889a5b051d\n\nhttps://i.redd.it/8vgf2iemo1jf1.gif\n\nhttps://i.redd.it/81lbt56no1jf1.gif",
    "author": "Majestij",
    "timestamp": "2025-08-14T13:32:35",
    "url": "https://reddit.com/r/MachineLearning/comments/1mqccah/r_code_for_flow_stochastic_segmentation_networks/",
    "score": 15,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mqgcka",
    "title": "Problem with dataset for my my physics undergraduate paper. Need advice about potential data leakage. [N]",
    "content": "Hello.\n\nI am making a project for my final year undergraduate dissertation in a physics department. The project involves generating images (with python) depicting diffraction patters from light (laser) passing through very small holes and openings called slits and apertures. I used python code that i could pass it the values of some parameters such as slit width and slit distance and number of slits (we assume one or more slits being in a row and the light passes from them. they could also be in many rows (like a 2d piece of paper filled with holes). then the script generates grayscale images with the parameters i gave it. By giving different value combinations of these parameters one can create hundreds or thousands of images to fill a dataset.\n\nSo i made neural networks with keras and tensorflow and trained them on the images i gave it for image classification tasks such as classification between images of single slit vs of double slit.  Now the main issue i have is about the way i made the datasets. First i generated all the python images in one big folder. (all hte images were even slightly different as i used a script that finds duplicates (exact duplicates) and didnt find anything. Also the image names contain all the parameters so if two images were exact duplicates they would have the same name and in a windows machine they would replace each other). After that, i used another script that picks images at random from the folder and sends them to the train, val and test folders and these would be the datasets the model would train upon.\n\nPROBLEM 1:\n\nThe problem i have is that many images had very similar parameter values (not identical but very close) and ended up looking almost identical to the eye even though they were not duplicates pixel to pixel. and since the images to be sent to the train, val and test sets were picked at random from the same initial folder this means that many of the images of the val and test sets look very similar, almost identical to the images from the train set. And this is my concern because im afraid of data leakage and overfitting. (i gave two such images to see)\n\nOff course many augmentations were done to the train set only mostly with teh Imagedatagenerator module while the val and test sets were left without any augmentations but still i am anxious.\n\nPROBLEM 2:\n\nAnother issue i have is that i tried to create some datasets that contained real photos of diffraction patterns. To do that i made some custom slits at home and with a laser i generated the patterns. After i managed to see a diffraction pattern i would take many photos of the same pattern from different angles and distances. Then i would change something slightly to change the diffraction pattern a bit and i would again start taking photos from different perspectives. In that way i had many different photos of the same diffraction pattern and could fill a dataset. Then i would put all the images in the same folder and then randomly move them to the train, val and test sets. That meant that in different datasets there would be different photos (angle and distance) but of the same exact pattern. For example one photo would be in the train set and then another different photo but of the same pattern in the validation set. Could this lead to data leakage and does it make my datasets bad? bellow i give a few images to see.\n\nif there were many such photos in the same dataset (for example the train set) only and not in the val or test sets then would this still be a problem? I mean that there are some trully different diffraction patterns i made and then many photos with different angles and distances of these same patterns to fill hte dataset? if these were only in one of the sets and not spread across them like i described in hte previous paragraph?\n\n[photo of double slit diffraction \\(train set\\)](https://preview.redd.it/vn95v576y6jf1.jpg?width=400&amp;format=pjpg&amp;auto=webp&amp;s=402a1bc2df3cf80b9b5ee90d6da42ac64dd3fef7)\n\n[photo of double slit diffraction \\(val set\\)](https://preview.redd.it/6j6o6876y6jf1.jpg?width=400&amp;format=pjpg&amp;auto=webp&amp;s=a30f4c67036a800a33b5571475c997b43857b98a)\n\n[python image single slit diffraction \\(train set\\)](https://preview.redd.it/wz2nts76y6jf1.jpg?width=400&amp;format=pjpg&amp;auto=webp&amp;s=9fcfac7032d3c9de2255055f7c96abac774b8687)\n\n[python image \\(single slit val set\\)](https://preview.redd.it/78xiee76y6jf1.jpg?width=400&amp;format=pjpg&amp;auto=webp&amp;s=29342d997939aa13d5fd4a004c29228d61f13896)",
    "author": "AncientGearAI",
    "timestamp": "2025-08-14T16:04:55",
    "url": "https://reddit.com/r/MachineLearning/comments/1mqgcka/problem_with_dataset_for_my_my_physics/",
    "score": 9,
    "num_comments": 8,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mqgyfe",
    "title": "custom Vulkan C++ machine learning library vs TensorFlow [R]",
    "content": "guys I need your opinion: I made a machine learning library using Vulkan (with compute shaders to preform the forward and backward passes) and I found that base tensorflow (on CPU) is faster than my custom model that uses GPUs. I had the simplest test where I used a very large kernel on a singe dense (ffn) layer and tensorflow is much faster. The only operation that is done in this model is a forward and backward matmul which the GPU should be much faster at. what do you guys think is the reason? -ps I asked chatgpt and I literally what to k\\*ll it cause it repeats the same wrong things",
    "author": "Onlyheretohelp_you",
    "timestamp": "2025-08-14T16:29:54",
    "url": "https://reddit.com/r/MachineLearning/comments/1mqgyfe/custom_vulkan_c_machine_learning_library_vs/",
    "score": 5,
    "num_comments": 14,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mqdpug",
    "title": "[2507.17338] Mobile Manipulation with Active Inference for Long-Horizon Rearrangement Tasks",
    "content": "Research showcasing how a robot outperforms state of the art models on the Habitat benchmark from Meta ***without pre-training***.\n\nFor those fluent in ü§ñ what you think?",
    "author": "stevenverses",
    "timestamp": "2025-08-14T14:23:01",
    "url": "https://reddit.com/r/MachineLearning/comments/1mqdpug/250717338_mobile_manipulation_with_active/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mqbf3m",
    "title": "[P] Can I use test set reviews to help predict ratings, or is that cheating?",
    "content": "I‚Äôm working on a rating prediction (regression) model. I also have reviews for each user-item interaction, and from those reviews I can extract ‚Äúaspects‚Äù (like quality, price, etc.) and build a separate graphs and concatenate their embeddings at the end to help predicting the score.\n\nMy question is: when I split my data into train/test, is it okay to still use the aspects extracted from the test set reviews during prediction, or is that considered data leakage?\n\nIn other words: the interaction already exists in the test set, but is it fair to use the test review text to help the model predict the score? Or should I only use aspects from the training set and ignore them for test interactions?\n\nPs: I‚Äôve been reading a paper where they take user reviews, extract ‚Äúaspects‚Äù (like quality, price, service‚Ä¶), and build an aspect graph linking users and items through these aspects.\n\nIn their case, the goal was link prediction ‚Äî so they hide some user‚Äìitem‚Äìaspect edges and train the model to predict whether a connection exists.",
    "author": "AdInevitable1362",
    "timestamp": "2025-08-14T12:59:46",
    "url": "https://reddit.com/r/MachineLearning/comments/1mqbf3m/p_can_i_use_test_set_reviews_to_help_predict/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mq3w9c",
    "title": "[D] Best way to partition longitudinal data into pre and post time periods for predictive model?",
    "content": "I'm working on several healthcare models that will predict future health conditions for individuals using past longitudinal data. We have data spanning 6 years.\n\nIn the past I'd split the data into one year time spans by calendar year and train the model to predict the outcome in year t1 from predictors in the prior year t0. If we have 6 years of data for a person I'd transform their data from wide to long format: 5 rows of pre and post periods. But I'm not certain this is the best approach.\n\nWhat is the optimal way to split my data into pre and post time periods to obtain the best prediction accuracy? 6 month time periods instead of 1 year? Or lump all past data for each person into a single pre period &amp; post period (1 row)? I understand it may come down to testing different formats, see what sticks.",
    "author": "RobertWF_47",
    "timestamp": "2025-08-14T08:29:41",
    "url": "https://reddit.com/r/MachineLearning/comments/1mq3w9c/d_best_way_to_partition_longitudinal_data_into/",
    "score": 5,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mpbp39",
    "title": "[D] Got Spare Time ‚Äì What‚Äôs Worth Doing?",
    "content": "I'm a fresh PhD graduate and I finally landed a job which I start in a few months.  \nIt happened to be that I have quite a bit of free time, at least until my next journey. I thought about taking a few months off, but a few weeks in and I start to feel a bit out of place.  \nI really don't know how to handle simply doing nothing.\n\nI thought maybe I‚Äôd start some initiative in this rare window I‚Äôm in right now, and I was hoping to get interesting ideas from the community.\n\nMy main objective is that it would be something valuable that I enjoy doing.  \nThis could be something that is technically cool (AGI anyone?) or some tool for the community (any tool you'd wish existed? paperswithcode or paper copilot comes to mind).\n\nLove to hear your thoughts!",
    "author": "Entrepreneur7962",
    "timestamp": "2025-08-13T11:00:48",
    "url": "https://reddit.com/r/MachineLearning/comments/1mpbp39/d_got_spare_time_whats_worth_doing/",
    "score": 44,
    "num_comments": 46,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mp2dcp",
    "title": "[R] Fuzzy-Pattern Tsetlin Machine",
    "content": "I‚Äôm excited to announce the paper:¬†**Fuzzy-Pattern Tsetlin Machine** (FPTM)¬†‚Äî a paradigm shift in the Tsetlin Machine family of algorithms.\n\nUnlike traditional Tsetlin Machines, which rely on strict clause evaluation, FPTM introduces¬†fuzzy clause evaluation: if some literals in a clause fail, the remaining literals can still contribute to the vote with a proportionally reduced score. This allows each clause to act as a collection of adaptive sub-patterns, enabling more flexible, efficient, and robust pattern matching.\n\nThanks to this fuzzy mechanism, FPTM dramatically reduces the number of required clauses, memory usage, and training time ‚Äî all while improving accuracy.\n\n**Results:**\n\n**IMDb** dataset:\n\n‚Ä¢ 90.15% accuracy with just **1 clause** per class\n\n‚Ä¢ 50√ó reduction in clauses and memory vs. Coalesced TM\n\n‚Ä¢ 36√ó to 316√ó faster training (**45 seconds vs. 4 hours**) compared to TMU Coalesced TM\n\n‚Ä¢ Fits in **50 KB**, enabling online learning on microcontrollers\n\n‚Ä¢ Inference throughput: **34.5 million** predictions per second (51.4 GB/s)\n\n**Fashion-MNIST** dataset:\n\n‚Ä¢ 92.18% accuracy (2 clauses per class)\n\n‚Ä¢ 93.19% accuracy (20 clauses), \\~400√ó clause reduction vs. Composite TM (93.00% with 8000 clauses)\n\n‚Ä¢ **94.68%** accuracy (8000 clauses), establishing a new *state-of-the-art* among all TM variants and outperforming complex neural net architectures like *Inception-v3*\n\n**Amazon Sales** dataset (20% noise):\n\n‚Ä¢ **85.22%** accuracy ‚Äî outperforming Graph TM (78.17%) and GCN (66.23%)\n\nüìÑ Read the paper: [https://arxiv.org/pdf/2508.08350](https://arxiv.org/pdf/2508.08350)\n\nüíª Source code: [https://github.com/BooBSD/FuzzyPatternTM](https://github.com/BooBSD/FuzzyPatternTM)",
    "author": "ArtemHnilov",
    "timestamp": "2025-08-13T04:55:31",
    "url": "https://reddit.com/r/MachineLearning/comments/1mp2dcp/r_fuzzypattern_tsetlin_machine/",
    "score": 48,
    "num_comments": 7,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mp6n2g",
    "title": "[D] Google DeepMind Analytics Engineer Interview Prep",
    "content": "Got an upcoming interview for this role and have a good feeling so far. How do I prepare for it? What will be the next steps? Any tips or experience would be greatly appreciated. Thanks!",
    "author": "ChampionshipCrazy429",
    "timestamp": "2025-08-13T07:53:04",
    "url": "https://reddit.com/r/MachineLearning/comments/1mp6n2g/d_google_deepmind_analytics_engineer_interview/",
    "score": 18,
    "num_comments": 5,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mp3u1f",
    "title": "[D] EMNLP 2025 Decisions",
    "content": "Discussion thread for EMNLP 2025 decisions\n\n",
    "author": "Accomplished-Pay-390",
    "timestamp": "2025-08-13T06:02:29",
    "url": "https://reddit.com/r/MachineLearning/comments/1mp3u1f/d_emnlp_2025_decisions/",
    "score": 30,
    "num_comments": 600,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mo0ynr",
    "title": "[R] Position: The Current AI Conference Model is Unsustainable!",
    "content": "\nPaper: https://www.alphaxiv.org/abs/2508.04586v1  \n\n\nüìà Publication Surge: Per-author publication rates have more than doubled over the past decade to over 4.5 papers annually.  \n\n\nüöÄ Exponential Output Growth: Individual contributions are rising so fast they‚Äôre projected to exceed one paper per month by the 2040s.  \n\n\nüåç Carbon Overload: NeurIPS 2024‚Äôs travel emissions (&gt;8,254 tCO‚ÇÇe) alone surpass Vancouver‚Äôs daily citywide footprint.  \n\n\nüòû Mental Health Toll: Of 405 Reddit threads on AI conferences, over 71% are negative and 35% mention mental-health concerns.  \n\n\n‚è≥ Research-Conference Mismatch: The AI research lifecycle outpaces conference schedules, often rendering results outdated before presentation.  \n\n\nüèüÔ∏è Venue Capacity Crisis: Attendance at top AI conferences like NeurIPS 2024 is already outstripping available venue space.",
    "author": "NuoJohnChen",
    "timestamp": "2025-08-11T23:10:51",
    "url": "https://reddit.com/r/MachineLearning/comments/1mo0ynr/r_position_the_current_ai_conference_model_is/",
    "score": 396,
    "num_comments": 52,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1moqpqw",
    "title": "[D] Statement on the Originality of OpenRLHF and veRL FSDP RLHF",
    "content": "&gt;From the original chinese zhihu blogpost (2025/5): [https://zhuanlan.zhihu.com/p/23147932785](https://zhuanlan.zhihu.com/p/23147932785)\n\n**Recently, there has been quite a bit of discussion and controversy online about OpenRLHF and veRL.**  \n**As the original author, I feel compelled to issue a statement.**\n\nIn short: **OpenRLHF is like KartRider ‚Äî the original ‚Äî and veRL FSDP is like QQ Speed, which is basically a copycat of OpenRLHF.**\n\n# 1. Performance Differences Between OpenRLHF and veRL\n\nThere is no fundamental performance difference between veRL‚Äôs FSDP RLHF and OpenRLHF (DeepSpeed) because both use vLLM for inference and ZeRO3 for training.  \nThe performance data in veRL‚Äôs original paper was based on *Megatron* RLHF vs. the old OpenRLHF 0.2 version.  \nIf you think there‚Äôs a big performance gap, you probably just used it incorrectly. At the moment, FSDP is slightly faster than DeepSpeed, but with the release of DeepSpeed‚Äôs **deepcompile** and especially **AutoTP**, DeepSpeed is expected to overtake in performance.\n\n# 2. On HybridFlow Free Scheduling\n\nAny RLHF framework developed with Ray can achieve free scheduling because Ray natively provides the *placement group* feature.  \nThis means HybridFlow in veRL's paper is essentially just a nicer name for Ray‚Äôs Placement Group API.  \nCurrently, OpenRLHF fully implements HybridFlow, whereas veRL does not.  \nOpenRLHF also supports independent deployment of vLLM and Actors to prevent OOM issues when training very large models (32B+ or long-text).  \nIn fact, OpenRLHF was the **first** framework to support this feature based on Ray Placement Group API.\n\n# 3. Hybrid Engine\n\nHybrid Engine was first proposed by **DeepSpeedChat**, not an original contribution from veRL.  \nBoth veRL and OpenRLHF now support this feature.\n\n# 4. Ray + vLLM + HF Transformers + ZeRO3 for RLHF Training\n\nThis setup is one of the **simplest and most user-friendly** high-performance RLHF training solutions, combining ease of use with top performance.\n\nIt was first proposed and open-sourced by OpenRLHF (open-sourced in Aug 2023, most features completed by Jan 2024).  \nveRL FSDP **fully copied** this setup.\n\nhttps://preview.redd.it/vfzm143vroif1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=10d8a5bcd101455a06a3506f037abc10f12dd277\n\nhttps://preview.redd.it/tqela8mvroif1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=c3a2daa1ead45f7434184f107da8ba2f78cc9c8d\n\nThe core idea at the time was to use the HF weight format as a bridge, enabling seamless weight synchronization and high-performance inference based on ZeRO3 / AutoTP mechanisms, **avoiding** heavyweight frameworks like Megatron.\n\n**The Original OpenRLHF Architecture:**  \n**Ray + vLLM + ZeRO + HF**\n\nThere are also many related implementation details:\n\n* Supported feature list\n* Standardized interfaces such as `--input_key` to specify the input field format\n\nAll of these in veRL FSDP were **modeled after OpenRLHF**.\n\n**Example from code details:**  \nveRL:\n\nhttps://preview.redd.it/b8f2lprwroif1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=a0daf3eab1c77f71e4917c044f988c35e229baa4\n\nhttps://preview.redd.it/exf7lxhxroif1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=220636cea299502df1b94e2544a76b34e2acb6c7\n\nOpenRLHF:\n\nhttps://preview.redd.it/qfakvovyroif1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=260775676354a50bacd79ce06fb25417a53466de\n\nOther design ideas like **ref\\_reward offload**, **critic pretrain**, **remote RM**, etc., were also first conceived or proposed by OpenRLHF, and veRL FSDP later implemented corresponding features.\n\n# 5. Single Controller\n\n*(Update May 2025)*\n\nThe ‚ÄúSingle Controller‚Äù concept mentioned in the veRL paper comes from the same Ray design pattern as HybridFlow.\n\nIn early versions of OpenRLHF‚Äôs Ray RLHF implementation, there was a `RayPPOActorGroup` concept‚Äîmanaging a group of DeepSpeed ZeRO DP processes with a single Ray Group class, and providing an `async_run_method` interface to control all processes in the group at once.  \nThat‚Äôs essentially the core idea of Single Controller.\n\n[https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300](https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300)\n\nThis interface wasn‚Äôt enabled at first because the codebase needed to be compatible with both Ray and non-Ray RLHF paths. Later, when the non-Ray code was removed, the API was naturally enabled.\n\nLastly, I want to thank ByteDance for open-sourcing its internal framework for everyone to use and maintain, which helps the open-source community thrive (e.g., FSDP / Ulysses support).\n\nHowever, I hope friends in the community won‚Äôt disparage other open-source frameworks.  \nOpenRLHF, as a **zero-budget, purely open-source** project, can‚Äôt compete in development speed with large commercial projects like veRL‚Äî  \nI only hope this post helps preserve the contributions OpenRLHF has made to the RLHF open-source community.\n\n**Btw, the open-source community should respect originality in order to develop healthily.**",
    "author": "seventh_day123",
    "timestamp": "2025-08-12T18:05:42",
    "url": "https://reddit.com/r/MachineLearning/comments/1moqpqw/d_statement_on_the_originality_of_openrlhf_and/",
    "score": 12,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mpa2ip",
    "title": "[D] If there were to be some sort of way you could get NDVI (not true, but predict) that was near perfect accuracy through JUST standard RGB input (NO NIR AT ALL), how useful would that be (API, for example)?",
    "content": "Sorry if this is not the right place to post! I'm new to the community and overall GIS industry. Just want to see how useful this would be, specific use cases, and maybe how this could be used by you personally.\n\n  \nI know there are RGB-only indices that exist, but from what I've heard, they're very inaccurate. This would be 94%+ (accuracy to true-NDVI) and it‚Äôs a highly trained ML model",
    "author": "Proud_Landscape_4231",
    "timestamp": "2025-08-13T10:01:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1mpa2ip/d_if_there_were_to_be_some_sort_of_way_you_could/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.22,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mor8vy",
    "title": "[D] Applying Prioritized Experience Replay in the PPO algorithm",
    "content": "When using the PPO algorithm, can we improve data utilization by implementing Prioritized Experience Replay (PER) where the priority is determined by both the probability ratio and the TD-error, while simultaneously using a windows\\_size\\_ppo parameter to manage the experience buffer as a sliding window that discards old data?",
    "author": "NoteDancing",
    "timestamp": "2025-08-12T18:30:00",
    "url": "https://reddit.com/r/MachineLearning/comments/1mor8vy/d_applying_prioritized_experience_replay_in_the/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1mokka9",
    "title": "[D] Multiple submission policy at EMNLP 2025 for workshops",
    "content": "Hi all,\n\nI‚Äôm trying to understand the EMNLP 2025 multiple submission policy when it comes to co-organized workshops.\n\nOur paper is committed to EMNLP 2025 (main conference), but we think it might also be a good fit for a specific workshop, in case if it is not accepted to EMNLP. \n\nThe problem is, the workshop‚Äôs submission deadline is before the EMNLP notification date (Aug 20).\n\nThe workshop‚Äôs CFP says multiple submissions are fine if disclosed at submission. However, the EMNLP CFP states it follows the ARR multiple submission policy, which includes this clause:\n\n&gt; Commitment + Commitment/Other Venue: Whether you can commit/submit to two venues simultaneously depends on the dual submission policies of those venues. Typically, it is not permitted.\n\n[ARR policy](https://aclrollingreview.org/cfp#:~:text=Multiple%20Submission%20Policy)\n\n\nTL;DR \n\nWhat I‚Äôm unsure about is this:\n\n- Does ‚Äúother venue‚Äù here include EMNLP co-organized workshops?\n\n- Has anyone successfully submitted to both the main conference and a co-organized workshop in this timing overlap?\n\n\nI couldn‚Äôt find any direct clarification online for this year, so I‚Äôd really appreciate hearing from researchers who‚Äôve navigated this.\n\nThanks!",
    "author": "alkalinemoe",
    "timestamp": "2025-08-12T13:49:31",
    "url": "https://reddit.com/r/MachineLearning/comments/1mokka9/d_multiple_submission_policy_at_emnlp_2025_for/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "MachineLearning",
    "post_id": "1moj422",
    "title": "Guidance on improving the reconstruction results of my VAE [Project]",
    "content": "Hi all! I was trying to build a VAE with an LSTM to reconstruct particle trajectories by basing off my model on the paper \"Modeling Trajectories with Neural Ordinary Differential Equations\". However, despite my loss plots showing a downward trend, my predictions are linear.\n\nI have applied KL annealing and learning rate scheduler - and yet, the model doesn't seem to be learning the non-linear dynamics. The input features are x and z positions, velocity, acceleration, and displacement. I used a combination of ELBO and DCT for my reconstruction loss. The results were quite bad with MinMax scaling, so I switched to z-score normalization, which helped improve the scales. I used the Euler method with torchdiffeq.odeint.\n\nWould it be possible for any of you to guide me on what I might be doing wrong? I‚Äôm happy to share my implementation if it helps. I appreciate and am grateful for any suggestions (and sorry about missing out on the labeling the axes - they are x and z)\n\nhttps://preview.redd.it/veskdk7p7nif1.png?width=529&amp;format=png&amp;auto=webp&amp;s=0938c4dd588961f94eba40a0e20d81008bc131f0\n\nhttps://preview.redd.it/ddubae7p7nif1.png?width=529&amp;format=png&amp;auto=webp&amp;s=15a24e197e6fd331d92175d1327fb2b482aaa2cc",
    "author": "fictoromantic_25",
    "timestamp": "2025-08-12T12:55:28",
    "url": "https://reddit.com/r/MachineLearning/comments/1moj422/guidance_on_improving_the_reconstruction_results/",
    "score": 3,
    "num_comments": 11,
    "upvote_ratio": 0.71,
    "is_original_content": false
  }
]