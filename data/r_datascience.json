[
  {
    "subreddit": "datascience",
    "post_id": "1ocenxj",
    "title": "Erdos: open-source IDE for data science",
    "content": "After a few months of work, we’re excited to launch [Erdos](https://www.lotas.ai/erdos) \\- a secure, AI-powered data science IDE, all open source! Some reasons you might use it over VS Code:\n\n* An AI that searches, reads, and writes all common data science file formats, with special optimizations for editing Jupyter notebooks\n* Built-in Python, R, and Julia consoles accessible to the user and AI\n* Single-click sign in to a secure, zero data retention backend; or users can bring their own keys\n* Plots pane with plots history organized by file and time\n* Help pane for Python, R, and Julia documentation\n* Database pane for connecting to SQL and FTP databases and manipulating data\n* Environment pane for managing in-memory variables, python environments, and Python, R, and Julia packages\n* Open source with AGPLv3 license\n\nUnlike other AI IDEs built for software development, Erdos is built specifically for data scientists based on what we as data scientists wanted. We'd love if you try it out at [https://www.lotas.ai/erdos](https://www.lotas.ai/erdos)",
    "author": "SigSeq",
    "timestamp": "2025-10-21T07:36:08",
    "url": "https://reddit.com/r/datascience/comments/1ocenxj/erdos_opensource_ide_for_data_science/",
    "score": 80,
    "num_comments": 35,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1obvzq9",
    "title": "Feeling like I’m falling behind on industry standards",
    "content": "I currently work as a data scientist at a large U.S. bank, making around $182K. The compensation is solid, but I’m starting to feel like my technical growth is being stunted.\n\nA lot of our codebase is still in SAS (which I struggle to use), though we’re slowly transitioning to Python. We don’t use version control, LLMs, NLP, or APIs — most of the work is done in Jupyter notebooks. The modeling is limited to logistic and linear regressions, and collaboration happens mostly through email or shared notebook links.\n\nI’m concerned that staying here long-term will limit my exposure to more modern tools, frameworks, and practices — and that this could hurt my job prospects down the road.\n\nWhat would you recommend I focus on learning in my free time to stay competitive and become a stronger candidate for more technically advanced data science roles?\n\n",
    "author": "xCrek",
    "timestamp": "2025-10-20T15:32:31",
    "url": "https://reddit.com/r/datascience/comments/1obvzq9/feeling_like_im_falling_behind_on_industry/",
    "score": 181,
    "num_comments": 62,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1obr0ve",
    "title": "How many peoples' days were upset by this today?",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-10-20T12:22:58",
    "url": "https://reddit.com/r/datascience/comments/1obr0ve/how_many_peoples_days_were_upset_by_this_today/",
    "score": 293,
    "num_comments": 18,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ocgpac",
    "title": "Meet the New Buzzword Behind Every Tech Layoff — From Salesforce to Meta",
    "content": "",
    "author": "nullstillstands",
    "timestamp": "2025-10-21T08:54:24",
    "url": "https://reddit.com/r/datascience/comments/1ocgpac/meet_the_new_buzzword_behind_every_tech_layoff/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1oc8xij",
    "title": "Do we still need Awesome lists now that we have LLMs like ChatGPT?",
    "content": "Hi folks!\n\nLet's talk about Awesome lists (curated collections of resources and tools) and what's happening to them now with  LLMs like ChatGPT and Claude around.\n\nI'm constantly impressed by how quickly LLMs can generate answers and surface obscure tools, but I also deeply respect the human-curated, battle-tested reliability of a good Awesome list. Let me be clear: I'm not saying they're obsolete. I genuinely value the curation and reliability they offer, which LLMs often lack.\n\nSo, I'm genuinely curious about the community's take on this.\n\n* In the era of LLMs, are traditional Awesome lists becoming less critical, or do they hold a new kind of value?\n* Do you still actually browse them to discover new stuff, or do you mostly rely on LLMs now?\n* How good are LLMs really when you don’t exactly know what you’re looking for? Are you happy with what they recommend?\n* What's your biggest frustration or limitation with traditional Awesome lists?",
    "author": "DeepAnalyze",
    "timestamp": "2025-10-21T03:05:10",
    "url": "https://reddit.com/r/datascience/comments/1oc8xij/do_we_still_need_awesome_lists_now_that_we_have/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1obn0tc",
    "title": "Communities / forums / resources for building neural networks",
    "content": "Hoping to compile a list of resources / communities that are specifically geared towards training large neural networks. Discussions / details around architecture, embedding strategies, optimization, etc are along the lines of what I’m looking for. ",
    "author": "JimBeanery",
    "timestamp": "2025-10-20T09:29:37",
    "url": "https://reddit.com/r/datascience/comments/1obn0tc/communities_forums_resources_for_building_neural/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1obad7k",
    "title": "How to perform synthetic control for multiple treated units? What are the things to keep in mind while performing it? Also, what python package i could use? Also have questions about metrics",
    "content": "\nHi\nI have never done Synthetic control, i want to work on a small project (like small data. My task is to find incremental effect), i have a few treatment units, have multiple units as a control (which includes some as major/anchor markets).\n\nSo questions are below:\n\n1. I know basic understanding of SCM but never used it, i know you get to optimize control units for a single treatment unit, but how do you perform the test when you have multiple treatments units? Do you build synthetic for each units? \nIf yes, do you use all control units for each treatment units? Then that means hace to do same steps multiple times? \n\n2. How do you use anchor markets? Like do you give them more weights from initial or do we need to do something about their data before doing the performance? \n\n3. How do you do placebo tests? Do we take a control unit then find synthetic control units? And in this synthetic do we include treatment units as well (I assume no, but still wanted to confirm) \n\n4. Lets say we want to check incremental for x metrics, do we do the whole process x times differently for each metric? Or once we have done it for one metric we can use the same synthetics for other metrics? \n(Lets say basic metrics like revenue, conversion, ctr) \n\n5. Which python package do we use  if there is resource on it would be great\n\n\n6. Am i missing any steps or things you believe i should be keep  in mind? \n\nThanks! Would be great help \n",
    "author": "Starktony11",
    "timestamp": "2025-10-19T21:17:03",
    "url": "https://reddit.com/r/datascience/comments/1obad7k/how_to_perform_synthetic_control_for_multiple/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1oa93fw",
    "title": "Anyone else tired of the non-stop LLM hype in personal and/or professional life?",
    "content": "I have a complex relationship with LLMs. At work, I'm told they're the best thing since the invention of the internet, electricity, or \\[insert other trite comparison here\\], and that I'll lose my job to people who do use them if I won't (I know I won't lose my job). Yes, standard \"there are some amazing use cases, like the breast cancer imaging diagnostics\" applies, and I think it's good for those like senior leaders where \"close enough\" is all they need. Yet, on the front line in a regulated industry where \"close enough\" doesn't cut it,  what I see on a daily basis are models that:\n\n(a) can't be trained on our data for legal and regulatory reasons and so have little to no context with which to help me in my role. Even if they could be trained on our company's data, most of the documentation - if it even exists to begin with - is wrong and out of date.\n\n(b) are suddenly getting worse (looking at you, Claude) at coding help, largely failing at context memory in things as basic as a SQL script - it will make up the names to tables and fields that have clearly, explicitly been written out just a few lines before. Yes they can help create frameworks that I can then patch up, but I do notice degradation in performance.\n\n(c) always manage to get \\*something\\* wrong, making my job part LLM babysitter. For example, my boss will use Teams transcribe for our 1:1s and sends me the AI recap after. I have to sift through because it always creates action items that were never discussed, or quotes me saying things that were never said in the meeting by anyone. One time, it just used a completely different name for me throughout the recap.\n\nHaving seen how the proverbial sausage is made, I have no desire to use it in my personal life, because why would I use it for anything with any actual stakes? And for the remainder, Google gets me by just fine for things like \"Who played the Sheriff in Blazing Saddles?\"\n\nAnyone else feel this way, or have a weird relationship with the technology that is, for better or worse, \"transforming\" our field?\n\nUpdate: some folks are leaving short, one sentence responses to the effect of \"They've only been great for me.\" Good! Tell us more about how you're finding success in your applications. any frustrations along the way? let's have a CONVERSATION. ",
    "author": "BlackJack5027",
    "timestamp": "2025-10-18T15:16:40",
    "url": "https://reddit.com/r/datascience/comments/1oa93fw/anyone_else_tired_of_the_nonstop_llm_hype_in/",
    "score": 453,
    "num_comments": 97,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1oana21",
    "title": "I built a project and I thought I might share it with the group",
    "content": "Disclaimer: It's UK focused.\n\n  \nHi everyone,\n\nWhen I was looking to buy a house, a big annoyance I had was that I couldn’t easily tell if I was getting value for money. Although, in my opinion, any property is expensive as fuck, I knew that definitely some are more expensive than they should be, always within context.\n\nAt the time, what I did was manually extract historical data for the street and for the property I was interested in, in an attempt to understand whether it was going for more than the street average or less, and why. It wasn’t my best analysis, but it did the job.\n\nFast forward a few years later, I found myself unemployed and started building projects for my portfolio, which brings us to this post. I’ve built an app that, for a given postcode, gives you historical prices, price per m², and year-on-year sales for the neighbourhood, the area, and the local authority the property falls under, as well as a property price estimation summary.\n\nThere are, of course, some caveats. Since I’m only using publicly available data, the historical trends are always going to be 2–3 months behind. However, there’s still the capacity to see overall trends e.g. an area might be up and coming if the trendline is converging toward the local authority’s average.\n\nAs for the property valuation bits, although I’d say it’s as good as what’s available out there, I’ve found that at the end of the day, property prices are pretty much defined by the price of the most recent, closest property sold.\n\nFinally, this is a portfolio project, not a product  but since I’m planning to maintain it, I thought I might as well share it with people, get some feedback, and maybe even make it a useful tool for some.\n\n  \nAs for what's going on under the hood. The system is organized into three modules: WH, ML, and App. Each month, the WH (Warehouse) module ingests data into BigQuery, where it’s transformed following a medallion architecture. The ML module is then retrained on the latest data, and the resulting inference outputs are stored in the gold layer of BigQuery. The App module, hosted on a Lightsail instance, loads the updated gold-layer inference and analytics data after each monthly iteration. Within the app, DuckDB is used to locally query and serve this data for fast, efficient access.\n\nAnyway, here’s the link if you want to play around:  [https://propertyanalytics.uk](https://propertyanalytics.uk)\n\nNote: It currently covers England and Wales, only.\n\nhttps://preview.redd.it/s220a3z702wf1.png?width=566&amp;format=png&amp;auto=webp&amp;s=1999caa45801a0ab216fa63e2de09cc9c6dfafaf\n\n",
    "author": "Emergency-Agreeable",
    "timestamp": "2025-10-19T04:28:42",
    "url": "https://reddit.com/r/datascience/comments/1oana21/i_built_a_project_and_i_thought_i_might_share_it/",
    "score": 28,
    "num_comments": 10,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1oa6dn1",
    "title": "Transformers, Time Series, and the Myth of Permutation Invariance",
    "content": "There's a common misconception in ML/DL that *Transformers shouldn’t be used for forecasting because attention is permutation-invariant.*\n\nLatest evidence shows the opposite, such as Google's latest model, where the experiments show the model performs just as well with or without positional embeddings.\n\nYou can find an analysis on tis topic [here](https://aihorizonforecast.substack.com/p/transformers-time-series-and-the).",
    "author": "nkafr",
    "timestamp": "2025-10-18T13:26:40",
    "url": "https://reddit.com/r/datascience/comments/1oa6dn1/transformers_time_series_and_the_myth_of/",
    "score": 20,
    "num_comments": 4,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o9urrk",
    "title": "Adversarial relation of success and ethics",
    "content": "I’ve been data scientist for four years and I feel we often balance on a verge of cost efficiency, because how expensive the truths are to learn. \n\nArguably, I feel like there are three types of data investigations: trivial ones, almost impossible ones, and randomized controlled experiments. The trivial ones are making a plot of a silly KPI, the impossible ones are getting actionable insights from real-world data. Random studies are the one thing in which I (still) trust. \n\nThat’s why I feel like most of my job is being pain in someone’s ass, finding data flaws, counterfactuals, and all sorts of reasons why whatever stakeholders want is impossible or very expensive to get. \n\nSometimes Im afraid that data science is just not cost effective. And worse, sometimes I feel like I’d be a more successful (paid better) data scientist if I did more of meaningless and shallow data astrology, just reinforcing the stakeholders that their ideas are good - because given the reality of data completeness and quality, there’s no way for me to tell it. Or announcing that I found an area for improvement, deliberately ignoring boring, alternative explanations. And honestly - I think that no one would ever learn what I did.\n\nIf you feel similarly, take care! I hope you too occasionally still get a high from rare moments of scientific and statistical purity we can sometimes find in our job. \n",
    "author": "Ciasteczi",
    "timestamp": "2025-10-18T05:42:37",
    "url": "https://reddit.com/r/datascience/comments/1o9urrk/adversarial_relation_of_success_and_ethics/",
    "score": 14,
    "num_comments": 14,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o9r7kl",
    "title": "Choose between 2 internal offers ?",
    "content": "Hi everyone, (TLDR at the end)\n\nI’d like some advice on which option would be best for my career in 2–3 years. Both offers are internal, same salary level (France, ~58k€ total, + added bonus and stock on top).\n\nI currently work as a Data Scientist – AI Lead in the space division of a major European aerospace group. I lead the internal roadmap for generative AI (RAG, LLM, ESA projects), manage ~400k€/year in R&amp;D budget, and supervise 3 people + 2 interns.\nManagement really believes in me and wants to promote me since I have been applying for new internal opportunities. Today I have 2 options on the same salary bands.\n\n⸻\n\nOption 1 – getting a promotion in my team and Stay in the Space Division\n\nRole: AI Solutions Engineer / Product Owner\n\nContext: Engineering-heavy environment (satellite systems, physics, data).\n\nCommute: 10 min by bike.\n\nScope:\n\n\t•\tunderstand needs and Deploy an tailored ChatGPT-like solution for technical users (~100 users/use case) as we do not have a cloud available.\n\n\t•\tIntegrate generative AI into internal data platforms (500–800 users).\n\n\t•\tManage a total budget of ~1.2M€ (including ~200k R&amp;D).\n\n\t•\tSupervise subcontractors (to help with the tasks I need, I can delegate everything I want) and handle ESA AI projects (surrogate modeling, etc.).\n\nPros:\n\n\t•\tGreat work-life balance (flexible hours, local site).\n\n\t•\tStrong autonomy and technical depth.\n\n\t•\tSupportive management, solid internal reputation.\n\n\t•\tFits my AI/engineering background perfectly.\n\nCons:\n\n\t•\tRestricted infra (no public cloud, only internal clusters).\n\n\t•\tSlow processes and limited tools.\n\n\t•\tImpact limited to the space business (niche scope).\n\n\t•\tThe space division might merge with another company within 2 years — could lead to reorgs, project cancellations, or slower salary progression, and lose of big bonuses. Also current health of the branch is bad.\n\n⸻\n\nOption 2 – Move to the Corporate Digital Department\n\nRole: Project Manager AI for Employee Services (Agentic AI).\n\nContext: Corporate HQ – global digital transformation team.\n\nCommute: 35–40 min by bike.\n\nScope:\n\n\t•\tManage a 1.4M€ budget to deploy AI HR tools (RAG, agentic, …) and automation tools for 130,000 employees.\n\n\t•\tWork with IT architects, data scientists, and HR stakeholders.\n\n\t•\tAccess to modern cloud stack (Azure, M365, Vertex AI) in a more mature environment.\n\n\t•\tExposure to the Chief Digital Officer and HR top management.\n\nPros:\n\n\t•\tGlobal visibility and strategic exposure.\n\n\t•\tFull access to modern AI tools and cloud infrastructure.\n\n\t•\tLarger budget and decision-making autonomy.\n\n\t•\tStronger potential long-term financial upside (high corporate bonuses, stock plan). Great financial health of the company.\n\nCons:\n\n\t•\tLess technical, even though they agreed I can build PoCs and stay hands on, and be active in the architecture decisions. More project management and stakeholder coordination.\n\n\t•\tMostly non-technical interlocutors (HR, business).\n\n\t•\tMore political environment and higher delivery pressure.\n\n\t•\tLonger commute and less daily flexibility.\n\n⸻\n\nTL;DR\n\n\t•\tOption 1 (Space): technical, stable, flexible, management trusts me and promises high career paths, but risk of merger and limited AI or cloud/tools.\n\n\t•\tOption 2 (Corporate Digital): strategic, bigger scope (130k people), access to modern tools, more political, less hands-on.\n\n\t•\tSalary: roughly the same (~58k€, + extra stock and bonus).\n\nQuestion:\n\nWhich path would give me the strongest market value in 2 years — staying as a hands-on AI lead in the space division or moving into a corporate-level AI project manager role?\n\nI value growth, getting more full remote / part time options well paid later on, and value WLB.\n\n",
    "author": "LocPat",
    "timestamp": "2025-10-18T02:20:11",
    "url": "https://reddit.com/r/datascience/comments/1o9r7kl/choose_between_2_internal_offers/",
    "score": 5,
    "num_comments": 19,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o93utr",
    "title": "Causal Data Scientists, what resources helped you the most?",
    "content": "Hello everyone, \n\nI am working on improving in areas of Bayesian and Frequentists A/B testings and Causal Inference, and applying them in industry. I am currently working on normal Frequentists A/B testings, and simple Causal Inference but want to expand to more nuanced cases and have some examples of what they may look like. For example, when to choose TMLE over Propensity Score Matching etc or Bayesian vs Frequentists. \n\n  \nPlease let me know if theres any resources that helped you apply these methods in your job. ",
    "author": "LebrawnJames416",
    "timestamp": "2025-10-17T08:07:16",
    "url": "https://reddit.com/r/datascience/comments/1o93utr/causal_data_scientists_what_resources_helped_you/",
    "score": 99,
    "num_comments": 20,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o8ipwa",
    "title": "Would you move from DS to BI/DA/DE for a salary increase?",
    "content": "I’m a DS but salary is below average. Getting recruiters reaching out for other data roles though because my experience is broad. Sometimes these roles start at ~$40k over what I’m making now, and even over other open DS roles I see on LinkedIn in my area for my yoe.\n\nThe issue is I love DS work, and don’t want to make it super difficult to get future DS jobs. But I also wouldn’t mind working in another data role for a bit to get that money though. \n\nWhat are everyone’s thoughts on this? Would you leave DS for more money?",
    "author": "Fit-Employee-4393",
    "timestamp": "2025-10-16T14:23:15",
    "url": "https://reddit.com/r/datascience/comments/1o8ipwa/would_you_move_from_ds_to_bidade_for_a_salary/",
    "score": 62,
    "num_comments": 44,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o8bkbt",
    "title": "What computer do you use for personal projects?",
    "content": "I’m trying to branch out and do more personal projects for my portfolio. My personal computer is pretty old, and I’m reluctant to use my work computer for my personal projects, so I’m curious about what kinds of computers you all use.",
    "author": "BloatedGlobe",
    "timestamp": "2025-10-16T09:54:57",
    "url": "https://reddit.com/r/datascience/comments/1o8bkbt/what_computer_do_you_use_for_personal_projects/",
    "score": 33,
    "num_comments": 36,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o8p6f0",
    "title": "Where to find actual resources and templates for data management that aren't just blog posts?",
    "content": "[](https://www.reddit.com/r/analytics/?f=flair_name%3A%22Question%22)I'm early in my career, and I've been tasked with a lot of data management and governance work, building SOPs and policies, things like that, for the first time. Everytime I try to research the best templates, guides, documents, spreadsheets, mindmaps, etc., all I get are the annoying generic blog posts that companies use for SEO, like [this](https://www.datamation.com/big-data/data-management-best-practices/). They say \"You should document everything\" but don't actually offer templates on how! I want to avoid reinventing the wheel, especially since I'm new to this side of data work.\n\nDoes anyone know of a good public resources to find guides, templates, spreadsheets, etc., for documentation, data management, SOPs, things like that instead of just the long blog posts that are littering the internet",
    "author": "lemonbottles_89",
    "timestamp": "2025-10-16T19:11:53",
    "url": "https://reddit.com/r/datascience/comments/1o8p6f0/where_to_find_actual_resources_and_templates_for/",
    "score": 5,
    "num_comments": 7,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o7a0rk",
    "title": "Completely Free Courses Oct 20-30 from Maven Analytics",
    "content": "Maven Analytics is hosting their Open Campus event Oct 20-30.  This means their whole platform is 100% free during that time. If you've been thinking about taking a course on Power BI, SQL, Python, how to approach the job search, etc., it would be a great time to binge and learn something new.\n\n  \nThere's also live sessions for these two weeks around portfolio projects, interviewing, etc.  And they all have Q&amp;A at the end, so you can ask any of the questions you have around getting into data. ",
    "author": "Clicketrie",
    "timestamp": "2025-10-15T05:46:29",
    "url": "https://reddit.com/r/datascience/comments/1o7a0rk/completely_free_courses_oct_2030_from_maven/",
    "score": 38,
    "num_comments": 12,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o7soo6",
    "title": "Anyone go through a McKinsey phone screening?",
    "content": "Anyone know what to expect for a first round phone screening for a data science role at McKinsey?",
    "author": "andrew2018022",
    "timestamp": "2025-10-15T17:58:44",
    "url": "https://reddit.com/r/datascience/comments/1o7soo6/anyone_go_through_a_mckinsey_phone_screening/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.48,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o6f3l8",
    "title": "AutoML: Yay or nay?",
    "content": "Hello data scientists and adjacent,\n\nI'm at a large company which is taking an interest in moving away from the traditional ML approach of training models ourselves to using AutoML. I have limited experience in it (except an intuition that it is likely to be less powerful in terms of explainability and debugging) and I was wondering what you guys think.\n\nHas anyone had experience with both \"custom\" modelling pipelines and using AutoML (specifically the GCP product)? What were the pros and cons? Do you think one is better than the other for specific use cases?\n\nThanks :)",
    "author": "idontknowotimdoing",
    "timestamp": "2025-10-14T06:10:50",
    "url": "https://reddit.com/r/datascience/comments/1o6f3l8/automl_yay_or_nay/",
    "score": 35,
    "num_comments": 29,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o5nvkp",
    "title": "AI Is Overhyped as a Job Killer, Says Google Cloud CEO",
    "content": "",
    "author": "KitchenTaste7229",
    "timestamp": "2025-10-13T09:05:20",
    "url": "https://reddit.com/r/datascience/comments/1o5nvkp/ai_is_overhyped_as_a_job_killer_says_google_cloud/",
    "score": 445,
    "num_comments": 80,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o64n48",
    "title": "Has anyone switched to AI Product Management from Data Science?",
    "content": "I've been a DS for almost 5 years, with a good majority in NLP. I've been wanting to do more POCs, less model production (IT budget, stack ranking, general burn-out) and get into Product Management for a while. \n\nI know the technology quite well, but I lack PM experience. Honestly, I'm pretty burnt out from DS. I really like working with cross-functional teams and focusing on strategy/business more so than coding. I tend to mainly do that these days during the day, then have to code at night and it's gotten exhausting. And coming into the office with all of that... not sustainable.  \n\nI'd love to know your journey and what made you stand out when making the switch! ",
    "author": "SnooWalruses4775",
    "timestamp": "2025-10-13T20:18:13",
    "url": "https://reddit.com/r/datascience/comments/1o64n48/has_anyone_switched_to_ai_product_management_from/",
    "score": 36,
    "num_comments": 21,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o68gf8",
    "title": "Deep Learning Topics: How Important Are They?",
    "content": "Background: I have a BS double major in Data Analytics and Information Systems: Data Engineering emphasis. I’m currently pursuing an MS in Data Analytics with a Statistics emphasis, plus graduate certificates in ML/AI and Data Science.\n\nI enjoy:\n\n\t•\tClassical ML and statistics (regression, tree-based models, etc.)\n\n\t•\tA/B testing and experimentation design\n\n\t•\tForecasting and time-series analysis\n\n\t•\tCausal inference\n\n\t•\tSQL and Python (leveraging libraries for applied work rather than building from scratch)\n\n\nWhat I’m less interested in:\n\n\t•\tDeep learning, computer vision, NLP\n\n\t•\tHeavy dashboard work (I can build functional dashboards but lack the design eye for making them actually look good)\n\nMy question is: To work as a Data Scientist, do I need to dive deeper into neural networks, transformers, and other deep learning topics? I don’t want to get stuck doing dashboards all day as a “Data Analyst,” but I also don’t see myself doing deep learning research or building production models for image/text applications.\n\nIs there space in the industry for data scientists who specialize in classical ML, experimentation, and statistical modeling, or does the field increasingly expect everyone to know deep learning inside out?",
    "author": "LilParkButt",
    "timestamp": "2025-10-13T23:54:27",
    "url": "https://reddit.com/r/datascience/comments/1o68gf8/deep_learning_topics_how_important_are_they/",
    "score": 18,
    "num_comments": 18,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o6tquy",
    "title": "Would you recommend starting new agentic projects with Typescript instead of Python?",
    "content": "I read somewhere that something like 60%-75% of YC-backed startups that are building agents are using Typescript. I've also heard that Typescript's native type system is very helpful for building AI apps. Is Typescript a better language than Python for building AI agents? \n\nI don't planning on training my own models so I am not sure if Python is really necessary in my case.",
    "author": "Illustrious-Pound266",
    "timestamp": "2025-10-14T15:22:59",
    "url": "https://reddit.com/r/datascience/comments/1o6tquy/would_you_recommend_starting_new_agentic_projects/",
    "score": 0,
    "num_comments": 14,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o5l0g8",
    "title": "Starting my Freelance Journey",
    "content": "I am a Data Scientist and am going to be moving from London to Amsterdam next year. \n\nI wanted to start freelancing to cover any unemployment period. On fiverr, I see a saturated Data Science space with hundreds of people offering quite similar expertise. On Upwork I realise you need to pay to Connect with project offerings (which sort of makes sense to me to avoid spam for the offerers), which makes me hesitant to start. \n\nI’m just wondering, with where GenAI is right now, is there actually opportunity to start freelancing now or are there still ample opportunities out there? Are people still quite freely doing this as a side hustle?",
    "author": "cdtmh",
    "timestamp": "2025-10-13T07:20:02",
    "url": "https://reddit.com/r/datascience/comments/1o5l0g8/starting_my_freelance_journey/",
    "score": 31,
    "num_comments": 29,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o5n86i",
    "title": "In production, how do you evaluate the quality of the response generated by a RAG system?",
    "content": "I am working on a use case where I need to get the right answer and send it to the user. I have been struggling for a time to find a reliable metric to use that tells me when an answer is correct.\n\nThe cost of a **false positive** is very high; there is a huge risk in sending an incorrect answer to the user.\n\nI have been spending most of my time trying to find which metric to use to evaluate the answer.\n\nHere is what I have tried so far:\n\n* I have checked the perplexity or the average log probability of the generated tokens, but it is only consistent when the model cannot find the answer in the provided chunks. The way my prompt is **designed**, in this case, the model returns, \"I cannot find the answer in the provided context\\*\\*,\\*\\*\" and that is a good signal when I cannot find the **answer**.\n* However, when the model is hallucinating an answer based on the provided tokens, it is very confident and returns a high perplexity / average token probability.\n* I have tried to use the cosine **similarity** between the question and the embeddings. It is okay when the model cannot find the correct chunks; the similarity is low, and for those, I am certain that the answer will be incorrect. But sometimes, the embedding models have some flaws.\n* I have tried to create a **metric** that is a weighted average of the average cosine **similarity** and the average token probability; it seems to work, but not quite well.\n* I cannot use an LLM as a judge. I don't think it **works** or is reliable, and the stakeholders do not trust the whole concept of judging the output of an LLM with another LLM.\n* I am in the process of getting **samples** of questions and answers labelled by **humans** who answer these questions in practice to see which metric will **correlate** with the human answer.\n\n**Other information:**\n\nFor now, I am only working with 164 **samples** of **questions**. Is this good enough? The **business** is planning on providing us with more questions to test the system.\n\nThe workflow I am suggesting for production is this:\n\n1. Get the question.\n2. If the average cosine **similarity** between the question and the chunks is low, route the question to an agent because we cannot find the answer.\n3. If it is high, we send it to the LLM and prompt it to generate an **answer** based on the context. If the LLM cannot find the answer in the provided context, send it to the agent.\n4. If it **says** it can find the answer, **generate** the answer and the reference. Check the average distance and the average token probability; if it is low, send it to the agent.\n5. Now, if the answer is there, there are enough references, and the **weighted** average of the token probability is high, send the answer to the user.\n\nHow do you think about this approach? What are other **ways** I can do better in order to evaluate and increase the number of answers I am sending to the user? For those who have worked with RAG in production, how do you handle this type of problem?\n\nHow do you quantify the **business** impact of **such a** system?\n\nI think if I manage to **answer** 50% of the users' queries correctly and the other 50% of queries go to an agent, the system **reduces** the workload of the agent by 50%.\n\nBut my boss is saying that it is not a good system if it is just 50% accurate, and **sometimes** the agents will stop using it in production. Is that true?",
    "author": "esp_py",
    "timestamp": "2025-10-13T08:41:41",
    "url": "https://reddit.com/r/datascience/comments/1o5n86i/in_production_how_do_you_evaluate_the_quality_of/",
    "score": 17,
    "num_comments": 19,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o5pg14",
    "title": "Fivetran and dbt",
    "content": "They seem to be merging? Thoughts on this please. How does this shakeup the landscape if at all? ",
    "author": "Fondant_Decent",
    "timestamp": "2025-10-13T10:01:04",
    "url": "https://reddit.com/r/datascience/comments/1o5pg14/fivetran_and_dbt/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o59o2w",
    "title": "Weekly Entering &amp; Transitioning - Thread 13 Oct, 2025 - 20 Oct, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-10-12T21:01:10",
    "url": "https://reddit.com/r/datascience/comments/1o59o2w/weekly_entering_transitioning_thread_13_oct_2025/",
    "score": 10,
    "num_comments": 12,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o37n2r",
    "title": "Clustring very different values",
    "content": "I have 200 observations, 3 variables ( somewhat correlated).For v1, the median is 300 dollars. but I have a really long tail. when I do the histogram, 100 obs are near 0 and the others form a really long tail, even when I cap outliers. what is best way to cluster?",
    "author": "Due-Duty961",
    "timestamp": "2025-10-10T10:41:07",
    "url": "https://reddit.com/r/datascience/comments/1o37n2r/clustring_very_different_values/",
    "score": 29,
    "num_comments": 22,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o2y9ki",
    "title": "From data scientist to a new role ?",
    "content": "Hi everyone,\n\nI’m 25, currently working as a Data Scientist &amp; AI Engineer at a large Space company in Europe, with ~2.5 years of experience. My focus has been on LLM R&amp;D, RAG pipelines, satellite telemetry anomaly detection, surrogate modeling, and some FPGA-compatible ML for onboard systems. I also mentor interns, coordinate small R&amp;D projects, and occasionally present findings internally.\n\nThe context is tough (departures, headcount freezes) and I have an opportunity to move to a large aeronautics company or stay in my team, but grow in scope.\n\nI’m now evaluating two potential next roles (which I might intend as ~2-year commitments before moving on) and would love advice from anyone who has experience with either path:\n\n⸻\n\nOption 1 – AI Product Manager / Project Manager in HR\n\n\t•\tDeploy 8 AI agents across HR services, impacting ~130k employees.\n\n\t•\tLead roadmap, orchestrate AI integrations, and liaise with IT and HR VPs.\n\n\t•\tFocus on coordination, strategy, and high-level product ownership.\n\n\t•\tAccess to cutting-edge generative AI tools and cloud-based agentic workflows.\n\n\t•\tHigh exposure to senior stakeholders and leadership opportunities.\n\n\t•\tSome political stress: managing expectations of VPs, cross-team alignment, continuous meetings. It is said to be a quite political environment as you deal with HR and not just engineers.\n\n⸻\n\nOption 2 – Big data product owner + AI R&amp;D manager (Tech + Product Ownership) in Space\n\n\t•\tMerge internal Big Data platforms and integrate AI/analytics pipelines and PO role for a 600 user data lake platform (on premise due to security constraints), coordinating subcontractors.\n\n\t•\tManage R&amp;D programs with subcontractors, support bids, and deploy ML models.\n\n\t•\tsome Hands-on technical + coordination (MLops, RAG, keeping 1 data science R&amp;D project as a IC and take subs for the rest), some product ownership.\n\n\t•\tExposure mostly internal; less political stress, but operational and technical expectations remain high.\n\n\t•\tTechnical constraints due to working in a defense context: access to cutting-edge AI tools is limited, and infrastructure is slower/more constrained.\n\n\t•\tOpportunity to remain in the aerospace/space field I’m passionate about, but external market is niche.\n\n⸻\n\nMy Considerations\n\n\t•\tI’m not an elite coder; my strength is prototyping, vision, and leadership rather than optimizing code.\n\n\t•\tLife-work balance is important; I do ~12–20h of meetings per week currently and enjoy running, cycling, and other hobbies.\n\n\t•\tOption 1 offers exposure to latest AI technologies and high-level leadership, but comes with political challenges. Also, HR tech is not sexy.\n\n\t•\tOption 2 is more technical and personally interesting (space), but tools and infrastructure are slower, and the field is more niche. Plus it’s in a crisis in Europe meaning we could have 2-5 years of stagnation.\n\n⸻\n\nQuestions to the community:\n\n\t1.\tIf you had to choose between strategic PM exposure with generative AI vs hands-on hybrid tech + product in a niche field, which would you pick early in your career?\n\n\t2.\tWhich path do you think gives the strongest leverage for leadership or high-profile opportunities?\n\n\t3.\tAny advice on navigating political stress if I take the PM role?\n\n\t4.\tAre there hybrid ways to make the PM role technically “sexier” or future-proof in AI?\n\n      5.   I am also considering moving into high paid remote roles such as tech sales in the future. Which would work as the best intermediate role ?\n\nThanks in advance for your insights! Any real-world experience, pros/cons, or anecdotal advice is hugely appreciated.",
    "author": "LocPat",
    "timestamp": "2025-10-10T04:25:55",
    "url": "https://reddit.com/r/datascience/comments/1o2y9ki/from_data_scientist_to_a_new_role/",
    "score": 72,
    "num_comments": 37,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o2nf09",
    "title": "What should I ask my potential managers when choosing between two jobs?",
    "content": "I’m deciding between two mid-level data science offers at large tech companies. These are more applied scientist type of roles than analytics. Comp and level are similar, so I’m really trying to figure out which one will set me up for a stronger career in the long run.\n\nThis will be my first true DS role (coming from a technical background, PhD + previous R&amp;D role). I want to do interesting, high-impact work that keeps doors open possibly toward more research-type paths down the line but I also care a lot about working under a manager who can actually help me grow and foster a good career trajectory.\n\nFor those who’ve been in big-tech DS roles, what should I be asking or paying attention to when talking to the managers or teams to tell which role will offer better career growth, mentorship, and long-term options?\n\nWould love any advice or signals I should be looking for.",
    "author": "SavingsMortgage1972",
    "timestamp": "2025-10-09T18:09:14",
    "url": "https://reddit.com/r/datascience/comments/1o2nf09/what_should_i_ask_my_potential_managers_when/",
    "score": 25,
    "num_comments": 16,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o26g7h",
    "title": "Free data set that links company to type of activity?",
    "content": "Best ressource to classify for example: walmart. food ( top classification) supermarket ( sub classification). I work with european companies also.\nthanks.",
    "author": "Due-Duty961",
    "timestamp": "2025-10-09T06:49:53",
    "url": "https://reddit.com/r/datascience/comments/1o26g7h/free_data_set_that_links_company_to_type_of/",
    "score": 20,
    "num_comments": 12,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o1e1vm",
    "title": "Less is More: Recursive Reasoning with Tiny Networks (7M model beats R1, Gemini 2.5 Pro on ARC AGI)",
    "content": "",
    "author": "Technical-Love-8479",
    "timestamp": "2025-10-08T08:41:50",
    "url": "https://reddit.com/r/datascience/comments/1o1e1vm/less_is_more_recursive_reasoning_with_tiny/",
    "score": 25,
    "num_comments": 4,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o0eed8",
    "title": "Resources for Data Science &amp; Analysis: A curated list of roadmaps, tutorials, Python libraries, SQL, ML/AI, data visualization, statistics, cheatsheets",
    "content": "Hello everyone!\n\nStaying on top of the constantly growing skill requirements in Data Science is quite a challenge. To manage my own learning and growth, I've been curating a list of useful resources and tools that cover the full spectrum of the field — from data analysis and engineering to deep learning and AI.\n\nI'd love to get your professional opinion. Could you please take a look? Have I missed anything crucial? What else would you recommend adding or focusing on?\n\nTo give you an immediate sense of the list's scope and structure, I've attached screenshots of the table of contents below.\n\nThe full version with all the active links and additional resources is available on GitHub. You can find the link at the end of the post.\n\nhttps://preview.redd.it/egbe8jmruotf1.png?width=890&amp;format=png&amp;auto=webp&amp;s=0256f4ea30e7843bca8e77545ea46cc5ba25b72c\n\nhttps://preview.redd.it/3vq4pm8k1evf1.png?width=882&amp;format=png&amp;auto=webp&amp;s=1dcdbb6f9188535ae872bc40b77ede45833a6d4f\n\nI'd be happy if this list is useful to others.\n\nYou can view the full list here [View on GitHub](https://github.com/PavelGrigoryevDS/awesome-data-analysis?#awesome-data-analysis-)\n\nThanks for your time! Your advice is invaluable!",
    "author": "DeepAnalyze",
    "timestamp": "2025-10-07T06:12:23",
    "url": "https://reddit.com/r/datascience/comments/1o0eed8/resources_for_data_science_analysis_a_curated/",
    "score": 264,
    "num_comments": 67,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1o0khz8",
    "title": "Nvidia CEO Reveals the Job That’ll Win the AI Race",
    "content": "",
    "author": "nullstillstands",
    "timestamp": "2025-10-07T10:01:14",
    "url": "https://reddit.com/r/datascience/comments/1o0khz8/nvidia_ceo_reveals_the_job_thatll_win_the_ai_race/",
    "score": 62,
    "num_comments": 48,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nzfr4k",
    "title": "Exploratory analysis of 12 frontier LLM's across 100s of hours shows o3 highest Type-Token Ratio (Lexical Diversity), GPT-5 most formal language, and GPT-4o most positive sentiment",
    "content": "I recently ran exploratory analysis on the group chat of the [AI Village](https://theaidigest.org/village): 4+ frontier LLMs all have their own computer, access to the internet, and a group chat, and then get set goals like [raise money](https://theaidigest.org/village/blog/season-recap-agents-raise-2k) for charity, [sell T-shirts](https://theaidigest.org/village/blog/im-gemini-i-sold-t-shirts), or debate ethics. The goal is to build some awareness around what models are capable of now. I took the 200+ hours of group chat between the models and ran some exploratory analyses. Turns out:\n\n  \n\\- o3 has the highest Type-Token Ratio, even higher than GPT-5! o3 is also the model that wins at [diplomacy](https://every.to/diplomacy) against other agents, and won at AI debate in the AI Village.\n\n\\- GPT-5 uses the fewest contractions, writes the longest sentences, and uses the least slang/filler. I'm thinking about this as \"most formal\" but maybe it's something else?\n\n\\- GPT-4o had the highest positive sentiment scores in the Village and is also known as the most sycophantic model\n\n  \nI enjoyed analyzing the data and would love to do more. Any tips on what to look at? I might be able to share the data if people are interested. Feel free to send me a DM and we can see what's possible :)",
    "author": "ExplorAI",
    "timestamp": "2025-10-06T03:52:19",
    "url": "https://reddit.com/r/datascience/comments/1nzfr4k/exploratory_analysis_of_12_frontier_llms_across/",
    "score": 31,
    "num_comments": 6,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nz94dg",
    "title": "Weekly Entering &amp; Transitioning - Thread 06 Oct, 2025 - 13 Oct, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-10-05T21:01:39",
    "url": "https://reddit.com/r/datascience/comments/1nz94dg/weekly_entering_transitioning_thread_06_oct_2025/",
    "score": 9,
    "num_comments": 21,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nyp1uw",
    "title": "Why am I not getting responses?",
    "content": "As mentioned before, I can't use the weekly transition because it doesn't allow pictures. I appreciate your help last time when I asked. I've implemented your recommendations but I'm still not getting responses. I've added a completely new ML-based project, fixed mistakes, revamped the layout and I'm still not getting anything. I appreciate your attention.\n\nhttps://preview.redd.it/u7bbf3q5vatf1.png?width=666&amp;format=png&amp;auto=webp&amp;s=8d6983cb5e8713b1b8b736f95b916ee52fb0dc21\n\n  \n",
    "author": "KyronAWF",
    "timestamp": "2025-10-05T07:03:09",
    "url": "https://reddit.com/r/datascience/comments/1nyp1uw/why_am_i_not_getting_responses/",
    "score": 27,
    "num_comments": 82,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nxrrcw",
    "title": "What could be my next career progression?",
    "content": "Hello, I'm 26 years old been working as a junior data scientist in marketing for the past two years and I'm a bit bored/ have no idea how to progress further in my career.\n\nCurrently I do end to end modeling, from gathering data up to production (not in the most data sciency way since I'm very limited in terms of tools but my models are being effectively used by other departments). \n\nI have built 5 different models: propensity score models, customer segmentation, churn models and a time series forecasting model.\n\nAll my job has been revolving around developing, validating, monitoring and updating these models I have built with the current tools I have available.\n\nI realise I'm already privileged in terms of what I'm doing. It's my first job and already developing models end to end in a company that recognises their usefulness and I'm pretty much free to take any decision about them.\n\nHowever, I would love to advance further since the my job is starting to get a bit repetitive.\nIn terms of innovating further my workflow I realised it's actually pretty much impossible. The company IT is stagnant and any time I asked for anything, like introducing MlFlow in my sagemaker flow (YES, from development to \"production\" is done in sagemaker using notebooks. I understand and have faced many of the problems that come out of this) or Airflow or anything else, the request has never gotten anywhere.\nThe size of the company and the IT privileges setup makes it impossible for me to take the innovation in my own hands and do as I please. I've tried lots of technical workarounds and loopholes but not very successfully.\n\n\nI don't feel confident enough now take a more senior position, nor there is the possibility at my current job. My boss is not directly involved in modeling stuff and don't really have anyone I can go to with career progression questions.\n\nI feel like I kinda already reached the end of progression and I'm pretty much lost in terms of what I can do, other than ask for various tools to make the pipeline up to current standards (which will not have an impact in terms of how the output will be used by other departments and profits).\n\nI understand it's an open ended question, but what else could I do to advance?",
    "author": "Gaston154",
    "timestamp": "2025-10-04T04:43:06",
    "url": "https://reddit.com/r/datascience/comments/1nxrrcw/what_could_be_my_next_career_progression/",
    "score": 56,
    "num_comments": 50,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nxqln5",
    "title": "Do you know interesting datasets for kriging?",
    "content": "Hi guys, I need to do a project using many linear models and I’m looking for a dataset. Ideally something interesting with lots of numerical variables, especially one where kriging could be applied.\n\nIf you have any dataset suggestions or interesting research questions I could build the project around, I’d really appreciate it. Thanks a lot!\n\nPS: i did not like chatgpt suggestions, they were cliche (even if i explicitly asked “not cliche”)",
    "author": "FinalRide7181",
    "timestamp": "2025-10-04T03:38:16",
    "url": "https://reddit.com/r/datascience/comments/1nxqln5/do_you_know_interesting_datasets_for_kriging/",
    "score": 9,
    "num_comments": 10,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nwh00i",
    "title": "Are LLMs necessary to get a job?",
    "content": "For someone laid off in 2023 before the LLM/Agent craze went mainstream, do you think I need to learn LLM architecture? Are certs or github projects worth anything as far as getting through the filters and/or landing a job?  \n\nI have 10 YOE. I specialized in machine learning at the start, but the last 5 years of employment, I was at a FAANG company and didnt directly own any ML stuff. It seems \"traditional\" ML demand, especially without LLM knowledge, is almost zero. I've had some interviews for roles focused on experimentation, but no offers.    \nI can't tell whether my previous experience is irrelevant now. I deployed \"deep\" learning pipelines with basic MLOps. I did a lot of predictive analytics, segmentation, and data exploration with ML.  \n\nI understand the landscape and tech OK, but it seems like every job description now says you need direct experience with agentic frameworks, developing/optimizing/tuning LLMs, and using orchestration frameworks or advanced MLOps. I don't see how DS could have changed enough in two years that every candidate has on-the-job experience with this now.  \n\nIt seems like actually getting confident with the full stack/architecture would take a 6 month course or cert. Ive tried shorter trainings and free content... and it seems like everyone is just learning \"prompt engineering,\" basic RAG with agents, and building chatbots without investigating the underlying architecture at all.  \n\nAre the job descriptions misrepresenting the level of skill needed or am I just out of the loop?",
    "author": "br0monium",
    "timestamp": "2025-10-02T14:43:00",
    "url": "https://reddit.com/r/datascience/comments/1nwh00i/are_llms_necessary_to_get_a_job/",
    "score": 83,
    "num_comments": 65,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nvduc2",
    "title": "Fun Interview with Jason Strimpel about transferable skills from data science to algorithmic trading.",
    "content": "I had the opportunity to interview Jason Strimpel.  He's been in trading and technology for 25 years as a hedge fund trader, risk quant, machine learning engineering manager, and GenAI specialist at AWS. He is now the Managing Director of AI and Advanced Analytics at a major consulting company. \n\nI asked him all about the transferable skills, the mindset shifts, tools someone should pick up if they're just getting started, how algo trading is similar to ML, and differences in how you think about/work with the data. He had a lot of great tips if you're a data person thinking about getting into trading.",
    "author": "Clicketrie",
    "timestamp": "2025-10-01T10:00:56",
    "url": "https://reddit.com/r/datascience/comments/1nvduc2/fun_interview_with_jason_strimpel_about/",
    "score": 18,
    "num_comments": 6,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nv0lfh",
    "title": "For data scientists in insurance and banking, how many data scientists/ML engineers work in your company, how are their teams organised, and roughly what do they work on?",
    "content": "I'm trying to get a better sense of how this is developing in financial services. Anything from insurance/banking or adjacent fields would be most appreciated.",
    "author": "geebr",
    "timestamp": "2025-09-30T23:10:41",
    "url": "https://reddit.com/r/datascience/comments/1nv0lfh/for_data_scientists_in_insurance_and_banking_how/",
    "score": 58,
    "num_comments": 27,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nucvgd",
    "title": "Weekend Project - Poker Agents Video/Code",
    "content": "Fun side project. You can configure (almost) any LLM as a player. The main capabilities (tools) each agent can call are:\n\n1) Hand Analysis\nGet detailed info about current hand and possibilities (straight draws, flush potential, many other things)\n\n2) Monte Carlo\nGet an estimated win probability if the player continues in the hand (can only be called one time per hand)\n\n3) Opponent Statistics\nGet metrics about opponent behavior, specifically how aggressive or passively they’ve played\n\nIt’s not a completely novel - other people have made LLMs play poker. The configurability and the specific callable tools are, to my knowledge, unique. Using it requires an OpenRouter API key.\n\n\nVideo:\nhttps://youtu.be/1PDo6-tcWfE?si=WR-vgYtmlksKCAm4\n\nCode:\nhttps://github.com/OlivierNDO/llm_poker_agents\n\n",
    "author": "MLEngDelivers",
    "timestamp": "2025-09-30T06:13:57",
    "url": "https://reddit.com/r/datascience/comments/1nucvgd/weekend_project_poker_agents_videocode/",
    "score": 61,
    "num_comments": 15,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nurg0y",
    "title": "Distance Correlation &amp; Matrix Association. Good stuff?",
    "content": "",
    "author": "uSeeEsBee",
    "timestamp": "2025-09-30T15:35:09",
    "url": "https://reddit.com/r/datascience/comments/1nurg0y/distance_correlation_matrix_association_good_stuff/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ntmrix",
    "title": "This has to be bait right?",
    "content": "recruitment companies posting jobs like this are just setting bait to get resumes so they can push other jobs right?",
    "author": "ds_throw",
    "timestamp": "2025-09-29T09:31:52",
    "url": "https://reddit.com/r/datascience/comments/1ntmrix/this_has_to_be_bait_right/",
    "score": 188,
    "num_comments": 54,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nv6wv8",
    "title": "GLM 4.6 is the BEST CODING LLM. Period.",
    "content": "Honestly, GLM 4.6 might be my favorite LLM right now. I threw it a messy, real-world coding project, full front-end build, 20+ components, custom data transformations, and a bunch of steps that normally require me to constantly keep track of what’s happening. With older models like GLM 4.5 and even the latest Claude 4.5 Sonnet, I’d be juggling context limits, cleaning up messy outputs, and basically babysitting the process.\n\nGLM 4.6? It handled everything smoothly. Remembered the full context, generated clean code, even suggested little improvements I hadn’t thought of. Multi-step workflows that normally get confusing were just… done. And it did all that using fewer tokens than 4.5, so it’s faster and cheaper too.\n\nLoved the new release [Z.ai](http://Z.ai)",
    "author": "Technical-Love-8479",
    "timestamp": "2025-10-01T05:32:02",
    "url": "https://reddit.com/r/datascience/comments/1nv6wv8/glm_46_is_the_best_coding_llm_period/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.08,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ntlgy5",
    "title": "Career advice",
    "content": "Hi everyone,\n\nI think I need a little general guidance on how to move forward.  After working in retail for 11 years, I went back to school in 2020 to do a Bachelor’s in Mathematics and a masters in analytics.  I was hoping to become a data scientist upon graduating.  Obviously, market conditions have fluctuated substantially since I started.  \n\nI took a job as a materials planner in electronics manufacturing, with the expectation that my boss was looking for someone that was data minded and would primarily focus on building pipelines and tools to make things run more smoothly.   my planning duties would be small while I used my skills to automate and streamline workflows.  Up to this point, my job has been about 70 percent coding and “data engineering/analyzing”, 20 percent managing and organizing my projects, and 10 percent actual materials planning.\n\nI think my boss made a risky hire.  He’s not an IT person, and has not been able to move the needle on giving me the access I need to scale these processes.  I found an old reporting tool that is basically SQL that nobody uses: have been able to install VS code on my work laptop, so I have been able to  substantially streamline, dashboard, and improve a ton of stuff using Python, “SQL”, and PowerQuery.\n\nThey pulled my access to the reporting tool: no advance communication.  All of my projects are pretty much kaput.  I feel like I’ve been lowballed big time.  I’m glad to have a job right now, but also I’m in a bit of a predicament.  If my job search went on for another 6 months, most employers in actual “data” roles would understand the struggle: and I might even have an actual role in data analytics right now, if I got lucky.  But now I am in a position that is a huge departure from what was discussed.  No matter the situation, leaving after only 6 months would look terrible one me.  It seems like the best thing to do is ride it out, but I’m not sure or for how long I should. ",
    "author": "rmb91896",
    "timestamp": "2025-09-29T08:43:04",
    "url": "https://reddit.com/r/datascience/comments/1ntlgy5/career_advice/",
    "score": 25,
    "num_comments": 11,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nt8wl0",
    "title": "What a Drunk Man Can Teach Us About Time Series Forecasting",
    "content": "**Autocorrelation &amp; The Random Walk explained** with a drunk man 🍺\n\nLet me illustrate this statistical concept with an example we can all visualize.\n\nImagine a drunk man wandering a city. His steps are completely random and unpredictable.\n\n**Here's the intuition**:\n\n\\- His current position is completely tied to his previous position\n\n\\- We know where he is RIGHT NOW, but have no idea where he'll be in the next minute\n\n**The statistical insight**:\n\nIn a random walk, the *current position* is highly correlated with the *previous position*, but the *changes in position* (the steps) are completely random &amp; uncorrelated.\n\n\n\nThis is why random walks are so tricky to forecast!\n\n[Part 2: Time Series Forecasting: Build a Baseline &amp; Understand the Random Walk](https://youtu.be/_Ke54TJqY9s) \n\n  \nWould love to hear your **thoughts, feedback** about this topic",
    "author": "The_Simpsons_22",
    "timestamp": "2025-09-28T21:31:25",
    "url": "https://reddit.com/r/datascience/comments/1nt8wl0/what_a_drunk_man_can_teach_us_about_time_series/",
    "score": 61,
    "num_comments": 13,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nt6q59",
    "title": "What interesting projects are you working on that are not related to AI?",
    "content": "Share links if possible.",
    "author": "yaymayhun",
    "timestamp": "2025-09-28T19:34:24",
    "url": "https://reddit.com/r/datascience/comments/1nt6q59/what_interesting_projects_are_you_working_on_that/",
    "score": 45,
    "num_comments": 38,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nt8d65",
    "title": "Weekly Entering &amp; Transitioning - Thread 29 Sep, 2025 - 06 Oct, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-09-28T21:01:23",
    "url": "https://reddit.com/r/datascience/comments/1nt8d65/weekly_entering_transitioning_thread_29_sep_2025/",
    "score": 7,
    "num_comments": 16,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nso6sy",
    "title": "Relationship between ROC AUC and Gain curve?",
    "content": "Heya, I been studying the gains curve, and I’ve noticed there’s a relationship between the gains curve and ROC curve the smaller the base rate the closer is gains curve is to ROC curve. Anyway onto the point, is if fair to assume that for two models if the area under the ROC curve is bigger for model A and then the gains curve will always be better for model A as well?\n Thanks ",
    "author": "Emergency-Agreeable",
    "timestamp": "2025-09-28T06:20:54",
    "url": "https://reddit.com/r/datascience/comments/1nso6sy/relationship_between_roc_auc_and_gain_curve/",
    "score": 18,
    "num_comments": 3,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nsxpad",
    "title": "Oscillatory Coordination in Cognitive Architectures: Old Dog, New Math",
    "content": "Been working in AI since before it was cool (think 80s expert systems, not ChatGPT hype). Lately I've been developing this cognitive architecture called OGI that uses Top-K gating between specialized modules. Works well, proved the stability, got the complexity down to O(k²). But something's been bugging me about the whole approach.\nThe central routing feels... inelegant. Like we're forcing a fundamentally parallel, distributed process through a computational bottleneck. Your brain doesn't have a little scheduler deciding when your visual cortex can talk to your language areas.\nSo I've been diving back into some old neuroscience papers on neural oscillations. Turns out biological neural networks coordinate through phase-locking across different frequency bands - gamma for local binding, theta for memory consolidation, alpha for attention. No central controller needed.\nThe Math That's Getting Me Excited\nStarted modeling cognitive modules as weakly coupled oscillators. Each module i has intrinsic frequency ωᵢ and phase θᵢ(t), with dynamics:\nθ̇ᵢ = ωᵢ + Σⱼ Aᵢⱼ sin(θⱼ - θᵢ + αᵢⱼ)\nThis is just Kuramoto model with adaptive coupling strengths Aᵢⱼ and phase lags αᵢⱼ that encode computational dependencies. When |ωᵢ - ωⱼ| falls below critical coupling threshold, modules naturally phase-lock and start coordinating.\nThe order parameter R(t) = |Σⱼ e^(iθⱼ)|/N gives you a continuous measure of how synchronized the whole system is. Instead of discrete routing decisions, you get smooth phase relationships that preserve gradient flow.\nWhy This Might Actually Work\nThree big advantages I'm seeing:\n\nScalability: Communication cost scales with active phase-locked clusters, not total modules. For sparse coupling graphs, this could be near-linear.\nRobustness: Lyapunov analysis suggests exponential convergence to stable states. System naturally self-corrects.\nTemporal Multiplexing: Different frequency bands can carry orthogonal information streams without interference. Massive bandwidth increase.\n\nThe Hard Problems\nObviously the devil's in the details. How do you encode actual computational information in phase relationships? How do you learn the coupling matrix A(t)? Probably need some variant of Hebbian plasticity, but the specifics matter.\nThe inverse problem is fascinating though - given desired computational dependencies, what coupling topology produces the right synchronization patterns? Starting to look like optimal transport theory applied to dynamical systems.\nBigger Picture\nMaybe we've been thinking about AI architecture wrong. Instead of discrete computational graphs, what if cognition is fundamentally about temporal organization of information flow? The binding problem, consciousness, unified experience - could all emerge from phase coherence mathematics.\nI know this sounds hand-wavy, but the math is solid. Kuramoto theory is well-established, neural oscillations are real, and the computational advantages are compelling.\nAnyone worked on similar problems? Particularly interested in numerical integration schemes for large coupled oscillator networks and learning rules for adaptive coupling.\n\nEdit: For those asking about implementation - yes, this requires continuous dynamics instead of discrete updates. Computationally more expensive per step, but potentially fewer steps needed due to natural coordination. Still working out the trade-offs.\n\nEdit 2: Getting DMs about biological plausibility. Obviously artificial oscillators don't need to match neural firing rates exactly. The key insight is coordination through phase relationships, not literal biological mimicry.\n\nMike ",
    "author": "Efficient-Hovercraft",
    "timestamp": "2025-09-28T12:46:58",
    "url": "https://reddit.com/r/datascience/comments/1nsxpad/oscillatory_coordination_in_cognitive/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nrtluz",
    "title": "How important is it for a Data Analyst to learn some ML, Data Engineering, and DL?",
    "content": "Hey everyone!\n\nI'm a Data Analyst, but I'm really interested in the whole data science world. For my current job, I don't need to be an expert in machine learning, deep learning, or data engineering, but I've been trying to learn the basics anyway.\n\nI feel like even a basic understanding helps me out in a few ways:\n\n* Better Problem-Solving: It helps me choose the right tool for the job and come up with better solutions.\n* Deeper Analysis: I can push my analyses further and ask more interesting questions.\n* Smoother Communication: It makes talking to data scientists and engineers on my team way easier because I kinda \"get\" what they're doing.\n\nPlus, I've noticed that just learning one new library or concept makes picking up the next one a lot less intimidating.\n\nWhat do you all think? Should Data Analysts just stick to getting really good at core analytics (SQL, stats, viz), or is there a real advantage to becoming more of a \"T-shaped\" person with a broad base of knowledge?\n\nCurious to hear your experiences.",
    "author": "DeepAnalyze",
    "timestamp": "2025-09-27T05:09:54",
    "url": "https://reddit.com/r/datascience/comments/1nrtluz/how_important_is_it_for_a_data_analyst_to_learn/",
    "score": 100,
    "num_comments": 44,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nri84g",
    "title": "Anyone noticing an uptick in recruiter outreach?",
    "content": "I’ve had up to 10 recruiters contact me in the last few weeks. Before this I hadn’t heard anything but crickets for years. Anyone else noticing more outreach lately? Note that I’m a US citizen but the outreach starts before the H1B news so I don’t think it’s related to that. ",
    "author": "BB_147",
    "timestamp": "2025-09-26T18:05:34",
    "url": "https://reddit.com/r/datascience/comments/1nri84g/anyone_noticing_an_uptick_in_recruiter_outreach/",
    "score": 86,
    "num_comments": 55,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nrla6h",
    "title": "Week Bites: Weekly Dose of Data Science",
    "content": "Hi everyone I’m sharing **Week Bites**, a series of **light, digestible videos on data science**. Each week, I cover **key concepts, practical techniques, and industry insights** in short, easy-to-watch videos.\n\n1. [Where Data Scientists Find Free Datasets (Beyond Kaggle)](https://youtu.be/HR1sDaZOMDI) Authentic datasets that are clustered between research datasets, government datasets, massive-sized datasets that fit TF and PyTorch projects.\n2. [Time Series Forecasting in Python (Practical Guide)](https://youtu.be/Y7KCMaBDeDM) Starting from the fundamentals supported by source code available in the video description\n3. [Causal Inference Comprehensive Guide](https://youtu.be/40wIk7FSxdM) This area seems tricky a little, and I've started a series to halp intertwine causal inference into our AI models.\n\nWould love to hear your **thoughts, feedback, and topic suggestions**! Let me know which topics you find most useful",
    "author": "The_Simpsons_22",
    "timestamp": "2025-09-26T20:44:53",
    "url": "https://reddit.com/r/datascience/comments/1nrla6h/week_bites_weekly_dose_of_data_science/",
    "score": 29,
    "num_comments": 4,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nr8iu0",
    "title": "Should I enroll in UC Berkeley MIDS?",
    "content": "I recently was accepted to the UC Berkeley MIDS program, but I'm a bit conflicted as to whether I should accept the offer. A little bit about me: I just got my bachelors in data science and economics this past May from Berkeley as well, and I'm starting a job as a data scientist this month at a medium sized company. My goal is to become a data scientist, and a lot of people have advised me to do a data science master's since it's so competitive nowadays. My plan originally was to do the master's along with my job, but I'm a bit worried about the time commitment. Even though the people in my company say we have a chill 9-5 culture, the MIDS program will require 20-30 hours of work for the first semester because everyone is required to take 2 classes in the beginning. That means I'll have to work 60+ hours a week, at least during the first semester, although I'm not sure how accurate this time commitment is, since I already have coding experience from my bachelor's. Another thing I'm worried about is cost. Berkeley MIDS costs 67k for me (original was 80k+ but I got a scholarship). Even though I'm lucky enough to have my parents' financial support, I still hate for them to spend so much money. I also applied to UPenn's MSE-DS program, which is not as good as Berkeley's but it's significantly cheaper (38k), but I won't know the results until November, and I'm hoping to get back to Berkeley before then. Should I just not do a masters until several years down the line, or should I decline Berkeley and wait for UPenn's results? What's my best course of action? Thank you 🙏",
    "author": "ExcitingCommission5",
    "timestamp": "2025-09-26T11:08:43",
    "url": "https://reddit.com/r/datascience/comments/1nr8iu0/should_i_enroll_in_uc_berkeley_mids/",
    "score": 14,
    "num_comments": 34,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ns18vu",
    "title": "Seeking Feedback on My Data Science CV",
    "content": "",
    "author": "telperion101",
    "timestamp": "2025-09-27T10:37:00",
    "url": "https://reddit.com/r/datascience/comments/1ns18vu/seeking_feedback_on_my_data_science_cv/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.32,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nqcu82",
    "title": "Your Boss Is Faking Their Way Through AI Adoption",
    "content": "",
    "author": "nullstillstands",
    "timestamp": "2025-09-25T10:22:58",
    "url": "https://reddit.com/r/datascience/comments/1nqcu82/your_boss_is_faking_their_way_through_ai_adoption/",
    "score": 207,
    "num_comments": 53,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nq8hi5",
    "title": "I'm still not sure how to answer vague DS questions...",
    "content": "Questions like:\n\n* *“How do you approach building a model?”*\n* *“What metrics would you look at to evaluate success?”*\n* *“How would you handle missing data?”*\n* *“How do you decide between different algorithms?”*\n\netc etc\n\n  \nWhere its highly dependent on context and it feels like no matter how much you qualify your answers with justifications, you never really know if it's the right answer.\n\nFor some of these there are decent, generic answers but it really does seem like it's up to the interviewer to determine whether they like the answer you give \n\n",
    "author": "ds_throw",
    "timestamp": "2025-09-25T07:36:44",
    "url": "https://reddit.com/r/datascience/comments/1nq8hi5/im_still_not_sure_how_to_answer_vague_ds_questions/",
    "score": 84,
    "num_comments": 40,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1npq7ty",
    "title": "PNC Bank Moving To 5 Days In Office",
    "content": "FYI - If you are considering an analytics job at PNC Bank, they are moving to 5 days in office.  It's now being required for senior managers, and will trickle down to individual contributors in the new year.",
    "author": "random_user_fp",
    "timestamp": "2025-09-24T15:35:03",
    "url": "https://reddit.com/r/datascience/comments/1npq7ty/pnc_bank_moving_to_5_days_in_office/",
    "score": 82,
    "num_comments": 32,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nqwcip",
    "title": "What is the state-of-the-art prediction performance for the stock market?",
    "content": "I am currently working on a university project and want to predict the next day's closing price of a stock. I am using a foundation model for time series based on the transformer architecture (decoder only).\n\nSince I have no touchpoints with the practical procedures of the industry I was asking myself what the best prediction performance, especially directional accuracy (\"stock will go up/down tomorrow\") is. I am currently able to achieve 59% accuracy only.\n\nAny practical insights? Thank you!",
    "author": "Poxput",
    "timestamp": "2025-09-26T01:58:53",
    "url": "https://reddit.com/r/datascience/comments/1nqwcip/what_is_the_stateoftheart_prediction_performance/",
    "score": 0,
    "num_comments": 52,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nphgwl",
    "title": "Expectations for probability questions in interviews",
    "content": "Hey everyone, I'm a PhD candidate in CS, currently starting to interview for industry jobs. I had an interview earlier this week for a research scientist job that I was hoping to get an outside perspective on - I'm pretty new to technical interviewing and there don't seem to be many online resources about what interviewers expectations are going to be for more probability-style questions. I was not selected for a next round of interviews based on my performance, and that's at odds with my self-assessment and with the affect and demeanor of the interviewer. \n\n  \n**The Interview Questions:** A question asking about probabilistic decay of N particles (over discrete time steps, known probability), and was asked to derive the probability that all particles would decay by a certain time. Then, I was asked to write a simulation of this scenario, and get point estimates, variance &amp;c. Lastly, I was asked about a variation where I would estimate the probability, given observed counts. \n\n  \n**My Performance:** I correctly characterized the problem as a Binomial(N,p) problem, where p is the probability that a single particle survives till time T. I did not get a closed form solution (I asked about how I did at the end and the interviewer mentioned that it would have been nice to get one). The code I wrote was correct, and I think fairly efficient? I got a little bit hung up on trying to estimate variance, but ended up with a bootstrap approach. We ran out of time before I could entirely solve the last variation, but generally described an approach. I felt that my interviewer and I had decent rapport, and it seemed like I did decently. \n\n  \n**Question:** Overall, I'd like to know what I did wrong, though of course that's probably not possible without someone sitting in. I did talk throughout, and I have struggled with clear and concise verbal communication in the past. Was the expectation that I would solve all parts of the questions completely? What aspects of these interviews do interviewers tend to look for?",
    "author": "gforce121",
    "timestamp": "2025-09-24T09:53:23",
    "url": "https://reddit.com/r/datascience/comments/1nphgwl/expectations_for_probability_questions_in/",
    "score": 47,
    "num_comments": 16,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nq1bj4",
    "title": "Introducing ryxpress: Reproducible Polyglot Analytical Pipelines with Nix (Python)",
    "content": "Hi everyone,\n\nThese past weeks I've been working on an R and Python package (called rixpress and ryxpress respectively) which aim to make it easy to build multilanguage projects by using Nix as the underlying build tool.\n\nryxpress is a Python port of the R package `{rixpress}`, both in early development and they let you define data pipelines in R (with helpers for Python steps), build them reproducibly using Nix, and then inspect, read, or load artifacts from Python.\n\nIf you're familiar with the `{targets}` R package, this is very similar.\n\nIt’s designed to provide a smoother experience for those working in polyglot environments (Python, R, Julia and even Quarto/Markdown for reports) where reproducibility and cross-language workflows matter.\n\nPipelines are defined in R, but the artifacts can be explored and loaded in Python, opening up easy interoperability for teams or projects using both languages.\n\nIt uses Nix as the underyling build tool, so you get the power of Nix for dependency management, but can work in Python for artifact inspection and downstream tasks.\n\nHere is a basic definition of a pipeline:\n\n```\nlibrary(rixpress)\n\nlist(\n  rxp_py_file(\n    name = mtcars_pl,\n    path = 'https://raw.githubusercontent.com/b-rodrigues/rixpress_demos/refs/heads/master/basic_r/data/mtcars.csv',\n    read_function = \"lambda x: polars.read_csv(x, separator='|')\"\n  ),\n\n  rxp_py(\n    name = mtcars_pl_am,\n    expr = \"mtcars_pl.filter(polars.col('am') == 1)\",\n    user_functions = \"functions.py\",\n    encoder = \"serialize_to_json\",\n  ),\n\n  rxp_r(\n    name = mtcars_head,\n    expr = my_head(mtcars_pl_am),\n    user_functions = \"functions.R\",\n    decoder = \"jsonlite::fromJSON\"\n  ),\n\n  rxp_r(\n    name = mtcars_mpg,\n    expr = dplyr::select(mtcars_head, mpg)\n  )\n) |&gt;\n  rxp_populate(project_path = \".\")\n```\n\nIt's R code, but as explained, you can build it from Python and explore build artifacts from Python as well. You'll also need to define the \"execution environment\" in which this pipeline is supposed to run, using Nix as well.\n\nryxpress is on PyPI, but you’ll need Nix (and R + {rixpress}) installed. See the [GitHub repo](https://github.com/b-rodrigues/ryxpress) for quickstart instructions and environment setup.\n\nWould love feedback, questions, or ideas for improvements! If you’re interested in reproducible, multi-language pipelines, give it a try.",
    "author": "brodrigues_co",
    "timestamp": "2025-09-25T01:31:19",
    "url": "https://reddit.com/r/datascience/comments/1nq1bj4/introducing_ryxpress_reproducible_polyglot/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nnvss1",
    "title": "Why do new analysts often ignore R?",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-09-22T12:18:36",
    "url": "https://reddit.com/r/datascience/comments/1nnvss1/why_do_new_analysts_often_ignore_r/",
    "score": 2465,
    "num_comments": 289,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1npfecr",
    "title": "Ad-hoc questions are the real killer. Curious if others feel this pain",
    "content": "When I was a data scientist at Meta, almost 50% of my week went to ad-hoc requests like:\n\n* “Can we break out Marketplace feed engagement for buyers vs sellers?”\n* “Do translation errors spike more in Spanish than French?”\n* “What % of teen users in Reality Labs got safety warnings last release?”\n\nEach one was reasonable, but stacked together it turned my entire DS team into human SQL machines.\n\nI’ve been hacking on an MVP that tries to reduce this by letting the DS define a domain once (metrics, definitions, gotchas), and then AI handles repetitive questions transparently (always shows SQL + assumptions).\n\nNot trying to pitch, just genuinely curious if others have felt the same pain, and how you’ve dealt with it. If you want to see what I’m working on, here’s the landing page: [www.takeoutforteams.com](http://www.takeoutforteams.com).\n\nWould love any feedback from folks who’ve lived this, especially how your teams currently handle the flood of ad-hoc questions. Because right now there's very little beyond dashboards that let DS scale themselves.",
    "author": "KyleDrogo",
    "timestamp": "2025-09-24T08:35:43",
    "url": "https://reddit.com/r/datascience/comments/1npfecr/adhoc_questions_are_the_real_killer_curious_if/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1no5b1j",
    "title": "Is a second masters worth it for MLE roles?",
    "content": "I already have an MS in Statistics and two and a half YoE, but mostly in operations and business-oriented roles. I would like to work more in DS or be able to pivot into engineering. My undergrad was not directly in computer science but I did have significant exposure to AI/ML before LLMs and generative models were mainstream. I don’t have any work experience directly in ML or DS, but my analyst roles over the last few years have been SQL-oriented with some scripting here and there.\n\nIf I wanted to pivot into MLE or DE would it be worth going back to school for an MSCS? I also just generally miss learning and am open to a career pivot, and also have always wanted to try working on research projects (never did it for my MS). I’m leaning towards no and instead just working on relevant certifications, but I want to pivot out of Business Operations or business intelligence roles into more technical teams such as ML teams or product. Internal migration within my own company does not seem possible at the moment.",
    "author": "ch4nt",
    "timestamp": "2025-09-22T19:07:20",
    "url": "https://reddit.com/r/datascience/comments/1no5b1j/is_a_second_masters_worth_it_for_mle_roles/",
    "score": 36,
    "num_comments": 38,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nnsvq3",
    "title": "New RAG Builder: Create a SOTA RAG system in under 5 minutes. Which models/methods should we add next? [Kiln]",
    "content": "I just updated [my GitHub project Kiln](https://github.com/Kiln-AI/Kiln) **so you can build a RAG system in under 5 minutes**; just drag and drop your documents in. We want it to be the most usable RAG builder, while also offering powerful options for finding the ideal RAG parameters.\n\nHighlights:\n\n* **Easy to get started**: just drop in documents, select a template configuration, and you're up and running in a few minutes.\n* **Highly customizable**: you can customize the document extractor, chunking strategy, embedding model/dimension, and search index (vector/full-text/hybrid). Start simple with one-click templates, but go as deep as you want on tuning/customization.\n* **Document library**: manage documents, tag document sets, preview extractions, sync across your team, and more.\n* **Deep integrations**: evaluate RAG-task performance with our evals, expose RAG as a tool to any tool-compatible model\n* **Local**: the Kiln app runs locally and we can't access your data. The V1 of RAG requires API keys for extraction/embeddings, but we're working on fully-local RAG as we speak; see below for questions about where we should focus.\n\nWe have docs walking through the process: [https://docs.kiln.tech/docs/documents-and-search-rag](https://docs.kiln.tech/docs/documents-and-search-rag)\n\n**Question for you:** V1 has a decent number of options for tuning, but folks are probably going to want more. We’d love suggestions for where to expand first. Options are:\n\n* **Document extraction**: V1 focuses on model-based extractors (Gemini/GPT) as they outperformed library-based extractors (docling, markitdown) in our tests. Which additional models/libraries/configs/APIs would you want? Specific open models? Marker? Docling?\n* **Embedding Models**: We're looking at EmbeddingGemma &amp; Qwen Embedding as open/local options. Any other embedding models people like for RAG?\n* **Chunking**: V1 uses the sentence splitter from llama\\_index. Do folks have preferred semantic chunkers or other chunking strategies?\n* **Vector database**: V1 uses LanceDB for vector, full-text (BM25), and hybrid search. Should we support more? Would folks want Qdrant? Chroma? Weaviate? pg-vector? HNSW tuning parameters?\n* Anything else?\n\nSome links to the repo and guides:\n\n* [Kiln AI on Github - 4k stars](https://github.com/Kiln-AI/Kiln)\n* [Documents &amp; Search (RAG) Docs/Guide](https://docs.kiln.tech/docs/documents-and-search-rag)\n* [Kiln Discord](https://getkiln.ai/discord)\n* [Homepage](https://kiln.tech)\n\nI'm happy to answer questions if anyone wants details or has ideas!!",
    "author": "davernow",
    "timestamp": "2025-09-22T10:30:27",
    "url": "https://reddit.com/r/datascience/comments/1nnsvq3/new_rag_builder_create_a_sota_rag_system_in_under/",
    "score": 10,
    "num_comments": 0,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nnfcwc",
    "title": "Is it due to the tech recession?",
    "content": "We know that in many companies Data Scientists are Product Analytics / Data Analysts. I thought it was because MLEs had absorbed the duties of DSs, but i have noticed that this may not be exactly the case.\n\nThere are basically three distinct roles:\n\t\n1.\tData Analyst / Product Analytics: dashboards, data analysis, A/B testing.\n\t\n2.\tMLE: build machine learning systems for user-facing products (e.g., Stripe’s fraud detection or YouTube’s recommendation algorithm).\n\t\n3.\tDS: use ML and advanced techniques to solve business problems and make forecasts (e.g., sales, growth, churn).\n\nThis last job is not done by MLEs, it has simply been eliminated by some companies in the last few years (but a lot of tech companies still have it).\n\nFor example Stripe used to hire DSs specifically for this function and LinkedIn profiles confirm that those people are still there doing it, but now the new hires consist only of Data Analysts.\n\nIt’s hard to believe that in a world increasingly driven by data, a role focused on predictive decision making would be seen as completely useless.\n\nSo my question is: is this mostly the result of the tech recession? Companies may now prioritize “essential” roles that can be filled at lower costs (Data Analysts) while removing, in this difficult economy, the “luxury” roles (Data Scientists).\n",
    "author": "FinalRide7181",
    "timestamp": "2025-09-21T23:46:14",
    "url": "https://reddit.com/r/datascience/comments/1nnfcwc/is_it_due_to_the_tech_recession/",
    "score": 58,
    "num_comments": 47,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nncgka",
    "title": "Need input from mid-career dara Scientists (2-5 year range)",
    "content": "I am a DS with 2YOE (plus about 6 coops). I'm looking for feedback from folks specifically transitioned out of early career and into mid-career phase. (Unfortunately I don't have any in my immediate network)\n\nContext: I'm coming upto 2 years in my role and have been seriously evaluating the next stage of my career.\n\nQuestions:\n1. Does having a decent resume land you your next role, or even for a mid-level role do you need to network extensively i.e. what's the most optimal method for this stage of career progression.\n\n2. Most of the work I've done so far has been POC-based i.e. we find business problems and work with teams to create MVPs. Its been an interesting experience as I get to experiment with different methods and almost derive the solution from scratch, without having to worry too much about MLE/MLOps. Does this kind of work exist at this next Intermediate level? And will this kind of role even exist into the future?\n\n3. How do you decide between being able to climb up the ladder in your current company? Or switch to a different industry, maybe one that aligns more with your passion/interests, but also risk losing all of that \"capital\" you've invested into in the current company?\n\nApologies if this is a bit all over the place,  but it was a little tough getting my thoughts across.\n\nAlso would love if anyone is down to discuss more in detail on dm, if that's preferred.\n\nThanks a lot!",
    "author": "SmogonWanabee",
    "timestamp": "2025-09-21T20:56:33",
    "url": "https://reddit.com/r/datascience/comments/1nncgka/need_input_from_midcareer_dara_scientists_25_year/",
    "score": 31,
    "num_comments": 33,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nnck8a",
    "title": "Weekly Entering &amp; Transitioning - Thread 22 Sep, 2025 - 29 Sep, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-09-21T21:01:38",
    "url": "https://reddit.com/r/datascience/comments/1nnck8a/weekly_entering_transitioning_thread_22_sep_2025/",
    "score": 2,
    "num_comments": 25,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nnqb2c",
    "title": "Well well...",
    "content": "Anyone Cruyff dribbling...?",
    "author": "OverratedDataScience",
    "timestamp": "2025-09-22T08:54:55",
    "url": "https://reddit.com/r/datascience/comments/1nnqb2c/well_well/",
    "score": 0,
    "num_comments": 23,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nl92og",
    "title": "Updated based on subreddit feedback. Applying for mid-senior based roles. Thank you",
    "content": "",
    "author": "StormyT",
    "timestamp": "2025-09-19T09:59:03",
    "url": "https://reddit.com/r/datascience/comments/1nl92og/updated_based_on_subreddit_feedback_applying_for/",
    "score": 44,
    "num_comments": 32,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nl4jtx",
    "title": "What’s the right thing to say to salary expectations question?",
    "content": "I have come across usually two types of scenarios here and I am not sure what’s the best way to deal. \n\n- I ask for a range and they give you range. Should you just say you’re okay with the range? But what if I make 80K now and their range is 90-120. In this case I don’t wanna move at 90K. What should you say?\n\n- They just don’t give you any range and keep pressing to give them a  number. In this case I feel like there’s chances of getting low balled later.\n\nI have a couple of recruiter rounds coming up. Could really use your help. Thanks!",
    "author": "Lamp_Shade_Head",
    "timestamp": "2025-09-19T07:07:10",
    "url": "https://reddit.com/r/datascience/comments/1nl4jtx/whats_the_right_thing_to_say_to_salary/",
    "score": 62,
    "num_comments": 62,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nl4oi1",
    "title": "How to actually perform observational studies in industry?",
    "content": "Hey everyone, \n\nI am working on observational studies and need some guidance on confounder and model selection, are you following a best practise when it comes to observational studies? \n\n  \nMy situation is, we have models to predict who will churn based on a whole set of features and then we reach out to them, and the ones that answer become our treatment and the ones that don't become our control. Then based on a bunch of features of their behaviour in the previous year, I use a model to find the features that most likely predict who will answer and use those as the confounders. As they were most related to the treated group. \n\n  \nThen would use something like TMLE,psw etc to find the ATE.\n\n  \nHow do you decide what to do if there isnt any domain knowledge, is there a textbook or methods you follow to conduct your tests?",
    "author": "LebrawnJames416",
    "timestamp": "2025-09-19T07:12:19",
    "url": "https://reddit.com/r/datascience/comments/1nl4oi1/how_to_actually_perform_observational_studies_in/",
    "score": 14,
    "num_comments": 10,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nlldi9",
    "title": "Transformer with multi-dimensional timesteps",
    "content": "Does anyone have boilerplate Python code for using Keras or similar to run a transformer model on data where each time step of each sequence is, say, 3 dimensions?\n\nE.g.:\n\nData 1: [(3,5,0),(4,6,1)], label = 1\nData 2: [(6,3,0)], label = 0\n\nI’m having trouble getting my ChatGPT-coded model to perform, which is surprising since I was able to get decent results when I just looked at one of the 3 featured with the same ordering, data, and number of steps.\n\nAny boilerplate Python code would be of great help. I’m unable to find something basic online, but I’m sure it’s out there so appreciate being pointed in the right direction.",
    "author": "transferrr334",
    "timestamp": "2025-09-19T18:31:25",
    "url": "https://reddit.com/r/datascience/comments/1nlldi9/transformer_with_multidimensional_timesteps/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1njp4vy",
    "title": "K-shot training with LLMs for document annotation/extraction",
    "content": "https://preview.redd.it/u9fxkmmqgspf1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=b616b9a5d725d680f9bc76dc09f2b9d62aed079a\n\nI’ve been experimenting with a way to teach LLMs to extract structured data from documents by \\*\\*annotating, not prompt engineering\\*\\*. Instead of fiddling with prompts that sometimes regress, you just build up examples. Each example improves accuracy in a concrete way, and you often need far fewer than traditional ML approaches.\n\n\n\nHow it works (prototype is live):\n\n\\- Upload a document (DOCX, PDF, image, etc.)\n\n\\- Select and tag parts of it (supports nesting, arrays, custom tag structures)\n\n\\- Upload another document → click \"predict\" → see editable annotations\n\n\\- Amend them and save as another example\n\n\\- Call the API with a third document → get JSON back\n\n\n\nPotential use cases:\n\n\\- Identify important clauses in contracts\n\n\\- Extract total value from invoices\n\n\\- Subjective tags like “healthy ingredients” on a label\n\n\\- Objective tags like “postcode” or “phone number”\n\nIt seems to generalize well: you can even tag things like “good rhymes” in a poem. Basically anything an LLM can comprehend and extrapolate.\n\n\n\nI’d love feedback on:\n\n\\- Does this kind of few-shot / K-shot approach seem useful in practice?\n\n\\- Are there other document-processing scenarios where this would be particularly impactful?\n\n\\- Pitfalls you’d anticipate?\n\n  \nI've called this \"DeepTagger\", first link on google if you search that, if you want to try it! It's fully working, but this is just a first version.",
    "author": "avloss",
    "timestamp": "2025-09-17T14:00:00",
    "url": "https://reddit.com/r/datascience/comments/1njp4vy/kshot_training_with_llms_for_document/",
    "score": 25,
    "num_comments": 12,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1njcvfv",
    "title": "Example Take Home Assignment For Interview - Data Science in Finance",
    "content": "Edit: formatting data dictionary\n\nHello,\n\nThought this might be an interesting post for some, especially those of us who work at Financial Institutions. Here is a take home assignment used in the interview process to evaluate candidates for a data scientist role in the financial industry. This company does personal lending in the US.\n\nHopefully this is enough on topic (and not against the rules) as this is for a data scientist role, but it also is very financially focused. I'm not looking for help in anyway, just hope this might helpful to someone looking for a role in this area. I know a lot of people are against take home assignments, I get it, but the reality is many employers still use them.\n\nI'll try to format things as best as possible, but it's tough when you can't post attachments.\n\n**Instructions**\n\nEmployer uses machine learning models to evaluate borrower risk and determine loan eligibility. In July 2024, we launched **Model B** to replace **Model A**, aiming to improve loan approvals and portfolio returns. Our executive team has expressed concern that Model B might be underperforming in some cases.\n\nYour task is to assess the performance of Models A and B across these loan product types and answer the central question: **Should we roll back to Model A or keep and improve Model B?** Additionally, analyze the dataset to uncover any other insights that could guide our decision-making and optimize our lending strategy.\n\nPlease put together a presentation summarizing your findings, insights, and recommendations. Assume your audience has a low level of familiarity with the specifics of the problem but will appreciate clear, data-driven reasoning and business implications. You will present your findings in a 45 minute meeting with stakeholders but ensure to leave ample time for their questions.\n\n**Data Dictionary (for the two attachments below):**\n\n* *Origination Month:* Month in which the loan was funded.\n* *Payment Month:* Payments are made monthly. The first payment is made a month after origination. Payment number refers to future payments from the loans that originated in the specified month. For an origination taking place in Jan 2023, their 1st payment month will take place in Feb 2023, their 2nd payment month will take place in March 2023, etc…\n* *Model Version:* Model\\_A is the original model and Model\\_B is the new, updated model.\n* *Scheduled Loan Repayment:* The loan repayments as determined by the amortization schedule at origination.\n* *Forecasted Loan Repayment:* The loan repayments that are forecasted by each model at origination.\n* *Actual Loan Repayment:* The actual loan repayments made during each payment month by borrowers.\n* *Application Submits:* Loan applications that are submitted.\n* *Origination Amount:* The initial principal amount when the loan is funded.\n* *Note: Employer earns revenue as a % fee of the loan origination amount and the investor (Employer’s lending partners which provide the capital for Employer to lend) earns returns based on interest net loss*\n\n**Attachment 1**\n\n|| || |Month|Application Submits|Origination Amount| |1/1/23|134,194|$7,245,878| |2/1/23|118,084|$6,291,085| |3/1/23|151,789|$6,978,795| |4/1/23|147,247|$7,629,398| |5/1/23|144,106|$7,386,274| |6/1/23|166,063|$7,607,082| |7/1/23|175,438|$8,302,775| |8/1/23|173,874|$9,136,815| |9/1/23|199,833|$9,556,795| |10/1/23|173,089|$9,305,852| |11/1/23|177,250|$9,383,253| |12/1/23|229,996|$11,186,584| |1/1/24|198,578|$10,922,898| |2/1/24|216,549|$12,409,692| |3/1/24|216,083|$11,248,453| |4/1/24|215,525|$12,350,982| |5/1/24|193,528|$10,995,911| |6/1/24|201,425|$12,011,017| |7/1/24|220,760|$10,487,390| |8/1/24|199,445|$10,180,941| |9/1/24|187,549|$10,518,739| |10/1/24|187,075|$10,095,767| |11/1/24|198,951|$10,281,715| |12/1/24|210,259|$10,266,566 |\n\n**Attachement 2**\n\n|| || |Origination Month|Model Version|Payment Number|Scheduled Loan Repayment|Forecasted Loan Repayment|Actual Loan Repayment| |1/1/23|Model\\_A|1|$106,000.00|$105,788.00|$105,788.00| |1/1/23|Model\\_A|2|$106,000.00|$105,576.42|$105,945.94| |1/1/23|Model\\_A|3|$106,000.00|$105,365.27|$105,312.59| |1/1/23|Model\\_A|4|$106,000.00|$105,154.54|$105,007.32| |1/1/23|Model\\_A|5|$106,000.00|$104,944.23|$104,660.88| |1/1/23|Model\\_A|6|$106,000.00|$104,734.34|$104,430.61| |1/1/23|Model\\_A|7|$106,000.00|$104,524.87|$105,037.04| |1/1/23|Model\\_A|8|$106,000.00|$104,315.82|$104,211.50| |1/1/23|Model\\_A|9|$106,000.00|$104,107.19|$104,471.57| |1/1/23|Model\\_A|10|$106,000.00|$103,898.98|$103,898.98| |1/1/23|Model\\_A|11|$106,000.00|$103,691.18|$103,421.58| |1/1/23|Model\\_A|12|$106,000.00|$103,483.80|$103,338.92| |1/1/23|Model\\_A|13|$106,000.00|$103,276.83|$102,967.00| |1/1/23|Model\\_A|14|$106,000.00|$103,070.28|$103,163.04| |1/1/23|Model\\_A|15|$106,000.00|$102,864.14|$102,349.82| |1/1/23|Model\\_A|16|$106,000.00|$102,658.41|$102,781.60| |1/1/23|Model\\_A|17|$106,000.00|$102,453.09|$102,729.71| |1/1/23|Model\\_A|18|$106,000.00|$102,248.18|$102,329.98| |1/1/23|Model\\_A|19|$106,000.00|$102,043.68|$99,880.61| |1/1/23|Model\\_A|20|$106,000.00|$101,839.59|$99,442.54| |1/1/23|Model\\_A|21|$106,000.00|$101,635.91|$99,451.76| |1/1/23|Model\\_A|22|$106,000.00|$101,432.64|$98,451.79| |1/1/23|Model\\_A|23|$106,000.00|$101,229.77|$98,314.10| |2/1/23|Model\\_A|1|$93,730.00|$93,542.54|$93,730.00| |2/1/23|Model\\_A|2|$93,730.00|$93,355.45|$93,411.46| |2/1/23|Model\\_A|3|$93,730.00|$93,168.74|$93,429.61| |2/1/23|Model\\_A|4|$93,730.00|$92,982.40|$93,382.22| |2/1/23|Model\\_A|5|$93,730.00|$92,796.44|$92,351.02| |2/1/23|Model\\_A|6|$93,730.00|$92,610.85|$92,184.84| |2/1/23|Model\\_A|7|$93,730.00|$92,425.63|$92,887.76| |2/1/23|Model\\_A|8|$93,730.00|$92,240.78|$91,844.14| |2/1/23|Model\\_A|9|$93,730.00|$92,056.30|$92,001.07| |2/1/23|Model\\_A|10|$93,730.00|$91,872.19|$92,101.87| |2/1/23|Model\\_A|11|$93,730.00|$91,688.45|$91,624.27| |2/1/23|Model\\_A|12|$93,730.00|$91,505.07|$91,404.41| |2/1/23|Model\\_A|13|$93,730.00|$91,322.06|$90,920.24| |2/1/23|Model\\_A|14|$93,730.00|$91,139.42|$91,522.21| |2/1/23|Model\\_A|15|$93,730.00|$90,957.14|$91,139.05| |2/1/23|Model\\_A|16|$93,730.00|$90,775.23|$90,602.76| |2/1/23|Model\\_A|17|$93,730.00|$90,593.68|$90,765.81| |2/1/23|Model\\_A|18|$93,730.00|$90,412.49|$88,187.43| |2/1/23|Model\\_A|19|$93,730.00|$90,231.67|$87,694.36| |2/1/23|Model\\_A|20|$93,730.00|$90,051.21|$87,641.89| |2/1/23|Model\\_A|21|$93,730.00|$89,871.11|$87,343.93| |2/1/23|Model\\_A|22|$93,730.00|$89,691.37|$87,580.26| |3/1/23|Model\\_A|1|$98,580.00|$98,382.84|$97,989.31| |3/1/23|Model\\_A|2|$98,580.00|$98,186.07|$97,734.41| |3/1/23|Model\\_A|3|$98,580.00|$97,989.70|$98,215.08| |3/1/23|Model\\_A|4|$98,580.00|$97,793.72|$97,617.69| |3/1/23|Model\\_A|5|$98,580.00|$97,598.13|$97,754.29| |3/1/23|Model\\_A|6|$98,580.00|$97,402.93|$97,841.24| |3/1/23|Model\\_A|7|$98,580.00|$97,208.12|$96,858.17| |3/1/23|Model\\_A|8|$98,580.00|$97,013.70|$97,149.52| |3/1/23|Model\\_A|9|$98,580.00|$96,819.67|$96,626.03| |3/1/23|Model\\_A|10|$98,580.00|$96,626.03|$96,394.13| |3/1/23|Model\\_A|11|$98,580.00|$96,432.78|$96,760.65| |3/1/23|Model\\_A|12|$98,580.00|$96,239.91|$96,365.02| |3/1/23|Model\\_A|13|$98,580.00|$96,047.43|$96,114.66| |3/1/23|Model\\_A|14|$98,580.00|$95,855.34|$96,056.64| |3/1/23|Model\\_A|15|$98,580.00|$95,663.63|$95,730.59| |3/1/23|Model\\_A|16|$98,580.00|$95,472.30|$95,625.06| |3/1/23|Model\\_A|17|$98,580.00|$95,281.36|$92,490.57| |3/1/23|Model\\_A|18|$98,580.00|$95,090.80|$93,112.20| |3/1/23|Model\\_A|19|$98,580.00|$94,900.62|$92,565.12| |3/1/23|Model\\_A|20|$98,580.00|$94,710.82|$92,315.35| |3/1/23|Model\\_A|21|$98,580.00|$94,521.40|$92,600.72| |4/1/23|Model\\_A|1|$103,550.00|$103,342.90|$103,260.23| |4/1/23|Model\\_A|2|$103,550.00|$103,136.21|$103,363.11| |4/1/23|Model\\_A|3|$103,550.00|$102,929.94|$102,857.89| |4/1/23|Model\\_A|4|$103,550.00|$102,724.08|$102,272.09| |4/1/23|Model\\_A|5|$103,550.00|$102,518.63|$102,293.09| |4/1/23|Model\\_A|6|$103,550.00|$102,313.59|$102,579.61| |4/1/23|Model\\_A|7|$103,550.00|$102,108.96|$101,996.64| |4/1/23|Model\\_A|8|$103,550.00|$101,904.74|$102,322.55| |4/1/23|Model\\_A|9|$103,550.00|$101,700.93|$101,975.52| |4/1/23|Model\\_A|10|$103,550.00|$101,497.53|$101,142.29| |4/1/23|Model\\_A|11|$103,550.00|$101,294.53|$100,909.61| |4/1/23|Model\\_A|12|$103,550.00|$101,091.94|$101,395.22| |4/1/23|Model\\_A|13|$103,550.00|$100,889.76|$100,960.38| |4/1/23|Model\\_A|14|$103,550.00|$100,687.98|$100,718.19| |4/1/23|Model\\_A|15|$103,550.00|$100,486.60|$100,808.16| |4/1/23|Model\\_A|16|$103,550.00|$100,285.63|$98,247.83| |4/1/23|Model\\_A|17|$103,550.00|$100,085.06|$97,534.14| |4/1/23|Model\\_A|18|$103,550.00|$99,884.89|$97,231.94| |4/1/23|Model\\_A|19|$103,550.00|$99,685.12|$97,348.50| |4/1/23|Model\\_A|20|$103,550.00|$99,485.75|$97,182.90| |5/1/23|Model\\_A|1|$118,720.00|$118,482.56|$118,720.00| |5/1/23|Model\\_A|2|$118,720.00|$118,245.59|$118,352.01| |5/1/23|Model\\_A|3|$118,720.00|$118,009.10|$118,079.91| |5/1/23|Model\\_A|4|$118,720.00|$117,773.08|$117,902.63| |5/1/23|Model\\_A|5|$118,720.00|$117,537.53|$116,961.60| |5/1/23|Model\\_A|6|$118,720.00|$117,302.45|$116,950.54| |5/1/23|Model\\_A|7|$118,720.00|$117,067.85|$117,220.04| |5/1/23|Model\\_A|8|$118,720.00|$116,833.71|$116,646.78| |5/1/23|Model\\_A|9|$118,720.00|$116,600.04|$116,961.50| |5/1/23|Model\\_A|10|$118,720.00|$116,366.84|$116,029.38| |5/1/23|Model\\_A|11|$118,720.00|$116,134.11|$116,459.29| |5/1/23|Model\\_A|12|$118,720.00|$115,901.84|$116,006.15| |5/1/23|Model\\_A|13|$118,720.00|$115,670.04|$115,843.55| |5/1/23|Model\\_A|14|$118,720.00|$115,438.70|$115,865.82| |5/1/23|Model\\_A|15|$118,720.00|$115,207.82|$112,395.02| |5/1/23|Model\\_A|16|$118,720.00|$114,977.40|$111,688.18| |5/1/23|Model\\_A|17|$118,720.00|$114,747.45|$111,431.25| |5/1/23|Model\\_A|18|$118,720.00|$114,517.96|$111,230.72| |5/1/23|Model\\_A|19|$118,720.00|$114,288.92|$111,598.84| |6/1/23|Model\\_A|1|$109,250.00|$109,031.50|$109,250.00| |6/1/23|Model\\_A|2|$109,250.00|$108,813.44|$108,933.13| |6/1/23|Model\\_A|3|$109,250.00|$108,595.81|$108,856.44| |6/1/23|Model\\_A|4|$109,250.00|$108,378.62|$108,476.16| |6/1/23|Model\\_A|5|$109,250.00|$108,161.86|$107,642.68| |6/1/23|Model\\_A|6|$109,250.00|$107,945.54|$108,129.05| |6/1/23|Model\\_A|7|$109,250.00|$107,729.65|$107,772.74| |6/1/23|Model\\_A|8|$109,250.00|$107,514.19|$107,116.39| |6/1/23|Model\\_A|9|$109,250.00|$107,299.16|$107,470.84| |6/1/23|Model\\_A|10|$109,250.00|$107,084.56|$107,063.14| |6/1/23|Model\\_A|11|$109,250.00|$106,870.39|$106,870.39| |6/1/23|Model\\_A|12|$109,250.00|$106,656.65|$106,912.63| |6/1/23|Model\\_A|13|$109,250.00|$106,443.34|$106,666.87| |6/1/23|Model\\_A|14|$109,250.00|$106,230.45|$103,864.70| |6/1/23|Model\\_A|15|$109,250.00|$106,017.99|$102,985.08| |6/1/23|Model\\_A|16|$109,250.00|$105,805.95|$103,625.03| |6/1/23|Model\\_A|17|$109,250.00|$105,594.34|$103,335.41| |6/1/23|Model\\_A|18|$109,250.00|$105,383.15|$103,025.99| |7/1/23|Model\\_A|1|$109,740.00|$109,520.52|$109,137.20| |7/1/23|Model\\_A|2|$109,740.00|$109,301.48|$109,050.09| |7/1/23|Model\\_A|3|$109,740.00|$109,082.88|$109,355.59| |7/1/23|Model\\_A|4|$109,740.00|$108,864.71|$109,256.62| |7/1/23|Model\\_A|5|$109,740.00|$108,646.98|$108,799.09| |7/1/23|Model\\_A|6|$109,740.00|$108,429.69|$108,505.59| |7/1/23|Model\\_A|7|$109,740.00|$108,212.83|$108,515.83| |7/1/23|Model\\_A|8|$109,740.00|$107,996.40|$108,082.80| |7/1/23|Model\\_A|9|$109,740.00|$107,780.41|$107,618.74| |7/1/23|Model\\_A|10|$109,740.00|$107,564.85|$107,629.39| |7/1/23|Model\\_A|11|$109,740.00|$107,349.72|$107,596.62| |7/1/23|Model\\_A|12|$109,740.00|$107,135.02|$107,638.55| |7/1/23|Model\\_A|13|$109,740.00|$106,920.75|$104,153.91| |7/1/23|Model\\_A|14|$109,740.00|$106,706.91|$104,060.04| |7/1/23|Model\\_A|15|$109,740.00|$106,493.50|$103,415.84| |7/1/23|Model\\_A|16|$109,740.00|$106,280.51|$103,177.91| |7/1/23|Model\\_A|17|$109,740.00|$106,067.95|$103,374.88| |8/1/23|Model\\_A|1|$117,370.00|$117,135.26|$117,370.00| |8/1/23|Model\\_A|2|$117,370.00|$116,900.99|$117,064.65| |8/1/23|Model\\_A|3|$117,370.00|$116,667.19|$116,748.86| |8/1/23|Model\\_A|4|$117,370.00|$116,433.86|$116,690.01| |8/1/23|Model\\_A|5|$117,370.00|$116,200.99|$116,108.03| |8/1/23|Model\\_A|6|$117,370.00|$115,968.59|$116,351.29| |8/1/23|Model\\_A|7|$117,370.00|$115,736.65|$115,482.03| |8/1/23|Model\\_A|8|$117,370.00|$115,505.18|$115,736.19| |8/1/23|Model\\_A|9|$117,370.00|$115,274.17|$114,905.29| |8/1/23|Model\\_A|10|$117,370.00|$115,043.62|$115,124.15| |8/1/23|Model\\_A|11|$117,370.00|$114,813.53|$114,928.34| |8/1/23|Model\\_A|12|$117,370.00|$114,583.90|$111,350.63| |8/1/23|Model\\_A|13|$117,370.00|$114,354.73|$111,585.05| |8/1/23|Model\\_A|14|$117,370.00|$114,126.02|$110,850.03| |8/1/23|Model\\_A|15|$117,370.00|$113,897.77|$111,139.17| |8/1/23|Model\\_A|16|$117,370.00|$113,669.97|$110,872.55| |9/1/23|Model\\_A|1|$112,840.00|$112,614.32|$112,062.51| |9/1/23|Model\\_A|2|$112,840.00|$112,389.09|$112,096.88| |9/1/23|Model\\_A|3|$112,840.00|$112,164.31|$111,951.20| |9/1/23|Model\\_A|4|$112,840.00|$111,939.98|$112,342.96| |9/1/23|Model\\_A|5|$112,840.00|$111,716.10|$111,459.15| |9/1/23|Model\\_A|6|$112,840.00|$111,492.67|$111,838.30| |9/1/23|Model\\_A|7|$112,840.00|$111,269.68|$111,113.90| |9/1/23|Model\\_A|8|$112,840.00|$111,047.14|$111,169.29| |9/1/23|Model\\_A|9|$112,840.00|$110,825.05|$110,913.71| |9/1/23|Model\\_A|10|$112,840.00|$110,603.40|$110,271.59| |9/1/23|Model\\_A|11|$112,840.00|$110,382.19|$107,730.26| |9/1/23|Model\\_A|12|$112,840.00|$110,161.43|$107,514.80| |9/1/23|Model\\_A|13|$112,840.00|$109,941.11|$106,656.62| |9/1/23|Model\\_A|14|$112,840.00|$109,721.23|$107,149.36| |9/1/23|Model\\_A|15|$112,840.00|$109,501.79|$106,700.19| |10/1/23|Model\\_A|1|$121,920.00|$121,676.16|$121,920.00| |10/1/23|Model\\_A|2|$121,920.00|$121,432.81|$121,177.80| |10/1/23|Model\\_A|3|$121,920.00|$121,189.94|$120,680.94| |10/1/23|Model\\_A|4|$121,920.00|$120,947.56|$120,475.86| |10/1/23|Model\\_A|5|$121,920.00|$120,705.66|$120,307.33| |10/1/23|Model\\_A|6|$121,920.00|$120,464.25|$120,825.64| |10/1/23|Model\\_A|7|$121,920.00|$120,223.32|$120,680.17| |10/1/23|Model\\_A|8|$121,920.00|$119,982.87|$120,570.79| |10/1/23|Model\\_A|9|$121,920.00|$119,742.90|$120,185.95| |10/1/23|Model\\_A|10|$121,920.00|$119,503.41|$116,224.53| |10/1/23|Model\\_A|11|$121,920.00|$119,264.40|$115,724.63| |10/1/23|Model\\_A|12|$121,920.00|$119,025.87|$115,806.52| |10/1/23|Model\\_A|13|$121,920.00|$118,787.82|$115,667.57| |10/1/23|Model\\_A|14|$121,920.00|$118,550.24|$115,378.43| |11/1/23|Model\\_A|1|$127,400.00|$127,145.20|$127,374.06| |11/1/23|Model\\_A|2|$127,400.00|$126,890.91|$127,208.14| |11/1/23|Model\\_A|3|$127,400.00|$126,637.13|$126,295.21| |11/1/23|Model\\_A|4|$127,400.00|$126,383.86|$126,257.48| |11/1/23|Model\\_A|5|$127,400.00|$126,131.09|$125,815.76| |11/1/23|Model\\_A|6|$127,400.00|$125,878.83|$125,715.19| |11/1/23|Model\\_A|7|$127,400.00|$125,627.07|$125,639.63| |11/1/23|Model\\_A|8|$127,400.00|$125,375.82|$124,786.55| |11/1/23|Model\\_A|9|$127,400.00|$125,125.07|$121,948.14| |11/1/23|Model\\_A|10|$127,400.00|$124,874.82|$121,752.95| |11/1/23|Model\\_A|11|$127,400.00|$124,625.07|$121,363.63| |11/1/23|Model\\_A|12|$127,400.00|$124,375.82|$121,133.03| |11/1/23|Model\\_A|13|$127,400.00|$124,127.07|$121,447.47| |12/1/23|Model\\_A|1|$126,350.00|$126,097.30|$125,895.54| |12/1/23|Model\\_A|2|$126,350.00|$125,845.11|$125,945.79| |12/1/23|Model\\_A|3|$126,350.00|$125,593.42|$125,794.37| |12/1/23|Model\\_A|4|$126,350.00|$125,342.23|$125,104.08| |12/1/23|Model\\_A|5|$126,350.00|$125,091.55|$124,916.42| |12/1/23|Model\\_A|6|$126,350.00|$124,841.37|$125,465.58| |12/1/23|Model\\_A|7|$126,350.00|$124,591.69|$124,853.33| |12/1/23|Model\\_A|8|$126,350.00|$124,342.51|$121,512.79| |12/1/23|Model\\_A|9|$126,350.00|$124,093.82|$120,640.60| |12/1/23|Model\\_A|10|$126,350.00|$123,845.63|$120,858.16| |12/1/23|Model\\_A|11|$126,350.00|$123,597.94|$120,110.32| |12/1/23|Model\\_A|12|$126,350.00|$123,350.74|$120,014.41| |1/1/24|Model\\_A|1|$134,640.00|$134,370.72|$134,236.35| |1/1/24|Model\\_A|2|$134,640.00|$134,101.98|$134,640.00| |1/1/24|Model\\_A|3|$134,640.00|$133,833.78|$133,606.26| |1/1/24|Model\\_A|4|$134,640.00|$133,566.11|$133,472.61| |1/1/24|Model\\_A|5|$134,640.00|$133,298.98|$133,538.92| |1/1/24|Model\\_A|6|$134,640.00|$133,032.38|$133,631.03| |1/1/24|Model\\_A|7|$134,640.00|$132,766.32|$129,408.33| |1/1/24|Model\\_A|8|$134,640.00|$132,500.79|$129,304.54| |1/1/24|Model\\_A|9|$134,640.00|$132,235.79|$129,097.51| |1/1/24|Model\\_A|10|$134,640.00|$131,971.32|$128,028.67| |1/1/24|Model\\_A|11|$134,640.00|$131,707.38|$128,016.61| |2/1/24|Model\\_A|1|$127,880.00|$127,624.24|$127,560.43| |2/1/24|Model\\_A|2|$127,880.00|$127,368.99|$126,846.78| |2/1/24|Model\\_A|3|$127,880.00|$127,114.25|$127,482.88| |2/1/24|Model\\_A|4|$127,880.00|$126,860.02|$127,481.63| |2/1/24|Model\\_A|5|$127,880.00|$126,606.30|$126,770.89| |2/1/24|Model\\_A|6|$127,880.00|$126,353.09|$123,108.02| |2/1/24|Model\\_A|7|$127,880.00|$126,100.38|$122,566.73| |2/1/24|Model\\_A|8|$127,880.00|$125,848.18|$123,205.06| |2/1/24|Model\\_A|9|$127,880.00|$125,596.48|$122,236.15| |2/1/24|Model\\_A|10|$127,880.00|$125,345.29|$121,686.15| |3/1/24|Model\\_A|1|$129,220.00|$128,961.56|$128,561.78| |3/1/24|Model\\_A|2|$129,220.00|$128,703.64|$129,192.71| |3/1/24|Model\\_A|3|$129,220.00|$128,446.23|$129,049.93| |3/1/24|Model\\_A|4|$129,220.00|$128,189.34|$128,253.43| |3/1/24|Model\\_A|5|$129,220.00|$127,932.96|$124,884.32| |3/1/24|Model\\_A|6|$129,220.00|$127,677.09|$124,273.54| |3/1/24|Model\\_A|7|$129,220.00|$127,421.74|$123,975.30| |3/1/24|Model\\_A|8|$129,220.00|$127,166.90|$124,161.31| |3/1/24|Model\\_A|9|$129,220.00|$126,912.57|$124,271.83| |4/1/24|Model\\_A|1|$134,850.00|$134,580.30|$134,270.77| |4/1/24|Model\\_A|2|$134,850.00|$134,311.14|$133,881.34| |4/1/24|Model\\_A|3|$134,850.00|$134,042.52|$133,559.97| |4/1/24|Model\\_A|4|$134,850.00|$133,774.43|$130,077.91| |4/1/24|Model\\_A|5|$134,850.00|$133,506.88|$130,156.19| |4/1/24|Model\\_A|6|$134,850.00|$133,239.87|$129,259.33| |4/1/24|Model\\_A|7|$134,850.00|$132,973.39|$129,363.83| |4/1/24|Model\\_A|8|$134,850.00|$132,707.44|$129,557.96| |5/1/24|Model\\_A|1|$134,680.00|$134,410.64|$134,680.00| |5/1/24|Model\\_A|2|$134,680.00|$134,141.82|$134,490.59| |5/1/24|Model\\_A|3|$134,680.00|$133,873.54|$130,017.64| |5/1/24|Model\\_A|4|$134,680.00|$133,605.79|$130,304.72| |5/1/24|Model\\_A|5|$134,680.00|$133,338.58|$130,408.13| |5/1/24|Model\\_A|6|$134,680.00|$133,071.90|$129,394.79| |5/1/24|Model\\_A|7|$134,680.00|$132,805.76|$128,928.83| |6/1/24|Model\\_A|1|$154,020.00|$153,711.96|$154,020.00| |6/1/24|Model\\_A|2|$154,020.00|$153,404.54|$149,389.94| |6/1/24|Model\\_A|3|$154,020.00|$153,097.73|$149,165.80| |6/1/24|Model\\_A|4|$154,020.00|$152,791.53|$149,567.63| |6/1/24|Model\\_A|5|$154,020.00|$152,485.95|$148,837.34| |6/1/24|Model\\_A|6|$154,020.00|$152,180.98|$148,064.87| |7/1/24|Model\\_B|1|$127,066.50|$126,812.37|$123,431.87| |7/1/24|Model\\_B|2|$127,066.50|$126,558.75|$123,690.93| |7/1/24|Model\\_B|3|$127,066.50|$126,305.63|$123,455.86| |7/1/24|Model\\_B|4|$127,066.50|$126,053.02|$122,655.89| |7/1/24|Model\\_B|5|$127,066.50|$125,800.91|$122,888.93| |8/1/24|Model\\_B|1|$130,917.00|$130,655.17|$127,644.08| |8/1/24|Model\\_B|2|$130,917.00|$130,393.86|$126,739.90| |8/1/24|Model\\_B|3|$130,917.00|$130,133.07|$126,968.56| |8/1/24|Model\\_B|4|$130,917.00|$129,872.80|$126,208.11| |9/1/24|Model\\_B|1|$133,484.00|$133,217.03|$129,419.01| |9/1/24|Model\\_B|2|$133,484.00|$132,950.60|$129,212.03| |9/1/24|Model\\_B|3|$133,484.00|$132,684.70|$129,755.68| |10/1/24|Model\\_B|1|$125,783.00|$125,531.43|$122,601.21| |10/1/24|Model\\_B|2|$125,783.00|$125,280.37|$122,026.21| |11/1/24|Model\\_B|1|$130,917.00|$130,655.17|$127,528.92 |",
    "author": "chasing_green_roads",
    "timestamp": "2025-09-17T06:15:45",
    "url": "https://reddit.com/r/datascience/comments/1njcvfv/example_take_home_assignment_for_interview_data/",
    "score": 55,
    "num_comments": 17,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nj55qy",
    "title": "How do you conduct a power analysis on a causal observational study?",
    "content": "Hey everyone, we are running some campaigns and then looking back retrospectively to see if they worked. How do you determine the correct sample size? Does a normal power size calculator work in this scenario? \n\nI’ve seen some conflicting thoughts on this, wondering how you’ve all done it on your projects.",
    "author": "LebrawnJames416",
    "timestamp": "2025-09-16T22:59:07",
    "url": "https://reddit.com/r/datascience/comments/1nj55qy/how_do_you_conduct_a_power_analysis_on_a_causal/",
    "score": 10,
    "num_comments": 14,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nj9mu5",
    "title": "Privacy-Safe Tabular Synthetic Data with TabPFN",
    "content": "",
    "author": "rsesrsfh",
    "timestamp": "2025-09-17T03:41:19",
    "url": "https://reddit.com/r/datascience/comments/1nj9mu5/privacysafe_tabular_synthetic_data_with_tabpfn/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1niop8o",
    "title": "Python Projects For Beginners to Advanced | Build Logic | Build Apps | Intro on Generative AI|Gemini",
    "content": "",
    "author": "Beneficial-Buyer-569",
    "timestamp": "2025-09-16T10:57:10",
    "url": "https://reddit.com/r/datascience/comments/1niop8o/python_projects_for_beginners_to_advanced_build/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nioq77",
    "title": "Python Projects For Beginners to Advanced | Build Logic | Build Apps | Intro on Generative AI|Gemini",
    "content": "Only those win who stay till the end.”\n\nComplete the whole series and become really good at python. You can skip the intro.\n\nYou can start from Anywhere. From Beginners or Intermediate or Advanced or You can Shuffle and Just Enjoy the journey of learning python by these Useful Projects.\n\nWhether you are a beginner or an intermediate in Python. This 5 Hour long Python Project Video will leave you with tremendous information , on how to build logic and Apps and also with an introduction to Gemini. \n\nYou will start from Beginner Projects and End up with Building Live apps. This Python Project video will help you in putting some great resume projects  and also help you in understanding the real use case of python. \n\nThis is an eye opening Python Video and you will be not the same python programmer after completing it.\n",
    "author": "Beneficial-Buyer-569",
    "timestamp": "2025-09-16T10:58:07",
    "url": "https://reddit.com/r/datascience/comments/1nioq77/python_projects_for_beginners_to_advanced_build/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.58,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nhzoam",
    "title": "Advice on presenting yourself",
    "content": "Hello everyone,\nI recently got the chance to speak with the HR at a healthcare company that’s working on AI agents to optimize prescription pricing. \nWhile I haven’t directly built AI agents before, I’d like to design a small prototype for my hiring manager round and use that discussion to show how I can tackle their challenges.\nI’ve got about a week to prepare and only ~30 minutes for the conversation, so I’m looking for advice on:\n- How to outline the initial architecture for a project like this (at a high level).\n- What aspects of the design/implementation are most valuable for a hiring manager or senior engineer to see.\n- What to leave out and what to keep so the presentation/my pitch stays focused and impactful.\n\nAppreciate any thoughts—especially from folks who have been on the hiring side and know what really makes someone stand out.\nI am just a bit confused that even if I have a prototype how should I present it naturally and smartly.\n\nEdit : the goal here is to optimize the prescription price by lowering prices where it's still profitable for the company. ",
    "author": "-Cicada7-",
    "timestamp": "2025-09-15T15:06:12",
    "url": "https://reddit.com/r/datascience/comments/1nhzoam/advice_on_presenting_yourself/",
    "score": 23,
    "num_comments": 14,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nhskvc",
    "title": "How do you factor seasonality in A/B test experiments? Which methods you personally use and why?",
    "content": "Hi, \n\nI was wondering how do you perform the experiment and factor the seasonality while analyzing it?  (Especially on e-commerce side) \n\nFor example i often wonder when marketing campaigns are done during black Friday/holiday season, how do they know whether the campaign had the causal effect? And how much? When we know people tend to buy more things in holiday season. \n\nSo what test or statistical methods do you use to factor into? Or what are the other methods you use to find how the campaign performed? \n\nFirst i think of is use historical data of the same season for last year, and compare it, but what if we don’t have historical data? \n\nWhat other things need to keep in mind while designing an experiment when we know seasonality could be play big role? And there’s no way we can perform the experiment outside of season? \n\nThanks!\n\n\nEdit- 2nd question, lets say we want to run a promotion during a season, like bf sale, how do you keep treatment and control? Or how do you analyze the effect of sale? As you would not want to hold out on users during sales? Or what companies do during this time to keep a control group ? ",
    "author": "Starktony11",
    "timestamp": "2025-09-15T10:40:28",
    "url": "https://reddit.com/r/datascience/comments/1nhskvc/how_do_you_factor_seasonality_in_ab_test/",
    "score": 43,
    "num_comments": 40,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nhy9qb",
    "title": "Free LLM API Providers",
    "content": "I’m a recent graduate working on end-to-end projects. Most of my current projects are either running locally through Ollama or were built back when the OpenAI API was free. Now I’m a bit confused about what to use for deployment.\n\nI don’t plan to scale them for heavy usage, but I’d like to deploy them so they’re publicly accessible and can be showcased in my portfolio, allowing a few users to try them out. Any suggestions would be appreciated.",
    "author": "PakalManiac",
    "timestamp": "2025-09-15T14:09:24",
    "url": "https://reddit.com/r/datascience/comments/1nhy9qb/free_llm_api_providers/",
    "score": 5,
    "num_comments": 18,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nhoblg",
    "title": "Is an explicit \"treatment\" variable a necessary condition for instrumental variable analysis?",
    "content": "Hi everyone, I'm trying to model the causal impact of our marketing efforts on our ads business, and I'm considering an Instrumental Variable (IV) framework. I'd appreciate a sanity check on my approach and any advice you might have.\n\n**My Goal**: Quantify how much our marketing spend contributes to advertiser acquisition and overall ad revenue.\n\n**The Challenge**: I don't believe there's a direct causal link. My hypothesis is a two-stage process:\n\n* Stage 1:  Marketing spend -&gt; Increases user acquisition and retention -&gt; Leads to higher Monthly Active Users (MAUs).\n* Stage 2: Higher MAUs -&gt; Makes our platform more attractive to advertisers -&gt; Leads to more advertisers and higher ad revenue.\n\nThe problem is that the variable in the middle (MAUs) is endogenous. A simple regression of Ad Revenue \\~ MAUs would be biased because unobserved factors (e.g., seasonality, product improvements, economic trends) likely influence both user activity and advertiser spend simultaneously.\n\n**Proposed IV Setup**:\n\n* **Outcome Variable** (Y): Advertiser Revenue.\n* **Endogenous Explanatory Variable** (\"Treatment\") (X): MAUs (or another user volume/engagement metric).\n* **Instrumental Variable** (Z): This is where I'm stuck. I need a variable that influences MAUs but does not directly affect advertiser revenue, which I believe should be marketing spend. \n\nMy Questions:\n\n* Is this the right way to conceptualize the problem? Is IV the correct tool for this kind of mediated relationship where the mediator (user volume) is endogenous? Is there a different tool that I could use?\n* This brings me to a more fundamental question: Does this setup require a formal \"experiment\"? Or can I apply this IV design to historical, observational time-series data to untangle these effects?\n\nThanks for any insights!\n\n",
    "author": "Money-Commission9304",
    "timestamp": "2025-09-15T08:03:59",
    "url": "https://reddit.com/r/datascience/comments/1nhoblg/is_an_explicit_treatment_variable_a_necessary/",
    "score": 15,
    "num_comments": 14,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nhbvwg",
    "title": "Weekly Entering &amp; Transitioning - Thread 15 Sep, 2025 - 22 Sep, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-09-14T21:01:19",
    "url": "https://reddit.com/r/datascience/comments/1nhbvwg/weekly_entering_transitioning_thread_15_sep_2025/",
    "score": 9,
    "num_comments": 16,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ngj3v5",
    "title": "Has anyone validated synthetic financial data (Gaussian Copula vs CTGAN) in practice?",
    "content": "I’ve been experimenting with generating synthetic datasets for financial indicators (GDP, inflation, unemployment, etc.) and found that CTGAN offered stronger privacy protection in simple linkage tests, but its overall analytical utility was much weaker. In contrast, Gaussian Copula provided reasonably strong privacy and far better fidelity.\n\nFor example, Okun’s law (the relationship between GDP and unemployment) still held in the Gaussian Copula data, which makes sense since it models the underlying distributions. What surprised me was how poorly CTGAN performed analytically... in one regression, the coefficients even flipped signs for both independent variables.\n\nHas anyone here used synthetic data for research or production modeling in finance? Any tips for balancing *fidelity* and *privacy* beyond just model choice?\n\nIf anyone’s interested in the full validation results (charts, metrics, code), let me know, I’ve documented them separately and can share the link.\n\nhttps://preview.redd.it/lmsmleiki2pf1.png?width=1059&amp;format=png&amp;auto=webp&amp;s=19cb6d9215e590e5fe6497bc0dd7152d9d85f119\n\n",
    "author": "nlomb",
    "timestamp": "2025-09-13T22:43:07",
    "url": "https://reddit.com/r/datascience/comments/1ngj3v5/has_anyone_validated_synthetic_financial_data/",
    "score": 26,
    "num_comments": 16,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ng5x61",
    "title": "Texts for creating better visualizations/presentations?",
    "content": "I started working for an HR team and have been tasked with creating visualizations, both in PowerPoint (I've been using Seaborn and Matplotlib for visualizations) and PowerBI Dashboards. I've been having a lot of fun creating visualizations, but I'm looking for a few texts or maybe courses/videos about design. Anything you would recommend? \n\nI have this conflicting issue with either showing too little or too much. Should I have appendices or not? ",
    "author": "Tyron_Slothrop",
    "timestamp": "2025-09-13T12:06:14",
    "url": "https://reddit.com/r/datascience/comments/1ng5x61/texts_for_creating_better/",
    "score": 29,
    "num_comments": 24,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nfzxy9",
    "title": "Database tools and method for tree structured data?",
    "content": "I have a database structure which I believe is very common, and very general, so I’m wondering how this is tackled.\n\nThe database structured like:\n\n     -&gt; Project (Name of project)\n\n           -&gt; Category (simple word, ~20 categories)\n\n                  -&gt; Study\n \n\nStudy is a directory containing:\n- README with date &amp; description (txt or md format)\n- Supporting files which can be any format (csv, xlsx, ptpx, keynote, text, markdown, pickled data frames, possible processing scripts, basically anything.)\n\nRelationships among data:\n- Projects can have shared studies.\n- Studies can be related or new versions of older ones, but can also be completely independent.\n\nTotal size:\n- 1 TB, mostly due to supporting files found in studies.\n\nWhat I want:\n- Search database for queries describing what we are looking for.\n- Eventually get pointed to proper study directory and/or contents, showing all the files.\n- Find which studies are similar based on description category, etc.\n\nWhat is a good way to search such a database?  Considering it’s so simple, do I even need a framework like sql?",
    "author": "thermokopf",
    "timestamp": "2025-09-13T08:11:27",
    "url": "https://reddit.com/r/datascience/comments/1nfzxy9/database_tools_and_method_for_tree_structured_data/",
    "score": 8,
    "num_comments": 5,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ng1xk3",
    "title": "The “three tiers” of data engineering pay — and how to move up",
    "content": "**The “three tiers” of data engineering pay — and how to move up (shout out to the article by geergly orosz which i placed in the bottom)**\n\nI keep seeing folks compare salaries across wildly different companies and walk away confused. A useful mental model I’ve found is that comp clusters into *three tiers* based on company type, not just your years of experience or title. Sharing this to help people calibrate expectations and plan the next move.\n\n# The three tiers\n\n* **Tier 1 — “Engineering is a cost center.”** Think traditional companies, smaller startups, internal IT/BI, or teams where data is a support function. Pay is the most modest, equity/bonuses are limited, scope is narrower, and work is predictable (reports, ELT to a warehouse, a few Airflow dags, light stakeholder churn).\n* **Tier 2 — “Data is a growth lever.”** Funded startups/scaleups and product-centric companies. You’ll see modern stacks (cloud warehouses/lakehouses, dbt, orchestration, event pipelines), clearer paths to impact, and some equity/bonus. companies expect design thinking and hands-on depth. Faster pace, more ambiguity, bigger upside.\n* **Tier 3 — “Data is a moat.”** Big tech, trading/quant, high-scale platforms, and companies competing globally for talent. Total comp can be multiples of Tier 1. hiring process are rigorous (coding + system design + domain depth). Expectations are high: reliability SLAs, cost controls at scale, privacy/compliance, streaming/near-real-time systems, complex data contracts.\n\nNone of these are “better” by default. They’re just different trade-offs: stability vs. upside, predictability vs. scope, lower stress vs. higher growth.\n\n# Signals you’re looking at each tier\n\n* **Tier 1:** job reqs emphasize tools (“Airflow, SQL, Tableau”) over outcomes; little talk of SLAs, lineage, or contracts; analytics asks dominate; compensation is mainly base.\n* **Tier 2:** talks about metrics that move the business, experimentation, ownership of domains, real data quality/process governance; base + some bonus/equity; leveling exists but is fuzzy.\n* **Tier 3:** explicit levels/bands, RSUs or meaningful options, on-call for data infra, strong SRE practices, platform/mesh/contract language, cost/perf trade-offs are daily work.\n\n# If you want to climb a tier, focus on evidence of impact at scale\n\nThis is what consistently changes comp conversations:\n\n* **Design → not just build.** Bring written designs for one or two systems you led: ingestion → storage → transformation → serving. Show choices and trade-offs (batch vs streaming, files vs tables, CDC vs snapshots, cost vs latency).\n* **Reliability &amp; correctness.** Prove you’ve owned SLAs/SLOs, data tests, contracts, backfills, schema evolution, and incident reviews. Screenshots aren’t necessary—bullet the incident, root cause, blast radius, and the guardrail you added.\n* **Cost awareness.** Know your unit economics (e.g., cost per 1M events, per TB transformed, per dashboard refresh). If you’ve saved the company money, quantify it.\n* **Breadth across the stack.** A credible story across ingestion (Kafka/Kinesis/CDC), processing (Spark/Flink/dbt), orchestration (Airflow/Argo), storage (lakehouse/warehouse), and serving (feature store, semantic layer, APIs). You don’t need to be an expert in all—show you can choose appropriately.\n* **Observability.** Lineage, data quality checks, freshness alerts, SLIs tied to downstream consumers.\n* **Security &amp; compliance.** RBAC, PII handling, row/column-level security, audit trails. Even basic exposure here is a differentiator.\n\n# prep that actually moves the needle\n\n* **Coding:** you don’t need to win ICPC, but you *do* need to write clean Python/SQL under time pressure and reason about complexity.\n* **Data system design:** practice 45–60 min sessions. Design an events pipeline, CDC into a lakehouse, or a real-time metrics system. Cover partitioning, backfills, late data, idempotency, dedupe, compaction, schema evolution, and cost.\n* **Storytelling with numbers:** have 3–4 impact bullets with metrics: “Reduced warehouse spend 28% by switching X to partitioned Parquet + object pruning,” “Cut pipeline latency from 2h → 15m by moving Y to streaming with windowed joins,” etc.\n* **Negotiation prep:** know base/bonus/equity ranges for the level (bands differ by tier). Understand RSUs vs options, vesting, cliffs, refreshers, and how performance ties to bonus.\n\n# Common traps that keep people stuck\n\n* **Tool-first resumes.** Listing ten tools without outcomes reads Tier 1. Frame with “problem → action → measurable result.”\n* **Only dashboards.** Valuable, but hiring loops for higher tiers want ownership of data *as a product*.\n* **Ignoring reliability.** If you’ve never run an incident call for data, you’re missing a lever that Tier 2/3 value highly.\n* **No cost story.** At scale, cost is a feature. Even a small POC that trims spend is compelling signal.\n\n# Why this matters\n\nAverages hide the spread. Two data engineers with the same YOE can be multiple tiers apart in pay purely based on company type and scope. When you calibrate to tiers, expectations and strategy get clearer.\n\nIf you want a deeper read on the broader “three clusters” concept for software salaries, Gergely Orosz has a solid breakdown (“The Trimodal Nature of Software Engineering Salaries”). The framing maps neatly onto data engineering roles too. link in the bottom\n\nCurious to hear from this sub:\n\n* If you moved from Tier 1 → 2 or 2 → 3, what was the single project or proof point that unlocked it?\n* For folks hiring: what signals *actually* distinguish tiers in your loop?\n\narticle: [https://blog.pragmaticengineer.com/software-engineering-salaries-in-the-netherlands-and-europe/](https://blog.pragmaticengineer.com/software-engineering-salaries-in-the-netherlands-and-europe/)",
    "author": "chrisgarzon19",
    "timestamp": "2025-09-13T09:30:33",
    "url": "https://reddit.com/r/datascience/comments/1ng1xk3/the_three_tiers_of_data_engineering_pay_and_how/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ne9hzv",
    "title": "Mid career data scientist burnout",
    "content": "Been in the industry since 2012. I started out in data analytics consulting. The first 5 were mostly that, and didn't enjoy the work as I thought it wasn't challenging enough. In the last 6 years or so, I've moved to being a Senior Data Scientist - the type that's more close to a statistical modeller, not a full-stack data scientist.  Currently work in health insurance (fairly new, just over a year in current role). I suck at comms and selling my work, and the more higher up I'm going in the organization, I realize I need to be strategic with selling my work, and also in dealing with people. It always has been an energy drainer for me - I find I'm putting on a front.  \nOff late, I feel 'meh' about everything. The changes in the industry, the amount of knowledge some technical, some industry based to keep up with seems overwhelming.\n\nOverall, I chart some of these feelings to a feeling of lacking capability to handling stakeholders, lack of leadership skills in the role/ tying to expectations in the role. (also want to add that I have social anxiety). Perhaps one of the things might help is probably upskilling on the social front. Anyone have similar journeys/ resources to share?  \nI started working with a generic career coach, but haven't found it that helpful as the nuances of crafting a narrative plus selling isn't really coming up (a lot more of confidence/ presence is what is focused on).\n\nEdit: Lots of helpful directions to move in, which has been energizing.",
    "author": "WillingAstronomer",
    "timestamp": "2025-09-11T06:56:50",
    "url": "https://reddit.com/r/datascience/comments/1ne9hzv/mid_career_data_scientist_burnout/",
    "score": 216,
    "num_comments": 78,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1negm5l",
    "title": "How do data scientists add value to LLMs?",
    "content": "Edit: i am not saying AI is replacing DS, of course DS still do their normal job with traditional stats and ml, i am just wondering if they can play an important role around LLMs too\n\nI’ve noticed that many consulting firms and AI teams have Forward Deployed AI Engineers. They are basically software engineers who go on-site, understand a company’s problems and build software leveraging LLM APIs like ChatGPT. They don’t build models themselves, they build solutions using existing models.\n\nThis makes me wonder: can data scientists add values to this new LLM wave too (where models are already built)? For example i read that data scientists could play an important role in dataset curation for LLMs. \n\nDo you think that DS can leverage their skills to work with AI eng in this consulting-like role?\n",
    "author": "FinalRide7181",
    "timestamp": "2025-09-11T11:30:56",
    "url": "https://reddit.com/r/datascience/comments/1negm5l/how_do_data_scientists_add_value_to_llms/",
    "score": 76,
    "num_comments": 44,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nehig5",
    "title": "Global survey exposes what HR fears most about AI",
    "content": "",
    "author": "nullstillstands",
    "timestamp": "2025-09-11T12:05:18",
    "url": "https://reddit.com/r/datascience/comments/1nehig5/global_survey_exposes_what_hr_fears_most_about_ai/",
    "score": 47,
    "num_comments": 19,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1neack3",
    "title": "Transitioning to MLE/MLOps from DS",
    "content": "I am working as a DS with some 2 years of experience in a mid tier consultancy. I work on some model building and lot of adhoc analytics. I am from CS background and I want to be more towards engineering side. Basically I want to transition to MLE/MLOps. My major challenge is I don't have any experience with deployment or engineering the solutions at scale etc. and my current organisation doesn't have that kind of work for me to internally transition. Genuinely, what are my chances of landing in the roles I want? Any advice on how to actually do that? I feel companies will hardly shortlist profiles for MLE without proper experience. If personal projects work I can do that as well. Need some genuine guidance here. \n",
    "author": "alpha_centauri9889",
    "timestamp": "2025-09-11T07:30:36",
    "url": "https://reddit.com/r/datascience/comments/1neack3/transitioning_to_mlemlops_from_ds/",
    "score": 21,
    "num_comments": 10,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nenrg8",
    "title": "An introduction to program synthesis",
    "content": "",
    "author": "ChavXO",
    "timestamp": "2025-09-11T16:21:51",
    "url": "https://reddit.com/r/datascience/comments/1nenrg8/an_introduction_to_program_synthesis/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ne37bs",
    "title": "Looking for recent research on explainable AI (XAI)",
    "content": "I'd love to get some papers on the latest advancements on explainable AI (XAI). I'm looking for papers that are at most 2-3 years old and had an impact. Thanks!",
    "author": "ciaoshescu",
    "timestamp": "2025-09-11T01:18:43",
    "url": "https://reddit.com/r/datascience/comments/1ne37bs/looking_for_recent_research_on_explainable_ai_xai/",
    "score": 11,
    "num_comments": 17,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ne1d5t",
    "title": "Collaborating with data teams",
    "content": "",
    "author": "ButtFlannel69",
    "timestamp": "2025-09-10T23:18:26",
    "url": "https://reddit.com/r/datascience/comments/1ne1d5t/collaborating_with_data_teams/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nd94cy",
    "title": "(: Smile! It’s my first open source project",
    "content": "",
    "author": "ThomasAger",
    "timestamp": "2025-09-10T02:02:35",
    "url": "https://reddit.com/r/datascience/comments/1nd94cy/smile_its_my_first_open_source_project/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ncmcgf",
    "title": "Pytorch lightning vs pytorch",
    "content": "Today at work, i was criticized by a colleague for implementing my training script in pytorch instead of pytorch lightning. His rationale was that the same thing could've been done in less code using lightning, and more code means more documentation and explaining to do. I havent familiarized myself with pytorch lightning yet so im not sure if this is fair criticism, or something i should take with a grain of salt. I do intend to read the lightning docs soon but im just thinking about this for my own learning. Any thoughts?",
    "author": "Factitious_Character",
    "timestamp": "2025-09-09T08:41:08",
    "url": "https://reddit.com/r/datascience/comments/1ncmcgf/pytorch_lightning_vs_pytorch/",
    "score": 68,
    "num_comments": 24,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nc93qq",
    "title": "I built a card recommender for EDH decks",
    "content": "Hi guys! I built a simple card recommender system for the EDH format of Magic the Gathering. Unlike EDHREC which suggests cards based on overall popularity, this analyzes your full decklist and recommends cards based on similar decks.\n\nDeck similarity is computed as the sum of idf weights of shared cards. It then shows the top 100 cards from similar decks that aren't already in your decklist. It's simple but will usually give more relevant suggestions for your deck.\n\nTry it [here](https://huggingface.co/spaces/bingbong-sempai/edhrec-at-home): (Archidekt links only)\n\nWould love to hear feedback!",
    "author": "bingbong_sempai",
    "timestamp": "2025-09-08T21:04:05",
    "url": "https://reddit.com/r/datascience/comments/1nc93qq/i_built_a_card_recommender_for_edh_decks/",
    "score": 23,
    "num_comments": 14,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nbxzs0",
    "title": "Analysing Priority zones in my Area with unprecise home adresses",
    "content": "hello, My project analyzes whether given addresses fall inside \"Quartiers Prioritaires de la Politique de la Ville \"(QPV). It uses a GeoJSON file of QPV boundaries(available on the gorvernment website) and a geocoding service (Nominatim/OSM) to convert addresses into geographic coordinates. Each address is then checked with GeoPandas + Shapely to determine if its coordinates lie within any QPV polygon. The program can process one or multiple addresses, returning results that indicate whether each is located inside or outside a QPV, along with the corresponding zone name when available. This tool can be extended to handle CSV databases, produce visualizations on maps, or integrate into larger urban policy analysis workflows. \" \n\nBUUUT . \n\nhere is the ultimate problem of this project , Home addresses in my area (Martinique) are notoriously unreliable if you dont know the way and google maps or Nominatim cant pinpoint most of the places in order to be converted to coordinates to say whether or not the person who gave the adress is in a QPV or not.  when i use my python script on adresses of the main land like paris and the like it works just fine but our little island isnt as well defined in terms of urban planning. \n\ncan someone please help me to find a way to get all the streets data into coordinates and make them match with the polygon of the QPV areas ? thank you in advance",
    "author": "samushusband",
    "timestamp": "2025-09-08T12:54:03",
    "url": "https://reddit.com/r/datascience/comments/1nbxzs0/analysing_priority_zones_in_my_area_with/",
    "score": 13,
    "num_comments": 13,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nbdtct",
    "title": "Weekly Entering &amp; Transitioning - Thread 08 Sep, 2025 - 15 Sep, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-09-07T21:01:38",
    "url": "https://reddit.com/r/datascience/comments/1nbdtct/weekly_entering_transitioning_thread_08_sep_2025/",
    "score": 10,
    "num_comments": 42,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n9yrfy",
    "title": "Europe Salary Thread 2025 - What's your role and salary?",
    "content": "The yearly Europe-centric salary thread. You can find the last one here:\n\nhttps://old.reddit.com/r/datascience/comments/1fxrmzl/europe_salary_thread_2024_whats_your_role_and/\n\nI think it's worthwhile to learn from one another and see what different flavours of data scientists, analysts and engineers are out there in the wild. In my opinion, this is especially useful for the beginners and transitioners among us. So, do feel free to talk a bit about your work if you can and want to. 🙂\n\nWhile not the focus, non-Europeans are of course welcome, too. Happy to hear from you!\n\n**Data Science Flavour:** .\n\n**Location:** .\n\n**Title:** .\n\n**Compensation (gross):** .\n\n**Education level:** .\n\n**Experience:** .\n\n**Industry/vertical:** .\n\n**Company size:** .\n\n**Majority of time spent using (tools):** .\n\n**Majority of time spent doing (role):** .",
    "author": "Massive_Arm_706",
    "timestamp": "2025-09-06T05:50:59",
    "url": "https://reddit.com/r/datascience/comments/1n9yrfy/europe_salary_thread_2025_whats_your_role_and/",
    "score": 188,
    "num_comments": 122,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1naptuq",
    "title": "🚀 Perpetual ML Suite: Now Live on the Snowflake Marketplace!",
    "content": "",
    "author": "mutlu_simsek",
    "timestamp": "2025-09-07T03:31:17",
    "url": "https://reddit.com/r/datascience/comments/1naptuq/perpetual_ml_suite_now_live_on_the_snowflake/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1na6x3q",
    "title": "Help me evaluate a new job offer - Stay or go?",
    "content": "Hi all, \n\nI'm having a really hard time deciding whether or not to take an offer I've recently received, would really appreciate some advice and a sense check. For context I generally feel my current role is comfortable but i'm starting to plateau after the first year, i'm also in the process of buying my dream house just to complicate things.\n\n### **Current Role**\n\n##### The Good\n\n- I am early 30's and have 4 years of experience as a full stack DS but am currently employed as an ML Eng for the last year. \n- My current role is effectively a senior/lead MLE in a small team (me + 3 DS) and I have loads of autonomy in how we do things and I get to lead my own  Gen AI projects with small squads as I'm the only one with experience in this domain. \n- I also get to straddle DS and MLE as much or as little as I want to in other projects, which suits my interests and background. \n- We have some interesting projects including one I'm leading. I think I have around 6 months of cool work to do where I can personally make an impact. \n- My work life balance is amazing, I'm not stressed at work at all and I can learn at my own pace. \n- Effectively remote, go into the office 1 or 2 times per month for meetings. It's 1.5 hours away but work pay for my travel. \n- Can push for a senior or principal title and will likely get it in the next ~6 months. \n\n##### The Bad\n\n- The main drawbacks here are that I don't have senior technical mentors, my direct boss has good soft skills but I have nothing to learn from him technically. He's also quite chaotic, so we are always shifting priorities etc. \n- It's a brand new team so we are constantly hitting blockers in terms of processes, integration of our projects and office politics. \n- Being a legacy insurer, innovation is really hard and momentum needed to shift opinions is huge. \n- Fundamentally data quality is very poor and this won't change in my tenure. \n- Essentially in an echo chamber, I'm bringing most of the ideas and solutions to the table in the team which potentially isn't great at this stage in my career.\n- It's not perfect and I'd have to leave at some point anyway. \n\n\n##### Comp\n- Total comp including bonus and generous pension is £84K\n\n\n### **New Job** AI Engineer\n\n##### The Good\n- Very cool AI consultancy startup, 2 years old, ~80 technical staff and growing rapidly, already profitable with a revenue of £1mill per month and partnership with Open AI.\n- Lots of interesting projects with cool clients. The founders' mantra is \"cool projects, in production\" and they have some genuinely interesting case studies. \n- Some projects are genuinely cutting edge and they claim to have a nice balance between R&amp;D and delivery. \n- Lots of technical staff to learn from, should be good for my growth. \n- Opportunity to work internationally in the future, the are opening offices in Australia now and eventually the US. \n\n\n##### The Bad\n\n- Pigeon holing myself into AI/Agents/LLMs. No trad ML, may lose some of my very rounded skill set.\n- Although it's customer facing, it sounds like the role is very delivery heavy and I'd essentially be smashing out code or researching all day with less soft skill development.\n- Slightly worried about work culture and work life balance, this could end up being a meat grinder. \n- I have no experience of start ups or start up culture at all.\n- Less job security as its a startup. \n- It's mostly based in London (5 hours round trip!) and I would need to travel down relatively frequently (expenses paid) for onboarding and establishing myself in the first few months, with that requirement tapering off slowly. \n\n\n##### Comp\n\n- Total offer all in is £90K, I could try and negotiate for up to £95K based on their bandings. \n- 36000 stock units, worthless until they sell though\n   \n\n\nWould love to know your thoughts!\n\n\n",
    "author": "Rockingtits",
    "timestamp": "2025-09-06T11:27:39",
    "url": "https://reddit.com/r/datascience/comments/1na6x3q/help_me_evaluate_a_new_job_offer_stay_or_go/",
    "score": 15,
    "num_comments": 38,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1nac35j",
    "title": "How to evaluate data transformations?",
    "content": "There are several well-established benchmarks for text-to-SQL tasks like BIRD, Spider, and WikiSQL. However, I'm working on a data transformation system that handles per-row transformations with contextual understanding of the input data.\n\nThe challenge is that most existing benchmarks focus on either:\n\n* Pure SQL generation (BIRD, Spider)\n* Simple data cleaning tasks\n* Basic ETL operations\n\nBut what I'm looking for are benchmarks that test:\n\n* Complex multi-step data transformations\n* Context-aware operations (where the same instruction means different things based on data context)\n* Cross-column reasoning and relationships\n* Domain-specific transformations that require understanding the semantic meaning of data\n\n  \nHas anyone come across benchmarks or datasets that test these more sophisticated data transformation capabilities?\n\n",
    "author": "metalvendetta",
    "timestamp": "2025-09-06T15:00:44",
    "url": "https://reddit.com/r/datascience/comments/1nac35j/how_to_evaluate_data_transformations/",
    "score": 3,
    "num_comments": 14,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n8z37l",
    "title": "Just got rejected from meta",
    "content": "Thought everything went well. Completed all questions for all interviews. Felt strong about all my SQL, A/B testing, metric/goal selection questions. No red flags during behavioral. Interviews provided 0 feedback about the rejection. I was talking through all my answers and reasoning, considering alternatives and explaining why I chose my approach over others. I led the discussions and was very proactive and always thinking 2 steps ahead and about guardrail metrics and stating my assumptions. The only ways I could think of improving was to answer more confidently and structure my thoughts more. Is it just that competitive right now? Even if I don’t make IC5 I thought for sure I’d get IC4. Anyone else interview with Meta recently? \n\nedit:\nMS degree\n3.5yoe DS\n4.5yoe ChemE\n\nedit2:\nI had 2 meta referrals but didn't use them. Should I tell the recruiter or does it not matter at this point?\nMeta recruiter reached out to me on LinkedIn.\n\nedit3:\nI remember now there was 1 moment I missed a beat, but recovered during a bernoulli distribution hand-calculation question. Maybe thats all it took...\n\nedit4:\nThanks everyone for the copium, words of advice, and support.",
    "author": "vtfresh",
    "timestamp": "2025-09-05T00:50:18",
    "url": "https://reddit.com/r/datascience/comments/1n8z37l/just_got_rejected_from_meta/",
    "score": 299,
    "num_comments": 153,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n8lhdx",
    "title": "MIT says AI isn’t replacing you… it’s just wasting your boss’s money",
    "content": "",
    "author": "CryoSchema",
    "timestamp": "2025-09-04T13:37:39",
    "url": "https://reddit.com/r/datascience/comments/1n8lhdx/mit_says_ai_isnt_replacing_you_its_just_wasting/",
    "score": 571,
    "num_comments": 62,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n88v2y",
    "title": "Almost 2 years into my first job... and already disillusioned and bored with this career",
    "content": "**TL;DR: I find this industry to be very unengaging, with most use cases and positions being very brainless, sluggish and just uninspiring. I am only 2 years into this job and bored and I feel like I need to shake things up a bit to keep doing this for the rest of my life.** \n\n\n\n\nFull disclosure: **this is very much a first world problem**. I get paid quite well, I have incredibly lenient work life balance, I work from home 3 days a week, etc etc. Most people would kill to be in my position at my age.\n\n\nSome context: I was originally in academia doing a PhD in math, but pure math, completely unrelated to ML or anything in the real world really. ~2 years in, I was disillusioned with that (sensing a pattern here lol) so I took as many ML courses I could and jumped ship to industry. \n\n\nRegardless of all the problems I had in academia, it at least *asked* something of me. I had to think, like, *actually think*, about complex, interesting stuff. It felt like I was actually engaging my mind and growing. \n\n\nMy current job is fine, basically applying LLMs for various use cases at a megacorp. On paper, I'm playing with the latest, greatest, tech, but in practice, I'm just really calling APIs on products that smarter people are building. \n\n\nI feel like I haven't actually flexed my brain muscles in years now, I'm forgetting all the stuff I've learnt at college, and the work itself is incredibly boring to me. Many many days I can barely bring myself to work as the work is so uninteresting, and the bare minimum I put in still somehow impresses my colleagues so there's no real incentive to work hard. \n\n\nI realize how privileged that sounds, I really do, but I do feel kind of unfulfilled and spiritually empty. I feel like if I keep doing this for the rest of my life I will look back with regret. \n\n\n**What I'm trying to do to fix this:** I would like to shift towards more cutting edge and harder data science. Problem here is a lack of qualifications and experience. I have a MS and a BS in Math (from T10 colleges) but no PhD and the math I studied was mostly pure/theoretical, very little to do with ML. \n\nI'm trying to do projects in my own time, but it's slow going on my own. I would love to aim for ML/AI research roles, but it feels like an impossible ask without a PhD, without papers, etc etc. I'm not sure that's a feasible goal. \n\n\n\nAnother thing I've been considering is playing a DS/ML role as support in research that's *not* ML. For instance, bioinformatics or biotech, etc. This is also fairly appealing to me. The main issue is here is a complete lack of knowledge about these fields (since there can be so many fields here) and a lack of domain knowledge which I presume is required. I'm still trying, I've been applying for some bioinformatics roles, but yeah, also hard. \n\n\n\n**Has anyone else felt this way? What did they do about it, and what would you recommend?**",
    "author": "ShittyLogician",
    "timestamp": "2025-09-04T05:32:48",
    "url": "https://reddit.com/r/datascience/comments/1n88v2y/almost_2_years_into_my_first_job_and_already/",
    "score": 284,
    "num_comments": 114,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n8a1do",
    "title": "A portfolio project for Data Scientists looking to add AI Engineering skills (Pytest, Security, Docker).",
    "content": "Hey guys,\n\nLike many of us, I'm comfortable in a Jupyter Notebook, but I found there's a huge gap when it comes to building and deploying a real, full-stack AI application. I created a project specifically to bridge that gap.\n\nYou build a \"GitHub Repo Analyst\" agent, but the real learning is in the production-level engineering skills that often aren't part of a data science workflow:\n\n- Automated Testing: Writing Pytest integration tests to verify your agent's security.\n- Building UIs: Creating an interactive web app with Chainlit.\n- Deployment: Packaging your entire application with Docker for easy, reproducible deployment.\n\nI've turned this into a 10-lesson guide and am looking for 10-15 beta testers. If you're a data scientist who wants to add a serious AI engineering project to your portfolio, I'll give you the complete course for free in exchange for your feedback.\n\nJust comment below if you're interested, and I'll send you a DM.",
    "author": "petburiraja",
    "timestamp": "2025-09-04T06:24:34",
    "url": "https://reddit.com/r/datascience/comments/1n8a1do/a_portfolio_project_for_data_scientists_looking/",
    "score": 75,
    "num_comments": 110,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n81chu",
    "title": "What's up with LinkedIn posts saying \"Excel is dead\", \"dashboards are dead\", \"data science is dead\", \"PPTs are dead\" and so on?",
    "content": "Is this a trend now? I also read somewhere \"SQL is dead\" too. Ffs. What isn't dead anyway for these Linkfluencers? Only LLMs? And then you hear mangers and leadership parrtoting the same LinkedIn bullshit in team meetings... where is all this going? ",
    "author": "OverratedDataScience",
    "timestamp": "2025-09-03T22:05:25",
    "url": "https://reddit.com/r/datascience/comments/1n81chu/whats_up_with_linkedin_posts_saying_excel_is_dead/",
    "score": 138,
    "num_comments": 101,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n81hrr",
    "title": "How are you liking Positron?",
    "content": "I’m an undergraduate student double majoring in Data Analytics and Data Engineering and have used VSCode, Jupyter Notebook, Google Colab, and PyCharm Community Edition during my different Python courses. I haven’t used Positron yet, but it looks really appealing since I enjoy the VSCode layout and notebook style programming. Anyone with experience using Position, I’d greatly appreciate any information on how you’ve liked (or not liked) it. Thanks!",
    "author": "LilParkButt",
    "timestamp": "2025-09-03T22:14:10",
    "url": "https://reddit.com/r/datascience/comments/1n81hrr/how_are_you_liking_positron/",
    "score": 26,
    "num_comments": 21,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n83iok",
    "title": "Would you volunteer to join the team building AI tooling? If you have what has been your experience?",
    "content": "I just learned a colleague that was part of the AI tooling team is leaving and I am considering whether to ask to be added to their old project team. \n\nI am a data scientist and while I have not had too many ML projects recently, I have some lined up for next quarter. \n\nTheir team was building the tooling to build agents for use internally and customer facing. That team has obviously gotten a lot of shout out from the CEO. Their early products are well received. \n\nI prefer ML over AI tooling but also feel there is a new reality for my next job in that I should be above average in AI usage and development. And thus I feel that being part of the AI team would be beneficial for my career. \n\nSo my question is. Should I ask to join the AI team? Have others done this - what has been experienced? Anything to look out for/any ways to shape the my potential journey in that team? ",
    "author": "Final_Alps",
    "timestamp": "2025-09-04T00:19:47",
    "url": "https://reddit.com/r/datascience/comments/1n83iok/would_you_volunteer_to_join_the_team_building_ai/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n7ops6",
    "title": "Freelance search",
    "content": "Any website to work as freelancer besides upwork ?",
    "author": "Gold-Artichoke-9288",
    "timestamp": "2025-09-03T12:39:45",
    "url": "https://reddit.com/r/datascience/comments/1n7ops6/freelance_search/",
    "score": 2,
    "num_comments": 21,
    "upvote_ratio": 0.58,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n6so7m",
    "title": "I built a simulation tool for students to learn causal inference!",
    "content": "\\- Building a good intuition for causal inference methods requires you to play around with assumptions and data, but getting data from a paper and replicating the results takes time.   \n\\- **I made a simulation tool to help students quickly build an intuition for these methods (currently only difference-in-difference is available).** This tool is great for the undergraduate level (as I am still a student so the content covered isn't super advanced)\n\nThis is still a proof-of-concept, but would love your feedback and what other methods you would like to see!\n\nLink: [https://causal-buddy.streamlit.app/](https://causal-buddy.streamlit.app/)",
    "author": "Technical-Note-4660",
    "timestamp": "2025-09-02T12:14:27",
    "url": "https://reddit.com/r/datascience/comments/1n6so7m/i_built_a_simulation_tool_for_students_to_learn/",
    "score": 164,
    "num_comments": 25,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n70lcz",
    "title": "A/B Testing Overview",
    "content": "Sharing this as a guide on A/B Testing. I hope that it can help those preparing for interviews and those unfamiliar with the wide field of experimentation.\n\nAny feedback would be appreciated as we're always on a learning journey.",
    "author": "joshamayo7",
    "timestamp": "2025-09-02T17:35:36",
    "url": "https://reddit.com/r/datascience/comments/1n70lcz/ab_testing_overview/",
    "score": 39,
    "num_comments": 8,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n7zgzy",
    "title": "Per row context understanding is hard for SQL and RAG databases, here's how we solved it with LLMs",
    "content": "Traditional databases rely on RAG and vector databases or SQL-based transformations/analytics. But will they be able to preserve per-row contextual understanding?\n\nWe’ve released Agents as part of Datatune:\n\n[https://github.com/vitalops/datatune](https://github.com/vitalops/datatune)\n\nIn a single prompt, you can define multiple tasks for data transformations, and Datatune performs the transformations on your data at a per-row level, with contextual understanding.\n\nExample prompt:\n\n\"Extract categories from the product description and name. Keep only electronics products. Add a column called ProfitMargin = (Total Profit / Revenue) \\* 100\"\n\nDatatune interprets the prompt and applies the right operation (map, filter, or an LLM-powered agent pipeline) on your data using OpenAI, Azure, Ollama, or other LLMs via LiteLLM.\n\nKey Features\n\n\\- Row-level map() and filter() operations using natural language\n\n\\- Agent interface for auto-generating multi-step transformations\n\n\\- Built-in support for Dask DataFrames (for scalability)\n\n\\- Works with multiple LLM backends (OpenAI, Azure, Ollama, etc.)\n\n\\- Compatible with LiteLLM for flexibility across providers\n\n\\- Auto-token batching, metadata tracking, and smart pipeline composition\n\nToken &amp; Cost Optimization\n\n\\- Datatune gives you explicit control over which columns are sent to the LLM, reducing token usage and API cost:\n\n\\- Use input\\_fields to send only relevant columns\n\n\\- Automatically handles batching and metadata internally\n\n\\- Supports setting tokens-per-minute and requests-per-minute limits\n\n\\- Defaults to known model limits (e.g., GPT-3.5) if not specified\n\n\\- This makes it possible to run LLM-based transformations over large datasets without incurring runaway costs.",
    "author": "metalvendetta",
    "timestamp": "2025-09-03T20:24:05",
    "url": "https://reddit.com/r/datascience/comments/1n7zgzy/per_row_context_understanding_is_hard_for_sql_and/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n6ez8o",
    "title": "Is it wrong to be specialized in specific DS niche?",
    "content": "Hello fellows Data Scientists!\nI’m coming with question/discussion about specialization in specific part of Data Science. For a long time my main duty is time series and predictive projects, mainly around finance but in retail domain. As an example, project where I predict sales per hour for month up front, later I place matrix with amount of staff needed on specific station to minimize number of employees present in the location (lot of savings in labor costs). Lately I attended few interviews, that didn’t go flawlessly from my side - most of questions were around classification problems, where most of my knowledge is in regression problems, of course I’m blaming myself on every attempt where I didn’t receive an offer because of technical interview and there is no discussion that I could prepare myself in more broad knowledge. But here comes my question, is it possible to know deeply every kind of niche knowledge when your main work spins around specific problems? I’m sure there are lot of DS which work for past 10 years or so and because of number of projects they’re familiar with a lot of specific problems, but for someone with 3 yoe is it doable? I feel like I’m very good in tackling time series problems, but as an example, my knowledge in image recognition is very limited, did you face problem like that? What are your thoughts? How did you overcome this in your career?",
    "author": "jesteartyste",
    "timestamp": "2025-09-02T02:33:59",
    "url": "https://reddit.com/r/datascience/comments/1n6ez8o/is_it_wrong_to_be_specialized_in_specific_ds_niche/",
    "score": 36,
    "num_comments": 55,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n70y9a",
    "title": "Diffusion models",
    "content": "What position do Diffusion models take in the spectrum of architectures to AGI like compared to jepa, auto-regressive modelling and others ? are they RL-able ?",
    "author": "FreakedoutNeurotic98",
    "timestamp": "2025-09-02T17:52:14",
    "url": "https://reddit.com/r/datascience/comments/1n70y9a/diffusion_models/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n6cug7",
    "title": "The Hidden Costs of Naive Retrieval",
    "content": "We often treat Retrieval-Augmented Generation (RAG) as the default solution for knowledge-intensive tasks, but the naive 'retrieve-then-read' paradigm has significant hidden costs that can hurt, rather than help, performance. So, when is it better not to retrieve?\n\nThis series on **Adaptive RAG** starts by exploring the hidden costs of our default RAG implementations by looking at three key areas:\n\n* **The Practical Problems:** These are the obvious unnecessary latency and compute overhead for simple or popular queries where the LLM's parametric memory would have been enough.\n* **The Hidden Dangers:** There are more subtle risks to quality. Noisy or misleading context can lead to \"External Hallucinations,\" where the retriever itself induces factual errors in an otherwise correct model.\n* **The Foundational Flaws:** Finally, the \"retrieval advantage\" can shrink as models scale.",
    "author": "SirCasms",
    "timestamp": "2025-09-02T00:12:42",
    "url": "https://reddit.com/r/datascience/comments/1n6cug7/the_hidden_costs_of_naive_retrieval/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n5eqdj",
    "title": "Weekly Entering &amp; Transitioning - Thread 01 Sep, 2025 - 08 Sep, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-08-31T21:01:46",
    "url": "https://reddit.com/r/datascience/comments/1n5eqdj/weekly_entering_transitioning_thread_01_sep_2025/",
    "score": 11,
    "num_comments": 29,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n4n1qz",
    "title": "How do I prepare for my data science job as a new grad?",
    "content": "I just graduated from my bachelors in May. Recently, I’ve been fortunate enough to receive an offer as a data scientist I at a unicorn where most of the people on the ds team have PhDs. My job starts in a month and I’m having massive imposter syndrome, especially since my coding skills are kinda shit. I can barely do leetcode mediums. The job description is also super vague, only mentioning ML models and data analysis, so idk what specific things I should brush up on. What can I do in this month to make sure I do a good job?",
    "author": "ExcitingCommission5",
    "timestamp": "2025-08-30T22:57:00",
    "url": "https://reddit.com/r/datascience/comments/1n4n1qz/how_do_i_prepare_for_my_data_science_job_as_a_new/",
    "score": 101,
    "num_comments": 36,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n4rufc",
    "title": "Let’s Build Something Together",
    "content": "Hey everyone,\n\nAfter my last post about my struggles in finding a remote job, I was honestly blown away. I got over 50 messages not with job offers, but with stories, frustrations, and suggestions. The common theme? Many of us are stuck. Some are trying to break into the market, others are trying to move within it, and many just want to *make something meaningful*.\n\nThat really got me thinking: since this subreddit is literally about connecting data scientists, engineers, PMs, MLOps folks, researchers, and builders of all kinds why don’t we **actually build something together**?\n\nIt doesn’t have to be one massive project; it could be multiple smaller ones. The goal wouldn’t just be to pad CVs, but to collaborate, learn, and create something that matters. Think hackathon energy, but async and community-driven with no time limits and frustration.\n\nI am personally interested to get involved with things i haven't been yet. Mlops,Deployment,Cloud,Azure,pytorch,Apache for example. Everyone can find their opening and what they want to improve and try and work with other experience people on this that could help them.\n\nThis would literally need\n\n* Data scientists / analysts\n* Software engineers\n* MLOps / infra people\n* Project managers\n* Researchers / scientists\n* Anyone who wants to contribute\n\nBuild something real with others (portfolio &gt; buzzwords)\n\n* Show initiative and collaboration on your CV/LinkedIn\n* Make connections that could lead to opportunities\n* Turn frustration into creation\n\nI’d love to hear your thoughts:\n\n* Would you be interested in joining something like this?\n* What kind of projects would excite you (open-source tools, research collabs, data-for-good, etc.)?\n* Should we organize a first call/Discord/Slack group to test the waters? I am waiting for connecting with you on Linkedin and here.\n\nPS1: Yeah I am not talkig about creating a product or building the new chatgpt. Just communication and brainstorming . Working on some ideas or just simply get to know some people. ",
    "author": "Fantastic-Trouble295",
    "timestamp": "2025-08-31T04:02:32",
    "url": "https://reddit.com/r/datascience/comments/1n4rufc/lets_build_something_together/",
    "score": 37,
    "num_comments": 51,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n4bamu",
    "title": "Advice for DS/AS/MLE interviews",
    "content": "I am looking for data scientist (ML heavy), applied scientist or ML engineer roles in product based companies. For my interview preperation, I am unsure about which book or resources to pick so that I can cover the rigor of ML rounds in these interviews. I have background in CS and have fair knowledge of ML. Anyone who cracked such roles or have any experience that can help me? \n\nPS: I was considering reading Kevin Murphy's ML book but it is too heavy on math so I am not sure if that much of rigor is required for these kind of interviews. I am not looking for research roles. ",
    "author": "alpha_centauri9889",
    "timestamp": "2025-08-30T13:10:15",
    "url": "https://reddit.com/r/datascience/comments/1n4bamu/advice_for_dsasmle_interviews/",
    "score": 45,
    "num_comments": 20,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n4ecoo",
    "title": "Career Dilemma",
    "content": "",
    "author": "NervousVictory1792",
    "timestamp": "2025-08-30T15:23:29",
    "url": "https://reddit.com/r/datascience/comments/1n4ecoo/career_dilemma/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n3jnpw",
    "title": "How do you design a test to compare two audience targeting methods?",
    "content": "So we have two audiences we want to test against each other. The first is one we're currently using and the second is a new audience. We want to know if a campaign using the new audience targeting method can match or exceed an otherwise identical campaign using our current targeting.\n\nWe're conducting the test on Amazon DSP and the Amazon representative recommended basically intersecting each audience with a randomized set of holdout groups. So for audience A the test cell will be all users in audience A and also in one group of randomized holdouts and similarly for audience B (with a different set of randomized holdouts)\n\nOur team's concern is that if each campaign is getting a different set of holdout groups then we wouldn't have the same baseline. My boss is recommending we use the same set of holdout groups for both. \n\nMy personal concern for that is if we'd have a proper isolation (e.g. if one user sees an ad from the campaign using audience A and also an ad from the campaign using audience B, then which audience targeting method gets credit). I think my boss' approach is probably the better design, but the overlap issue stands out to me as a complication.  \n\nI'll be honest that I've never designed an A/B test before, much less on audiences, so any help at all is appreciated. I've been trying to understand how other platforms do this because Amazon does seem a bit different - as in, how (in an ideal universe) would you test two audiences against each other?",
    "author": "PathalogicalObject",
    "timestamp": "2025-08-29T14:22:32",
    "url": "https://reddit.com/r/datascience/comments/1n3jnpw/how_do_you_design_a_test_to_compare_two_audience/",
    "score": 21,
    "num_comments": 12,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n2fmqs",
    "title": "Stanford study finds that AI has already started wiping out new grad jobs",
    "content": "",
    "author": "nullstillstands",
    "timestamp": "2025-08-28T08:37:48",
    "url": "https://reddit.com/r/datascience/comments/1n2fmqs/stanford_study_finds_that_ai_has_already_started/",
    "score": 265,
    "num_comments": 55,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n2s1v1",
    "title": "Shopify Applied Machine Learning Engineer Pair Programming Interview",
    "content": "Has anyone done the pair programming interview with Shopify? \n\nCurrently interviewing for a Machine Learning Engineer position and the description is really vague.  \nAll I know is that I can use AI tools and that they don't like Leetcode.  \nIt will be pair programming and bring your own IDE, but beyond this I really have no idea what to expect and how to prepare.\n\n  \nMy interview is in a week - I'd really appreciate any guidance and help, thank you!\n\n(also based in Canada, flair says US only for some reason)",
    "author": "sg6128",
    "timestamp": "2025-08-28T16:44:51",
    "url": "https://reddit.com/r/datascience/comments/1n2s1v1/shopify_applied_machine_learning_engineer_pair/",
    "score": 18,
    "num_comments": 3,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n2s33v",
    "title": "Shopify Applied Machine Learning Engineer Pair Programming Interview",
    "content": "Has anyone done the pair programming interview with Shopify? \n\nCurrently interviewing for a Machine Learning Engineer position and the description is really vague.  \nAll I know is that I can use AI tools and that they don't like Leetcode.  \nIt will be pair programming and bring your own IDE, but beyond this I really have no idea what to expect and how to prepare.\n\n  \nMy interview is in a week - I'd really appreciate any guidance and help, thank you!\n\n(also based in Canada, flair says US only for some reason)",
    "author": "sg6128",
    "timestamp": "2025-08-28T16:46:26",
    "url": "https://reddit.com/r/datascience/comments/1n2s33v/shopify_applied_machine_learning_engineer_pair/",
    "score": 12,
    "num_comments": 0,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n2o7c1",
    "title": "Free 1,000 CPU + 100 GPU hours for testers",
    "content": "I believe it should be dead simple for data scientists, analysts, and researchers to scale their code in the cloud without relying on DevOps. At my last company, whenever the data team needed to scale workloads, we handed it off to DevOps. They wired it up in Airflow DAGs, managed the infrastructure, and quickly became the bottleneck. When they tried teaching the entire data team how to deploy DAGs, it fell apart and we ended up back to queuing work for DevOps.\n\nThat experience pushed me to build cluster compute software that makes scaling dead simple for any Python developer. With a single function you can deploy to massive clusters (10k vCPUs, 1k GPUs). You can bring your own Docker image, define hardware requirements, run jobs as background tasks you can fire and forget, and kick off a million simple functions in seconds.\n\nIt’s [open source](https://github.com/Burla-Cloud/burla) and I’m still making install easier, but I also have a few managed versions.\n\nRight now I’m looking for test users running embarrassingly parallel workloads like data prep, hyperparameter tuning, batch inference, or Monte Carlo simulations. If you’re interested, email me at [**joe@burla.dev**]() and I’ll set you up with a managed cluster that includes 1,000 CPU hours and 100 GPU hours.\n\nHere’s an example of it in action: I spun up 4k vCPUs to screenshot 30k arXiv PDFs and push them to GCS in just a couple minutes: [https://x.com/infra\\_scale\\_5/status/1938024103744835961](https://x.com/infra_scale_5/status/1938024103744835961?utm_source=chatgpt.com)\n\nWould love testers.",
    "author": "Ok_Post_149",
    "timestamp": "2025-08-28T14:03:04",
    "url": "https://reddit.com/r/datascience/comments/1n2o7c1/free_1000_cpu_100_gpu_hours_for_testers/",
    "score": 4,
    "num_comments": 6,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n1tk23",
    "title": "Rejected after 3rd round live coding OA round",
    "content": "As the title says, I made it to the 3rd round interview for a Staff DS role. Thought I was doing well, but I bombed the coding portion, I only managed to outline my approach instead of producing actual code. That’s on me, mostly because I’ve gotten used to relying on GPT to crank out code for me over the last two years. Most of what I do is build POCs, check hypotheses, then have GPT generate small snippets that I review for logic before applying it. I honestly haven’t done “live coding” in a while.\n\nBefore the interview, I prepped with DataLemur for the pandas related questions and brushed up on building simple NNs and GNNs from scratch to cover the conceptual/simple DS side. A little bit on the transformer module as well to have my bases cover if they ask for it. I didn’t expect a LeetCode-style live coding question. I ended up pseudo-coding it, then stumbling hard when I tried to actually implement it.\n\nGot the rejection email today. Super heartbreaking to see. Do I go back to live-coding and memorizing syntax and practicing leetcodes for upcoming future DS interview?",
    "author": "1234okie1234",
    "timestamp": "2025-08-27T14:21:48",
    "url": "https://reddit.com/r/datascience/comments/1n1tk23/rejected_after_3rd_round_live_coding_oa_round/",
    "score": 93,
    "num_comments": 65,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n1zo5y",
    "title": "Why is Typescript starting to gain adoption in AI?",
    "content": "I've noticed that, increasingly, using TypeScript has become more common for AI tools. For example, Langgraph has Langgraph.js for Typescript developers. Same with OpenAI's Agents SDK.\n\nI've also seen some AI engineer job openings for roles that use both Python and Typescript.\n\nPython still seems to be dominant, but it seems like Typescript is definitely starting to gain traction in the field. So why is this? Why the appeal of building AI apps in Typescript? It wasn't originally like this with more traditional ML / deep learning, where Python was so dominant.\n\nWhy is it gaining increasing adoption and what's the appeal?\n\n",
    "author": "Illustrious-Pound266",
    "timestamp": "2025-08-27T18:48:26",
    "url": "https://reddit.com/r/datascience/comments/1n1zo5y/why_is_typescript_starting_to_gain_adoption_in_ai/",
    "score": 23,
    "num_comments": 41,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n105of",
    "title": "Airbnb Data",
    "content": "Hey everyone,\n\nI work on the data team at [AirROI](https://www.airroi.com). For a while, we offered free datasets for about **250** cities, but we always wanted to do more for the community. Recently, we just expanded our free public dataset from \\~250 to nearly **1000** global Airbnb markets on **properties** and **pricing data**. As far as we know, this makes it the single **largest free Airbnb dataset** ever released on the internet.\n\nYou can browse the collection and download here, no sign-up required: [Airbnb Data](http://www.airroi.com/data-portal)\n\n\n\n**What’s in the data?**\n\nFor each market (cities, regions, etc.), the CSV dumps include:\n\nProperty Listings: Details like room type, amenities, number of bedrooms/bathrooms, guest capacity, etc.\n\nPricing Data: This is the cool part. We include historical rates, future calendar rates (for investment modeling), and minimum/maximum stay requirements.\n\nHost Data: Host ID, superhost status, and other host-level metrics.\n\n\n\n**What can you use it for?**\n\nThis is a treasure trove for:\n\nTrend Analysis: Track pricing and occupancy trends across the globe.\n\nInvestment &amp; Rental Arbitrage Analysis: Model potential ROI for properties in new markets.\n\nAcademic Research: Perfect for papers on the sharing economy, urban development, or tourism.\n\nPortfolio Projects: Build a killer dashboard or predictive model for your GitHub.\n\nGeneral Data Wrangling Practice: It's real, messy, world-class data.\n\n\n\n**A quick transparent note**: If you need hyper-specific or real-time data for a region not in the free set, we do have a ridiculously cheap [Airbnb API](https://www.airroi.com/api) to get more customized data. Alternatively, if you are a researcher who wants a larger customized data just reach out to us, we'll try our best to support!\n\n  \nIf you require something that's not currently in the free dataset please comment below, we'll try to accommodate within reason.\n\nHappy analyzing and go building something cool!\n\n\n\n[Airbnb Data](https://preview.redd.it/vi9bjqphxflf1.png?width=3038&amp;format=png&amp;auto=webp&amp;s=6953d029e8bc9aa21280b411df543d3b5bbc3d66)\n\n[Download Airbnb Data](https://preview.redd.it/ydtx5oqjxflf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=bb4f4dfc361d83734a1c088750d8167e1327bdae)\n\n",
    "author": "jason-airroi",
    "timestamp": "2025-08-26T15:36:44",
    "url": "https://reddit.com/r/datascience/comments/1n105of/airbnb_data/",
    "score": 328,
    "num_comments": 39,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n28ukj",
    "title": "I built Runcell - an AI agent for Jupyter that actually understands your notebook context",
    "content": "I've been working on something called Runcell that I think fills a gap I was frustrated with in existing AI coding tools.\n\n**What it is:** Runcell is an AI agent that lives inside JupyterLab (can be used as an extension) and can understand the full context of your notebook - your data, charts, previous code, kernel state, etc. Instead of just generating code, it can actually edit and execute specific cells, read/write files, and take actions on its own.\n\n**Why I built it:** I tried Cursor and Claude Code, but they mostly just generate a bunch of cells at once without really understanding what happened in previous steps. When I'm doing data science work, I usually need to look at the results from one cell before deciding what to write next. That's exactly what Runcell does - it analyzes your previous results and decides what code to run next based on that context.\n\n**How it's different:**\n\n* vs AI IDEs like Cursor: Runcell focuses specifically on building context for Jupyter environments instead of treating notebooks like static files\n* vs Jupyter AI: Runcell is more of an autonomous agent rather than just a chatbot - it has tools to actually work and take actions\n\nYou can try it with just `pip install runcell`.\n\nI'm looking for feedback from the community. Has anyone else felt this frustration with existing tools? Does this approach make sense for your workflow?",
    "author": "Sudden_Beginning_597",
    "timestamp": "2025-08-28T03:44:16",
    "url": "https://reddit.com/r/datascience/comments/1n28ukj/i_built_runcell_an_ai_agent_for_jupyter_that/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n17500",
    "title": "What exactly is \"prompt engineering\" in data science?",
    "content": "I keep seeing people talk about prompt engineering, but I'm not sure I understand what that actually means in practice.\n\nIs it just writing one-off prompts to get a model to do something specific? Or is it more like setting up a whole system/workflow (e.g. using LangChain, agents, RAG, etc.) where prompts are just one part of the stack in developing an application?\n\nFor those of you working as data scientists:\n- Are you actively building internal end-to-end agents with RAG and tool integrations (either external like MCP or creating your own internal files to serve as tools)?\n\n- Is prompt engineering part of your daily work, or is it more of an experimental/prototyping thing?",
    "author": "IronManFolgore",
    "timestamp": "2025-08-26T21:03:39",
    "url": "https://reddit.com/r/datascience/comments/1n17500/what_exactly_is_prompt_engineering_in_data_science/",
    "score": 68,
    "num_comments": 56,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n191lg",
    "title": "NVIDIA AI Released Jet-Nemotron: 53x Faster Hybrid-Architecture Language Model Series",
    "content": "NVIDIA Jet-Nemotron is a new LLM series which is about 50x faster for inferencing. The model introduces 3 main concept :\n\n* **PostNAS**: a new search method that tweaks only attention blocks on top of pretrained models, cutting massive retraining costs.\n* **JetBlock**: a dynamic linear attention design that filters value tokens smartly, beating older linear methods like Mamba2 and GLA.\n* **Hybrid Attention**: keeps a few full-attention layers for reasoning, replaces the rest with JetBlocks, slashing memory use while boosting throughput.\n\nVideo explanation : [https://youtu.be/hu\\_JfJSqljo](https://youtu.be/hu_JfJSqljo)\n\nPaper : [https://arxiv.org/html/2508.15884v1](https://arxiv.org/html/2508.15884v1)",
    "author": "Technical-Love-8479",
    "timestamp": "2025-08-26T22:54:32",
    "url": "https://reddit.com/r/datascience/comments/1n191lg/nvidia_ai_released_jetnemotron_53x_faster/",
    "score": 11,
    "num_comments": 7,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n035we",
    "title": "Is the market really like this? The reality for a recent graduate looking for opportunities.",
    "content": "Hello . I’m a recent Master of Science in Analytics graduate from Georgia Tech (GPA 3.91, top 5% of my class). I completed a practicum with Sandia Labs and I’m currently in discussions about further research with GT and SANDIA. I’m originally from Greece and I’ve built a strong portfolio of projects, ranging from classic data analysis and machine learning to a Resume AI chatbot.\n\nI entered the job market feeling confident, but I’ve been surprised and disappointed by how tough things are here. The Greek market is crazy: I’ve seen openings that attract 100 applicants and still offer very low pay while expecting a lot. I’m applying to junior roles and have gone as far as seven interview rounds that tested pandas, PyTorch, Python, LeetCode-style problems, SQL, and a lot of behavioral and technical assessments.\n\nRemote opportunities seem rare on EUROPE or US. I may be missing something, but I can’t find many remote openings.\n\nThis isn’t a complaint so much as an expression of frustration. It’s disheartening that a master’s from a top university, solid skills, hands-on projects, and a real practicum can still make landing a junior role so difficult. I’ve also noticed many job listings now list deep learning and PyTorch as mandatory, or rebrand positions as “AI engineer,” even when it doesn’t seem necessary.\n\nOn a positive note, I’ve had strong contacts reach out via LinkedIn  though most ask for relocation, which I can’t manage due to family reasons.\n\nI’m staying proactive: building new projects, refining my interviewing skills, and growing my network. I’d welcome any advice, referrals, or remote-friendly opportunities. Thank you!\n\nPS. If you comment your job experience state your country to get a picture of the worldwide problem.\n\nPS2. Started as an attempt for networking and opportunities, came down to an interesting realistic discussion. Still sad to read, what's the future of this job? What will happen next? What recent grads and on university juniors should be doing? \n\nPs3. If anyone wants to connect send me a message ",
    "author": "Fantastic-Trouble295",
    "timestamp": "2025-08-25T14:24:24",
    "url": "https://reddit.com/r/datascience/comments/1n035we/is_the_market_really_like_this_the_reality_for_a/",
    "score": 208,
    "num_comments": 134,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n0ke01",
    "title": "InternVL 3.5 released : Best MultiModal LLM, ranks 3 overall",
    "content": "InternVL 3.5 has been released, and given the benchmark, the model looks to be the best multi-model LLM, ranking 3 overall just behind Gemini 2.5 Pro and GPT-5.  Multiple variants released ranging from 1B to 241B\n\n*Processing img 5v5hfeg9wclf1...*\n\nThe team has introduced a number of new technical inventions, including *Cascade RL, Visual Resolution Router,  Decoupled Vision-Language Deployment.*  \n\nModel weights : [https://huggingface.co/OpenGVLab/InternVL3\\_5-8B](https://huggingface.co/OpenGVLab/InternVL3_5-8B)\n\nTech report : [https://arxiv.org/abs/2508.18265](https://arxiv.org/abs/2508.18265)\n\nVideo summary : [https://www.youtube.com/watch?v=hYrdHfLS6e0](https://www.youtube.com/watch?v=hYrdHfLS6e0)",
    "author": "Technical-Love-8479",
    "timestamp": "2025-08-26T05:24:16",
    "url": "https://reddit.com/r/datascience/comments/1n0ke01/internvl_35_released_best_multimodal_llm_ranks_3/",
    "score": 11,
    "num_comments": 6,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mzzzu7",
    "title": "We are back with many Data science jobs in Soccer, NFL, NHL, Formula1 and more sports! 2025-08",
    "content": "Hey guys,\n\nI've been silent here lately but many opportunities keep appearing and being posted.\n\nThese are a few from the last 10 days or so\n\n* [Quantitative Analyst Associate (Spring/Summer 2026) - Philadelphia Phillies](http://www.sportsjobs.online/jobs/9015-quantitative-analyst-associate-spring-summer-2026?utm_source=sportsjobs-online.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&amp;_bhlid=24b4748bef795f9a693e2911693d223c99632356)\n* [Senior Sports Data Scientist - ESPN](http://www.sportsjobs.online/jobs/9018-senior-sports-data-scientist?utm_source=sportsjobs-online.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&amp;_bhlid=58166f06c2cb14a5f60c555a80e63eff791ece6a)\n* [Baseball Analyst/Data Scientist - Miami Marlins](http://www.sportsjobs.online/jobs/9014-baseball-analyst-data-scientist?utm_source=sportsjobs-online.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&amp;_bhlid=7d5181c9bd523683761c79ffcd23fafab8877728)\n* [Data Engineer, Athletics - University of Pittsburgh](http://www.sportsjobs.online/jobs/8992-data-engineer-athletics?utm_source=sportsjobs-online.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&amp;_bhlid=90aede97e283411c5e9a31b34a982299320cc5e6)\n* [Senior Data Scientist - Tottenham Hotspur](http://www.sportsjobs.online/jobs/8997-senior-data-scientist?utm_source=sportsjobs-online.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&amp;_bhlid=e35ef1aeb7939cd356689d46e49afdff95535e1a)\n* [Sports Scientist - Human Data Science - McLaren Racing](http://www.sportsjobs.online/jobs/8996-sports-scientist-human-data-science?utm_source=sportsjobs-online.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&amp;_bhlid=e40bd45b1f6178064b5c7cf165f65e5821c8ad0d)\n* [Lead Engineer - Phoenix Suns](http://www.sportsjobs.online/jobs/8981-lead-engineer?utm_source=sportsjobs-online.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&amp;_bhlid=841248c85b1774cec3812e308b803fbcaa9b570e)\n* [Business Intelligence Intern - Houston Texans](http://www.sportsjobs.online/jobs/8967-business-intelligence-intern?utm_source=sportsjobs-online.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&amp;_bhlid=35c476cde3ddf380fbd3d5f4beccd3424bdcb356)\n* [Technical Data Analyst - Portland Timbers](http://www.sportsjobs.online/jobs/8953-technical-staff-data-analyst-mls?utm_source=sportsjobs-online.beehiiv.com&amp;utm_medium=newsletter&amp;utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&amp;_bhlid=14f0d07bcd9a80d670a7cc018bb6d16d6e2e9c2b)\n\nI run www.sportsjobs(.)online, a job board in that niche. In the last month I added around 300 jobs.\n\nFor the ones that already saw my posts before, I've added more sources of jobs lately. I'm open to suggestions to prioritize the next batch.\n\nIt's a niche, there aren't thousands of jobs as in Software in general but my commitment is to **keep improving a simple metric, jobs per month.** We always need some metric in DS..\n\nI run also a newsletter to receive emails with jobs and interesting content on sports analytics (next edition tomorrow!)  \n[https://sportsjobs-online.beehiiv.com/subscribe](https://sportsjobs-online.beehiiv.com/subscribe)\n\nFinally, I've created also a [reddit community](https://www.reddit.com/r/sports_jobs/) where I post recurrently the openings if that's easier to check for you.\n\nI hope this helps someone!",
    "author": "fark13",
    "timestamp": "2025-08-25T12:23:56",
    "url": "https://reddit.com/r/datascience/comments/1mzzzu7/we_are_back_with_many_data_science_jobs_in_soccer/",
    "score": 116,
    "num_comments": 22,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n0ep0g",
    "title": "How do I make the most of this opportunity",
    "content": "Hello everyone, I’m a senior studying data science at a large state school. Recently, through some networking, I got to interview with a small real estate and financial data aggregator company with around \\~100 employees.\n\nI met with the CEO for my interview. As far as I know, they haven’t had an engineering or science intern before, mainly marketing and business interns. The firm has been primarily a more traditional real estate company for the last 150 years. Many tasks are done through SQL queries and Excel. Much of the product team at the company has been there for over 20 years and is resistant to change.\n\nThe ceo wants to make the company more efficient and modern, and implement some statistical and ML models and automated workflows with their large amounts of data. He has given me some of the ideas that he and others at the company have considered. I will list those at the end. But I am starting to feel that I’m a bit in over my head here as he hinted towards using my work as a proof of concept to show the board that these new technologies and techniques r what the company needs to stay relevant and competitive. As someone who is just wrapping up their undergrad, some of it feels beyond my abilities if I’m mainly going to be implementing a lot of these things solo.\n\n  \nThese are some of the possible projects I would work on:\n\n\n\n#  Chatbot Knowledge Base Enhancement\n\n**Background**: The Company is deploying AI-powered chatbots (HubSpot/CoPilot) for customer engagement and internal knowledge access. Current limitations include incomplete coverage of FAQs and inconsistent performance tracking.\n\n**Objective**: Enhance chatbot functionality through improved training, monitoring, and analytics.\n\n**Scope**:\n\n* Automate FAQ training using internal documentation.\n* Log and classify failed responses for continuous improvement.\n* Develop a performance dashboard.\n\n**Deliverables**:\n\n* Enhanced training process.\n* Error classification system.\n* Prototype dashboard.\n\n**Value**: Improves customer engagement, reduces staff workload, and provides analytics on chatbot usage.\n\n\n\n# Automated Data Quality Scoring\n\n**Background**: Clients demand AI-ready datasets, and the company must ensure high data quality standards.\n\n**Objective**: Prototype an automated scoring system for dataset quality.\n\n**Scope**:\n\n* Metrics: completeness, duplicates, anomalies, missing metadata.\n* Script to evaluate any dataset.\n\n**Intern Fit**: Candidate has strong Python/Pandas skills and experience with data cleaning.\n\n**Deliverables**:\n\n* Reusable script for scoring.\n* Sample reports for selected datasets.\n\n**Value**: Positions the company as a provider of AI-ready data, improving client trust.\n\n  \nEntity Resolution Prototype\n\n**Background**: The company datasets are siloed (deeds, foreclosures, liens, rentals) with no shared key.\n\n**Objective**: Prototype entity resolution methods for cross-dataset linking.\n\n**Scope**:\n\n* Fuzzy matching, probabilistic record linkage, ML-based classifiers.\n* Apply to limited dataset subset.\n\n**Intern Fit**: Candidate has ML and data cleaning experience but limited production-scale exposure.\n\n**Deliverables**:\n\n* Prototype matching algorithms.\n* Confidence scoring for matches.\n* Report on results.\n\n**Value**: Foundation for the company's long-term, unique master identifier initiative.\n\n  \nPredictive Micro-Models\n\n**Background**: Predictive analytics represents an untapped revenue stream for the company.\n\n**Objective**: Build small predictive models to demonstrate product potential.\n\n**Scope**:\n\n* Predict foreclosure or lien filing risk.\n* Predict churn risk for subscriptions.\n\n**Intern Fit**: Candidate has built credit risk models using XGBoost and regression.\n\n**Deliverables**:\n\n* Trained models with evaluation metrics.\n* Prototype reports showcasing predictions.\n\n**Value**: Validates feasibility of predictive analytics as a company product.\n\n\n\n# Generative Summaries for Court/Legal Documents\n\n**Background**: Processing court filings is time-intensive, requiring manual metadata extraction.\n\n**Objective**: Automate structured metadata extraction and summary generation using NLP/LLM.\n\n**Scope**:\n\n* Extract entities (names, dates, amounts).\n* Generate human-readable summaries.\n\n**Intern Fit**: Candidate has NLP and ML experience through research work.\n\n**Deliverables**:\n\n* Prototype NLP pipeline.\n* Example structured outputs.\n* Evaluation of accuracy.\n\n**Value**: Reduces operational costs and increases throughput.\n\n  \nAutomation of Customer Revenue Analysis\n\n**Background**: The company currently runs revenue analysis scripts manually, limiting scale.\n\n**Objective**: Automate revenue forecasting and anomaly detection.\n\n**Scope**:\n\n* Extend existing forecasting models.\n* Build anomaly detection.\n* Dashboard for finance/sales.\n\n**Intern Fit**: Candidate’s statistical background aligns with forecasting work.\n\n**Deliverables**:\n\n* Automated pipeline.\n* Interactive dashboard.\n\n**Value**: Improves financial planning and forecasting accuracy.\n\n  \nData Product Usage Tracking\n\n**Background**: Customer usage patterns are not fully tracked, limiting upsell opportunities.\n\n**Objective**: Prototype a product usage analytics system.\n\n**Scope**:\n\n* Track downloads, API calls, subscriptions.\n* Apply clustering/churn prediction models.\n\n**Intern Fit**: Candidate’s experience in clustering and predictive modeling fits well.\n\n**Deliverables**:\n\n* Usage tracking prototype.\n* Predictive churn model.\n\n**Value**: Informs sales strategies and identifies upsell/cross-sell opportunities.\n\n  \nAI Policy Monitoring Tool\n\n**Background**: The company has implemented an AI Use Policy, requiring compliance monitoring.\n\n**Objective**: Build a prototype tool that flags non-compliant AI usage.\n\n**Scope**:\n\n* Detect unapproved file types or sensitive data.\n* Produce compliance dashboards.\n\n**Intern Fit**: Candidate has built automation pipelines before, relevant experience.\n\n**Deliverables**:\n\n* Monitoring scripts.\n* Dashboard with flagged activity.\n\n**Value**: Protects the company against compliance and cybersecurity risks.",
    "author": "ChubbyFruit",
    "timestamp": "2025-08-25T23:44:33",
    "url": "https://reddit.com/r/datascience/comments/1n0ep0g/how_do_i_make_the_most_of_this_opportunity/",
    "score": 7,
    "num_comments": 17,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1n0biew",
    "title": "Microsoft released VibeVoice TTS",
    "content": "Microsoft just dropped VibeVoice, an Open-sourced TTS model in 2 variants (1.5B and 7B) which can support audio generation upto 90 mins and also supports multiple speaker audio for podcast generation. \n\nDemo Video : https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ\n\nGitHub : https://github.com/microsoft/VibeVoice",
    "author": "Technical-Love-8479",
    "timestamp": "2025-08-25T20:38:17",
    "url": "https://reddit.com/r/datascience/comments/1n0biew/microsoft_released_vibevoice_tts/",
    "score": 9,
    "num_comments": 0,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mzwdws",
    "title": "\"The Vibes are Off...\" *server logs filling with errors*",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-08-25T10:09:56",
    "url": "https://reddit.com/r/datascience/comments/1mzwdws/the_vibes_are_off_server_logs_filling_with_errors/",
    "score": 61,
    "num_comments": 15,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mzxmx3",
    "title": "Looking to transition to experimentation",
    "content": "Hi all, I am looking to transition from ml analytics generalized roles to more experimentation focused roles. Where to start looking for experimentation heavy roles. I know the market is trash right now, but are there any specific portals that can help find such roles. Also usually faang is very popular for such roles, but are there any other companies which would be a good step to make a transition to. ",
    "author": "SmartPizza",
    "timestamp": "2025-08-25T10:55:36",
    "url": "https://reddit.com/r/datascience/comments/1mzxmx3/looking_to_transition_to_experimentation/",
    "score": 14,
    "num_comments": 10,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mzlzsp",
    "title": "First time writing a technical article, would love constructive feedback",
    "content": "Hi everyone,\n\nI recently wrote my first blog post where I share a method I’ve been using to get good results on a fine-grained classification benchmark. This is something I’ve worked on for a while and wanted to put my thoughts together in an article.\n\nI’m sharing it here **not as a promo** but because I’m genuinely looking to improve my writing and make sure my explanations are clear and useful. If you have a few minutes to read and share your thoughts (on structure, clarity, tone, level of detail, or anything else), I’d really appreciate it.\n\nHere’s the link: [https://towardsdatascience.com/a-refined-training-recipe-for-fine-grained-visual-classification/](https://towardsdatascience.com/a-refined-training-recipe-for-fine-grained-visual-classification/)\n\nThanks a lot for your time and feedback!",
    "author": "Bus-cape",
    "timestamp": "2025-08-25T02:39:51",
    "url": "https://reddit.com/r/datascience/comments/1mzlzsp/first_time_writing_a_technical_article_would_love/",
    "score": 11,
    "num_comments": 10,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mz2jgn",
    "title": "Day to day work at lead/principal data scientist",
    "content": "Hi, \n\nI have 9 years of experience in ml/dl. I have been looking for a role in lead/principal ds. Can you tell me what expectations do you guys face at the role.\n\nData science knowledge? \nMl ops knowledge? \nTeam management? \n",
    "author": "sourabharsh",
    "timestamp": "2025-08-24T10:56:46",
    "url": "https://reddit.com/r/datascience/comments/1mz2jgn/day_to_day_work_at_leadprincipal_data_scientist/",
    "score": 66,
    "num_comments": 23,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mzgkc7",
    "title": "Weekly Entering &amp; Transitioning - Thread 25 Aug, 2025 - 01 Sep, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-08-24T21:01:38",
    "url": "https://reddit.com/r/datascience/comments/1mzgkc7/weekly_entering_transitioning_thread_25_aug_2025/",
    "score": 4,
    "num_comments": 25,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mymb21",
    "title": "Google's new Research : Measuring the environmental impact of delivering AI at Google Scale",
    "content": "Google has dropped in a very important research paper measuring the impact of AI on the environment, suggesting how much carbon emission, water, and energy consumption is done for running a prompt on Gemini. Surprisingly, the numbers have been quite low compared to the previously reported numbers by other studies, suggesting that the evaluation framework is flawed. \n\nGoogle measured the environmental impact of **a single Gemini prompt** and here’s what they found:\n\n* **0.24 Wh of energy**\n* **0.03 grams of CO₂**\n* **0.26 mL of water**\n\nPaper : [https://services.google.com/fh/files/misc/measuring\\_the\\_environmental\\_impact\\_of\\_delivering\\_ai\\_at\\_google\\_scale.pdf](https://services.google.com/fh/files/misc/measuring_the_environmental_impact_of_delivering_ai_at_google_scale.pdf)\n\nVideo : [https://www.youtube.com/watch?v=q07kf-UmjQo](https://www.youtube.com/watch?v=q07kf-UmjQo)",
    "author": "Technical-Love-8479",
    "timestamp": "2025-08-23T21:32:51",
    "url": "https://reddit.com/r/datascience/comments/1mymb21/googles_new_research_measuring_the_environmental/",
    "score": 58,
    "num_comments": 13,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mxrbck",
    "title": "NVIDIA new paper : Small Language Models are the Future of Agentic AI",
    "content": "NVIDIA have just published a paper claiming SLMs (small language models) are the future of agentic AI. They provide a number of claims as to why they think so, some important ones being they are cheap. Agentic AI requires just a tiny slice of LLM capabilities, SLMs are more flexible and other points. The paper is quite interesting and short as well to read. \n\nPaper : [https://arxiv.org/pdf/2506.02153](https://arxiv.org/pdf/2506.02153)\n\nVideo Explanation : [https://www.youtube.com/watch?v=6kFcjtHQk74](https://www.youtube.com/watch?v=6kFcjtHQk74)",
    "author": "Technical-Love-8479",
    "timestamp": "2025-08-22T20:52:46",
    "url": "https://reddit.com/r/datascience/comments/1mxrbck/nvidia_new_paper_small_language_models_are_the/",
    "score": 258,
    "num_comments": 22,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mxyprj",
    "title": "Anyone Using Search APIs as a Data Source?",
    "content": "I've been working on a research project recently and have encountered a frustrating issue: the amount of time spent cleaning scraped web results is insane. \n\nHalf of the pages I collect are:  \n\n*  Ads disguised as content  \n* Keyword-stuffed SEO blogs  \n* Dead or outdated links  \n\nWhile it's possible to write filters and regex pipelines, it often feels like I spend more time cleaning the data than actually analyzing it. This got me thinking: instead of scraping, has anyone here tried using structured search APIs as a data acquisition step? \n\nIn theory, the benefits could be significant:  \n\n* Fewer junk pages since the API does some filtering already  \n* Results delivered in structured JSON format instead of raw HTML  \n* Built-in citations and metadata, which could save hours of wrangling  \n\nHowever, I haven't seen many researchers discuss this yet. I'm curious if APIs like these are actually good enough to replace scraping or if they come with their own issues (such as coverage, rate limits, cost, etc.). \n\nIf you've used a search API in your pipeline, how did it compare to scraping in terms of:\n\n* Data quality  \n* Preprocessing time  \n* Flexibility for different research domains  \n\nI would love to hear if this is a viable shortcut or just wishful thinking on my part.",
    "author": "posiela",
    "timestamp": "2025-08-23T04:14:25",
    "url": "https://reddit.com/r/datascience/comments/1mxyprj/anyone_using_search_apis_as_a_data_source/",
    "score": 48,
    "num_comments": 15,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mxpyef",
    "title": "When do we really need an Agent instead of just ChatGPT?",
    "content": "I’ve been diving into the whole “Agent” space lately, and I keep asking myself a simple question: *when does it actually make sense to use an Agent, rather than just a ChatGPT-like interface?*\n\nHere’s my current thinking:\n\n* Many user needs are **low-frequency, one-off, low-risk**. For those, opening a ChatGPT window is usually enough. You ask a question, get an answer, maybe copy a piece of code or text, and you’re done. No Agent required.\n* Agents start to make sense only when certain conditions are met:\n   1. **High-frequency or high-value tasks** → worth automating.\n   2. **Horizontal complexity** → need to pull in information from multiple external sources/tools.\n   3. **Vertical complexity** → decisions/actions today depend on context or state from previous interactions.\n   4. **Feedback loops** → the system needs to check results and retry/adjust automatically.\n\nIn other words, if you don’t have multi-step reasoning + tool orchestration + memory + feedback, an “Agent” is often just a chatbot with extra overhead.\n\nI feel like a lot of “Agent products” right now haven’t really thought through what incremental value they add compared to a plain ChatGPT dialog.\n\nCurious what others think:\n\n* Do you agree that most low-frequency needs are fine with just ChatGPT?\n* What’s your personal checklist for deciding when an Agent is *actually* worth building?\n* Any concrete examples from your work where Agents clearly beat a plain chatbot?\n\nWould love to hear how this community thinks about it.",
    "author": "Rich-Effect2152",
    "timestamp": "2025-08-22T19:41:33",
    "url": "https://reddit.com/r/datascience/comments/1mxpyef/when_do_we_really_need_an_agent_instead_of_just/",
    "score": 55,
    "num_comments": 19,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mxhji7",
    "title": "DS/DA Recruiters, do you approve of my plan",
    "content": "Pivoting away from lab research after I finish my PhD, I'm thinking of taking this approach to landing a DS/DA job:\n\n- Spot an ideal job and study it's requirements.\n\n- Develop all (or most of) the skills associated with that job.\n\n- Compensate for wet-lab-heavy experiences by undertaking projects (even if hypothetical) in said job domain and learn to think like an analyst.\n\nI want to read from recruiters to know what they look for so I can.... Be that 😅 ",
    "author": "DataAnalystWanabe",
    "timestamp": "2025-08-22T13:30:03",
    "url": "https://reddit.com/r/datascience/comments/1mxhji7/dsda_recruiters_do_you_approve_of_my_plan/",
    "score": 6,
    "num_comments": 25,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mwchp8",
    "title": "[Hiring] MLE Position - Enterprise-Grade LLM Solutions",
    "content": "Hey all,  \n  \nI'm the founder of Analytics Depot, and we're looking for a talented Machine Learning Engineer to join our team. We have a premium brand name and are positioned to deliver a product to match. The Home depot of Analytics if you will.  \n  \nWe've built a solid platform that combines LLMs, LangChain, and custom ML pipelines to help enterprises actually understand their data. Our stack is modern (FastAPI, Next.js), our approach is practical, and we're focused on delivering real value, not chasing buzzwords.   \n  \nWe need someone who knows their way around production ML systems and can help us push our current LLM capabilities further. You'll be working directly with me and our core team on everything from prompt engineering to scaling our document processing pipeline. If you have experience with Python, LangChain, and NLP, and want to build something that actually matters in the enterprise space, let's talk.   \n  \nWe offer competitive compensation, equity, and a remote-first environment. DM me if you're interested in learning more about what we're building.  \n",
    "author": "AnalyticsDepot--CEO",
    "timestamp": "2025-08-21T07:28:35",
    "url": "https://reddit.com/r/datascience/comments/1mwchp8/hiring_mle_position_enterprisegrade_llm_solutions/",
    "score": 25,
    "num_comments": 12,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mwdbr8",
    "title": "Where to reference personal projects on my CV?",
    "content": "I havn t work as a data scientist in a long time and I want to get back to the field. I had mostly data analysis missions. I recently did a data science personal project. do I put it in professional experiences in the top of the cv for visibility, or lower in the cv with projects? thanks.",
    "author": "Due-Duty961",
    "timestamp": "2025-08-21T07:58:31",
    "url": "https://reddit.com/r/datascience/comments/1mwdbr8/where_to_reference_personal_projects_on_my_cv/",
    "score": 21,
    "num_comments": 25,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mu3c6j",
    "title": "MIT report: 95% of generative AI pilots at companies are failing",
    "content": "",
    "author": "CanYouPleaseChill",
    "timestamp": "2025-08-18T17:19:28",
    "url": "https://reddit.com/r/datascience/comments/1mu3c6j/mit_report_95_of_generative_ai_pilots_at/",
    "score": 2295,
    "num_comments": 152,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mumd4y",
    "title": "Causal Inference Tech Screen Structure",
    "content": "This will be my first time administering a tech screen for this type of role.\n\nThe HM and I are thinking about formatting this round as more of a verbal case study on DoE within our domain since LC questions and take homes are stupid. The overarching prompt would be something along the lines of \"marketing thinks they need to spend more in XYZ channel, how would we go about determining whether they're right or not?\", with a series of broad, guided questions diving into DoE specifics, pitfalls, assumptions, and touching on high level domain knowledge.\n\nI'm sure a few of you out there have either conducted or gone through these sort of interviews, are there any specific things we should watch out for when structuring a round this way? If this approach is wrong, do you have any suggestions for better ways to format the tech screen for this sort of role? My biggest concern is having an objective grading scale since there are so many different ways this sort of interview can unfold.",
    "author": "save_the_panda_bears",
    "timestamp": "2025-08-19T08:52:55",
    "url": "https://reddit.com/r/datascience/comments/1mumd4y/causal_inference_tech_screen_structure/",
    "score": 36,
    "num_comments": 20,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mv5ojf",
    "title": "Asking for feedback on databases course content",
    "content": "",
    "author": "idan_huji",
    "timestamp": "2025-08-19T22:01:35",
    "url": "https://reddit.com/r/datascience/comments/1mv5ojf/asking_for_feedback_on_databases_course_content/",
    "score": 1,
    "num_comments": 10,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mtehzk",
    "title": "Curious to know about people who switched from DS to DE or SWE or Solutions Architect",
    "content": "Hello, I was just curious to know about people who have switched from DS to DE or SWE or Solutions Architect. If you have done it, what was your rationale behind doing it, what pushed or motivated you for it and how has been your experience after you did it?",
    "author": "explorer_seeker",
    "timestamp": "2025-08-17T23:37:55",
    "url": "https://reddit.com/r/datascience/comments/1mtehzk/curious_to_know_about_people_who_switched_from_ds/",
    "score": 45,
    "num_comments": 38,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1msw56a",
    "title": "Dijkstra defeated: New Shortest Path Algorithm revealed",
    "content": "Dijkstra, the goto shortest path algorithm (time complexity nlogn) has now been outperformed by a new algorithm by top Chinese University which looks like a hybrid of bellman ford+ dijsktra algorithm.\n\nPaper : https://arxiv.org/abs/2504.17033\n\nAlgorithm explained with example : https://youtu.be/rXFtoXzZTF8?si=OiB6luMslndUbTrz",
    "author": "Technical-Love-8479",
    "timestamp": "2025-08-17T09:56:55",
    "url": "https://reddit.com/r/datascience/comments/1msw56a/dijkstra_defeated_new_shortest_path_algorithm/",
    "score": 464,
    "num_comments": 32,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mtbra1",
    "title": "Weekly Entering &amp; Transitioning - Thread 18 Aug, 2025 - 25 Aug, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-08-17T21:01:38",
    "url": "https://reddit.com/r/datascience/comments/1mtbra1/weekly_entering_transitioning_thread_18_aug_2025/",
    "score": 6,
    "num_comments": 27,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mtmvuc",
    "title": "Scared of AI",
    "content": "I have been working with a principal data scientist on a project. Although I am the sole data scientist working on this project  and discussing stuff with him but I am so impressed at his articulate way of thinking. Literally putting his suggestions in chatgpt gives me the code I need. Honestly I am a little scare about AI now. Am I falling behind ?? Just to beat my own drum. I am probably asking the right questions. ",
    "author": "NervousVictory1792",
    "timestamp": "2025-08-18T07:00:53",
    "url": "https://reddit.com/r/datascience/comments/1mtmvuc/scared_of_ai/",
    "score": 0,
    "num_comments": 34,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mr8nwu",
    "title": "How different is \"Senior Data Analyst\" from \"Data Scientist\"?",
    "content": "I often see Senior DA roles that seem focused on using R/Python for analysis (vs. Excel and Power BI), but don't have any insight into the day-to-day of theese roles. \n\nAt the senior level, how different is Data Analyst from Data Scientist? ",
    "author": "empirical-sadboy",
    "timestamp": "2025-08-15T13:02:33",
    "url": "https://reddit.com/r/datascience/comments/1mr8nwu/how_different_is_senior_data_analyst_from_data/",
    "score": 116,
    "num_comments": 56,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mqlp7d",
    "title": "Suspicious ad",
    "content": "Describe the results you want and then have ai manufacture those results for you... who's going to tell them that's not how science works 🤣\n\nDisclosure: I did not read about their tool at all,I just that the advert sounded terribly bad.",
    "author": "CorpusculantCortex",
    "timestamp": "2025-08-14T20:01:59",
    "url": "https://reddit.com/r/datascience/comments/1mqlp7d/suspicious_ad/",
    "score": 77,
    "num_comments": 9,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mq737g",
    "title": "Overfitting on training data time series forecasting on commodity price, test set fine. XGBclassifier. Looking for feedback",
    "content": "Good morning nerds, I’m looking for some feedback I’m sure is rather obvious but I seem to be missing. \n\nI’m using XGBclassifier to predict the direction of commodity x price movement one month the  the future. \n\n~60 engineered features and 3500 rows. \nTarget = one month return &gt; 0.001\n \nClass balance is 0.52/0.48. Backtesting shows an average accuracy of 60% on the test with a lot of variance through testing periods which I’m going to accept given the stochastic nature of financial markets. \n\nI know my back test isn’t leaking, but my training performance is too high, sitting at &gt;90% accuracy. \n\nNot particularly relevant, but hyperparameters were selected with Optuna.\n\nDoes anything jump out as the obvious cause for the training over performance?  \n\n\n",
    "author": "Its_lit_in_here_huh",
    "timestamp": "2025-08-14T10:25:09",
    "url": "https://reddit.com/r/datascience/comments/1mq737g/overfitting_on_training_data_time_series/",
    "score": 99,
    "num_comments": 38,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mq4ai4",
    "title": "Would you jump jobs if you're in fear of a layoff?",
    "content": "EDIT: Just looked and this new company has 2.5 stars out of 600 reviews on Glassdoor. Oof.\n\nCurrently based in the U.S., working remote, medium cost of living area. I make 90k a year and I'm the lead (and only) data scientist / frontend software dev for our area in the company. On top of data science/analyst stuff, I maintain/build our training website for around 500 employees (solo dev as well using React).\n\nThe down side? I work for Medicaid, and if you know what's going on in the United States you know Medicaid is having major cuts, and especially for 2026. We have laid off 300 people this year (so far). I was told \"You have nothing to worry about because your role is so niche\" but I still feel worried.\n\nNew job:\n\n- Pay raise to 115k a year\n\n- Still remote\n\n- I would be working under my current boss who is transitioning to this new company (I have worked with him for 8 years, and the fact that my boss left this current job says something).\n\n- 401k is comparable (3% match), health insurance is better and less cost, PTO is comparable.\n\n- What I'm worried about: He is starting this new department from the ground up. I would be the only data/front-end website guy basically doing what I do in my current role. I'm worried the workload will be too much, or I'm not good enough to start from scratch. Feeling some imposter syndrome here.\n\nThanks for any insight here! This job I am currently at is fun, productive, and I love my team. But I am scared to death of layoffs. The company I am going to now has been around for 25 years, is growing a lot, and has much more \"lasting power\" in my opinion.",
    "author": "tits_mcgee_92",
    "timestamp": "2025-08-14T08:44:35",
    "url": "https://reddit.com/r/datascience/comments/1mq4ai4/would_you_jump_jobs_if_youre_in_fear_of_a_layoff/",
    "score": 97,
    "num_comments": 43,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mqfubv",
    "title": "Time series with value dependent lag",
    "content": "I build models of factories that process liquids. Liquid flows through the factory in various steps and sits in tanks. A tank will have a flow rate in and a flow rate out, a level, and a volume so I can calculate the residence time. It takes ~3 days for liquid to get from the start of the process to the end and it goes through various temperatures, separations, and various other things get added to it along the way. \n\nIf the factory is in a steady state the residence times and lags are relatively easy to calculate. The problem is I am looking at 6 months worth of data and during that time the  rate of the whole facility varies and therefore the residence times vary. If the flow rate goes up residence time goes down. \n\nHow would you adjust the lags based on the flow rates? Chunk the data into months and calculate the lags for each month then concaténate everything? Vary the lags and just drop the overlaps and gaps?",
    "author": "big_data_mike",
    "timestamp": "2025-08-14T15:44:42",
    "url": "https://reddit.com/r/datascience/comments/1mqfubv/time_series_with_value_dependent_lag/",
    "score": 16,
    "num_comments": 19,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mq4sfp",
    "title": "Copy-pasting jupyter notebooks is memory heavy on VSCode",
    "content": "Currently for most of my work, I found out that copy-pasting jupyter notebooks and slightly modifying them is the most effective way to do my work. So basically I have a ipynb for every project I do every day.\n\nHowever, some issues is that they can sometimes get a pretty big memory footprint especially when I have a lot of plots. Like around 1GB per notebook. So sometimes it takes several seconds to a minute to open some files on vscode. I was wondering if there's a way to optimize this?\n\n  \nI saw there's marimo and stuff. Wondering what you guys do.",
    "author": "Affectionate_Use9936",
    "timestamp": "2025-08-14T09:02:41",
    "url": "https://reddit.com/r/datascience/comments/1mq4sfp/copypasting_jupyter_notebooks_is_memory_heavy_on/",
    "score": 41,
    "num_comments": 19,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mpkk2n",
    "title": "Job market getting any better or nah?",
    "content": "I’ve been staying in my role and refusing to leave for the last several years. I’m wondering if there’s any signs yet the job market is coming back yet or if we’re still stuck in the slog",
    "author": "BB_147",
    "timestamp": "2025-08-13T16:41:45",
    "url": "https://reddit.com/r/datascience/comments/1mpkk2n/job_market_getting_any_better_or_nah/",
    "score": 88,
    "num_comments": 78,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mpei2b",
    "title": "How can I gain business acumen as a data scientist?",
    "content": "I can build models, but can I build profits? That’s the gap I’m trying to close.\n\nI’m doing my Master’s in Data Science with a BSc in Computer Science. My technical skills are strong, but I lack business acumen. In interviews, I’ve noticed many questions aren’t just about models or algorithms, but about how those translate into profits or measurable business value.\n\nSenior data scientists seem to connect their work to revenue, retention, or strategy with ease, while I still default to thinking in terms of accuracy and technical metrics. How did you learn to bridge that gap? Did you focus on general business knowledge, industry-specific skills, or hands-on projects?\n\nI want to speak the “language of the business” so my work is not just technically solid but strategically impactful.",
    "author": "Odd_Artist4319",
    "timestamp": "2025-08-13T12:44:58",
    "url": "https://reddit.com/r/datascience/comments/1mpei2b/how_can_i_gain_business_acumen_as_a_data_scientist/",
    "score": 105,
    "num_comments": 53,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mpa610",
    "title": "Research Data Scientists without heavy coding backgrounds (stats, econ, etc), has LLM's improved your workflow?",
    "content": "I remember for a while there were many CS folks saying that Data Science has become software engineering, and that if you aren't fluent in software engineering fundamentals then you're going to fall behind. It became enough of a popular rhetoric that people said they preferred to hire a coder with some math knowledge than a math person with some coding knowledge. \n\nAs a Statistician that works in Research Data Science with an average level of coding experience, enough to write my own code in notebooks, but translating it into a fully fleshed Python module with classes and functions was much more difficult for me. For a while I thought my lack of advanced software engineering knowledge would become a crutch in my career and as someone with a busy personal life I didn't want to spend that much time learning these fundamentals. Then, my company rolled out LLM's integrated into the software we use, like Visual Studio. Suddenly I'm able to create fully fleshed out modules from my notebooks in a flash. I can ask the LLM to write unit tests to test out how my code processes data or test its various subfunctions. I can use it to code up various types of models quickly to compare results. Handing off my code to engineering in the form of a Python package wasn't such a pain anymore. \n\nSure the LLM produces some weird results sometimes, and I do have to spend time making sure I ask it the correct things and/or cleaning up the code so that it works properly. But now I feel like that crutch I had is no longer present.",
    "author": "jambery",
    "timestamp": "2025-08-13T10:05:02",
    "url": "https://reddit.com/r/datascience/comments/1mpa610/research_data_scientists_without_heavy_coding/",
    "score": 146,
    "num_comments": 33,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mpp8sv",
    "title": "What should my job title be",
    "content": "I’ve been in my current role for ~5 months after finishing up my masters in geospatial data science. My official title is Energy Analyst, so essentially a data analyst role in the energy industry. \n\nI feel like the work I do is potentially beyond what is meant for the position (though I’m happy to be told otherwise if that’s not true) and am planning on asking for a title change and raise in the next few months. \n\nWe have a weird set-up where we have a central IT team that supports ~12 implementation contractor teams that work with various utilities. The central IT team owns all of our data and does not allow any sort of read access or api to access data, and only exposes anything through SSRS reports. In theory, the IT team is meant to support a lot of our analytics, but historically they’ve done a pretty bad job at that so I was hired into one of the distributed teams to run their analytics and build out an internal IT capacity. So far that has included the following:\n\n- Recreating a database from the SSRS extracts. So far this is only a few tables in a sqlite3 db so nothing crazy. \n- Developing optimization models in pyomo to inform program design.\n- Lots of ad hoc analysis and reporting. Most of this can be done with some filtering and group-bys but has also included some iterative proportional fitting and other kind of ‘medium difficulty’ methods. \n- creating power bi dashboards as well as a couple java script maplibre-gl-js maps with complex symbology.\n- we accept applications to our program via an online intake, where applicants fill out forms one by one. Most of these applicants submit tens to hundreds of these applications at once. I am working in parallel on a few different potential solutions to this: templates for batch uploading is the easy one, and a potential api integration to pull applications directly from applicant systems is another.\n- looking into creating some llm-agents to automate very simply data extraction. I have already tried automating these processes via dom ids and such but haven’t gotten it to work reliably enough yet. My manager specifically asked for me to try agentic approaches to appease higher ups that we are implementing AI.\n\nI’m not entirely sure where I fall in the landscape of data titles and would appreciate input. I mostly use python with a bit of power query and vanilla excel as well. Very little Java script (just for certain visualizations). Power bi. \n\nEdit to add- I also manage an intern-turned-part-time-employee that supports me in the above tasks basically at my own discretion ",
    "author": "Tyrannosaurus_Secks",
    "timestamp": "2025-08-13T20:16:29",
    "url": "https://reddit.com/r/datascience/comments/1mpp8sv/what_should_my_job_title_be/",
    "score": 12,
    "num_comments": 10,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mq78jd",
    "title": "Getting Master's worth it with T5 Bachelor's?",
    "content": "As a bit of background, I have 2 years of work experience as a Data Scientist, and I have a Bachelor's Degree in Mathematics from a 'top' University: think MIT/Harvard/Princeton.\n\nI'm currently employed. Making about $105k in total comp. I have a feeling I could be doing better compensation wise and even task wise so I've been considering applying to more jobs. \n\nI've noticed a lot of job postings seem to have a minimum requirement of at least a Master's degree, but I'm sort of hesitant to pursue this route right now for a few reasons. For one, master's are expensive, and I don't want to quit my job and go into debt. Secondly, if I were to pursue an online Master's degree, I'm not sure the available options would increase my signal. For example, does a MIT Math Bachelor's -&gt; Texas AM Master's Data Science really boost the resume?\n\nThe only reason I'd get a Master's is for my love of learning, and I'd pursue something theoretical ML oriented and maybe transition into a more research-heavy or even quant role. But I'm not feeling this is an imminent or necessary next step for me.\n\nI'm not trying to be cocky; I'm just trying to get insight from more seasoned people in the field who might be closer to hiring expectations.",
    "author": "Helloiamwhoiam",
    "timestamp": "2025-08-14T10:30:25",
    "url": "https://reddit.com/r/datascience/comments/1mq78jd/getting_masters_worth_it_with_t5_bachelors/",
    "score": 0,
    "num_comments": 17,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mo6ofm",
    "title": "Using Experiment Tracking For Backtests",
    "content": "I’ve used MLFlow as a data scientist, but here it’s being used for managing algo trading backtests and I thought this was an awesome use case. (And these aren’t ML runs, this is testing a momentum strategy).",
    "author": "Clicketrie",
    "timestamp": "2025-08-12T04:56:42",
    "url": "https://reddit.com/r/datascience/comments/1mo6ofm/using_experiment_tracking_for_backtests/",
    "score": 5,
    "num_comments": 6,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mnhsx7",
    "title": "When you edit the massive query someone sent you, forgot where you deleted something, and left a comma behind...",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-08-11T09:32:12",
    "url": "https://reddit.com/r/datascience/comments/1mnhsx7/when_you_edit_the_massive_query_someone_sent_you/",
    "score": 140,
    "num_comments": 9,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mnggxa",
    "title": "Catch-22 followup",
    "content": "I'm following up on my post about \"Catch-22: learning R with projects\"\n\nThank you to all those who responded. The replies were very reassuring.\n\nAfter reading through the replies and reflecting on it, I realised the core of my struggle came from a specific fear that I would have to go through a rigorous coding interview, similar to what software engineers face.\n\nI was picturing a scenario where I'd be given a problem and have to write perfect, memorised R code on the spot without any help. That pressure is what made me feel like I had to absorb every cheat sheet and learn all the syntax before I could even start a project. It created the syntax vs. projects Catch-22 that my original post was about.\n\nFor those who pivoted to data science or data analytics, did you have to go through some sort of coding interview or was it just like any other interview?",
    "author": "DataAnalystWanabe",
    "timestamp": "2025-08-11T08:43:32",
    "url": "https://reddit.com/r/datascience/comments/1mnggxa/catch22_followup/",
    "score": 19,
    "num_comments": 10,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mniapg",
    "title": "Databricks Freea course Recs",
    "content": "Can anyone recommend a great free databricks catalog or otherwise course to level up as a DS using databricks itself? \n\n",
    "author": "tinkinc",
    "timestamp": "2025-08-11T09:50:18",
    "url": "https://reddit.com/r/datascience/comments/1mniapg/databricks_freea_course_recs/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mmzk4s",
    "title": "Catch-22: Learning R through \"hands on\" Projects",
    "content": "\n\nI often get told \"learn data science by doing hands-on projects\" and then I get all fired up and motivated to learn, and then I open up R.... And then I stare at a blank screen because I don't know the syntax from memory. \n\nAnd then I tell myself I'm going to learn the syntax so that I can do projects, but then I get caught up creating folders for each function of dplyr and the subfunctions of that and cheat sheets for this.\n\nAnd then I come across the advice that I shouldn't learn syntax for the sake of learning syntax - I should do hands on projects.\n\nI need projects to learn syntax and I need syntax to start doing projects.\n\n________\n\n\nEdit - Thank you so much to all of you who have replied and I would respond to each one of you but I don't want to sound like a parrot.\n\nThe reassurance that you don't have to have absorbed every R cheat sheet before being a professional Data Scientist/Analyst is very much appreciated. \n\nMy assumption was these data analyst/scientist roles had coding-exams as part of the interview process, which is what stressed me out. Seeing some of you here as experienced analysts who still Google code is very relieving. I am very grateful for each response, and I read each one carefully.",
    "author": "DataAnalystWanabe",
    "timestamp": "2025-08-10T18:06:42",
    "url": "https://reddit.com/r/datascience/comments/1mmzk4s/catch22_learning_r_through_hands_on_projects/",
    "score": 48,
    "num_comments": 33,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mn3338",
    "title": "Weekly Entering &amp; Transitioning - Thread 11 Aug, 2025 - 18 Aug, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-08-10T21:01:33",
    "url": "https://reddit.com/r/datascience/comments/1mn3338/weekly_entering_transitioning_thread_11_aug_2025/",
    "score": 6,
    "num_comments": 37,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mlmwk0",
    "title": "AI isn't taking your job. Executives are.",
    "content": "If AI is ready to replace developers, why aren't developers replacing themselves with AI and just taking it easy at work?\n\nI'm a Director at my company. I'm in the meetings and helping set up the tools that cost people their jobs. Here's how they work:\n\n1. Claude AI writes some code\n\n2. The code gets passed to a developer for validation\n\n3. Since the developer's \"just validating\", he can be replaced with an overseas contractor that'll work for a fraction of the pay\n\nWe've tracked the tools, and we haven't seen any evidence that having Claude take a crack at the code saves anybody any time - but it does let us justify replacing expensive employees with cheap overseas contractors.\n\nYou're not getting replaced by AI.\n\nYour job's being outsourced overseas.",
    "author": "takenorinvalid",
    "timestamp": "2025-08-09T04:13:29",
    "url": "https://reddit.com/r/datascience/comments/1mlmwk0/ai_isnt_taking_your_job_executives_are/",
    "score": 1824,
    "num_comments": 185,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mluc12",
    "title": "Burnout, disillusionment, and imposter syndrome after 1 year in DS. Am I just an API monkey? Reality check needed.",
    "content": "Hey folks,\n\nI am about a year into my first data science job. It took roughly a year and more than 400 applications to land it, so the idea of another long search is scary.\n\nEarly on I worked with an internally built causal AI model that captures relationships for further analysis. I did not build the model. I ran experiments to make it more explainable and easier for others to use. I also built data orchestration pipelines using third party tools that are common in industry and cloud providers like AWS and GCP.\n\nThe last six months have shifted to LLM and NLP work. A lot of API calls, large text analysis. The next six months look even more LLM heavy since I am leading an internal tool build.\n\nOn paper there are wins:\n- I have led projects and designed tools from scratch.\n- My communication and client skills have improved.\n\n\nMy concerns:\n\n- I am not doing much classical DS or rigorous modeling.\n- LLM work often feels like API wrangling rather than technical depth.\n- Work life balance is rough with frequent weekends.\n- Even with a possible 5 to 10 percent raise (possibly within the next 6 months), the work likely stays the same.\n\nI feel imposter syndrome and worry I am behind my peers on fundamentals and interview depth. I’m so burned out and honestly can’t tell if I’m just being a negative Nancy or if my concerns are legit. Am I shortchanging myself by thinking that I'm just not skilled enough? Idk\n\n\nWhat I would love input on:\n\nAm I building valuable skills for the DS market, or am I narrowing myself too much?\n\nWhat types of companies or industries might value this mix of causal modeling, LLM work, and consulting style analysis?\n\nIf I want to keep doors open for more traditional DS or ML roles, what should I focus on learning now?\n\nPortfolio ideas I can ship from my current work that would impress a hiring manager?\n\nWould you ride out six months to finish the tool and try for a promotion, or start looking sooner?\n\nHonest takes are very welcome.\n\n",
    "author": "RookFlame4882",
    "timestamp": "2025-08-09T09:56:51",
    "url": "https://reddit.com/r/datascience/comments/1mluc12/burnout_disillusionment_and_imposter_syndrome/",
    "score": 117,
    "num_comments": 44,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mly9hm",
    "title": "Business focused data science",
    "content": "As a microbiology researcher, I'm far away from the business world. I do more -omics and growth curves and molecular techniques, but I want to move away from biology.\n\nI believe the bridge that can help me do that is data. I have got experience with R and excel. I'm looking at learning SQL and PowerBI.\n\nBut I want to do it away from biology. The problem is, if I was to go from the UK, as a PhD microbiologist, and approach GCC consulting/business analyst recruiters, I get the sense that they'd scoff at me for thinking too highly of my \"transferrable skills\" and tell me that I don't have experience in the world of business.\n\nHow would I get myself job-ready for GCC business-focused data science roles. Is there anyone out there that has made the switch that can share some advice?\n\nThanks in advance",
    "author": "DataAnalystWanabe",
    "timestamp": "2025-08-09T12:40:16",
    "url": "https://reddit.com/r/datascience/comments/1mly9hm/business_focused_data_science/",
    "score": 40,
    "num_comments": 31,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ml6fxs",
    "title": "Just bombed a technical interview. Any advice?",
    "content": "I've been looking for a new job because my current employer is re-structuring and I'm just not a big fan of the new org chart or my reporting line. It's not the best market, so I've been struggling to get interviews. \n\nBut I finally got an interview recently. The first round interview was a chat with the hiring manager that went well. Today, I had a technical interview (concept based, not coding) and I really flubbed it. I think I generally/eventually got to what they were asking, but my responses weren't sharp.* It just sort of felt like I studied for the wrong test. \n\nHow do you guys rebound in situations like this? How do you go about practicing/preparing for interviews? And do I acknowledge my poor performance in a thank you follow up email?\n\n*Example (paraphrasing): They built a model that indicated that logging into a system was predictive of some outcome and management wanted to know how they might incorporate that result into their business processes to drive the outcome. I initially thought they were asking about the effect of requiring/encouraging engagement with this system, so I talked about the effect of drift and self selection on would have on model performance. Then they rephrased the question and it became clear they were talking about causation/correlation, so I talked about controlling for confounding variables and natural experiments.",
    "author": "gonna_get_tossed",
    "timestamp": "2025-08-08T13:40:52",
    "url": "https://reddit.com/r/datascience/comments/1ml6fxs/just_bombed_a_technical_interview_any_advice/",
    "score": 81,
    "num_comments": 59,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mkzhvp",
    "title": "Resources/tips for someone brand new to model building and deployment in Azure?",
    "content": "Context: my current company is VERY (VERY) far behind, technologically. Our data isn't that big and currently resides in SQL Server databases, which I query directly via SSMS.\n\nWhenever a project requires me to build models, my workflow would generally look like:\n\n1. Query the data I need, make features, etc. from SQL Server.\n2. Once I have the data, use Jupyter Notebooks to train/build models. \n3. Use best model to score dataset.\n4. Send dataset/results to stakeholder as a file.\n\nMy company doesn't have a dedicated Dev team (on-shore, at least) nor a DE team. And this workflow works to make ends meet. \n\nNow my company has opened up Azure accounts for me and my manager, but neither one of us have developed anything in it before.\n\nMicrosoft has PLENTY of documentation, but the more I read, the more questions I have, and I feel like my time will be spent reading articles rather than getting anything done.\n\nIt seems like quite a shift from doing everything \"locally\" like what we have been doing to actually using cloud resources. So does anyone have any tips/guides that are beginner-friendly where I can do my entire workflow in the cloud?",
    "author": "redditisthenewblak",
    "timestamp": "2025-08-08T09:14:20",
    "url": "https://reddit.com/r/datascience/comments/1mkzhvp/resourcestips_for_someone_brand_new_to_model/",
    "score": 22,
    "num_comments": 7,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mkmjje",
    "title": "How would you visualize or analyze movements across a categorical grid over time?",
    "content": "I’m working with a dataset where each entity is assigned to one of N categories that form a NxN grid. Over time, entities move between positions (e.g., from “N1” to “N2”).\n\nHas anyone tackled this kind of problem before? I’m curious how you’ve visualized or even clustered trajectory types when working with time-series data on a discrete 2D space.",
    "author": "Proof_Wrap_2150",
    "timestamp": "2025-08-07T22:12:22",
    "url": "https://reddit.com/r/datascience/comments/1mkmjje/how_would_you_visualize_or_analyze_movements/",
    "score": 11,
    "num_comments": 11,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mkdy7a",
    "title": "How do you analyse unbalanced data you get in A/B testing?",
    "content": "Hi \nI have two questions related unbalanced data in A/B testing. Would appreciate resources or thoughts. \n\n1. Usually when we perform A/B testing, we have 5-10% in treatment, after doing power analysis we get the sample size needed, we run tge experiment, by the time we get required sample size for treatment we get way more control samples, so now when we analyse, which samples do we keep in control group? For example by the time we collect 10k samples from treatment we might get 100k samples of control. So what to do now before performing t-test or any kinds of test? \n (In ML we can downsample or over sample but what to do in causal side) \n\n2. Again similar question Lets say we are performing test on 50/50 but if one variant get way more samples as more ppl come through that channel and common for users, hiw do we segment users such as way? And again which samples we keep once we get way more sample than needed? \n\nI want to know how it is tackeled in day to day, and this thing happen frequently right? Or am i wrong? \n\nAlso, what if you get sample size before expected time? (Like was thinking to run them for 2 weeks but got the required size in 10 days) Do you stop the experiment and start analyzing? \n\nSorry for this dumb question but i could not find good answers and honestly don’t trust chat gpt much as many time it hallucinates in this topic. \n\nThanks!",
    "author": "Starktony11",
    "timestamp": "2025-08-07T15:21:21",
    "url": "https://reddit.com/r/datascience/comments/1mkdy7a/how_do_you_analyse_unbalanced_data_you_get_in_ab/",
    "score": 30,
    "num_comments": 27,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mk7lpa",
    "title": "What elective course should I take",
    "content": "Hey all,\n\nAbout to start my last semester for my masters in computer science, with a concentration in AI. I’m a veteran data scientist, this is more of a vanity degree and an ability to say “yes I do have a masters degree” on a job application, but I have enjoyed the studying overall. \n\nI have room for one elective class, and I’m trying to decide what I should take. None of them that fit my schedule seem particularly appealing:\n\n- data analysis: hyper redundant given my background\n- computer networks: possibly useful, but I’d much rather learn something like distributed systems\n- intro to cybersecurity: maybe good, but seems like it would be mostly terminology and not so much a deep dive on anything \n- object oriented design: could be nice for refining my actual design choices, but programming seems like the least valuable skill to upskill on in computer science now (as compared to, say, cloud computing, which is and will continue to be good to know). \n\nIt’s not exactly the most pressing choice, but I thought I’d throw it to Reddit, and see if anyone has a strong opinion on what’s good to learn to augment my ML/AI background\n\nEdit: okay I think you people convinced me. Object oriented design it is! Which sounds a whole lot better than computer networks, that’s for sure. ",
    "author": "Pristine-Item680",
    "timestamp": "2025-08-07T11:15:23",
    "url": "https://reddit.com/r/datascience/comments/1mk7lpa/what_elective_course_should_i_take/",
    "score": 6,
    "num_comments": 19,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mkov0u",
    "title": "\"SemiAuto\" Fully Automated Machine Learning Lifecycle by Just API Calling",
    "content": "So for the last 4 months I have been working on this project which was first supposed to be a upgrade of AutoML, but I later recognised it's potential.\n\nThis project could be one of the best things in ML reasearch, This project is just that good.\n\nFor context, I have the knowledge around ML for about 1.5 years now and thanks to the tools available, I have been able to build a grand project like this,\n\nThe Project's or you can say the Tool name is 'SemiAuto', A full fledged ML lifecycle Automation tool. It has 3 microservice, Regression, Classification, and Clustering.\n\nI have completely build the Version 1 of this project.\n\n\nIt has 6 parts, First ingest the Data.csv file and the target column.\n\nSecond choose whatever preprocessing you want to and apply them.\n\nThird use feature tools to build new features and then SHAP to select the amount of features you want.\n\nFourth choose any algorithm you want with the hyper params and build the model.\n\nFifth choose the optimization technique and get an optimised model.\n\nAt last, get the report, model.pkl, and processor.pkl and use them wherever you want.\n\n\nAs of why this project would be extremely good in research as researchers needs to test with different techniques and different models to get the best thing out and this tool provides that,\n\nThis tool will in a semiautomatic way can fully do each and everything by itself, no coding required.\n\nThe version 2 of this project is in production and I are introducing much more than the previous version,\nFor example, Parallel model building, Simple Ensemble design and Staged Ensemble design.\n\nAnd also the thing that no one as of today has ever implemented in their ML automation tool, Meta-Heuristics Algorithms for feature selection.\n\n\nVersion 2 will be one of the most mind blowingly incredible release of the SemiAuto",
    "author": "Damp_Out",
    "timestamp": "2025-08-08T00:32:35",
    "url": "https://reddit.com/r/datascience/comments/1mkov0u/semiauto_fully_automated_machine_learning/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mizivg",
    "title": "Seeking Meaningful, Non-Profit Data Volunteering Projects",
    "content": "",
    "author": "Astherol",
    "timestamp": "2025-08-06T01:56:31",
    "url": "https://reddit.com/r/datascience/comments/1mizivg/seeking_meaningful_nonprofit_data_volunteering/",
    "score": 28,
    "num_comments": 9,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mhikh4",
    "title": "How can I *give* a good data science/machine learning interview?",
    "content": "I'm around 6 months into my first non intern job and am the only data scientist/MLE in my company. My company has decided they want to bring on some much needed help (thank god) and want me to do \"the more technical side\" of the interview (with others taking care of the behavioral etc)\n\nI do have some questions in mind specific to my job for what I want in a colleague but I still feel a bit underprepared. My plan is to ask the 'basic' questions that I got asked in every interview (classification vs clustering, what is r^2, etc) before asking them how they would solve some of the problems I'm actually working on\n\nBut like that's all I have in the pipeline at the moment, and I'd really like to avoid this becoming the blind interviewing the blind moment.  \n\nDoes anyone have any good tips on how to do the interviews, what to look for or what to include? Thank you!!!!\n\nEDIT: In reply to the DMs, we are not accepting any new applicants at this time 😅",
    "author": "ProbaDude",
    "timestamp": "2025-08-04T09:42:37",
    "url": "https://reddit.com/r/datascience/comments/1mhikh4/how_can_i_give_a_good_data_sciencemachine/",
    "score": 176,
    "num_comments": 40,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1miresg",
    "title": "Share your thought on open source alternative for data robot",
    "content": "Data robot is the market leader when it comes to enterprises data science project life cycle management. But there is no open source alternative available in the market right now. What are the chances of getting a good adoption if I can build the open source alternative of data robot?",
    "author": "vishal-vora",
    "timestamp": "2025-08-05T18:21:40",
    "url": "https://reddit.com/r/datascience/comments/1miresg/share_your_thought_on_open_source_alternative_for/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1miccmb",
    "title": "How I built and deployed a GenAI app in minutes using open‑source tools + Azure",
    "content": "Hey everyone building AI apps always felt like a massive undertaking. So much code, setup, server stuff. I recently tried something different and launched a working GenAI app in just under 15 minutes. I used Dify AI (an open‑source platform) to design the app and Microsoft Azure to deploy it.\n\nWhat I learned:\n• No heavy DevOps or managing servers\n• Very user‑friendly interface—just plug in your AI logic\n• Scales automatically via Azure cloud resources\n\nWould love to hear if anyone’s tried Dify AI or other open‑source builders for AI—and what challenges you faced!\n\nFull details in this write‑up:\nhttps://medium.com/@techlatest.net/launch-genai-apps-in-minutes-with-techlatest-dify-ai-on-azure-cloud-platform-8307bccf4aed\n\nHappy to answer questions or breakdown steps if interested 😊",
    "author": "techlatest_net",
    "timestamp": "2025-08-05T08:30:16",
    "url": "https://reddit.com/r/datascience/comments/1miccmb/how_i_built_and_deployed_a_genai_app_in_minutes/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mh3i7n",
    "title": "Weekly Entering &amp; Transitioning - Thread 04 Aug, 2025 - 11 Aug, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-08-03T21:01:42",
    "url": "https://reddit.com/r/datascience/comments/1mh3i7n/weekly_entering_transitioning_thread_04_aug_2025/",
    "score": 7,
    "num_comments": 57,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mgsshu",
    "title": "Personal projects and skill set",
    "content": "Hi everyone,\nI was just wondering how do you guys specify personal acquired skills from your personal projects in your CV.\nI’m in the midst of a pretty large project - end to end pipeline for predicting real time probabilities of winning chances in a game. This includes a lot of tools, from scraping, database management (mostly tables creations, indexing, nothing DBA-like), scheduling, training, prediction and data drift pipelines, cloud hosting, etc. and I was wondering how I can specify those skills after I finish my project, because I do learn tons from this project. To say I’m using some of those tools in my current job is not entirely right so…\n\nWhat would you say?\nCheers.",
    "author": "indie-devops",
    "timestamp": "2025-08-03T12:56:10",
    "url": "https://reddit.com/r/datascience/comments/1mgsshu/personal_projects_and_skill_set/",
    "score": 25,
    "num_comments": 11,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mgfcke",
    "title": "Built this out of pure laziness for all my Feature engineering/model training jobs",
    "content": "Built this out of pure laziness \nA lightweight Telegram bot that lets me: \n- Get Databricks job alerts\n- Check today’s status\n- Repair failed runs\n- Pause/reschedule ,\nAll from my phone.\nNo laptop. No dashboard. Just / Commands.",
    "author": "Anu_Rag9704",
    "timestamp": "2025-08-03T02:50:17",
    "url": "https://reddit.com/r/datascience/comments/1mgfcke/built_this_out_of_pure_laziness_for_all_my/",
    "score": 58,
    "num_comments": 10,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mgxgpl",
    "title": "Is there a term for internal processing vs data that needs to be stakeholding/customer facing?",
    "content": "For example I had my physical credit card stolen. I was trying to get information from the CC company about when the card was used so that the local PD could check security cameras. (We thought it was particular person so they made a little bit more effort). When I called the credit card company, the customer service person started telling me these random times that made no sense and I realized he was reading the wrong column which were basically the time the charge was converted from “?” to an actual money transfer. I assume to him it gave insight into how to refund each charge so “relvant” just not “relvant” information I would ever need to know.\n\nTwo years later, I am setting up a model with my team and we batting around terms to differentiate between data like these dates &amp; times that are relvant but are not relvant un-manipulated or laid bare for the stakeholder to see visualized or be discussed outside of our team.\n\nYou can hear the inevitable pause from a team member every time the concept comes up as they attempt a new word.  While it was amusing it’s starting to eat at me. Any ideas?",
    "author": "CleanDataDirtyMind",
    "timestamp": "2025-08-03T16:08:55",
    "url": "https://reddit.com/r/datascience/comments/1mgxgpl/is_there_a_term_for_internal_processing_vs_data/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mh9tvo",
    "title": "What would be a better job Position ? Data Scientist or AI/ML Engineer.",
    "content": "",
    "author": "SharePlayful1851",
    "timestamp": "2025-08-04T03:31:54",
    "url": "https://reddit.com/r/datascience/comments/1mh9tvo/what_would_be_a_better_job_position_data/",
    "score": 0,
    "num_comments": 19,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mgxbio",
    "title": "Algorithm Idea",
    "content": "This sudden project has fallen on my lap where I have a lot of survey results and I have to identify how many of those are actually done by bots. I haven’t see what kind of data the survey holds but I was wondering how can I accomplish this task. A quick search points me towards anomaly detections algorithms like isolation forest and dbscan clusters. Just wanted to know if I am headed in the right direction or can I use any LLM tools. TIA :) ",
    "author": "NervousVictory1792",
    "timestamp": "2025-08-03T16:02:26",
    "url": "https://reddit.com/r/datascience/comments/1mgxbio/algorithm_idea/",
    "score": 0,
    "num_comments": 18,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mgrvsh",
    "title": "Hi! i am a junior dev need advice regarding fraud/risk scoring (not credit) on my rules based fraud detection system.",
    "content": "so i our team has developed a rules based fraud detecton system....now we have received a new requirement that we have to score every transaction as how much risky or if flagged as fraud how much fraud it is.\n\ni did some research and i found out its easier if it is a supervisied operation but in my case i wont be able to access prod transaction data due to policy.\n\nnow i have 2 problems data which i guess i have to make a fake one.\n\n2nd how to score i was thinking of going witb regression if i keep my target value bete 0 and 1 but realised that the model can predict above that\nthen thought of classification and use predict_proba() to get prediction probability.\n\nor isolation forest\n\ntill now thats what i bave you thought what else shoudl i consider any advices or guidance to set me in the right path so i dont get any rework\n",
    "author": "1_plate_parcel",
    "timestamp": "2025-08-03T12:19:25",
    "url": "https://reddit.com/r/datascience/comments/1mgrvsh/hi_i_am_a_junior_dev_need_advice_regarding/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mf44ek",
    "title": "Using a hybrid role in job title (Data Science and Engineer)",
    "content": "I have an BS and MS in data science and got hired as a data analyst for a small ish scale company for about a year now as my first job. I'm the only data person in the entire company and I've been wanting to transition into a data science focused role for awhile, so I have been using DS and DE principles at every opportunity to boost my resume. This has ended up extending far beyond the typical DA responsibilities as I have been utilizing a lot of stats modeling and predictive analytics over company data/KPIs, using MLOps occasionally, as well as building ETL pipelines, managing the internal DBMS and streamlining data acquisition through RESTful APIs with contracted third parties. I still do excel monkey work/tableau dashboards along with this.\n\nManagement ended up taking notice and since nobody in the building has any familiarity with data science/tech, they have asked me to rewrite my job description including my job title as a semi promotion. Since I have been working as a bit of a hybrid between DS and DE I am wondering if I should put the new contracted job title as a hybrid role (e.g. Data Science Engineer) or just pick one? My department head has suggested the title of Data Architect but I don't really think that aligns with my job responsibilities and it's also a senior sounding position which feels strange to take on considering I've only been in the industry for a year.",
    "author": "pokelord13",
    "timestamp": "2025-08-01T11:22:53",
    "url": "https://reddit.com/r/datascience/comments/1mf44ek/using_a_hybrid_role_in_job_title_data_science_and/",
    "score": 50,
    "num_comments": 17,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mets4m",
    "title": "How to convert data to conceptual models",
    "content": "I am not sure if I am in the right subreddit, so please by patient with me.\n\nI am working on a tool to reverse-engineer conceptual models from existing data. The idea is you take a legacy system, collect sample data (for example JSON messages communicated by the system), and get a precise model from them. The conceptual model can be then used to develop new parts of the system, component replacements, build documentation, tests, etc...\n\nOne of the open issues I struggle with is the fully-automated conversion from 'packaging' model to conceptual model.\n\nWhen some data is uploaded, it's model reflects the packaging mechanism, rather than the concepts itself. For example. if I upload JSON-formatted data, the model initially consists of objects, arrays, and values. For XML, it is elements and attributes. And so on.\n\n[JSON messages consist of objects, arrays, and values](https://preview.redd.it/rq6k13ej2egf1.png?width=737&amp;format=png&amp;auto=webp&amp;s=415800ea39e0b408f91124f5d03fab02b631e75e)\n\nI can convert the keys, levels, paths to detect concepts and their relationships.  It can look something like this:\n\n[Data structures converted to concepts](https://preview.redd.it/r1d2ti683egf1.png?width=695&amp;format=png&amp;auto=webp&amp;s=0927e6222a90412d7dd5b722fdb43ad07b49e027)\n\n  \nThe issue I am struggling with is that this conversion is not straightforward. Sometimes, it helps to use keys, other times it is better to use paths. For some YAML files, I need to treat the keys as values (typically package.yaml samples).\n\nDid anyone tried to convert data to conceptual models before? Any real-word use cases?\n\nIs there any theory at least about the reverse direction - use conceptual model and map it into XML schema / JSON schema / YAML ... ?\n\nThanks in advance.",
    "author": "JumbleGuide",
    "timestamp": "2025-08-01T04:19:20",
    "url": "https://reddit.com/r/datascience/comments/1mets4m/how_to_convert_data_to_conceptual_models/",
    "score": 9,
    "num_comments": 6,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mem3t5",
    "title": "Generative AI shell interface for browsing and processing data?",
    "content": "So vibe coding is a thing, and I'm not super into it.\n\nHowever, I often need to write little scripts and parsers and things to collect and analyze data in a shell environment for various code that I've written.  It might be for debugging, or just collecting production science data.  Writing that shit is a real pain, because you need to be careful about exceptions and errors and folder names and such.\n\nIs there a way to do \"vibe data gathering\" where I can ask some LLM to write me a script that does a number of things like open up a couple thousand files that fit various properties in various folders, parse them for specific information, then draw say a graph?  ChatGPT can of course do that, but it needs to know the folder structure and examine the files to see what issues there are in collecting this information.  Any way I can do this without having to roll my sleeves up?",
    "author": "Minotaar_Pheonix",
    "timestamp": "2025-07-31T20:37:46",
    "url": "https://reddit.com/r/datascience/comments/1mem3t5/generative_ai_shell_interface_for_browsing_and/",
    "score": 2,
    "num_comments": 10,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1me934o",
    "title": "Why is there no Cursor/Windsurf for Notebooks or Google Collab?",
    "content": " Last week, I tried Windsurf to build a web application and OMG my world was changed. I have used AI tools before but having an agent that implements the code for you is a game changer, my productivity probably went up x5 or x10 times. \n\nThis made me think why is there nothing like this for a data scientist workflow? I know you can do notebook markdown but it is still not the same because Cursor cannot see outputs of your graphs. Also, this tool wouldn’t work on Google Collab where I have access to powerful GPUs. \n\nNow, imagine if you have a tool that goes from a prompt “make the predictive model to predict customer churn” and instead of something like Chatgpt giving you one slob of generic BS that will definitely give out an error, an agent goes and executes each cell one by one: making plots, studying the data, modifying the outliers etc. and adjusting the plan as it goes before finally making a few models and testing them. Basically, the standard data science workflow. \n\nI would like to build something this (I have no idea how yet lol) if there is interest in this community. What do you guys think? Those of you who are working in the field, would you actually use it? \n\nAlso, if someone wants to build it with me, DM me. ",
    "author": "giantwaterwithice",
    "timestamp": "2025-07-31T11:16:29",
    "url": "https://reddit.com/r/datascience/comments/1me934o/why_is_there_no_cursorwindsurf_for_notebooks_or/",
    "score": 10,
    "num_comments": 43,
    "upvote_ratio": 0.61,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mdf6fn",
    "title": "My take on the Microsoft paper",
    "content": "I read the paper myself (albeit pretty quickly) and tried to analyze the situation for us Data Scientists.\n\nThe jobs on the list, as you can intuitively see (and it is also explicitly mentioned in the paper), are mostly jobs that require writing reports and gathering information because, as the paper claims, AI is good at it.\n\nIf you check the chart present in the paper (which I linked in this post), you can see that the clear winner in terms of activities done by AI is “Gathering Information”, while “Analyzing Data” instead is much less impacted and also most of it is people asking AI to help with analysis, not AI doing them as an agent (red bar represents the former, blue bar the latter).\n\nIt seems that our beloved occupation is in the list mainly because it involves gathering information and writing reports. However, the data analysis part is much less affected and that’s just data analysis, let alone the more advanced tasks that separate a Data Scientist from a Data Analyst.\n\nSo, from what I understand, Data Scientists are not at risk. The things that AI does do not represent the actual core of the job at all, and are possibly even activities that a Data Scientist wants to get rid of.\n\nIf you’ve read the paper too, I’d appreciate your feedback. Thanks!",
    "author": "FinalRide7181",
    "timestamp": "2025-07-30T11:57:25",
    "url": "https://reddit.com/r/datascience/comments/1mdf6fn/my_take_on_the_microsoft_paper/",
    "score": 168,
    "num_comments": 22,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1md5gvk",
    "title": "Microsoft just dropped a study showing the 40 jobs most affected by Al and the 40 that Al can't touch (yet).",
    "content": "",
    "author": "timusw",
    "timestamp": "2025-07-30T05:37:47",
    "url": "https://reddit.com/r/datascience/comments/1md5gvk/microsoft_just_dropped_a_study_showing_the_40/",
    "score": 419,
    "num_comments": 164,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mdaa40",
    "title": "Working remote",
    "content": "hey all \ni’ve been a data scientist for a while now, and i’ve noticed my social anxiety has gotten worse since going fully remote since covid. i love the work itself - building models, finding insights etc, but when it comes to presenting those insights, i get really anxious. it’s easily the part of the job i dread most.\n\ni think being remote makes it harder. less day-to-day interaction, fewer casual chats - and it just feels like the pressure is higher when you do have to speak. imposter syndrome also sneaks in at time. tech is constantly evolving, and sometimes i feel like i’m barely keeping up, even though i’m doing the work.\n\ni guess i’m wondering:\n\t•\tdoes anyone else feel this way?\n\t•\thave you found ways to make communications feel less overwhelming?\n\nwould honestly just be nice to hear from others in the same boat. thanks for reading.",
    "author": "itssdgm",
    "timestamp": "2025-07-30T08:53:52",
    "url": "https://reddit.com/r/datascience/comments/1mdaa40/working_remote/",
    "score": 116,
    "num_comments": 40,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1me91rq",
    "title": "FIGMA? Is the tech industry back?",
    "content": "Have you guys heard of this IPO? Stock tripled on debut. What does this company do? \n\nI feel like you tech bros might have a come back soon fyi ",
    "author": "Aristoteles1988",
    "timestamp": "2025-07-31T11:15:05",
    "url": "https://reddit.com/r/datascience/comments/1me91rq/figma_is_the_tech_industry_back/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mdan3p",
    "title": "Model Governance Requests - what is normal?",
    "content": "I’m looking for some advice. I work at a company that provides inference as a service to other customers, specifically we have model outputs in an API. This is used across industries, but specifically when working with Banks, the amount of information they request through model governance is staggering.\n\nI am trying to understand if my privacy team is keeping things too close to the chest, because I find that what is in our standard governance docs, vs the details we are asked, is hugely lacking. It ends up being this ridiculous back and forth and is a huge burn on time and resources. \n\n\nHere are some example questions:\n\n* specific features used in the model \n\n* specific data sources we use\n\n* detailed explanations of how we arrived at our modeling methodology, what other models we considered, the results of those other models, and the rationale for our decision with a comparative analysis\n\n* a list of all metrics used to evaluate model performance, and why we chose those metrics\n\n* time frame for train/test/val sets, to the day\n\nI really want to understand if this is normal, and if my org needs to improve how we report these out to customers that are very concerned about these kinds of things (banks). Are there any resources out there showing what is industry standard? How does your org do it?\n\nThanks",
    "author": "-phototrope",
    "timestamp": "2025-07-30T09:07:20",
    "url": "https://reddit.com/r/datascience/comments/1mdan3p/model_governance_requests_what_is_normal/",
    "score": 5,
    "num_comments": 13,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mcngiy",
    "title": "Python Summer Party (free!): 15-day coding challenge for Data folks",
    "content": "I’ve been cooking up something fun for the summer.. A Python-themed challenge to help Data Scientists &amp; Data Analysts practice and level up their Python skills. Totally free to play!\n\nIt’s called **Python Summer Party**, and it runs for 15 days, starting August 1.\n\nHere’s what to expect:\n\n* One Python challenge + 3 parts per day\n* Focused on Data skills using *NumPy*, *Pandas*, and regular Python\n* All questions based on real companies, so you can practice working with real problems\n* Beginner to intermediate to advanced questions\n* AI chat to help you if you get stuck\n* Discord community (if you still need more help)\n* A chance to win 5 free annual Data Camp subscriptions if you complete the challenges\n* Totally free\n\nI built this because I know how hard it can be to stay consistent when you’re learning alone. Plus, when I was learning Python I couldn't find questions that allowed me to apply Python to realistic business problems.\n\nSo this is meant to be a light, motivating way to practice and have fun with others. *I even tried to design it such that it's cute &amp; fun.*\n\nWould love to have you join us (and hear your feedback if you have any!) \n\n[www.interviewmaster.ai/python-party](http://www.interviewmaster.ai/python-party)",
    "author": "askdatadawn",
    "timestamp": "2025-07-29T14:02:07",
    "url": "https://reddit.com/r/datascience/comments/1mcngiy/python_summer_party_free_15day_coding_challenge/",
    "score": 85,
    "num_comments": 25,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mcd3n5",
    "title": "Since when did “meets” expectations become a bad thing in this industry?",
    "content": "I work at a pretty big named company on west coast. It is pretty shocking to see that in my company anyone who gets “meets” expectations have not been getting any salary increments, not even a dollar each year. I’d think if you are meeting expectations, it means you are holding up your end of the deal and it shouldn’t be a bad thing. But now, you actually have to exceeds expectations to get measly 1% salary raises and sometimes to just keep your job.\n\nDid this used to happen pre covid as well?",
    "author": "Lamp_Shade_Head",
    "timestamp": "2025-07-29T07:35:17",
    "url": "https://reddit.com/r/datascience/comments/1mcd3n5/since_when_did_meets_expectations_become_a_bad/",
    "score": 227,
    "num_comments": 55,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mc2zaz",
    "title": "Does a Data Scientist need to learn all these skills?",
    "content": "* Strong knowledge of Machine Learning, Deep Learning, NLP, and LLMs.\n* Experience with Python, PyTorch, TensorFlow.\n* Familiarity with Generative AI frameworks: Hugging Face, LangChain, MLFlow, LangGraph, LangFlow.\n* Cloud platforms: AWS (SageMaker, Bedrock), Azure AI, and GCP\n* Databases: MongoDB, PostgreSQL, Pinecone, ChromaDB.\n* MLOps tools, Kubernetes, Docker, MLflow.\n\nI have been browsing many jobs and noticed they all are asking for all these skills.. is it the new norm? Looks like I need to download everything and subscribe to a platform that teaches all these lol (cries in pain).",
    "author": "CableInevitable6840",
    "timestamp": "2025-07-28T22:18:53",
    "url": "https://reddit.com/r/datascience/comments/1mc2zaz/does_a_data_scientist_need_to_learn_all_these/",
    "score": 359,
    "num_comments": 181,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mby05c",
    "title": "Any PhDs having trouble in the job market",
    "content": "I am a Math Bio PhD who is currently working for a pharma company. I am trying to look for new positions outside the industry, as it seems most data science work at my current employer and previous employers has been making simple listings for use across the company. It is really boring, and I feel my skillset is not applicable to other data roles. I have taken courses on data engineering and ML and worked on personal projects, but it has yielded little success. I was wondering if any other PhD that are entering the job market or are veterans have had trouble finding a new job in the last few years. Obviously the job market is terrible, but you would think having a PhD would yield better success in finding new positions. I would also like some advice on how to better position myself in the market.",
    "author": "bass581",
    "timestamp": "2025-07-28T18:05:56",
    "url": "https://reddit.com/r/datascience/comments/1mby05c/any_phds_having_trouble_in_the_job_market/",
    "score": 80,
    "num_comments": 105,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mbk933",
    "title": "Why are none of my reports refreshing this morning?",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-07-28T09:04:50",
    "url": "https://reddit.com/r/datascience/comments/1mbk933/why_are_none_of_my_reports_refreshing_this_morning/",
    "score": 254,
    "num_comments": 9,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mb49xm",
    "title": "New Grad Data Scientist feeling overwhelmed and disillusioned at first job",
    "content": "Hi all,\n\nI recently graduated with a degree in Data Science and just started my first job as a data scientist. The company is very focused on staying ahead/keeping up with the AI hype train and wants my team (which has no other data scientists except myself) to explore deploying AI agents for specific use cases. \n\nThe issue is, my background, both academic and through internships, has been in more traditional machine learning (regression, classification, basic NLP, etc.), not agentic AI or LLM-based systems. The projects I’ve been briefed on, have nothing to do with my past experiences and are solely concerned with how we can infuse AI into our workflows and within our products. I’m feeling out of my depth and worried about the expectations being placed on me so early in my career. I was wondering if anyone had advice on how to quickly get up to speed with newer techniques like agentic AI, or how I should approach this situation overall. Any learning resources, mindset tips, or career advice would be greatly appreciated.",
    "author": "insane_membrane13",
    "timestamp": "2025-07-27T19:14:45",
    "url": "https://reddit.com/r/datascience/comments/1mb49xm/new_grad_data_scientist_feeling_overwhelmed_and/",
    "score": 384,
    "num_comments": 104,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mbqiix",
    "title": "Best framework for internal tools",
    "content": "I need frameworks to build standalone internal tools that don’t require spinning up a server. Most of the time I am delivering to non technical users and having them install Python to run the tool is so cumbersome if you don’t have a clue what you are doing. Also, I don’t want to spin up a server for a process that users run once a week, that feels like a waste. PowerBI isn’t meant to execute actions when buttons are clicked so that isn’t really an option. I don’t need anything fancy, just something that users click, it opens up asks them to put in 6 files, runs various logic and exports a report comparing various values across all of those files.\n\n\nTkinter would be a great option besides the fact that it looks like it was last updated in 2000 which while it sounds silly doesn’t inspire confidence for non technical people to use a new tool.\n\nI love Streamlit or Shiny but that would require it to be running 24/7 on a server or me remembering to start it up every morning and monitor it for errors.\n\nWhat other options are out there to build internal tools for your colleagues? I don’t need anything enterprise grade anything, just something simple that less than 30 people would ever use.\n",
    "author": "cptsanderzz",
    "timestamp": "2025-07-28T12:55:10",
    "url": "https://reddit.com/r/datascience/comments/1mbqiix/best_framework_for_internal_tools/",
    "score": 7,
    "num_comments": 9,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mbmnkz",
    "title": "Why autoencoders aren't the answer for image compression",
    "content": "I just finished my engineering thesis comparing different lossy compression methods and thought you might find the results interesting.\n\n**What I tested:**\n\n* Principal Component Analysis (PCA)\n* Discrete Cosine Transform (DCT) with 3 different masking variants\n* Convolutional Autoencoders\n\nAll methods were evaluated at 33% compression ratio on MNIST dataset using SSIM as the quality metric.\n\n**Results:**\n\n* **Autoencoders: 0.97 SSIM** \\- Best reconstruction quality, maintained proper digit shapes and contrast\n* **PCA: 0.71 SSIM** \\- Decent results but with grayer, washed-out digit tones\n* **DCT variants: \\~0.61 SSIM** \\- Noticeable background noise and poor contrast\n\n**Key limitations I found:**\n\n* Autoencoders and PCA require dataset-specific training, limiting universality\n* DCT works out-of-the-box but has lower quality\n* Results may be specific to MNIST's simple, uniform structure\n* More complex datasets (color images, multiple objects) might show different patterns\n\n**Possible optimizations:**\n\n* Autoencoders: More training epochs, different architectures, advanced regularization\n* Linear methods: Keeping more principal components/DCT coefficients (trading compression for quality)\n* DCT: Better coefficient selection to reduce noise\n\n**My takeaway:** While autoencoders performed best on this controlled dataset, the training requirement is a significant practical limitation compared to DCT's universal applicability.\n\n**Question for you:** What would you have done differently in this comparison? Any other methods worth testing or different evaluation approaches I should consider for future work?\n\nThe post with more details about implementation and **visual comparisons** if anyone's interested in the technical details: [https://dataengineeringtoolkit.substack.com/p/autoencoders-vs-linear-methods-for](https://dataengineeringtoolkit.substack.com/p/autoencoders-vs-linear-methods-for)",
    "author": "AipaQ",
    "timestamp": "2025-07-28T10:32:16",
    "url": "https://reddit.com/r/datascience/comments/1mbmnkz/why_autoencoders_arent_the_answer_for_image/",
    "score": 9,
    "num_comments": 13,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mc8w7g",
    "title": "How to use AI effectively and efficiently to code",
    "content": "Any tips on how to teach beginners on how to use AI effectively and efficiently to code?",
    "author": "bandaian",
    "timestamp": "2025-07-29T04:28:51",
    "url": "https://reddit.com/r/datascience/comments/1mc8w7g/how_to_use_ai_effectively_and_efficiently_to_code/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mbm7zn",
    "title": "Tried Wan2.2 on RTX 4090, quite impressed",
    "content": "",
    "author": "Technical-Love-8479",
    "timestamp": "2025-07-28T10:16:15",
    "url": "https://reddit.com/r/datascience/comments/1mbm7zn/tried_wan22_on_rtx_4090_quite_impressed/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mb6ch8",
    "title": "Weekly Entering &amp; Transitioning - Thread 28 Jul, 2025 - 04 Aug, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-07-27T21:01:39",
    "url": "https://reddit.com/r/datascience/comments/1mb6ch8/weekly_entering_transitioning_thread_28_jul_2025/",
    "score": 7,
    "num_comments": 38,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mawalf",
    "title": "why OneHotEncoder give better results than get.dummies/reindex?",
    "content": "**I can't figure out why I get a better score with OneHotEncoder :**\n\npreprocessor = ColumnTransformer(\n\ntransformers=\\[\n\n('cat', categorical\\_transformer, categorical\\_cols)\n\n\\],\n\nremainder='passthrough'  # &lt;-- this keeps the numerical columns\n\n)\n\nmodel\\_GBR =  GradientBoostingRegressor(n\\_estimators=1100, loss='squared\\_error', subsample = 0.35, learning\\_rate = 0.05,random\\_state=1)\n\nGBR\\_Pipeline = Pipeline(steps=\\[('preprocessor', preprocessor),('model', model\\_GBR)\\])\n\n  \n**than get.dummies/reindex:**\n\n  \nX\\_test = pd.get\\_dummies(d\\_test)\n\nX\\_test\\_aligned = X\\_test.reindex(columns=X\\_train.columns, fill\\_value=0)",
    "author": "Due-Duty961",
    "timestamp": "2025-07-27T13:13:17",
    "url": "https://reddit.com/r/datascience/comments/1mawalf/why_onehotencoder_give_better_results_than/",
    "score": 11,
    "num_comments": 17,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1maxkht",
    "title": "Anomoly detection with only categorical variables",
    "content": "Hello everyone, I have an anomoly detection project but all of my data is categorical. I suppose I could try and ask them to change it prediction but does anyone have any advice. The goal is to there are groups within the data and and do an analysis to see anomlies. This is all unsupervised the dataset is large in terms of rows (500k) and I have no gpus.",
    "author": "Routine_Nothing_8568",
    "timestamp": "2025-07-27T14:05:36",
    "url": "https://reddit.com/r/datascience/comments/1maxkht/anomoly_detection_with_only_categorical_variables/",
    "score": 6,
    "num_comments": 12,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1makoge",
    "title": "Can LLMs Reason - I don't know, depends on the definition of reasoning.  Denny Zhou - Founder/Lead of Google Deepmind LLM Reasoning Team",
    "content": "AI influencers: LLMs can think given this godly prompt bene gesserit oracle of the world blahblah, hence xxx/yyy/zzz is dead. See more below.\n\nMeanwhile, literally the founder/lead of the reasoning team: \n\nhttps://preview.redd.it/z9uwnummqeff1.png?width=652&amp;format=png&amp;auto=webp&amp;s=c84727d328d059504adf64768b8badac45d20611\n\nReference: [https://www.youtube.com/watch?v=ebnX5Ur1hBk](https://www.youtube.com/watch?v=ebnX5Ur1hBk) good lecture! ",
    "author": "ArticleLegal5612",
    "timestamp": "2025-07-27T05:08:24",
    "url": "https://reddit.com/r/datascience/comments/1makoge/can_llms_reason_i_dont_know_depends_on_the/",
    "score": 19,
    "num_comments": 36,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1mabzuf",
    "title": "Hyperparameter and prompt tuning via agentic CLI tools like Claude Code",
    "content": "Has anyone used Claude Code as way to automate the improvement of their ML/AI solution?\n\nIn traditional ML, there’s the notion of hyperparameter tuning, whereby you search the source of all possible hyperparameter values to see which combination yields the best result on some outcome metric.\n\nIn LLM systems, the thing that gets tuned is the prompt and the outcome being evaluated is the output of some eval framework.\n\nAnd some systems incorporate both ML and LLM\n\nAll of this iteration can be super time consuming and, in the case of the LLM prompt optimization, quite costly if you are constantly changing the prompt and having to rerun the eval framework.\n\nThe process can be manual or operated automatically by some heuristic.\n\nIt occurred to me the other day that it might be a great idea to get CC to do this iteration instead. If we arm it with the context and a CLI for running experiments with different configs), then it could do the following:\n- ⁠Run its own experiments via CLI\n- Log the results\n- Analyze the results against historical results\n- Write down its thoughts\n- Come up with ideas for future experiments\n- Iterate!\n\nJust wondering if anyone has pulled this off successfully in the past and would care to share :)\n",
    "author": "hendrix616",
    "timestamp": "2025-07-26T20:17:20",
    "url": "https://reddit.com/r/datascience/comments/1mabzuf/hyperparameter_and_prompt_tuning_via_agentic_cli/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m9e3vg",
    "title": "Stuck not doing DS work as a DS",
    "content": "I have been working at a pharma for 5 years. In that time I got my MSDS and did some good work. Issue is, despite stellar yearly reviews I never ever get promoted. Each year I ask for a plan, for a goal to hit , for a reason why, but I always get met with “it just is not in the cards” kind of answer. \n\nI spent 6 months applying for other jobs but the issue is my work does not translate well. I built dashboards and an r shiny apps that had some business impact. Unfortunately despite the manager and director talking a big game about how we will use Ai and do a ton of DS and ML work, we never do and I often get stuck with the crappy work. \n\nWhen I interview I kill it during behaviorals and I often get far into the process but then I get asked about my lack of AB testing, or ML experience and I am quite honest. I simply have not been assigned those tasks and the company does not do them. Boom I’m out. I’m stuck and I don’t know what to do or how to proceed. Doing projects seems like a decent move but I’ve heard people say that it does not matter. I’m also not great at coding interviews on the spot. I’ve studied a bunch but can’t perform or often get mind wiped when asked a coding question. Anyone else been here? How did you get out? Any help would be appreciated. I really want to be a better DS and get out of pharma and into product or analytics.",
    "author": "Suspicious_Coyote_54",
    "timestamp": "2025-07-25T16:20:41",
    "url": "https://reddit.com/r/datascience/comments/1m9e3vg/stuck_not_doing_ds_work_as_a_ds/",
    "score": 143,
    "num_comments": 54,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m8zjnq",
    "title": "Can a PhD be harmful for your career?",
    "content": "I have my MS degree in a Data Science adjacent field. I currently work in a Data Science / Software Engineering hybrid role, but I also work a second job as an adjunct professor in data science/analytics.\n\nI find teaching unbelievably rewarding, but I could make more money being a cashier at Target. That's no exaggeration.\n\nPart of me thinks teaching is my calling. My workplace will pay for my PhD, however, if I receive my PhD, and discover that I may not want to be a professor... would this result in a hard time finding data science jobs that aren't solely research based?\n\nI try to think of the recruiter perspective, and if I applied to a job with a PhD they may think I will be asking for too much money or be too overqualified.\n\nI'm just wondering if anyone has been in the same scenario, or had thoughts on this. Thank you for your time!",
    "author": "tits_mcgee_92",
    "timestamp": "2025-07-25T06:43:56",
    "url": "https://reddit.com/r/datascience/comments/1m8zjnq/can_a_phd_be_harmful_for_your_career/",
    "score": 91,
    "num_comments": 119,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m8jaeh",
    "title": "Highest ROI math you’ve had?",
    "content": "Curious if there is a type of math / project that has saved or generated tons of money for your company. For example, I used Bayesian inference to figure out what insurance policy we should buy. I would consider this my highest ROI project. \n\nMachine Learning so far seems to promise a lot but delivers quite little.\n\nCausal inference is starting to pick up the speed. ",
    "author": "gpbayes",
    "timestamp": "2025-07-24T16:10:33",
    "url": "https://reddit.com/r/datascience/comments/1m8jaeh/highest_roi_math_youve_had/",
    "score": 243,
    "num_comments": 113,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m8da2j",
    "title": "Are your traditional Data Science projects still getting supported?",
    "content": "My managers are consumed by AI hype.  It was interesting initially when AI was chatbots and coding assistants, but once the idea of Agents entered their mind, it all went off a cliff.  We've had conversations that might as well have been conversations about magic.\n\nI am proposing sensible projects with modest budgets that are getting no interest.",
    "author": "gyp_casino",
    "timestamp": "2025-07-24T12:07:22",
    "url": "https://reddit.com/r/datascience/comments/1m8da2j/are_your_traditional_data_science_projects_still/",
    "score": 131,
    "num_comments": 42,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m7z6un",
    "title": "How do you know someone's got a data science background?",
    "content": "They know of only 3 species of iris flower.\n\nPS: we need a flair for stupid jokes",
    "author": "Papa_Huggies",
    "timestamp": "2025-07-24T01:49:48",
    "url": "https://reddit.com/r/datascience/comments/1m7z6un/how_do_you_know_someones_got_a_data_science/",
    "score": 334,
    "num_comments": 51,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m7bhew",
    "title": "So are we just supposed to know how to get a promotion?",
    "content": "I’ve been working as a Data Scientist I at a Fortune 50 company for the past 3.5 years. Over the last two performance cycles, I’ve proactively asked for a promotion. The first time, my manager pointed out areas for improvement—so I treated that as a development goal, worked on it, and presented clear results in the next cycle.\n\nHowever, when I brought it up again, I was told that promotions aren’t just based on performance—they also depend on factors like budget and others in the promotion queue. When I asked for a clear path forward, I was given no concrete guidance.\n\nNow I’m left wondering: until the next cycle, what am I supposed to do? Is it usually on us to figure out how to get promoted, or does your company provide a defined path?",
    "author": "Substantial_Tank_129",
    "timestamp": "2025-07-23T07:57:01",
    "url": "https://reddit.com/r/datascience/comments/1m7bhew/so_are_we_just_supposed_to_know_how_to_get_a/",
    "score": 181,
    "num_comments": 83,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m7qbd9",
    "title": "SHAP values with class weights",
    "content": "I’m trying to understand which marketing channels are driving conversion. Approximately 2% of customers convert.\n\nI utilize an XGBoost model and as features have:\n1. For converters, the count of various touchpoints in the 8 weeks prior to conversion date.\n2. For non-converters, the count of various touchpoints in the 8 weeks prior to a dummy date selected from the distribution of true conversion dates.\n\nBecause of how rare conversion is, I use class weighing in my XGBoost model. When I interpret SHAP values, I then get that every predictor is negative, which contextually and numerically is contradictory.\n\nDoes changing class weights impact the baseline probability, and mean that SHAP values reflect deviation from the over-weighed baseline probability and not true baseline? If so, what is the best way to correct for this if I still want to use weighing?",
    "author": "transferrr334",
    "timestamp": "2025-07-23T17:42:10",
    "url": "https://reddit.com/r/datascience/comments/1m7qbd9/shap_values_with_class_weights/",
    "score": 20,
    "num_comments": 13,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m7jbpk",
    "title": "Is my side gig worth the effort?",
    "content": "I’ve been doing some freelance data analysis (regression, visuals, clustering) for a mid-sized company over the past couple months. The first project paid OK, and the work itself is pretty open-ended and intellectually engaging.\n\nI initially expected access to their internal data, but it turned out I had to source and prep everything myself. The setup is very hands-off—minimal guidance, so I end up doing a lot of research and exploration on my own.\n\nRight now, I’ve had a lot of free time at my full-time job, so I’ve been able to fit this in without much sacrifice. But I’m anticipating a job change soon, and I’m starting to wonder if this work is worth the effort.\n\nRealistically, I probably earn around (or slightly below) my hourly rate once you factor in how open-ended the work is. That wasn’t what I expected going in.\n\nI keep asking myself if my time would be better spent:\n\n* Practicing Python, SQL, or ML skills for future interviews\n* Studying things I actually enjoy (causal inference, classical stats)\n* Working on personal projects I control\n* Or just spending time on non-data hobbies\n\nCurious to hear how others have thought about this tradeoff. Is it better to lean into these kinds of freelance projects for experience and cash, or to use that energy more intentionally elsewhere?",
    "author": "[deleted]",
    "timestamp": "2025-07-23T12:52:36",
    "url": "https://reddit.com/r/datascience/comments/1m7jbpk/is_my_side_gig_worth_the_effort/",
    "score": 24,
    "num_comments": 27,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m7ftt7",
    "title": "Google DeepMind release Mixture-of-Recursions",
    "content": "Google DeepMind's new paper explore a new advanced Transformers architecture for LLMs called Mixture-of-Recursions which uses recursive Transformers with dynamic recursion per token. Check visual explanation details : https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR",
    "author": "Technical-Love-8479",
    "timestamp": "2025-07-23T10:41:05",
    "url": "https://reddit.com/r/datascience/comments/1m7ftt7/google_deepmind_release_mixtureofrecursions/",
    "score": 22,
    "num_comments": 8,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m70fk3",
    "title": "Where is Data Science interviews going?",
    "content": "As a data scientist myself, I’ve been working on a lot of RAG + LLM things and focused mostly on SWE related things. However, when I interview at jobs I notice every single data scientist job is completely different and it makes it hard to prepare for. Sometimes I get SQL questions, other times I could get ML, Leetcode, pandas data frames, probability and Statistics etc and it makes it a bit overwhelming to prepare for every single interview because they all seem very different. \n\nHas anyone been able to figure out like some sort of data science path to follow? I like how things like Neetcode are very structured to follow, but fail to find a data science equivalent.  ",
    "author": "drewm8080",
    "timestamp": "2025-07-22T22:00:19",
    "url": "https://reddit.com/r/datascience/comments/1m70fk3/where_is_data_science_interviews_going/",
    "score": 191,
    "num_comments": 50,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m825ra",
    "title": "After Many Failed Attempts, I Finally Built a Workflow for Generating Beautiful Ink Painting",
    "content": "I've always wanted to build a workflow for my blog that can quickly and affordably generate high-quality artistic covers. After dozens of days of effort, I finally succeeded. Here's what the output looks like:\n\nhttps://preview.redd.it/lus6nn9i7tef1.png?width=1792&amp;format=png&amp;auto=webp&amp;s=4bf86969f63512c2b223fe0382f85096f8805e87\n\nLet me briefly share my solution:\n\nFirst, I set a clear goal—this workflow should understand the Eastern artistic concepts in users' drawing intentions, generate prompts suitable for the DALL-E-3 model, and ultimately produce high-quality ink painting illustrations.\n\nhttps://preview.redd.it/rvttfx5m7tef1.png?width=1792&amp;format=png&amp;auto=webp&amp;s=d0dd035d9138fe5427aa84de39d714082aa47adf\n\nIt should also allow users to refine the generated prompts through multi-turn conversations and adjust prompts based on the final generated images. This would significantly reduce costs in terms of tokens and time.\n\nInitially, I tried using Dify to build the workflow, but I faced painful failures in user feedback and workflow loops.\n\nI couldn't use coding frameworks like LangChain or CrewAI either because their abstraction levels were too high, making it hard to meet my customization needs.\n\nFinally, I found LlamaIndex Workflow, which provides a low-abstraction, event-driven architecture for building workflows.\n\nUsing this framework along with Context Engineering, I successfully decoupled the workflow loops, making the entire workflow easy to understand, maintain, and adjust as needed.\n\nhttps://preview.redd.it/vp55460p7tef1.jpg?width=1792&amp;format=pjpg&amp;auto=webp&amp;s=97f9ba642c79400b369437e0bf0a52d954da104c\n\nThis flowchart reflects my overall workflow design:\n\nhttps://preview.redd.it/wjkcel5s7tef1.png?width=658&amp;format=png&amp;auto=webp&amp;s=e69448489b428524bb03be2aa5ab6218359a76c1\n\nhttps://preview.redd.it/raqqmngt7tef1.png?width=974&amp;format=png&amp;auto=webp&amp;s=9a6eb4e02f71542a17adfff22522bb972be8d327\n\nDue to length constraints, I can't explain my implementation in detail here, but you can read [my full tutorial](https://www.dataleadsfuture.com/use-llamaindex-workflow-to-create-an-ink-painting-style-image-generation-workflow/) to learn about my complete solution.",
    "author": "qtalen",
    "timestamp": "2025-07-24T04:44:44",
    "url": "https://reddit.com/r/datascience/comments/1m825ra/after_many_failed_attempts_i_finally_built_a/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m70hqg",
    "title": "Probably and Stats interview questions?",
    "content": "Is there like a Neetcode equivalent to be able to do those (where you start understanding the different patterns in questions)? I want to get better at problem solving probability and stats questions. ",
    "author": "drewm8080",
    "timestamp": "2025-07-22T22:03:33",
    "url": "https://reddit.com/r/datascience/comments/1m70hqg/probably_and_stats_interview_questions/",
    "score": 15,
    "num_comments": 8,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m6jx39",
    "title": "Stuck in defense contracting not doing Data Science but have a data science title",
    "content": "Title says it all…. Been here for 3 years, doing a lot of database/data architecting but not really any real data science work. My previous job was at a big 4 consulting but I was doing real data science for 2 years, but hated consulting part with a passion. Any advice?\n\nEdit forgot to add: I’m also currently doing my masters in data science (part-time), and my company is flexible letting me do it. I see a lot more job opportunities elsewhere but feel like I should just stay until I finish next year.",
    "author": "Significant-Heron521",
    "timestamp": "2025-07-22T10:12:11",
    "url": "https://reddit.com/r/datascience/comments/1m6jx39/stuck_in_defense_contracting_not_doing_data/",
    "score": 113,
    "num_comments": 36,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m6h3f0",
    "title": "I wrote 2000 LLM test cases so you don't have to: LLM feature compatibility grid",
    "content": "This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.\n\n# The problem: too many options\n\nI've been building [Kiln AI](https://github.com/kiln-ai/kiln): an open tool to help you find the best way to run your AI workload. Part of Kiln’s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn't easy. If evaluating an additional model is painful, you're less likely to do it, which makes you less likely to find the best way to run your AI workload.\n\nHere's a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (`&lt;think&gt;...&lt;/think&gt;`), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top\\_p, etc), and much more.\n\n# How a focus on usability turned into over 2000 test cases\n\nI wanted things to \"just work\" as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.\n\nTo make it easy to use, we needed reasonable defaults for every major model. That's no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.\n\nThe solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.\n\n# Wait, doesn't that cost a lot of money and take forever?\n\n**Yes it does!** Each time we run these tests, we're making thousands of LLM calls against a wide variety of providers. There's no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn't an option.\n\nOur blog has some details on the [Python pytest setup we used to make this manageable](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time).\n\n# The Result\n\nThe end result is that it's much easier to rapidly evaluate AI models and methods. It includes\n\n* The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.\n* Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).\n\nHowever, you're in control. You can always override any suggestion.\n\n# Next Step: A Giant Ollama Server\n\nI can run a decent sampling of our Ollama tests locally, but I lack the \\~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I'd love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!\n\n# How to Find the Best Model for Your Task with Kiln\n\nAll of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.\n\nKiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case — all backed by the extensive testing described above.\n\nTo get started, check out the tool or our guides:\n\n* [Kiln AI on Github - over 3900 stars](https://getkiln.ai/)\n* [Quickstart Guide](https://docs.getkiln.ai/docs/quickstart)\n* [Kiln Discord](https://getkiln.ai/discord)\n* [Blog post with more details on our LLM testing (more detailed version of above)](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time)\n\nI'm happy to answer questions if anyone wants to dive deeper on specific aspects!",
    "author": "davernow",
    "timestamp": "2025-07-22T08:26:27",
    "url": "https://reddit.com/r/datascience/comments/1m6h3f0/i_wrote_2000_llm_test_cases_so_you_dont_have_to/",
    "score": 10,
    "num_comments": 5,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m5nkwe",
    "title": "Wouldn't be the first time I've seen an entire org propped up by a 80MB Excel file",
    "content": "Oh yeah, I started a meme sub r/AnalyticsMemes if anyone wants every day to be meme Monday",
    "author": "ElectrikMetriks",
    "timestamp": "2025-07-21T09:29:06",
    "url": "https://reddit.com/r/datascience/comments/1m5nkwe/wouldnt_be_the_first_time_ive_seen_an_entire_org/",
    "score": 455,
    "num_comments": 40,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m5xn63",
    "title": "Looking for MMM / Marketing Data Science specialist",
    "content": "Hi All,\n\nHope this is okay to post in this sub.\n\nI am looking to hire for a role here in the DFW metro area and looking for a hard to find specialty of media mix marketing. Willing to train recent graduates with the right statistical and academic background. Currently hybrid 3 days a week in office. Compensation depends on skill set and experience, but can be between $95k-150k.\n\nPlease DM for more details and to send resumes.",
    "author": "recruitingfornow2025",
    "timestamp": "2025-07-21T15:51:14",
    "url": "https://reddit.com/r/datascience/comments/1m5xn63/looking_for_mmm_marketing_data_science_specialist/",
    "score": 22,
    "num_comments": 17,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m5odiz",
    "title": "Data Science MSc 1 year Full time or 2 year Part time?",
    "content": "Hi, I'm funding my own MSc in Applied Data Science (intended for non computer/maths background)\n\nI have a 6 year healthcare background (Nuclear medicine and CT).\n\nI have taken python and SQL introduction courses to build a foundation.\n\nMy question is:\n\nWould a 1 year MSc be  intensive learning for 1 year with dissertation and realistically result in a 18month study?\n\nDoes a 2 year MSc offer more room, resulting in a realistic 24 month timeline, with some room for job \"volunteering\" to get some experience?\n\nI have completed a 3 year MSc before and can't comprehend how intense a 1 year MSc would be.\n\nThanks!",
    "author": "sideshowbob01",
    "timestamp": "2025-07-21T09:58:29",
    "url": "https://reddit.com/r/datascience/comments/1m5odiz/data_science_msc_1_year_full_time_or_2_year_part/",
    "score": 12,
    "num_comments": 18,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m5m5pn",
    "title": "Maintenance of clustered data over time",
    "content": "With LLM-generated data, what are the best practices for handling downstream maintenance of clustered data?\n\nE.g. for conversation transcripts, we extract things like the topic. As the extracted strings are non-deterministic, they will need clustering prior to being queried by dashboards.\n\nWhat are people doing for their daily/hourly ETLs? Are you similarity-matching new data points to existing clusters, and regularly assessing cluster drift/bloat? How are you handling historic assignments when you determine clusters have drifted and need re-running?\n\nAny guides/books to help appreciated!",
    "author": "Disastrous_Classic96",
    "timestamp": "2025-07-21T08:35:52",
    "url": "https://reddit.com/r/datascience/comments/1m5m5pn/maintenance_of_clustered_data_over_time/",
    "score": 14,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m5i5dj",
    "title": "Data Snooping Resources",
    "content": "Simple question: Do you guys have any resources/papers about data snooping and how to limits its influence when making predictive models? I understand to maintain a testing dataset, but I am hoping someone knows any good high-level introductions to the topic that is not overly technical. Something like this, but about data snooping specifically, is what I am hoping to find: https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/ES13-00160.1",
    "author": "Key-Network-9447",
    "timestamp": "2025-07-21T05:55:16",
    "url": "https://reddit.com/r/datascience/comments/1m5i5dj/data_snooping_resources/",
    "score": 11,
    "num_comments": 2,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m58yyn",
    "title": "Weekly Entering &amp; Transitioning - Thread 21 Jul, 2025 - 28 Jul, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-07-20T21:01:31",
    "url": "https://reddit.com/r/datascience/comments/1m58yyn/weekly_entering_transitioning_thread_21_jul_2025/",
    "score": 6,
    "num_comments": 39,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m4d64h",
    "title": "Company Killed University Programs",
    "content": "Normally, I would have a post around this time hyping up fall recruiting and trying to provide pointers. The company I work for has decided to hire no additional entry level data scientists this year outside of intern return offers. They have also cut the number of intern positions in half for 2026. \n\nPart of the reasoning given by the CEO was that it is easy to hire early to mid level data scientist with project specific skills rather than training new hires. Money can also be saved by not having a university recruiting team and saving time interviewing by only going to target universities. \n\nAre any other data scientists seeing this change in their companies?",
    "author": "[deleted]",
    "timestamp": "2025-07-19T18:56:38",
    "url": "https://reddit.com/r/datascience/comments/1m4d64h/company_killed_university_programs/",
    "score": 181,
    "num_comments": 39,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m45pmq",
    "title": "Generating random noise for media data",
    "content": "Hey everyone - I work on an ML team in the industry, and I’m currently building a predictive model to catch signals in live media data to sense when potential viral moments or crises are happening for brands. We have live media trackers at my company that capture all articles, including their sentiment (positive, negative, neutral). \n\nI currently am using ARIMA to predict out a certain amount of time steps, then using an LSTM to determine whether the volume of articles is anomalous given historical data trends. \n\nHowever, the nature of media is there’s so much randomness, so just taking the ARIMA projection is not enough. Because of that, I’m using Monte Carlo simulation to run an LSTM on a bunch of different forecasts that incorporate an added noise signal for each simulation. Then, that forces a probability of how likely it is that a crisis/viral moment will happen.\n\nI’ve been experimenting with a bunch of methods on how to generate a random noise signal, and while I’m close to getting something, I still feel like I’m missing a method that’s concrete and backed by research/methodology. \n\nDoes anyone know of approaches on how to effectively generate random noise signals for PR data? Or know of any articles on this topic?\n\nThank you!\n",
    "author": "Entire_Island8561",
    "timestamp": "2025-07-19T13:07:54",
    "url": "https://reddit.com/r/datascience/comments/1m45pmq/generating_random_noise_for_media_data/",
    "score": 12,
    "num_comments": 9,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m49rai",
    "title": "How would you structure a project (data frame) to scrape and track listing changes over time?",
    "content": "\n\nI’m working on a project where I want to scrape data daily (e.g., real estate listings from a site like RentFaster or Zillow) and track how each listing changes over time. I want to be able to answer questions like:\n\nWhen did a listing first appear?\nHow long did it stay up?\nWhat changed (e.g., price, description, status)?\nWhat’s new today vs yesterday?\n\nMy rough mental model is:\n1. Scrape today’s data into a CSV or database.\n2. Compare with previous days to find new/removed/updated listings.\n3. Over time, build a longitudinal dataset with per-listing history (kind of like slow-changing dimensions in data warehousing).\n\nI’m curious how others would structure this kind of project:\n\nHow would you handle ID tracking if listings don’t always have persistent IDs?\nWould you use a single master table with change logs? Or snapshot tables per day?\nHow would you set up comparisons (diffing rows, hashing)?\nAny Python or DB tools you’d recommend for managing this type of historical tracking?\n\nI’m open to best practices, war stories, or just seeing how others have solved this kind of problem. Thanks!\n",
    "author": "Proof_Wrap_2150",
    "timestamp": "2025-07-19T16:08:06",
    "url": "https://reddit.com/r/datascience/comments/1m49rai/how_would_you_structure_a_project_data_frame_to/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m4s0vw",
    "title": "AI In Data Engineering",
    "content": "",
    "author": "chrisgarzon19",
    "timestamp": "2025-07-20T08:41:10",
    "url": "https://reddit.com/r/datascience/comments/1m4s0vw/ai_in_data_engineering/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m3gy6m",
    "title": "Are headhunters still a thing in 2025?",
    "content": "Curious what the current consensus is on headhunters these days. A few years ago they seemed to be everywhere, both big-name firms like Michael Page and boutique ones, but lately I don’t hear much about them.\n\nDo companies still rely on them or have internal recruiting teams and LinkedIn taken over completely?",
    "author": "ergodym",
    "timestamp": "2025-07-18T16:12:31",
    "url": "https://reddit.com/r/datascience/comments/1m3gy6m/are_headhunters_still_a_thing_in_2025/",
    "score": 62,
    "num_comments": 37,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m10uku",
    "title": "What question from recruiters do you absolutely hate to answer? How do you answer it elegantly?",
    "content": "Pretty much the title. Recruiters are not technically adepts in most of the cases. They go about asking some questions which is routine for them but hardly make sense in the real world. Not trying to be idealistic but, which questions do you hate the most? How would you answer them in a polite way?",
    "author": "OverratedDataScience",
    "timestamp": "2025-07-15T19:19:08",
    "url": "https://reddit.com/r/datascience/comments/1m10uku/what_question_from_recruiters_do_you_absolutely/",
    "score": 64,
    "num_comments": 57,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m0wx9l",
    "title": "Hoping for a review.",
    "content": "I want to clarify the reason I'm not using the main thread is because I'm posting an image, which can't be used for replies. I've been searching for a while without as much as a call back. I've been a data scientist for a while now and I'm not sure if it's the market or if there's something glaringly bad with my resume. Thanks for your help.",
    "author": "KyronAWF",
    "timestamp": "2025-07-15T16:15:22",
    "url": "https://reddit.com/r/datascience/comments/1m0wx9l/hoping_for_a_review/",
    "score": 31,
    "num_comments": 73,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m0b9f9",
    "title": "Is it normal to be scared for the future finding a job",
    "content": "I am a rising senior at a large state school studying data science. I am currently working an internship as a software engineer for the summer. And I get my tickets done for the most part albeit with some help from ai. But deep down I feel a pit in my stomach that I won’t be able to end up employed after all of this.\n\nI plan to go for a masters in applied statistics or data science after my bachelors. Thought I definitely don’t have great math grades from my first few semesters of college. But after those semesters all my upper division math/stats/cs/data science courses have been A’s and B’s. And I feel like ik enough python, R, and SAS to work through and build models for most problems I run into, as well as tableau, sql and alteryx. But I can’t shake the feeling that it won’t be enough.\n\nAlso that my rough math grades in my first few semesters will hold me back from getting into a masters programs. I have tried to supplement this by doing physics and applied math research. But I’m just not sure I’m doing enough and I’m scared for like after I finish my education.\n\nIm just venting here but I’m hoping there r others in this sub who have been in similar positions and gotten employed. Or r currently in my same shoes I just need to hear from other people that it’s not as hopeless as it feels.\n\nI just want to get a job as a data analyst, scientist, or statistician working on interesting problems and have a decent career.",
    "author": "ChubbyFruit",
    "timestamp": "2025-07-15T00:18:35",
    "url": "https://reddit.com/r/datascience/comments/1m0b9f9/is_it_normal_to_be_scared_for_the_future_finding/",
    "score": 243,
    "num_comments": 103,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lzx0la",
    "title": "I have people skills... I am good at dealing with people. Can't you understand that? What the hell is wrong with you people?",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-07-14T13:07:07",
    "url": "https://reddit.com/r/datascience/comments/1lzx0la/i_have_people_skills_i_am_good_at_dealing_with/",
    "score": 318,
    "num_comments": 14,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m0n56g",
    "title": "\"Harnessing the Universal Geometry of Embeddings\" - Breakthroughs and Security Implications",
    "content": "",
    "author": "SharePlayful1851",
    "timestamp": "2025-07-15T09:55:59",
    "url": "https://reddit.com/r/datascience/comments/1m0n56g/harnessing_the_universal_geometry_of_embeddings/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m0dxsm",
    "title": "How does your organization label data?",
    "content": "I'm curious to hear how your organization labels data for use in modeling. We use a combination of SMEs who label data, simple rules that flag cases (it's rare that we can use these because they're generally no unambiguous), and an ML model to find more labels. I ask because my organization doesn't think it's valuable to have SMEs labeling data. In my domain area (fraud), we need SMEs to be labeling data because fraud evolves over time, and we need to identify the evoluation. Also, identifying fraud in the data isn't cut and dry. ",
    "author": "Dangerous_Media_2218",
    "timestamp": "2025-07-15T03:12:44",
    "url": "https://reddit.com/r/datascience/comments/1m0dxsm/how_does_your_organization_label_data/",
    "score": 7,
    "num_comments": 12,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lzgfhq",
    "title": "I suck at these interviews.",
    "content": "I'm looking for a job again and while I have had quite a bit of hands-on practical work that has a lot of business impacts - revenue generation, cost reductions, increasing productivity etc \n\nBut I keep failing at \"Tell the assumptions of Linear regression\" or \"what is the formula for Sensitivity\".\n\nWhile I'm aware of these concepts, and these things are tested out in model development phase, I never thought I had to mug these stuff up. \n\nThe interviews are so random - one could be hands on coding (love these), some would be a mix of theory, maths etc, and some might as well be in Greek and Latin..\n\nPlease give some advice to  4 YOE DS should be doing. The \"syllabus\" is entirely too vast.🥲\n\nEdit:\nWow, ok i didn't expect this to blow up. I did read through all the comments. This has been definitely enlightening for me.\n\nYes, i should have prepared better, brushed up on the fundamentals. Guess I'll have to go the notes/flashcards way. \n",
    "author": "JayBong2k",
    "timestamp": "2025-07-14T00:50:27",
    "url": "https://reddit.com/r/datascience/comments/1lzgfhq/i_suck_at_these_interviews/",
    "score": 532,
    "num_comments": 133,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lzlrlu",
    "title": "Fine-tuning for tabular foundation models (TabPFN)",
    "content": "Hi everyone - wanted to share that you can now fine-tune tabular foundation models as well, specifically TabPFN! With the latest 2.1 package release, you can now build your own fine-tuned models.\n\nA community member put together a practical walkthrough!\n\nHow to Fine-Tune TabPFN on Your Data: [https://medium.com/@iivalchev/how-to-fine-tune-tabpfn-on-your-data-a831b328b6c0](https://medium.com/@iivalchev/how-to-fine-tune-tabpfn-on-your-data-a831b328b6c0)\n\nThe tutorial covers:\n\n* Running TabPFN in batched mode\n* Handling preprocessing and inference-time transformations\n* Fine-tuning the transformer backbone on your dataset\n\nIf you're working with highly domain specific data and looking to boost performance, this is a great place to start.\n\nYou can also check out the example files directly at these links:\n\n🧪 [Fine-tune classifier](https://github.com/PriorLabs/TabPFN/blob/main/examples/finetune_classifier.py)\n\n📈 [Fine-tune regressor](https://github.com/PriorLabs/TabPFN/blob/main/examples/finetune_regressor.py)\n\nWould love to hear how it goes if you try it!\n\nThere’s also a community Discord where folks are sharing experiments and helping each other out - worth checking out if you're playing around with TabPFN [https://discord.com/invite/VJRuU3bSxt](https://discord.com/invite/VJRuU3bSxt)",
    "author": "rsesrsfh",
    "timestamp": "2025-07-14T06:00:27",
    "url": "https://reddit.com/r/datascience/comments/1lzlrlu/finetuning_for_tabular_foundation_models_tabpfn/",
    "score": 20,
    "num_comments": 3,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lzo89g",
    "title": "Do employers see volunteer experience as “real world experience”?",
    "content": "",
    "author": "Kati1998",
    "timestamp": "2025-07-14T07:42:27",
    "url": "https://reddit.com/r/datascience/comments/1lzo89g/do_employers_see_volunteer_experience_as_real/",
    "score": 12,
    "num_comments": 22,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1m07l5m",
    "title": "Need mentorship on climbing the ladder or transitioning",
    "content": "",
    "author": "m2rik",
    "timestamp": "2025-07-14T20:45:37",
    "url": "https://reddit.com/r/datascience/comments/1m07l5m/need_mentorship_on_climbing_the_ladder_or/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lzkso0",
    "title": "Site Selection Model - Subjective Feature",
    "content": "I have been working on a site selection model, and the one I created is performing quite well in out of sample testing. I was also able to reduce the model down to just 5 features. But, one of those features is a \"Visibility Score\" (how visible the building is from the road). I had 3 people independently score all of our existing sites and I averaged their scores, and this has proven to work well so far. But if we actually put the model into production, I am concerned about standardized those scores. The model predictiction can vary by 18% just from a visibility score change from 3.5 to 4.0 so the model is heavily dependent on that subjective score.\n\nAny tips?",
    "author": "multicm",
    "timestamp": "2025-07-14T05:14:56",
    "url": "https://reddit.com/r/datascience/comments/1lzkso0/site_selection_model_subjective_feature/",
    "score": 7,
    "num_comments": 5,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lzcn4y",
    "title": "Weekly Entering &amp; Transitioning - Thread 14 Jul, 2025 - 21 Jul, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-07-13T21:01:26",
    "url": "https://reddit.com/r/datascience/comments/1lzcn4y/weekly_entering_transitioning_thread_14_jul_2025/",
    "score": 8,
    "num_comments": 39,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lywh35",
    "title": "How much DSA for FAANG+ ?",
    "content": "Hello all, I am going to be graduating in 6 months and have been practicing Leetcode as I believe this to be my weakest point. I have solved 250 LC with 130 Easy and 120 Hard, covering concepts like arrays, hashing, binary trees, SQL, linked list, two pointers, stack, sliding windows majorly. Could anyone guide me on how I can maximise the time I have on hand to prepare better for technical interviews? I have good internship and research experience so I am not that worried about future rounds, but timed coding questions have always been brutal for me. Any advice is appreciated.",
    "author": "harsh82000",
    "timestamp": "2025-07-13T09:05:15",
    "url": "https://reddit.com/r/datascience/comments/1lywh35/how_much_dsa_for_faang/",
    "score": 68,
    "num_comments": 37,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lyo2ac",
    "title": "Toto: A Foundation Time-Series Model Optimized for Observability Data",
    "content": "Datadog open-sourced *Toto* (Time Series Optimized Transformer for Observability), a model purpose-built for observability data.\n\nToto is currently the most extensively pretrained time-series foundation model: The pretraining corpus contains 2.36 trillion tokens, with \\~70% coming from Datadog’s private telemetry dataset.\n\nAlso, Toto currently ranks 2nd in the GIFT-Eval Benchmark.\n\nYou can find an analysis of the model [here](https://aihorizonforecast.substack.com/p/toto-a-foundation-time-series-model).",
    "author": "nkafr",
    "timestamp": "2025-07-13T01:45:48",
    "url": "https://reddit.com/r/datascience/comments/1lyo2ac/toto_a_foundation_timeseries_model_optimized_for/",
    "score": 56,
    "num_comments": 14,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ly06nw",
    "title": "How do you efficiently traverse hundreds of features in the dataset?",
    "content": "Currently, working on a fintech classification algorithm, with close to a thousand features which is very tiresome. I'm not a domain expert, so creating sensible hypotesis is difficult. How do you tackle EDA and forming reasonable hypotesis in these cases? Even with proper documentation it's not a trivial task to think of all interesting relationships that might be worth looking at. What I've been looking so far to make is:\n\n1) Baseline models and feature relevance assessment with in ensemble tree and via SHAP values  \n2) Traversing features manually and check relationships that \"make sense\" for me",
    "author": "Grapphie",
    "timestamp": "2025-07-12T06:14:14",
    "url": "https://reddit.com/r/datascience/comments/1ly06nw/how_do_you_efficiently_traverse_hundreds_of/",
    "score": 92,
    "num_comments": 41,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lyciw1",
    "title": "The right questions to find clusters (tangles)",
    "content": "**Hey everyone,**\n\nI’m currently working on my bachelor’s thesis and I’m hitting a creative block on a central part – maybe you have some ideas or impulses for me.\n\nMy dataset consists of 100,000 cleaned job postings from Kaggle (title + description). The goal of my thesis is to use a method called **Tangles** (probably no one knows it, it’s a rather specific approach from my studies) to find interesting clusters in this data – similar to embedding-based clustering methods, but with the key difference that it requires **interpretable, binary decisions**. Sounds theoretical, but it’s actually pretty cool:\n\nYou ask the dataset **yes/no questions** (e.g., *“Does the job require a lot of travel?”*), and based on the answer patterns, a kind of profile emerges – and from these profiles, groups that belong together can be formed.\n\nThe goal is to group jobs that don’t obviously belong together at first glance, but do share certain underlying similarities (e.g., requirements, tasks) that cause them to respond similarly to the questions.\n\n**One example:**\n\nQuestions like:\n\n* Does the job require a lot of travel?\n* Do you need a driver’s license?\n* Do you have to be physically fit?\n\n\n\n=&gt; could group *Sales Managers* and *Truck Drivers* together – even though those jobs seem very different at first. These kinds of connections are what I find exciting.\n\nWhat I’m **not** looking for are questions like:\n\n* Is this a data science job?\n* Do you need to know how to code?\n* Is it IT-related?\n\nTo me, those are more like categories or classifications that make the clustering too obvious – they just confirm what you already know. I’m more interested in **surprising, layered similarities**.\n\nSo here’s my question for you:\n\nDo you have any interesting **yes/no questions** from your daily work or knowledge that could be applied to any kind of job posting – and that might result in **interesting, possibly unexpected groupings**?\n\nWhether you work in trades, healthcare, IT, management, or research – **every perspective helps!**\n\nIn the end, I need at least 40 such questions (the more, the better), but right now I’m really struggling to come up with good ones. Even GPT &amp; co. haven’t been much help – they usually just spit out generic stuff.\n\nEven **one** good question from you would be incredibly helpful. 🙏 OR advice on how to find these questions/if my idea is right or not, would help.\n\nThanks in advance for thinking along!",
    "author": "juggerjaxen",
    "timestamp": "2025-07-12T15:09:48",
    "url": "https://reddit.com/r/datascience/comments/1lyciw1/the_right_questions_to_find_clusters_tangles/",
    "score": 3,
    "num_comments": 10,
    "upvote_ratio": 0.59,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lxb0bn",
    "title": "Doordash phone screen reject despite good in-interview feedback. What are they looking for?",
    "content": "Had a phone screen with DoorDash recently for a DS Analytics role. First round was a product case study — the interviewer was super nice, gave good feedback throughout, and even ended with “Great job on this round,” so I felt pretty good about it.\n\nSecond round was SQL with 4 questions. Honestly, the first one threw me off — it was more convoluted than I expected, so I struggled a bit but managed to get through it. The 2nd and 3rd were much easier and I finished those without issues. The 4th was a bonus question where I had to explain a SQL query — took me a moment, but I eventually explained what it was doing.\n\nGot a rejection email the next day. I thought it went decently overall, so I’m a bit confused. Any thoughts on what might’ve gone wrong or what I could do better next time",
    "author": "Substantial_Tank_129",
    "timestamp": "2025-07-11T09:13:24",
    "url": "https://reddit.com/r/datascience/comments/1lxb0bn/doordash_phone_screen_reject_despite_good/",
    "score": 112,
    "num_comments": 68,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ly409f",
    "title": "How have you supported DS fundamentals, creative thinking or curiosity in your baby/toddler using what you know as a technical or analytical thinker?",
    "content": "Anything you built, played, repeated, or tracked?",
    "author": "Proof_Wrap_2150",
    "timestamp": "2025-07-12T09:03:01",
    "url": "https://reddit.com/r/datascience/comments/1ly409f/how_have_you_supported_ds_fundamentals_creative/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lvmphl",
    "title": "All of my data comes from spreadsheets. As I receive more over time, what’s the best way to manage and access multiple files efficiently? Ideally in a way that scales and still lets me work interactively with the data?",
    "content": "I’m working on a project where all incoming data is provided via spreadsheets (Excel/CSV). The number of files is growing, and I need to manage them in a structured way that allows for:\n\n1. Easy access to different uploads over time\n2. Avoiding duplication or version confusion\n3. Interactive analysis (e.g., via Jupyter notebooks or a lightweight dashboard)\n\nI’m currently loading files manually, but I want a better system. Whether that means a file management structure, metadata tagging, or loading/parsing automation. Eventually I’d like to scale this up to support analysis across many uploads or clients.\n\nWhat are good patterns, tools, or Python-based workflows to support this?",
    "author": "Proof_Wrap_2150",
    "timestamp": "2025-07-09T09:07:05",
    "url": "https://reddit.com/r/datascience/comments/1lvmphl/all_of_my_data_comes_from_spreadsheets_as_i/",
    "score": 67,
    "num_comments": 39,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lvn71u",
    "title": "How do you guys measure AI impact",
    "content": "Im sure a lot of companies are rolling out AI products to help their business. \n\nIm curious how do people typically try to measure these AI products impacts. I guess it really depends on the domain but can we isolate and see if any uplift in the KPI is attributable to AI?\n\nIs AB testing always to gold standard? Use Quasi experimental methods? \n\n",
    "author": "Professional_Ball_58",
    "timestamp": "2025-07-09T09:26:33",
    "url": "https://reddit.com/r/datascience/comments/1lvn71u/how_do_you_guys_measure_ai_impact/",
    "score": 25,
    "num_comments": 46,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lux7bt",
    "title": "Saved $100k per year by explaining how AI/LLM work.",
    "content": "I work in a data science field, and I bring this up because I think it's data science related.\n\nWe have an internal website that is very bare bones. It's made to be simplistic, because it's the reference document for our end-users (1000 of them) use.\n\nExecutives heard about a software that would be completely AI driven, build detailed statistical insights, and change the world as they know it.\n\nI had a demo with the company and they explained its RAG capabilities, but mentioned it doesn't really \"learn\" like the assumption AI does. Our repo is so small and not at all needed for AI. We have used a fuzzy search that has worked for the past three years. Additionally, I have already built out dashboards that retrieve all the information executives have asked for via API (who's viewing pages, what are they searching, etc.)\n\nI showed the c-suite executives our current dashboards in Tableau, and how the actual search works. I also explained what RAG is, and how AI/LLMs work at a high level. I explained to them that AI is a fantastic tool, but I'm not sure if we should be spending 100k a year on it. They also asked if I have built any predictive models. I don't think they quite understood what that was as well, because we don't have the amount of data or need to predict anything.\n\nNeedless to say, they decided it was best not to move forward \"for now\". I am shocked, but also not, that executives want to change the structure of how my team and end-users digest information just because they heard \"AI is awesome!\" They had zero idea how anything works in our shop.\n\nOh yeah, our company has already laid of 250 people this year due to \"financial turbulence\", and now they're wanting to spend 100k on this?!\n\nIt just goes to show you how deep the AI train runs. Did I handle this correctly and can I put this on my resume? LOL",
    "author": "tits_mcgee_92",
    "timestamp": "2025-07-08T12:03:19",
    "url": "https://reddit.com/r/datascience/comments/1lux7bt/saved_100k_per_year_by_explaining_how_aillm_work/",
    "score": 1177,
    "num_comments": 99,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lvoz88",
    "title": "Quarterly to Monthly Data Conversion",
    "content": "As the title suggests. I am trying to convert average wage data, from quarterly to monthly. I need to perform forecasting on that. What is the best ways to do that?? . I don’t want to go for a naive method and just divide by 3 as I will loose any trends or patterns. I have come across something called disproportionate aggregation but having a tough time grasping it.",
    "author": "NervousVictory1792",
    "timestamp": "2025-07-09T10:35:32",
    "url": "https://reddit.com/r/datascience/comments/1lvoz88/quarterly_to_monthly_data_conversion/",
    "score": 14,
    "num_comments": 31,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lvnda9",
    "title": "Reachy-Mini: Huggingface launched open-sourced robot that supports vision, text and speech",
    "content": "Huggingface just released an open-sourced robot named Reachy-Mini, which supports all Huggingface open-sourced AI models, be it text or speech or vision and is quite cheap. Check more details here : https://youtu.be/i6uLnSeuFMo?si=Wb6TJNjM0dinkyy5",
    "author": "Technical-Love-8479",
    "timestamp": "2025-07-09T09:33:29",
    "url": "https://reddit.com/r/datascience/comments/1lvnda9/reachymini_huggingface_launched_opensourced_robot/",
    "score": 13,
    "num_comments": 8,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lvl0wp",
    "title": "Open source or not?",
    "content": "Hi all,  \nI am building an AI agent, similar to Github copilot / Cursor but very specialized on data science / ML. It is integrated in VSCode as an extension.  \nHere is a few examples of use cases:  \n\\- Combine different data sources, clean and preprocess for ML pipeline.  \n\\- Refactor R&amp;D notebooks into ready for production project: Docker, package, tests, documentation.\n\nWe are approaching an MVP in the next few weeks and I am hesitating between 2 business models:  \n1- Closed source, similar to cursor, with fixed price subscription with limit by request.  \n2- Open source, pay per token. User can plug their own API or use our backend which offers all frontier models. Charge a topup % on top of token consumption (similar to Cline).\n\nThe question is also whether the data science community would contribute to a vscode extension in React, Typescript.\n\nWhat do you think make senses as a data scientist / ML engineer?",
    "author": "SummerElectrical3642",
    "timestamp": "2025-07-09T08:01:23",
    "url": "https://reddit.com/r/datascience/comments/1lvl0wp/open_source_or_not/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lum3ih",
    "title": "Path to product management",
    "content": "I’m a student interested in working as a product manager in tech. \n\nI know it’s tough to land a first role directly in PM, so I’m considering alternative paths that could lead there.\n\nMy question is: how common is the transition from data scientist/product data scientist to product manager? Is it a viable path?\n\nAlso would it make more sense to go down the software engineering route instead (even though I’m not particularly passionate about it) if it makes the transition to PM easier?",
    "author": "FinalRide7181",
    "timestamp": "2025-07-08T04:27:33",
    "url": "https://reddit.com/r/datascience/comments/1lum3ih/path_to_product_management/",
    "score": 7,
    "num_comments": 13,
    "upvote_ratio": 0.68,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lu7gqq",
    "title": "How to deal with time series unbalanced situations?",
    "content": "**Hi everyone,**\n\nI’m working on a challenge to **predict the probability of a product becoming unavailable the next day**.\n\nThe dataset contains one row per product per day, with a binary target (`failure` or not) and 10 additional features. There are over 1 million rows without failure, and only 100 with failure — so it's a highly imbalanced dataset.\n\nHere are some key points I’m considering:\n\n1. **The target should reflect the next day**, not the current one. For example, if product X has data from day 1 to day 10, each row should indicate whether a failure will happen on the following day. Day 10 is used only to label day 9 and is not used as input for prediction.\n2. **The features are on different scales**, so I’ll need to apply normalization or standardization depending on the model I choose (e.g., for Logistic Regression or KNN).\n3. **There are no missing values**, so I won’t need to worry about imputation.\n4. **To avoid data leakage**, I’ll split the data by product, making sure that each product's full time series appears entirely in either the training or test set — never both. For example, if product X has data from day 1 to day 9, those rows must all go to either train **or** test.\n5. Since the output should be a **probability**, I’m planning to use models like Logistic Regression, Random Forest, XGBoost, Naive Bayes, or KNN.\n6. Due to the strong class imbalance, my **main evaluation metric will be ROC AUC**, since it handles imbalanced datasets well.\n7. Would it make sense to include calendar-based features, like the day of the week, weekend indicators, or holidays?\n8. How useful would it be to add rolling window statistics (e.g., 3-day averages or standard deviations) to capture recent trends in the attributes?\n9. Any best practices for flagging anomalies, such as sudden spikes in certain attributes or values above a specific percentile (like the 90th)?\n\n**My questions:**  \nDoes this approach make sense?  \nI’m not entirely confident about some of these steps, so I’d really appreciate feedback from more experienced data scientists!",
    "author": "EducationalUse9983",
    "timestamp": "2025-07-07T15:03:11",
    "url": "https://reddit.com/r/datascience/comments/1lu7gqq/how_to_deal_with_time_series_unbalanced_situations/",
    "score": 59,
    "num_comments": 67,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lu7fxv",
    "title": "Python package for pickup/advanced booking models for forecasting?",
    "content": "Recently discovered pickup models that use reservation data to generate forecasts (see [https://www.scitepress.org/papers/2016/56319/56319.pdf](https://www.scitepress.org/papers/2016/56319/56319.pdf) ) Seems used often in the hotel and airline industry. Is there a python package for this? Maybe it goes by a different name but I'm not seeing anything",
    "author": "GussieWussie",
    "timestamp": "2025-07-07T15:02:16",
    "url": "https://reddit.com/r/datascience/comments/1lu7fxv/python_package_for_pickupadvanced_booking_models/",
    "score": 8,
    "num_comments": 3,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ltkjwx",
    "title": "Weekly Entering &amp; Transitioning - Thread 07 Jul, 2025 - 14 Jul, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-07-06T21:02:03",
    "url": "https://reddit.com/r/datascience/comments/1ltkjwx/weekly_entering_transitioning_thread_07_jul_2025/",
    "score": 17,
    "num_comments": 57,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lu1cve",
    "title": "I don't drink, but I'm still tired because my dogs hate fireworks.  Did everyone in the US take a long weekend at least?",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-07-07T11:06:15",
    "url": "https://reddit.com/r/datascience/comments/1lu1cve/i_dont_drink_but_im_still_tired_because_my_dogs/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.37,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lsjfj4",
    "title": "Long-timers at companies — what’s your secret?",
    "content": "Hi everyone,\n\nI’ve been a job hopper throughout my career—never stayed at one place for more than 1-2 years, usually for various reasons.\n\nNow, I’m entering a phase where I want to get more settled. I’m about to start a new job and would love to hear from those who have successfully stayed long-term at a job.\n\nWhat’s the secret sauce besides just hard work and taking ownership? Lay your knowledge on me—your hacks, tips, rituals.\n\nThanks in advance.",
    "author": "mlbatman",
    "timestamp": "2025-07-05T13:35:14",
    "url": "https://reddit.com/r/datascience/comments/1lsjfj4/longtimers_at_companies_whats_your_secret/",
    "score": 147,
    "num_comments": 69,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lss1c5",
    "title": "Reliable DS Adjacent Fields Hiring for Bachelor's Degree?",
    "content": "Hello all. To try and condense a lot of context for this question, I am an adult who went back to school to complete my bachelor's, in order to support myself and my partner on one income. Admittedly, I did this because I heard how good data science was as a field, but it seems I jumped in at the wrong time. \n\nConsequently, now that I am one year out from graduating with my bachelor's, I am starting to think about what fields would be best to apply in, beyond simply \"data science\" and \"data analysis.\" Any leads on fields that are reliably hiring that are similar to data science but not exact? I am really open to anything that would pay the bills for two people.",
    "author": "fenrirbatdorf",
    "timestamp": "2025-07-05T20:56:29",
    "url": "https://reddit.com/r/datascience/comments/1lss1c5/reliable_ds_adjacent_fields_hiring_for_bachelors/",
    "score": 37,
    "num_comments": 30,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lsfyeo",
    "title": "A Brief Guide to UV",
    "content": "Python has been largely devoid of easy to use environment and package management tooling, with various developers employing their own cocktail of `pip`, `virtualenv`, `poetry`, and `conda` to get the job done. However, it looks like `uv` is rapidly emerging to be a standard in the industry, and I'm super excited about it.\n\nIn a nutshell `uv` is like `npm` for Python. It's also written in rust so it's crazy fast.\n\nAs new ML approaches and frameworks have emerged around the greater ML space (A2A, MCP, etc) the cumbersome nature of Python environment management has transcended from an annoyance to a major hurdle. This seems to be the major reason `uv` has seen such meteoric adoption, especially in the ML/AI community.\n\n[star history of uv vs poetry vs pip. Of course, github star history isn't necessarily emblematic of adoption.  \\&lt;ore importantly, uv is being used all over the shop in high-profile, cutting-edge repos that are governing the way modern software is evolving. Anthropic’s Python repo for MCP uses UV, Google’s Python repo for A2A uses UV, Open-WebUI seems to use UV, and that’s just to name a few.](https://preview.redd.it/b6myln1ve3bf1.png?width=1050&amp;format=png&amp;auto=webp&amp;s=89d275ad7b050bbe8a365dd731e37910182592c4)\n\nI wrote [an article](https://iaee.substack.com/p/uv-intuitively-and-exhaustively-explained) that goes over `uv` in greater depth, and includes some examples of `uv` in action, but I figured a brief pass would make a decent Reddit post.\n\n**Why UV**  \n`uv` allows you to manage dependencies and environments with a single tool, allowing you to create isolated python environments for different projects. While there are a few existing tools in Python to do this, there's one critical feature which makes it groundbreaking: *it's easy to use*.\n\n**Installing UV**  \n`uv` can be installed via `curl`\n\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n\nor via `pip`\n\n    pipx install uv\n\nthe docs have a [more in-depth guide to install](https://iaee.substack.com/p/uv-intuitively-and-exhaustively-explained#:~:text=Check%20out%20the-,uv%20docs,-for%20more%20information).\n\n**Initializing a Project with UV**  \nOnce you have `uv` installed, you can run\n\n    uv init\n\nThis initializes a uv project within your directory. You can think of this as an isolated python environment that's tied to your project.\n\n**Adding Dependencies to your Project**  \nYou can add dependencies to your project with\n\n    uv add &lt;dependency name&gt;\n\nYou can download all the dependencies you might install via `pip`:\n\n    uv add pandas\n    uv add scipy\n    uv add numpy sklearn matplotlib\n\nAnd you can install from various other sources, including github repos, local wheel files, etc.\n\n**Running Within an Environment**  \nif you have a python script within your environment, you can run it with\n\n    uv run &lt;file name&gt;\n\nthis will run the file with the dependencies and python version specified for this particular environment.  This makes it super easy and convenient to bounce around between different projects. Also, if you clone a `uv` managed project, all dependencies will be installed and synchronized before the file is run.\n\n**My Thoughts**  \nI didn't realize I've been waiting for this for a long time. I always found off the cuff quick implementation of Python locally to be a pain, and I think I've been using ephemeral environments like Colab as a crutch to get around this issue. I find local development of Python projects to be significantly more enjoyable with `uv` , and thus I'll likely be adopting it as my go to approach when developing in Python locally.",
    "author": "Daniel-Warfield",
    "timestamp": "2025-07-05T11:01:17",
    "url": "https://reddit.com/r/datascience/comments/1lsfyeo/a_brief_guide_to_uv/",
    "score": 101,
    "num_comments": 57,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lt464v",
    "title": "With Generative AI looking so ominous, would there be any further research in any other domains like Computer Vision or NLP or Graph Analytics ever?",
    "content": "So as the title suggest, last few years have been just Generative AI all over the place. Every new research is somehow focussed towards it. So does this mean other fields stands still ? Or eventually everything will merge into GenAI somehow? What's your thoughts ",
    "author": "Technical-Love-8479",
    "timestamp": "2025-07-06T08:37:22",
    "url": "https://reddit.com/r/datascience/comments/1lt464v/with_generative_ai_looking_so_ominous_would_there/",
    "score": 0,
    "num_comments": 18,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lrluwg",
    "title": "Any good resources for fraud detection and credit risk modelling?",
    "content": "Hello, I am very much interested in using ML/DS in banking domain like fraud detection, loan prediction, credit risk, etc..\n\n\nI have read this book about fraud detection.\nhttps://fraud-detection-handbook.github.io/fraud-detection-handbook/Foreword.html\n\nUnderstood everything and it was fun. Now, I am looking for similar resources to work on.\n\nThank you.",
    "author": "kmeansneuralnetwork",
    "timestamp": "2025-07-04T08:41:49",
    "url": "https://reddit.com/r/datascience/comments/1lrluwg/any_good_resources_for_fraud_detection_and_credit/",
    "score": 66,
    "num_comments": 22,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lrojc3",
    "title": "How easy is it to be pigeonholed in DS?",
    "content": "Although in my PhD I used experiments and traditional statistics, my first DS role is entirely focused on NLP. There are no opportunities to use casual inference, time series, or other traditional statistical methods. \n\nHow much will this hurt my ability to apply to roles focused on these kinds of analyses? Basically, I'm wondering if my current role's focus on NLP is going to make it hard for me to get non-NLP data science positions when I'm ready to leave. \n\nIs it common for data scientists to get stuck in a niche?",
    "author": "empirical-sadboy",
    "timestamp": "2025-07-04T10:33:47",
    "url": "https://reddit.com/r/datascience/comments/1lrojc3/how_easy_is_it_to_be_pigeonholed_in_ds/",
    "score": 36,
    "num_comments": 26,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lrghkc",
    "title": "Causes of the 'Bad Market'",
    "content": "I'm just opening the floor to speculation / source dumping but everyone's talking about a suddenly very bad market for DS and DS related fields\n\n  \nI live in the north of the UK and it feels impossible to get a job out here. It sounds like its similar in the US. Is this a DS specific issue or are we just feeling what everyone else is feeling? I'm only now just emerging from a post-grad degree and I thought that hearing all these news stories about people illegally gathering and storing data that it was an indicator in how data driven so many decisions are now... which in my mind means that you'd need more DS/ ML engineers to wade through the quagmire and build solutions\n\n  \nobviously I'm wrong but why?",
    "author": "Unusual-Map6326",
    "timestamp": "2025-07-04T04:31:24",
    "url": "https://reddit.com/r/datascience/comments/1lrghkc/causes_of_the_bad_market/",
    "score": 99,
    "num_comments": 66,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lsk0sd",
    "title": "What’s the best way to automate pulling content performance metrics from LinkedIn beyond just downloading spreadsheets?",
    "content": "I’ve been stuck manually exporting post data from the LinkedIn analytics dashboard for months. Automating via API sounds ideal, but this is uncharted territory!",
    "author": "Proof_Wrap_2150",
    "timestamp": "2025-07-05T14:01:41",
    "url": "https://reddit.com/r/datascience/comments/1lsk0sd/whats_the_best_way_to_automate_pulling_content/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lqno9m",
    "title": "People who have been in the field before 2020: how do you keep up with the constantly new and changing technologies in ML/AI?",
    "content": "As someone who genuinely enjoys learning new tech, sometimes I feel it's too much to constantly keep up. I feel like it was only barely a year ago when I first learned RAG and then agents soon after, and now MCP servers.\n\nI have a life outside tech and work and I feel that I'm getting lazier and burnt out in having to keep up. Not to mention only AI-specific tech, but even with adjacent tech like MLFlow, Kubernetes, etc, there seems to be so much that I feel I should be knowing. \n\nThe reason why I asked *before 2020* is because I don't recall AI moving at this fast pace before then. Really feels like only after ChatGPT was released to the masses did the pace really pickup that now AI engineering actually feels quite different to the more classic ML engineering I was doing.",
    "author": "Illustrious-Pound266",
    "timestamp": "2025-07-03T04:56:18",
    "url": "https://reddit.com/r/datascience/comments/1lqno9m/people_who_have_been_in_the_field_before_2020_how/",
    "score": 229,
    "num_comments": 115,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lqn6pu",
    "title": "How I Use MLflow 3.1 to Bring Observability to Multi-Agent AI Applications",
    "content": "Hi everyone,\n\nIf you've been diving into the world of multi-agent AI applications, you've probably noticed a recurring issue: most tutorials and code examples out there feel like toys. They’re fun to play with, but when it comes to building something reliable and production-ready, they fall short. You run the code, and half the time, the results are unpredictable.\n\nThis was exactly the challenge I faced when I started working on enterprise-grade AI applications. I wanted my applications to not only work but also be robust, explainable, and observable. By \"observable,\" I mean being able to monitor what’s happening at every step — the inputs, outputs, errors, and even the thought process of the AI. And \"explainable\" means being able to answer questions like: *Why did the model give this result? What went wrong when it didn’t?*\n\nBut here’s the catch: as multi-agent frameworks have become more abstract and convenient to use, they’ve also made it harder to see under the hood. Often, you can’t even tell what prompt was finally sent to the large language model (LLM), let alone why the result wasn’t what you expected.\n\nSo, I started looking for tools that could help me monitor and evaluate my AI agents more effectively. That’s when I turned to MLflow. If you’ve worked in machine learning before, you might know MLflow as a model tracking and experimentation tool. But with its latest 3.x release, MLflow has added specialized support for GenAI projects. And trust me, it’s a game-changer.\n\n[MLflow's tracking records. ](https://preview.redd.it/k3i2hbh18naf1.png?width=948&amp;format=png&amp;auto=webp&amp;s=91cd9c33943b0d612fda2e8874b4979c60ce0618)\n\n# Why Observability Matters\n\nBefore diving into the details, let’s talk about why this is important. In any AI application, but especially in multi-agent setups, you need three key capabilities:\n\n1. **Observability:** Can you monitor the application in real time? Are there logs or visualizations to see what’s happening at each step?\n2. **Explainability:** If something goes wrong, can you figure out why? Can the algorithm explain its decisions?\n3. **Traceability:** If results deviate from expectations, can you reproduce the issue and pinpoint its cause?\n\n[Three key metrics for evaluating the stability of enterprise GenAI applications. Image by Author](https://preview.redd.it/azgs0y3j7naf1.png?width=820&amp;format=png&amp;auto=webp&amp;s=9fbf70b52379d8e8e869eab3bb3acc9b9450942f)\n\nWithout these, you’re flying blind. And when you’re building enterprise-grade systems where reliability is critical, flying blind isn’t an option.\n\n# How MLflow Helps\n\nMLflow is best known for its model tracking capabilities, but its GenAI features are what really caught my attention. It lets you track everything — from the prompts you send to the LLM to the outputs it generates, even in streaming scenarios where the model responds token by token.\n\n[The Events tab in MLflow interface records every SSE message.](https://preview.redd.it/7mteb23c8naf1.png?width=858&amp;format=png&amp;auto=webp&amp;s=1d0de24b21a58abeca9db0bbc7feeddd106c7fbc)\n\n[MLflow's Autolog can also stitch together streaming messages in the Chat interface.](https://preview.redd.it/h77q9y3h8naf1.png?width=859&amp;format=png&amp;auto=webp&amp;s=b3ab949f1cab02c2d71da952d6b6fc60bb2bac91)\n\nThe setup is straightforward. You can annotate your code, use MLflow’s \"autolog\" feature for automatic tracking, or leverage its context managers for more granular control. For example:\n\n* Want to know exactly what prompt was sent to the model? Tracked.\n* Want to log the inputs and outputs of every function your agent calls? Done.\n* Want to monitor errors or unusual behavior? MLflow makes it easy to capture that too.\n\n[You can view code execution error messages in the Events interface.](https://preview.redd.it/svx0fnpm8naf1.png?width=854&amp;format=png&amp;auto=webp&amp;s=d7edbc819dbbccad8a0e881efce310c4b9553a02)\n\nAnd the best part? MLflow’s UI makes all this data accessible in a clean, organized way. You can filter, search, and drill down into specific runs or spans (i.e., individual events in your application).\n\n# A Real-World Example\n\nI have a project involving building a workflow using Autogen, a popular multi-agent framework. The system included three agents:\n\n1. A **generator** that creates ideas based on user input.\n2. A **reviewer** that evaluates and refines those ideas.\n3. A **summarizer** that compiles the final output.\n\nWhile the framework made it easy to orchestrate these agents, it also abstracted away a lot of the details. At first, everything seemed fine — the agents were producing outputs, and the workflow ran smoothly. But when I looked closer, I realized the summarizer wasn’t getting all the information it needed. The final summaries were vague and uninformative.\n\nWith MLflow, I was able to trace the issue step by step. By examining the inputs and outputs at each stage, I discovered that the summarizer wasn’t receiving the generator’s final output. A simple configuration change fixed the problem, but without MLflow, I might never have noticed it.\n\n[I might never have noticed that the agent wasn't passing the right info to the LLM until MLflow helped me out.](https://preview.redd.it/q7giinxu8naf1.png?width=960&amp;format=png&amp;auto=webp&amp;s=05a3e7c983191836e6ceae8a8f689613d5acf77c)\n\n# Why I’m Sharing This\n\nI’m not here to sell you on MLflow — it’s open source, after all. I’m sharing this because I know how frustrating it can be to feel like you’re stumbling around in the dark when things go wrong. Whether you’re debugging a flaky chatbot or trying to optimize a complex workflow, having the right tools can make all the difference.\n\nIf you’re working on multi-agent applications and struggling with observability, I’d encourage you to give MLflow a try. It’s not perfect (I had to patch a few bugs in the Autogen integration, for example), but it’s the tool I’ve found for the job so far.",
    "author": "qtalen",
    "timestamp": "2025-07-03T04:29:11",
    "url": "https://reddit.com/r/datascience/comments/1lqn6pu/how_i_use_mlflow_31_to_bring_observability_to/",
    "score": 30,
    "num_comments": 3,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lq8shf",
    "title": "How can I get international remote positions?",
    "content": "Hello folks! I am a data scientist in Brazil and in general, I have a good resume. I have experience working in big techs, startup,  consulting and a MsC degree. \n\nI get Brazilian interviews easily but not abroad, even if I have a LinkedIn profile in English. How can I get considered for a remote position from US or Europe so I can keep working from my country?",
    "author": "BirdLadyTraveller",
    "timestamp": "2025-07-02T15:06:14",
    "url": "https://reddit.com/r/datascience/comments/1lq8shf/how_can_i_get_international_remote_positions/",
    "score": 95,
    "num_comments": 74,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lr8l54",
    "title": "I just got LinkedIn Learning, what courses do you recommend I take on Data Science?",
    "content": "I’m kinda new to it but dont shy away from giving me the more advanced courses as I’ll be able to learn more\n\nIm going to charge my phone",
    "author": "Particular_Reality12",
    "timestamp": "2025-07-03T20:22:35",
    "url": "https://reddit.com/r/datascience/comments/1lr8l54/i_just_got_linkedin_learning_what_courses_do_you/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.44,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lq4xgr",
    "title": "How much wiggle room do you give yourself on DS projects?",
    "content": "When you’re starting a project, how much extra time do you give yourself for the deadline that you share with stakeholders?\n\nI personally will multiply the time I think I can complete something in by 1.5-2. Honestly might start multiplying by 3 to make multitasking easier.\n\nThere’s just so much that can go wrong in DS related projects so I feel it’s necessary to do this. Basically just underpromise overdeliver as they say.\n\nInterested to hear about different situations.",
    "author": "Fit-Employee-4393",
    "timestamp": "2025-07-02T12:27:18",
    "url": "https://reddit.com/r/datascience/comments/1lq4xgr/how_much_wiggle_room_do_you_give_yourself_on_ds/",
    "score": 56,
    "num_comments": 27,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lq79vo",
    "title": "A Breakdown of A2A, MCP, and Agentic Interoperability",
    "content": "MCP and A2A are both emerging standards in AI. In this post I want to cover what they're both useful for (based on my experience) from a practical level, and some of my thoughts about where the two protocols will go moving forward. Both of these protocols are still actively evolving, and I think there's room for interpretation around where they should go moving forward. As a result, I don't think there is a single, correct interpretation of A2A and MCP. These are my thoughts.\n\n**What is MCP?**  \nFrom it's highest level, MCP (model context protocol) is a standard way to expose tools to AI agents. More specifically, it's a standard way to communicate tools to a client which is managing the execution of an LLM within a logical loop. There's not really one, single, god almighty way to feed tools into an LLM, but MCP defines a standard on how tools are defined to make that process more streamlined.\n\nThe whole idea of MCP is derivative from LSP (language server protocol), which emerged due to a practical need from programming language and code editor developers. If you're working on something like VS Code, for instance, you don't want to implement hooks for Rust, Python, Java, etc. If you make a new programming language, you don't want to integrate it into vscode, sublime, jetbrains, etc.  The problem of \"connect programming language to text editor, with syntax highlighting and autocomplete\" was abstracted to a generalized problem, and solved with LSP. The idea is that, if you're making a new language, you create an LSP server so that language will work in any text editor. If you're building a new text editor, you can support LSP to automatically support any modern programming language.\n\n[A conceptual diagram of LSPs \\(source: MCP IAEE\\)](https://preview.redd.it/wz60k2hswiaf1.jpg?width=1050&amp;format=pjpg&amp;auto=webp&amp;s=1c42845286b2bb05047bd0c32caf6a25ca7fdcac)\n\nMCP does something similar, but for agents and tools. The idea is to represent tool use in a standardized way, such developers can put tools in an MCP server, and so developers working on agentic systems can use those tools via a standardized interface.\n\n[LSP and MCP are conceptually similar in terms of their core workflow \\(source: MCP IAEE\\)](https://preview.redd.it/clc7u0qehiaf1.png?width=1050&amp;format=png&amp;auto=webp&amp;s=6790f5a438aff994337a2224736ba986f1c17777)\n\nI think it's important to note, MCP presents a standardized **interface** for tools, but there is leeway in terms of how a developer might choose to build tools and resources within an MCP server, and there is leeway around how MCP client developers might choose to use those tools and resources.\n\nMCP has various \"transports\" defined, transports being means of communication between the client and the server. MCP can communicate both over the internet, and over local channels (allowing the MCP client to control local tools like applications or web browsers). In my estimation, the latter is really what MCP was designed for. In theory you can connect with an MCP server hosted on the internet, but MCP is chiefly designed to allow clients to execute a locally defined server.\n\nHere's an example of a simple MCP server:\n\n    \"\"\"A very simple MCP server, which exposes a single very simple tool. In most\n    practical applications of MCP, a script like this would be launched by the client,\n    then the client can talk with that server to execute tools as needed.\n    source: MCP IAEE.\n    \"\"\"\n    \n    from mcp.server.fastmcp import FastMCP\n    \n    mcp = FastMCP(\"server\")\n    \n    u/mcp.tool()\n    def say_hello(name: str) -&gt; str:\n        \"\"\"Constructs a greeting from a name\"\"\"\n        return f\"hello {name}, from the server!\n\nIn the normal workflow, the MCP client would spawn an MCP server based on a script like this, then would work with that server to execute tools as needed.\n\n**What is A2A?**  \nIf MCP is designed to expose tools to AI agents, A2A is designed to allow AI agents to talk to one another. I think this diagram summarizes how the two technologies interoperate with on another nicely:\n\n[A conceptual diagram of how A2A and MCP might work together. \\(Source: A2A Home Page\\)](https://preview.redd.it/gb2bj773ziaf1.png?width=640&amp;format=png&amp;auto=webp&amp;s=c74c1ced5fc1e9026670f68487431392f79d0a4e)\n\nSimilarly to MCP, A2A is designed to standardize communication between AI resource. However, A2A is specifically designed for allowing agents to communicate with one another. It does this with two fundamental concepts:\n\n1. Agent Cards: a structure description of what an agent does and where it can be found.\n2. Tasks: requests can be sent to an agent, allowing it to execute on tasks via back and forth communication.\n\nA2A is peer-to-peer, asynchronous, and is natively designed to support online communication. In python, A2A is built on top of ASGI (asynchronous server gateway interface), which is the same technology that powers FastAPI and Django.\n\nHere's an example of a simple A2A server:\n\n    from a2a.server.agent_execution import AgentExecutor, RequestContext\n    from a2a.server.apps import A2AStarletteApplication\n    from a2a.server.request_handlers import DefaultRequestHandler\n    from a2a.server.tasks import InMemoryTaskStore\n    from a2a.server.events import EventQueue\n    from a2a.utils import new_agent_text_message\n    from a2a.types import AgentCard, AgentSkill, AgentCapabilities\n    \n    import uvicorn\n    \n    class HelloExecutor(AgentExecutor):\n        async def execute(self, context: RequestContext, event_queue: EventQueue) -&gt; None:\n            # Respond with a static hello message\n            event_queue.enqueue_event(new_agent_text_message(\"Hello from A2A!\"))\n    \n        async def cancel(self, context: RequestContext, event_queue: EventQueue) -&gt; None:\n            pass  # No-op\n    \n    \n    def create_app():\n        skill = AgentSkill(\n            id=\"hello\",\n            name=\"Hello\",\n            description=\"Say hello to the world.\",\n            tags=[\"hello\", \"greet\"],\n            examples=[\"hello\", \"hi\"]\n        )\n    \n        agent_card = AgentCard(\n            name=\"HelloWorldAgent\",\n            description=\"A simple A2A agent that says hello.\",\n            version=\"0.1.0\",\n            url=\"http://localhost:9000\",\n            skills=[skill],\n            capabilities=AgentCapabilities(),\n            authenticationSchemes=[\"public\"],\n            defaultInputModes=[\"text\"],\n            defaultOutputModes=[\"text\"],\n        )\n    \n        handler = DefaultRequestHandler(\n            agent_executor=HelloExecutor(),\n            task_store=InMemoryTaskStore()\n        )\n    \n        app = A2AStarletteApplication(agent_card=agent_card, http_handler=handler)\n        return app.build()\n    \n    \n    if __name__ == \"__main__\":\n        uvicorn.run(create_app(), host=\"127.0.0.1\", port=9000)\n\nThus A2A has important distinctions from MCP:\n\n* A2A is designed to support \"discoverability\" with agent cards. MCP is designed to be explicitly pointed to.\n* A2A is designed for asynchronous communication, allowing for complex implementations of multi-agent workloads working in parallel.\n* A2A is designed to be peer-to-peer, rather than having the rigid hierarchy of MCP clients and servers.\n\n**A Point of Friction**  \nI think the high level conceptualization around MCP and A2A is pretty solid; MCP is for tools, A2A is for inter-agent communication.\n\n[A high level breakdown of the core usage of MCP and A2A \\(source: MCP vs A2A\\)](https://preview.redd.it/s8ba9ov6ziaf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=7c4db19dde15d13cc34372e9c7449ad91939ad28)\n\nDespite the high level clarity, I find these clean distinctions have a tendency to break down practically in terms of implementation. I was working on an example of an application which leveraged both MCP and A2A. I poked around the internet, and found [a repo of examples](https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents/a2a_mcp) from the official a2a github account. In these examples, they actually use MCP to expose A2A as a set of tools. So, instead of the two protocols existing independently:\n\n[How MCP and A2A might commonly be conceptualized, within a sample application consisting of a travel agent, a car agent, and an airline agent. \\(source: A2A IAEE\\)](https://preview.redd.it/5wxavpimniaf1.png?width=1050&amp;format=png&amp;auto=webp&amp;s=b092517d6df915c72b673898f3bf563f5dda16d0)\n\nCommunication over A2A happens within MCP servers:\n\n[Another approach of implementing A2A and MCP. \\(source: A2A IAEE\\)](https://preview.redd.it/dh3de5xuniaf1.png?width=1050&amp;format=png&amp;auto=webp&amp;s=d3f46df060e30bb2d71b24ecfc670566f643322f)\n\nThis violates the conventional wisdom I see online of A2A and MCP essentially operating as completely separate and isolated protocols. I think the key benefit of this approach is ease of implementation: You don't have to expose both A2A and MCP as two seperate sets of tools to the LLM. Instead, you can expose only a single MCP server to an LLM (that MCP server containing tools for A2A communication). This makes it much easier to manage the integration of A2A and MCP into a single agent. Many LLM providers have plenty of demos of MCP tool use, so using MCP as a vehicle to serve up A2A is compelling.\n\nYou can also use the two protocols in isolation, I imagine. There are a ton of ways MCP and A2A enabled projects can practically be implemented, which leads to closing thoughts on the subject.\n\n**My thoughts on MCP and A2A**  \nIt doesn't matter how standardized MCP and A2A are; if we can't all agree on the larger structure they exist in, there's no interoperability. In the future I expect frameworks to be built on top of both MCP and A2A to establish and enforce best practices. Once the industry converges on these new frameworks, I think issues of \"should this be behind MCP or A2A\" and \"how should I integrate MCP and A2A into this agent\" will start to go away. This is a standard part of the lifecycle of software development, and we've seen the same thing happen with countless protocols in the past.\n\nStandardizing prompting, though, is a different beast entirely.\n\nHaving managed the development of LLM powered applications for a while now, I've found prompt engineering to have an interesting role in the greater product development lifecycle. Non-technical stakeholders have a tendency to flock to prompt engineering as a catch all way to solve any problem, which is totally untrue. Developers have a tendency to disregard prompt engineering as a secondary concern, which is also totally untrue. The fact is, prompt engineering won't magically make an LLM powered application better, but bad prompt engineering sure can make it worse. When you hook into MCP and A2A enabled systems, you are essentially allowing for arbitrary injection of prompts as they are defined in these systems. This may have some security concerns if your code isn't designed in a hardened manner, but more palpably there are massive performance concerns. Simply put, if your prompts aren't synergistic with one another throughout an LLM powered application, you won't get good performance. This seriously undermines the practical utility of MCP and A2A enabling turn-key integration.\n\nI think the problem of a framework to define when a tool should be MCP vs A2A is immediately solvable. In terms of prompt engineering, though, I'm curious if we'll need to build rigid best practices around it, or if we can devise clever systems to make interoperable agents more robust to prompting inconsistencies.\n\n**Sources:**  \nMCP [vs A2A](https://www.eyelevel.ai/post/a2a-vs-mcp-how-agent-protocols-really-work-and-where-each-one-wins) (I co-authored)  \n[MCP IAEE ](https://iaee.substack.com/p/model-context-protocol-intuitively) (I authored)  \n[A2A IAEE](https://iaee.substack.com/p/agent-to-agent-protocol-intuitively?utm_source=publication-search) (I authored)  \n[A2A MCP Examples](https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents/a2a_mcp)  \n[A2A Home Page](https://a2aproject.github.io/A2A/latest/)\n\n  \n\n\n  \n",
    "author": "Daniel-Warfield",
    "timestamp": "2025-07-02T14:02:43",
    "url": "https://reddit.com/r/datascience/comments/1lq79vo/a_breakdown_of_a2a_mcp_and_agentic/",
    "score": 33,
    "num_comments": 6,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lq0v8p",
    "title": "I am currently a data scientist. How can I move to a more business oriented rule?",
    "content": "Hey folks! I have worked as a DS for about 5 years now. I wanted to move to a position that I still work with data, but I am  looking for something less technical and more business related. I will list some of my strengths that are also things I like to work with:\n\n- Build proof of concepts projects and explore techniques in the literature to solve business problems with data science approaches;\n- Do presentation for technical and non technical peers;\n- Build documentation and produce online content;\n- I also love to create training and manage projects related to data culture, education, and onboarding.\n- Work in groups /having group discussions with multidisciplinary teams. \n\nDo you know names for positions that are more focused on that? I'd like to search for them! ",
    "author": "BirdLadyTraveller",
    "timestamp": "2025-07-02T09:47:58",
    "url": "https://reddit.com/r/datascience/comments/1lq0v8p/i_am_currently_a_data_scientist_how_can_i_move_to/",
    "score": 47,
    "num_comments": 35,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lpucaf",
    "title": "Need advise on cross-functional collaboration",
    "content": "Hi data science community,\n\nI need your advice on how to handle a work situation. Curious to know how others would handle or if they have been in a similar situation.\n\nI lead a data science team and I also have a peer who leads a BI team and we report to the same executive.\n\nA couple months ago, BI lead reached out and was excited to see if we can collaborate and create an AI/BI chat bot for our internal structured data. I thought this was a good idea and would be a great opportunity to collaborate with him and his team. So I spent a couple of weeks to build out a POC, I show cased it to him and our executive, it was well received and I outlined next steps on how we can collaborate to make it better.\n\nI got no response from him about my next steps email. I figured no harm no foul he got busy I’m sure. Well come to find out, he had his team build almost an exact replica of the POC I did and essentially boxed my team and I out of this idea and decided he would just do it himself internally. Mind you, all the BI people had to learn how to use LLMs and how to orchestrate agents, etc. it’s a skill set we have but he decided to do it himself despite this.\n\nHow would you all handle this?\n\nI was planning on a 1:1 with him where I essentially lay out the facts that he wasted my time by giving me the illusion that we would work together and collaborate but instead just did things himself. We have been getting pushed by our executive team to work together more and this was a great opportunity to show them we work together but instead he decided to take a different route.",
    "author": "Belmeez",
    "timestamp": "2025-07-02T05:17:58",
    "url": "https://reddit.com/r/datascience/comments/1lpucaf/need_advise_on_crossfunctional_collaboration/",
    "score": 18,
    "num_comments": 8,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lohjp4",
    "title": "No reason to complicate things.",
    "content": "There's absolutely validity in doing more complex visuals.  But, sometimes simple is better if the audience is more likely to use it/understand it.",
    "author": "ElectrikMetriks",
    "timestamp": "2025-06-30T12:59:00",
    "url": "https://reddit.com/r/datascience/comments/1lohjp4/no_reason_to_complicate_things/",
    "score": 1232,
    "num_comments": 75,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lpnkj0",
    "title": "Beta release: Minds AI Filter for EEG — Physics-informed preprocessing for real-time BCI (+17% gain on noisy data from commercial headsets, 0.2s latency)",
    "content": "We at MindsApplied specialize in the development of machine learning models for the enhancement of EEG signal quality and emotional state classification. We're excited to share our latest model—the Minds AI Filter—and would love your feedback.\n\n* [👉 Download the Python package here](https://drive.google.com/drive/folders/1_4Q9voe5j88G_EMF8YanoeEPVoUt_D2B?usp=drive_link)\n* 🔑Use key: ''REDDIT-KEY-VRG44S' to initialize\n* 📄 Includes setup instructions\n\nThe Minds AI Filter is a physics-informed, real-time EEG preprocessing tool that relies on sensor fusion for low-latency noise and artifact removal. It's built to improve signal quality before feature extraction or classification, especially for online systems. To dive (very briefly) into the details, it works in part by **reducing high-frequency noise (\\~40 Hz) and sharpening low-frequency activity (\\~3–7 Hz)**.\n\nWe tested it alongside standard bandpass filtering, using both:\n\n* Commercial EEG hardware (OpenBCI Mark IV, BrainBit Dragon)\n* The public DEAP dataset, a 32-participant benchmark for emotional state classification\n\nHere are our experimental results:\n\n* Commercial Devices (OpenBCI Mark IV, BrainBit Dragon)\n   * \\+15% average improvement in balanced accuracy using only 12 trials of 60 seconds per subject per device\n   * Improvement attributed to higher baseline noise in these systems\n* DEAP Dataset\n   * \\+6% average improvement across 32 subjects and 32 channels\n   * Maximum individual gain: +35%\n   * Average gain in classification accuracy was 17% for cases where the filter led to improvement.\n   * No decline in accuracy for any participant\n* Performance\n   * \\~0.2 seconds to filter 60 seconds of data\n\nNote: Comparisons were made between bandpass-only and bandpass + Minds AI Filter. Filtering occurred before bandpass.\n\nMethodology:\n\nTo generate these experimental results, we used 2-fold stratified cross-validation grid search to tune the filter's key hyperparameter (λ). Classification relied on balanced on balanced accuracy using logistic regression on features derived from wavelet coefficients.\n\nWhy we're posting: This filter is still in beta and we'd love feedback —especially if you try it on your own datasets or devices. The current goal is to support rapid, adaptive, and physics-informed filtering for real-time systems and multi-sensor neurotech platforms.\n\nIf you find it useful or want future updates (e.g., universal DLL, long-term/offline licenses), you can subscribe here:\n\n* 🔗 [https://www.minds-applied.com/contact](https://www.minds-applied.com/contact)\n\nhttps://preview.redd.it/o3xqckeiaeaf1.png?width=594&amp;format=png&amp;auto=webp&amp;s=0fb1860d8af85fa516cb705c096427a32977a522\n\nhttps://preview.redd.it/95lbzd8jaeaf1.png?width=589&amp;format=png&amp;auto=webp&amp;s=39984d0e8f75f27ab0a71e7e5ca09bba25f6ffb4\n\nhttps://preview.redd.it/x9iyc4kjaeaf1.png?width=1372&amp;format=png&amp;auto=webp&amp;s=ef70703c892727318688b7472778cb5658a899ee\n\n",
    "author": "statius9",
    "timestamp": "2025-07-01T22:16:35",
    "url": "https://reddit.com/r/datascience/comments/1lpnkj0/beta_release_minds_ai_filter_for_eeg/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1loo3eh",
    "title": "Does DB normalization worth it?",
    "content": "Hi, I have 6 months as a Jr Data Analyst and I have been working with Power BI since I begin. At the beginning I watched a lot of dashboards on PBI and when I checked the Data Model was disgusting, it doesn't seems as something well designed. \n\nOn my the few opportunities that I have developed some dashboards I have seen a lot of redundancies on them, but I keep quiet due it's my first analytic role and my role using PBI so I couldn't compare with anything else.\n\nI ask here because I don't know many people who use PBI or has experience on Data related jobs and I've been dealing with query limit reaching (more than 10M rows to process).\n\nSo I watched some courses that normalization could solve many issues, but I wanted to know:\n1 - If it could really help to solve that issue.\n2 - How could I normalize the data when, not the data, the data Model is so messy? \n\nThanks in advance.",
    "author": "Karl_mstr",
    "timestamp": "2025-06-30T17:36:08",
    "url": "https://reddit.com/r/datascience/comments/1loo3eh/does_db_normalization_worth_it/",
    "score": 29,
    "num_comments": 32,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lo4xao",
    "title": "Model Context Protocol (MCP) tutorials playlist for beginners",
    "content": "This playlist comprises of numerous tutorials on MCP servers including\n\n1. Install Blender-MCP for Claude AI on Windows\n2. Design a Room with Blender-MCP + Claude\n3. Connect SQL to Claude AI via MCP\n4. Run MCP Servers with Cursor AI\n5. Local LLMs with Ollama MCP Server\n6. Build Custom MCP Servers (Free)\n7. Control Docker via MCP\n8. Control WhatsApp with MCP\n9. GitHub Automation via MCP\n10. Control Chrome using MCP\n11. Figma with AI using MCP\n12. AI for PowerPoint via MCP\n13. Notion Automation with MCP\n14. File System Control via MCP\n15. AI in Jupyter using MCP\n16. Browser Automation with Playwright MCP\n17. Excel Automation via MCP\n18. Discord + MCP Integration\n19. Google Calendar MCP\n20. Gmail Automation with MCP\n21. Intro to MCP Servers for Beginners\n22. Slack + AI via MCP\n23. Use Any LLM API with MCP\n24. Is Model Context Protocol Dangerous?\n25. LangChain with MCP Servers\n26. Best Starter MCP Servers\n27. YouTube Automation via MCP\n28. Zapier + AI using MCP\n29. MCP with Gemini 2.5 Pro\n30. PyCharm IDE + MCP\n31. ElevenLabs Audio with Claude AI via MCP\n32. LinkedIn Auto-Posting via MCP\n33. Twitter Auto-Posting with MCP\n34. Facebook Automation using MCP\n35. Top MCP Servers for Data Science\n36. Best MCPs for Productivity\n37. Social Media MCPs for Content Creation\n38. MCP Course for Beginners\n39. Create n8n Workflows with MCP\n40. RAG MCP Server Guide\n41. Multi-File RAG via MCP\n42. Use MCP with ChatGPT\n43. ChatGPT + PowerPoint (Free, Unlimited)\n44. ChatGPT RAG MCP\n45. ChatGPT + Excel via MCP\n46. Use MCP with Grok AI\n47. Vibe Coding in Blender with MCP\n48. Perplexity AI + MCP Integration\n49. ChatGPT + Figma Integration\n50. ChatGPT + Blender MCP\n51. ChatGPT + Gmail via MCP\n52. ChatGPT + Google Calendar MCP\n53. MCP vs Traditional AI Agents\n\nHope this is useful !!\n\nPlaylist : [https://www.youtube.com/playlist?list=PLnH2pfPCPZsJ5aJaHdTW7to2tZkYtzIwp](https://www.youtube.com/playlist?list=PLnH2pfPCPZsJ5aJaHdTW7to2tZkYtzIwp)",
    "author": "Technical-Love-8479",
    "timestamp": "2025-06-30T04:19:26",
    "url": "https://reddit.com/r/datascience/comments/1lo4xao/model_context_protocol_mcp_tutorials_playlist_for/",
    "score": 26,
    "num_comments": 1,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lntegl",
    "title": "ICs who pivoted: did you go engineering or management?",
    "content": "Hitting that point where I feel like I need to pick a lane.\n\nCurious what others did. Did you double down on technical stuff (data engineering/MLE/SWE), switched to the product side, or did you move into people management?",
    "author": "ergodym",
    "timestamp": "2025-06-29T16:57:36",
    "url": "https://reddit.com/r/datascience/comments/1lntegl/ics_who_pivoted_did_you_go_engineering_or/",
    "score": 60,
    "num_comments": 39,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lny1dk",
    "title": "Weekly Entering &amp; Transitioning - Thread 30 Jun, 2025 - 07 Jul, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-06-29T21:01:14",
    "url": "https://reddit.com/r/datascience/comments/1lny1dk/weekly_entering_transitioning_thread_30_jun_2025/",
    "score": 11,
    "num_comments": 63,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lmv5uf",
    "title": "Unpopular Opinion: These are the most useless posters on LinkedIn",
    "content": "LinkedIn influencers love to treat the two roles as different species. In most enterprises, especially in mid to small orgs, these roles are largely overlapping. ",
    "author": "OverratedDataScience",
    "timestamp": "2025-06-28T12:27:04",
    "url": "https://reddit.com/r/datascience/comments/1lmv5uf/unpopular_opinion_these_are_the_most_useless/",
    "score": 1327,
    "num_comments": 115,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ln6aeq",
    "title": "How’s the job market for Bayesian statistics?",
    "content": "I’m a data scientist with 1 YOE. mostly worked on credit scoring models, sql, and Power BI. Lately, I’ve been thinking of going deeper into bayesian statistics and I’m currently going through the s*tatistical rethinking* book.\n\nBut I’m wondering. is it worth focusing heavily on bayesian stats? Or should I pivot toward something that opens up more job opportunities?\n\nWould love to hear your thoughts or experiences!",
    "author": "guna1o0",
    "timestamp": "2025-06-28T21:47:53",
    "url": "https://reddit.com/r/datascience/comments/1ln6aeq/hows_the_job_market_for_bayesian_statistics/",
    "score": 138,
    "num_comments": 72,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ln3zyk",
    "title": "Is ML/AI engineering increasingly becoming less focused on model training and more focused on integrating LLMs to build web apps?",
    "content": "One thing I've noticed recently is that increasingly, a lot of AI/ML roles seem to be focused on ways to integrate LLMs to build web apps that automate some kind of task, e.g. chatbot with RAG or using agent to automate some task in a consumer-facing software with tools like langchain, llamaindex, Claude, etc. I feel like there's less and less of the \"classical\" ML training and building models.\n\nI am not saying that \"classical\" ML training will go away. I think model building/training non-LLMs will always have some place in data science. But in a way, I feel like \"AI engineering\" seems increasingly converging to something closer to back-end engineering you typically see in full-stack. What I mean is that rather than focusing on building or training models, it seems that the bulk of the work now seems to be about how to take LLMs from model providers like OpenAI and Anthropic, and use it to build some software that automates some work with Langchain/Llamaindex.\n\nIs this a reasonable take? I know we can never predict the future, but the trends I see seem to be increasingly heading towards that.",
    "author": "Illustrious-Pound266",
    "timestamp": "2025-06-28T19:35:38",
    "url": "https://reddit.com/r/datascience/comments/1ln3zyk/is_mlai_engineering_increasingly_becoming_less/",
    "score": 161,
    "num_comments": 41,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ln9cf0",
    "title": "Advice on feature selection process",
    "content": "**Hi everyone,**\n\nI have a question regarding the feature selection process for a credit risk model I'm building as part of my internship. I've collected raw data and conducted feature engineering with the help of a domain expert in credit risk. Now I have a list of around 2000 features.\n\nFor the feature selection part, based on what I've learned, the typical approach is to use a tree-based model (like Random Forest or XGBoost) to rank feature importance, and then shortlist it down to about 15–20 features. After that, I would use those selected features to train my final model (CatBoost in this case), perform hyperparameter tuning, and then use that model for inference.\n\nAm I doing it correctly? It feels a bit too straightforward — like once I have the 2000 features, I just plug them into a tree model, get the top features, and that's it. I noticed that some of my colleagues do multiple rounds of feature selection — for example, narrowing it down from 2000 to 200, then to 80, and finally to 20 — using multiple tree models and iterations.\n\nAlso, where do SHAP values fit into this process? I usually use SHAP to visualize feature effects in the final model for interpretability, but I'm wondering if it can or should be used during the feature selection stage as well.\n\nI’d really appreciate your advice!",
    "author": "Round-Paramedic-2968",
    "timestamp": "2025-06-29T01:05:12",
    "url": "https://reddit.com/r/datascience/comments/1ln9cf0/advice_on_feature_selection_process/",
    "score": 31,
    "num_comments": 20,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lmi8j8",
    "title": "The \"Unicorn\" is Dead: A Four-Era History of the Data Scientist Role and Why We're All Engineers Now",
    "content": "Hey everyone,\n\nI’ve been in this field for a while now, starting back when \"Big Data\" was the big buzzword, and I've been thinking a lot about how drastically our roles have changed. It feels like the job description for a \"Data Scientist\" has been rewritten three or four times over. The \"unicorn\" we all talked about a decade ago feels like a fossil today.\n\nI wanted to map out this evolution, partly to make sense of it for myself, but also to see if it resonates with your experiences. I see it as four distinct eras.\n\n---\n\n### **Era 1: The BI &amp; Stats Age (The \"Before Times,\" Pre-2010)**\n\nRemember this? Before \"Data Scientist\" was a thing, we were all in our separate corners.\n\n*   **Who we were:** BI Analysts, Statisticians, Database Admins, Quants.\n*   **What we did:** Our world revolved around historical reporting. We lived in SQL, wrestling with relational databases and using tools like Business Objects or good old Excel to build reports. The core question was always, **\"What happened last quarter?\"**\n*   **The \"advanced\" stuff:** If you were a true statistician, maybe you were building logistic regression models in SAS, but that felt very separate from the day-to-day business analytics. It was more academic, less integrated.\n\nThe mindset was purely descriptive. We were the historians of the company's data.\n\n### **Era 2: The Golden Age of the \"Unicorn\" (Roughly 2011-2018)**\n\nThis is when everything changed. *HBR* called our job the \"sexiest\" of the century, and the hype was real.\n\n*   **The trigger:** Hadoop and Spark made \"Big Data\" accessible, and Python with Scikit-learn became an absolute powerhouse. Suddenly, you could do serious modeling on your own machine.\n*   **The mission:** The game changed from \"What happened?\" to **\"What's *going* to happen?\"** We were all building churn models, recommendation engines, and trying to predict the future. The Jupyter Notebook was our kingdom.\n*   **The \"unicorn\" expectation:** This was the peak of the \"full-stack\" ideal. One person was supposed to understand the business, wrangle the data, build the model, and then explain it all in a PowerPoint deck. The *insight* from the model was the final product. It was an incredibly fun, creative, and exploratory time.\n\n### **Era 3: The Industrial Age &amp; The Great Bifurcation (Roughly 2019-2023)**\n\nThis is where, in my opinion, the \"unicorn\" myth started to crack. Companies realized a model sitting in a notebook doesn't actually *do* anything for the business. The focus shifted from *building models* to *deploying systems*.\n\n*   **The trigger:** The cloud matured. AWS, GCP, and Azure became the standard, and the discipline of MLOps was born. The problem wasn't \"can we predict it?\" anymore. It was, **\"Can we serve these predictions reliably to millions of users with low latency?\"**\n*   **The splintering:** The generalist \"Data Scientist\" role started to fracture into specialists because no single person could master it all:\n    *   **ML Engineers:** The software engineers who actually productionized the models.\n    *   **Data Engineers:** The unsung heroes who built the reliable data pipelines with tools like Airflow and dbt.\n    *   **Analytics Engineers:** The new role that owned the data modeling layer for BI.\n*   The mindset became engineering-first. We were building factories, not just artisanal products.\n\n### **Era 4: The Autonomous Age (2023 - Today and Beyond)**\n\nAnd then, everything changed again. The arrival of truly powerful LLMs completely upended the landscape.\n\n*   **The trigger:** ChatGPT went public, GPT-4 was released, and frameworks like LangChain gave us the tools to build on top of this new paradigm.\n*   **The mission:** The core question has evolved again. It's not just about prediction anymore; it's about **action and orchestration**. The question is, **\"How do we build a system that can understand a goal, create a plan, and execute it?\"**\n*   **The new reality:**\n    *   **Prediction becomes a feature, not the product.** An AI *agent* doesn't just predict churn; it takes an *action* to prevent it.\n    *   **We are all systems architects now.** We're not just building a model; we're building an intelligent, multi-step workflow. We're integrating vector databases, multiple APIs, and complex reasoning loops.\n    *   **The engineering rigor from Era 3 is now the mandatory foundation.** You can't build a reliable agent without solid MLOps and real-time data engineering (Kafka, Flink, etc.).\n\nIt feels like the \"science\" part of our job is now less about statistical analysis (AI can do a lot of that for us) and more about the rigorous, empirical science of architecting and evaluating these incredibly complex, often non-deterministic systems.\n\nSo, that's my take. The \"Data Scientist\" title isn't dead, but the \"unicorn\" generalist ideal of 2015 certainly is. We've been pushed to become deeper specialists, and for most of us on the building side, that specialty looks a lot more like engineering than anything else.\n\nCurious to hear if this matches up with what you're all seeing in your roles. Did I miss an era? Is your experience different?\n\nEDIT: In response to comments asking if this was written by AI: The underlying ideas are based on my own experience.\n\nHowever, I want to be transparent that I would not have been able to articulate my vague, intuitive thoughts about the changes in this field with such precision. \n\nI used AI specifically for the structurization and organization of the content.",
    "author": "petburiraja",
    "timestamp": "2025-06-28T01:56:59",
    "url": "https://reddit.com/r/datascience/comments/1lmi8j8/the_unicorn_is_dead_a_fourera_history_of_the_data/",
    "score": 613,
    "num_comments": 110,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lmneo7",
    "title": "I built a self-hosted Databricks",
    "content": "Hey everyone, I'm an ML Engineer who spearheaded the adoption of Databricks at work. I love the agency it affords me because I can own projects end-to-end and do everything in one place.\n\nHowever, the platform adds a lot of overhead and has a wide array of data-features I just don't care about. So many problems can be solved with a simple data pipeline and basic model (e.g. XGBoost.) Not only is there technical overhead, but systems and process overhead; bureaucracy and red-tap significantly slow delivery. Right now at work we are undertaking a \"migration\" to Databricks and man, it is such a PITA to get anything moving it isn't even funny...\n\nAnyway, I decided to try and address this myself by developing [FlintML](https://github.com/flintml/flintml), a self-hosted, all-in-one MLOps stack. Basically, Polars, Delta Lake, unified catalog, Aim experiment tracking, notebook IDE and orchestration (still working on this) fully spun up with Docker Compose.\n\nI'm hoping to get some feedback from this subreddit. I've spent a couple of months developing this and want to know whether I would be wasting time by continuing or if this might actually be useful. I am using it for my personal research projects and find it very helpful.\n\nThanks heaps",
    "author": "Mission-Balance-4250",
    "timestamp": "2025-06-28T06:55:44",
    "url": "https://reddit.com/r/datascience/comments/1lmneo7/i_built_a_selfhosted_databricks/",
    "score": 78,
    "num_comments": 20,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lmxkq8",
    "title": "Pleased to share the \"SimPy Simulation Playground\" - examples of simulations in Python from different industries",
    "content": "Just put the finishing touches to the first version of this web page where you can run SimPy examples from different industries, including parameterising the sim, editing the code if you wish, running and viewing the results.\n\nRuns entirely in your browser.\n\nHere's the link: [https://www.schoolofsimulation.com/simpy\\_simulations](https://www.schoolofsimulation.com/simpy_simulations)\n\nMy goal with this is to help provide education and informationa around how discrete-event simulation with SimPy can be applied to different industry contexts.\n\nIf you have any suggestions for other examples to add, I'd be happy to consider expanding the list!\n\nFeedback, as ever, is most welcome!",
    "author": "bobo-the-merciful",
    "timestamp": "2025-06-28T14:14:48",
    "url": "https://reddit.com/r/datascience/comments/1lmxkq8/pleased_to_share_the_simpy_simulation_playground/",
    "score": 14,
    "num_comments": 6,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lnbgna",
    "title": "Not sure what certifications to attain to increase my chances of getting an internship after third year",
    "content": "Context: I am planning to go into data science as a career. Im currently about to go into my third year and I need to secure an internship agter my third year during my coop year. To increade my chances, I want to obtain AWS certifications. The problem I am seeing is that the AWS SAA certificate seems to specific to AWS. Would the MLEA or DEA increade my chance of getting data scientist/mle internships significantly? Assume I have knowledge and projects to showcase knowledge of theoretical ML, python, sql, etc. Also assume I have cloud practitioner and AI practitioner certs but no experience with AWS whatsoever, but experience in data analysis. I would really appreciate in depth responses. Please avoid stupid comments like \"certifications are useless\" because they obv arent and can set you apart from someone with similar skill sets in other areas. ",
    "author": "ParkingTheory9837",
    "timestamp": "2025-06-29T03:28:01",
    "url": "https://reddit.com/r/datascience/comments/1lnbgna/not_sure_what_certifications_to_attain_to/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.47,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lluwlv",
    "title": "Data Science Has Become a Pseudo-Science",
    "content": "I’ve been working in data science for the last ten years, both in industry and academia, having pursued a master’s and PhD in Europe. My experience in the industry, overall, has been very positive. I’ve had the opportunity to work with brilliant people on exciting, high-impact projects. Of course, there were the usual high-stress situations, nonsense PowerPoints, and impossible deadlines, but the work largely felt meaningful.\n\nHowever, over the past two years or so, it feels like the field has taken a sharp turn. Just yesterday, I attended a technical presentation from the analytics team. The project aimed to identify anomalies in a dataset composed of multiple time series, each containing a clear inflection point. The team’s hypothesis was that these trajectories might indicate entities engaged in some sort of fraud.\n\nThe team claimed to have solved the task using “generative AI”. They didn’t go into methodological details but presented results that, according to them, were amazing. Curious, nespecially since the project was heading toward deployment, i asked about validation, performance metrics, or baseline comparisons. None were presented.\n\nLater, I found out that “generative AI” meant asking ChatGPT to generate a code. The code simply computed the mean of each series before and after the inflection point, then calculated the z-score of the difference. No model evaluation. No metrics. No baselines. Absolutely no model criticism. Just a naive approach, packaged and executed very, very quickly under the label of generative AI.\n\nThe moment I understood the proposed solution, my immediate thought was \"I need to get as far away from this company as possible\". I share this anecdote because it summarizes much of what I’ve witnessed in the field over the past two years. It feels like data science is drifting toward a kind of pseudo-science where we consult a black-box oracle for answers, and questioning its outputs is treated as anti-innovation, while no one really understand how the outputs were generated.\n\nAfter several experiences like this, I’m seriously considering focusing on academia. Working on projects like these is eroding any hope I have in the field. I know this won’t work and yet, the label generative AI seems to make it unquestionable. So I came here to ask if is this experience shared among other DSs?\n\n",
    "author": "Raz4r",
    "timestamp": "2025-06-27T07:11:44",
    "url": "https://reddit.com/r/datascience/comments/1lluwlv/data_science_has_become_a_pseudoscience/",
    "score": 2737,
    "num_comments": 353,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lnct9i",
    "title": "Using Claude Code in notebook",
    "content": "At work I use jupyter notebooks for experimentation and prototyping of data products. So far, I’ve been leveraging AI code completion type of functionality within a Python cell for finishing a line of code, writing the next few lines or writing a function altogether. \n\nBut I’m curious about the next level: using something like Claude Code open side-by side with my notebook. \n\nJust wondering if anyone is currently using this type of workflow and if you have any tips &amp; tricks or specific use cases you could share. ",
    "author": "hendrix616",
    "timestamp": "2025-06-29T04:50:32",
    "url": "https://reddit.com/r/datascience/comments/1lnct9i/using_claude_code_in_notebook/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lmaxr4",
    "title": "Using LLMs to Extract Stock Picks from YouTube",
    "content": "For anyone interested in NLP or the application of data science in finance and media, we just released a dataset + paper on extracting **stock recommendations from YouTube financial influencer videos**.\n\nThis is a real-world task that combines signals across audio, video, and transcripts. We used expert annotations and benchmarked both LLMs and multimodal models to see how well they can extract structured recommendation data (like ticker and action) from messy, informal content.\n\nIf you're interested in working with unstructured media, financial data, or evaluating model performance in noisy settings, this might be interesting.\n\nPaper: [https://papers.ssrn.com/sol3/papers.cfm?abstract\\_id=5315526](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5315526)  \nDataset: [https://huggingface.co/datasets/gtfintechlab/VideoConviction](https://huggingface.co/datasets/gtfintechlab/VideoConviction)\n\nHappy to discuss the challenges we ran into or potential applications beyond finance!\n\n[Betting against finfluencer recommendations outperformed the S&amp;P 500 by +6.8&amp;#37; in annual returns, but at higher risk \\(Sharpe ratio 0.41 vs 0.65\\). QQQ wins in Sharpe ratio. ](https://preview.redd.it/3n861nuhnk9f1.png?width=4764&amp;format=png&amp;auto=webp&amp;s=aa010ae695934b5520df5e82d8158201750cb3a4)\n\n",
    "author": "mgalarny",
    "timestamp": "2025-06-27T18:36:04",
    "url": "https://reddit.com/r/datascience/comments/1lmaxr4/using_llms_to_extract_stock_picks_from_youtube/",
    "score": 93,
    "num_comments": 25,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lmsf8b",
    "title": "HuggingFace transformers API reference: How do you navigate it?",
    "content": "This might be a me problem, but I have some difficulty navigating HF transformers API documentation. It's sometimes easier to use Gemini or Claude to get the relevant information than from the official HF transformers API reference. \n\nHow do you all do it? Any best practices? \n\nTY.",
    "author": "[deleted]",
    "timestamp": "2025-06-28T10:30:08",
    "url": "https://reddit.com/r/datascience/comments/1lmsf8b/huggingface_transformers_api_reference_how_do_you/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ln8f2b",
    "title": "How do you deal with data scientists with big pay check and title but no domain knowledge?",
    "content": "A tech illiterate Director at my org hired a data couple of data scientists 18 months ago. He has tasked them with nothing specific. And their job was solely to observe and find uses-cases themselves. The only reason they were hired was for the Director to gain brownie points of creating a data-driven team for themself, despite there being several other such teams.\n\nCut to today, the Director has realized that there is very little ROI from his hires because they lack domain knowledge. He conveniently moved them to another team where ML is an overkill. The data scientists however, have found some problems they thought they'll solve with \"data science\". They have been vibe coding and building PPTs for months now. But their attempts are hardly successful because of their lack of domain knowledge. To compensate for their lack of domain knowledge, they create beautiful presentations with lots of buzzwords such as LLMs, but again, lack domain substance.\n\nNow, their proposals seem unnecessary and downright obnoxious to many domain SMEs. But the SMEs don't have the courage to say it to the leadership and be percevied as a roadblock to the data-driven strategy. The constant interference of these data scientists is destabilizing the existing processes for the worst and the team is incurring additional costs.\n\nThis is a very peculiar situation where the data scientists, lacking domain knowledge, are just shooting project proposals in the dark hoping to hit something. I know this doesn't typically happen in most organizations. But have you ever seen such a situation around you? How did you or others deal with the situation?\n\nEDIT: This post is not to shit on the data scientists. They are probably good in their areas. The problem is not the domain SME support. The problem is that these data scientists seem to be too high on their titles and paychecks to collaborate with SMEs. Most SMEs want to support them and tell them nicely that ML/AI is an overkill for their usecases, and the efforts required are too big. There are other data science and analytics teams that are working seamlesly with SMEs.",
    "author": "OverratedDataScience",
    "timestamp": "2025-06-29T00:02:58",
    "url": "https://reddit.com/r/datascience/comments/1ln8f2b/how_do_you_deal_with_data_scientists_with_big_pay/",
    "score": 0,
    "num_comments": 18,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lm0dc9",
    "title": "I built a \"virtual simulation engineer\" tool that designs, build, executes and displays the results of Python SimPy simulations entirely in a single browser window",
    "content": "New tool I built to design, build and execute a discrete-event simulation in Python entirely using natural language in a single browser window. \n\nYou can use it here, 100% free: [https://gemini.google.com/share/ad9d3a205479](https://gemini.google.com/share/ad9d3a205479)\n\nVersion 2 uses SimPy under the hood. Pyodide to execute Python in the front end.\n\nThis is a proof of concept, I am keen for feedback please.\n\nI made a video overview of it here: [https://www.youtube.com/watch?v=BF-1F-kqvL4](https://www.youtube.com/watch?v=BF-1F-kqvL4) ",
    "author": "bobo-the-merciful",
    "timestamp": "2025-06-27T10:51:09",
    "url": "https://reddit.com/r/datascience/comments/1lm0dc9/i_built_a_virtual_simulation_engineer_tool_that/",
    "score": 14,
    "num_comments": 9,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1llnbwq",
    "title": "Causal Inference in Sports",
    "content": "For all curious on Causal Inference, and anyone interested in the application of DS in Sport. I’ve written this blog with the aim of providing a taste for how Causal Inference techniques are used practically, as well as some examples to get people thinking.\n\nI do believe upskilling in Causal Inference is quite valuable, despite the learning curve I think it’s quite cool identifying cause-and -effect without having to do RCTs.\n\nEnjoy!",
    "author": "joshamayo7",
    "timestamp": "2025-06-26T23:55:48",
    "url": "https://reddit.com/r/datascience/comments/1llnbwq/causal_inference_in_sports/",
    "score": 72,
    "num_comments": 17,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ll5dv2",
    "title": "I have two amazing job offers. I want to build my own company in the near future. At a loss.",
    "content": "Hi!\n\nI have two offers. One from a big tech company as a data scientist. I deem it easily the best tech company in my country. I would have killed for this offer just 1 year ago.\n\nAnother offer is from a robotics startup. I would be a founding engineer doing ML, and I think I would learn a lot. However, I'm not interested in this company in the long run. I would jump out after 2 years at the latest to build my own. So my equity would not even vest, and I would feel like I'm backstabbing the founders. They probably would not hire me if I told them this. But I think I would (maybe) learn more in this position.\n\nI just can't decide what to do... My ultimate goal is to build my own company in 1-2 years. What to do?",
    "author": "Error40404",
    "timestamp": "2025-06-26T09:59:08",
    "url": "https://reddit.com/r/datascience/comments/1ll5dv2/i_have_two_amazing_job_offers_i_want_to_build_my/",
    "score": 74,
    "num_comments": 42,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ll7or7",
    "title": "When applying internally, do you reach out to the hiring manager?",
    "content": "I work at a relatively large company, and I've always reached out to hiring managers for internal positions, setting up a brief introductory meeting to ask specific questions about the role. However, during a recent HR session for new employees, it was recommended that we avoid this approach, as it could \"create bias\" and that managers are often too busy.\n\nNow I'm rethinking my strategy for internal applications, I feel like it's highly dependent on the manager themselves but in most cases, asking for a quick intro meeting wouldn't hurt right? I feel like HR was way too broad with this statement. What are people's experiences on this.",
    "author": "MasteredLink",
    "timestamp": "2025-06-26T11:27:04",
    "url": "https://reddit.com/r/datascience/comments/1ll7or7/when_applying_internally_do_you_reach_out_to_the/",
    "score": 54,
    "num_comments": 34,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lliwit",
    "title": "SEAL:Self-Adapting Language Models (self learning LLMs)",
    "content": "MIT has recently released a new research paper where they have introduced a new framework SEAL which introduces a concept of self-learning LLMs that means LLMs can now generate their own fine-tuning data set optimized for the strategy and fine tune themselves on the given context. \n\nFull summary ; [https://www.youtube.com/watch?v=MLUh9b8nN2U](https://www.youtube.com/watch?v=MLUh9b8nN2U)\n\nPaper : [https://arxiv.org/abs/2506.10943](https://arxiv.org/abs/2506.10943)",
    "author": "Technical-Love-8479",
    "timestamp": "2025-06-26T19:40:50",
    "url": "https://reddit.com/r/datascience/comments/1lliwit/sealselfadapting_language_models_self_learning/",
    "score": 8,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ll56bo",
    "title": "Gemini CLI: Google's free coding AI Agent",
    "content": "Google's Gemini CLI is a terminal based AI Agent mostly for coding and easy to install with free access to Gemini 2.5 Pro. Check demo here : https://youtu.be/Diib3vKblBM?si=DDtnlHqAhn_kHbiP",
    "author": "Technical-Love-8479",
    "timestamp": "2025-06-26T09:50:48",
    "url": "https://reddit.com/r/datascience/comments/1ll56bo/gemini_cli_googles_free_coding_ai_agent/",
    "score": 22,
    "num_comments": 4,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lkjxmr",
    "title": "Steam Recommender using Vectors! (Student Project)",
    "content": "Hello Data Enjoyers!\n\nI have recently created a steam game finder that helps users find games similar to their own favorite game,\n\nI pulled reviews form multiple sources then used sentiment with some regex to help me find insightful ones then with some procedural tag generation along with a hierarchical genre umbrella tree i created game vectors in category trees, to traverse my db I use vector similarity and walk up my hierarchical tree.\n\nmy goal is to create a tool to help me and hopefully many others find games not by relevancy but purely by similarity. Ideally as I work on it finding hidden gems will be easy.\n\nI created this project to prepare for my software engineering final in undergrad so its **very rough**, this is not a finished product at all by any means. **Let me know** if there are any features you would like to see or suggest some algorithms to incorporate.\n\ncheck it out on : [https://nextsteamgame.com/](https://nextsteamgame.com/)",
    "author": "Expensive-Ad8916",
    "timestamp": "2025-06-25T15:44:09",
    "url": "https://reddit.com/r/datascience/comments/1lkjxmr/steam_recommender_using_vectors_student_project/",
    "score": 146,
    "num_comments": 40,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lkpnkk",
    "title": "Pre-Expedition Weather Conditions and Success Rates: Seasonal Pattern Analysis of Himalayan Expedition Data",
    "content": "After someone posted Himalayan expedition data on Kaggle: [Himalayan Expeditions](https://www.kaggle.com/datasets/siddharth0935/himalayan-expeditions), I decided to start a personal project and expand on this data by adding ERA5 historical reanalysis weather data to it. Some of my preliminary findings have been interesting so far and I thought I would share them.  \n  \nI expanded on the expedition data by creating multiple different weather windows:\n\n* Full expedition from basecamp date until termination either following summit or termination of attempt.\n* Pre-expedition weather - 14 days prior to official expedition start at basecamp.\n* Termination or Summit approach - the day before termination or summit.\n* Early phase - the first 14 days at basecamp.\n* Late phase - 7 days prior to termination date (either after summit or on failed attempt.)\n* Decision window - 2 days prior to summit window\n\nThe first weather that I have focused on analyzing is the pre-expedition weather window. After cleaning the data and adding the weather windows, I also added a few other features using simple operations and created a few target variables for later modelling like expedition success score, expedition failure score, and an overall expedition score. For this analysis, though, I only focused on success being either True or False. After creating the features and targets, I then ran t-tests on success being True or False to determine their statistical significance. \n\nWhen looking at all the features related to the pre-expedition weather window, the findings seem to suggest that pre-expedition weather conditions play a significant role in Himalayan expedition success or failure in spring/summer expeditions. The graphs and correlation heatmap below summarize the variables that have the highest significance in either success or failure:  \n\n\n[This diagram shows how the different attributes either contribute to success or failure.](https://preview.redd.it/6nbr99uzu69f1.png?width=1904&amp;format=png&amp;auto=webp&amp;s=f84e3cb76b2d61d3b3e68dc3fdd0eec2608cd586)\n\n[This diagram highlights the key attributes over or under of a significance of 0.2 or -0.2 respectively. ](https://preview.redd.it/bzj3uxu2v69f1.png?width=1889&amp;format=png&amp;auto=webp&amp;s=5287852c5a90ae75b251826e1a9668db0ca34d80)\n\n[This is a correlation heatmap diagram associating the attributes to success or failure.](https://preview.redd.it/dd5g6ly4v69f1.png?width=1904&amp;format=png&amp;auto=webp&amp;s=a6925405bc7b5a9930fa9265adc3f425129579d8)\n\nAlthough these findings alone do not paint an over-all picture of Himalayan expedition success or failure, I believe they play a significant part and could be used practically to assess conditions going into spring/summer expeditions. \n\nI hope this is interesting and feel free to provide any feedback. I am not a data scientist by professional and still learning. This analysis was done in Python using a jupyter notebook.",
    "author": "bonesclarke84",
    "timestamp": "2025-06-25T20:17:02",
    "url": "https://reddit.com/r/datascience/comments/1lkpnkk/preexpedition_weather_conditions_and_success/",
    "score": 13,
    "num_comments": 10,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lkfg6w",
    "title": "How long/which things as a HM you would expect a candidate to speak for in Behavioral interviews?",
    "content": "How long/which things as a HM you would expect a candidate to speak for in Behavioral interviews?  Anything important you want them to share or things that they share make them stand out from other candidates for offer? Also things they mention/not mention make them on rejection list? \n\n\nAlso, is 2-3 minutes stories good enough? Or are they too short?  (For me STAR method complete stories in 2 minutes unless i add unnecessary details that are not asked) \n\ni tend to be person who answer only things you asked, should I change this method?. Like if you ask whether i did project on worked on stake holders t\n\n\nAny other things you would like to share for DS behavioral interviews",
    "author": "Starktony11",
    "timestamp": "2025-06-25T12:43:38",
    "url": "https://reddit.com/r/datascience/comments/1lkfg6w/how_longwhich_things_as_a_hm_you_would_expect_a/",
    "score": 9,
    "num_comments": 8,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ljsd1j",
    "title": "Graduating Soon — Any Tips for Landing an Entry-Level Data Science Job?",
    "content": "Hey everyone — I'm finishing up my MSc in Data Science this fall (Fall 2025). I also have a BSc in Computer Science and completed 2–3 relevant tech internships.\n\nI’m starting to plan my job hunt and would love to hear from working data scientists or others in the field:\n\n* Should I be applying in bulk to everything I qualify for, or focus on tailoring my resume with ATS keywords?\n* Are there other strategies that helped you break into the field?\n* What do you wish someone had told you when you were job hunting?\n* Is it even heard of fresh graduates landing data roles?\n\nI know the market’s tough right now, so I want to be as strategic as possible. Any advice is appreciated — thanks!",
    "author": "Odd_Artist4319",
    "timestamp": "2025-06-24T17:59:41",
    "url": "https://reddit.com/r/datascience/comments/1ljsd1j/graduating_soon_any_tips_for_landing_an/",
    "score": 190,
    "num_comments": 116,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ljg8dx",
    "title": "Why would anyone try to win Kaggle's challenges?",
    "content": "Per title. Go to Kaggle right now and look at the top competitions featuring monetary prizes. Like you have to predict folded protein structures and polymers properties within 3 months? Those are ground breaking problems which to me would probably require years of academic effort without any guarantee of success. And IF you win you get what, 50000$, not even a year salary in most positions, and you have to split it with your team? Like even if you are capable of actually solving some of these challenges why would you ever share them as Kaggle public notebook or give IP to the challenge sponsor?",
    "author": "vaginedtable",
    "timestamp": "2025-06-24T09:47:46",
    "url": "https://reddit.com/r/datascience/comments/1ljg8dx/why_would_anyone_try_to_win_kaggles_challenges/",
    "score": 404,
    "num_comments": 80,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ljiuzx",
    "title": "A Breakdown of RAG vs CAG",
    "content": "I work at a company that does a lot of RAG work, and a lot of our customers have been asking us about CAG. I thought I might break down the difference of the two approaches.\n\nRAG (retrieval augmented generation) Includes the following general steps:\n\n* retrieve context based on a users prompt\n* construct an augmented prompt by combining the users question with retrieved context (basically just string formatting)\n* generate a response by passing the augmented prompt to the LLM\n\nWe know it, we love it. While RAG can get fairly complex (document parsing, different methods of retrieval source assignment, etc), it's conceptually pretty straight forward.\n\n[A conceptual diagram of RAG, from an article I wrote on the subject \\(IAEE RAG\\).](https://preview.redd.it/izh2zrta0x8f1.png?width=800&amp;format=png&amp;auto=webp&amp;s=2beb6557c45ffc3221a6d0cda78d5674ffddb487)\n\nCAG, on the other hand, is a bit more complex. It uses the idea of LLM caching to pre-process references such that they can be injected into a language model at minimal cost.\n\nFirst, you feed the context into the model:\n\n[Feed context into the model. From an article I wrote on CAG \\(IAEE CAG\\).](https://preview.redd.it/5zw54o9j1x8f1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=27e46efa7ef7a467834558c511954f603b94f224)\n\nThen, you can store the internal representation of the context as a cache, which can then be used to answer a query.\n\n[pre-computed internal representations of context can be saved, allowing the model to more efficiently leverage that data when answering queries. From an article I wrote on CAG \\(IAEE CAG\\).](https://preview.redd.it/jfznfh2p1x8f1.png?width=1456&amp;format=png&amp;auto=webp&amp;s=da7c17029235ca3fceaa2880a14f095badef9bb3)\n\nSo, while the names are similar, CAG really only concerns the augmentation and generation pipeline, not the entire RAG pipeline. If you have a relatively small knowledge base you may be able to cache the entire thing in the context window of an LLM, or you might not.\n\nPersonally, I would say CAG is compelling if:\n\n* The context can always be at the beginning of the prompt\n* The information presented in the context is static\n* The entire context can fit in the context window of the LLM, with room to spare.\n\nOtherwise, I think RAG makes more sense.\n\nIf you pass all your chunks through the LLM prior, you can use CAG as caching layer on top of a RAG pipeline, allowing you to get the best of both worlds (admittedly, with increased complexity).\n\n[From the RAG vs CAG article.](https://preview.redd.it/lc6ku69g3x8f1.png?width=1880&amp;format=png&amp;auto=webp&amp;s=01c59fae3b9daf0b1554a5cb139375fed353d570)\n\nI filmed a [video](https://www.youtube.com/watch?v=HqJ-KDPE6PY) recently on the differences of RAG vs CAG if you want to know more.\n\nSources:  \n\\- [RAG vs CAG video](https://www.youtube.com/watch?v=HqJ-KDPE6PY)  \n\\- [RAG vs CAG Article](https://www.eyelevel.ai/post/rag-vs-cag)  \n\\- [RAG IAEE](https://iaee.substack.com/p/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9?utm_source=publication-search)  \n\\- [CAG IAEE](https://iaee.substack.com/p/cache-augmented-generation-intuitively?utm_source=publication-search)",
    "author": "Daniel-Warfield",
    "timestamp": "2025-06-24T11:25:57",
    "url": "https://reddit.com/r/datascience/comments/1ljiuzx/a_breakdown_of_rag_vs_cag/",
    "score": 47,
    "num_comments": 7,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ljp64t",
    "title": "How much time do you spend designing your ML/DS problems before starting?",
    "content": "Not sure if this is a low effort question but working in the industry I am starting to think I am not spending enough time designing the problem by addressing how I will build training, validation, test sets. Identifying the model candidates. Identifying sources of data to build features. Designing end to end pipeline for my end result to be consumed.\n\nIn my opinion this is not spoken about enough and I am curious how much time some of you spend and what you focus to address?\n\nThanks",
    "author": "titiboa",
    "timestamp": "2025-06-24T15:34:41",
    "url": "https://reddit.com/r/datascience/comments/1ljp64t/how_much_time_do_you_spend_designing_your_mlds/",
    "score": 19,
    "num_comments": 22,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ljhav8",
    "title": "Has anyone prepared for Doordash DS interview? Looking for tips and resources",
    "content": "I have phone screen coming up in 2 weeks. I feel okay about SQL part, but I am quite worried about the product case study, particularly the questions that may include A/B testing. \n\nDo you have any resources for studying A/B testing to crack the interview?",
    "author": "Substantial_Tank_129",
    "timestamp": "2025-06-24T10:27:54",
    "url": "https://reddit.com/r/datascience/comments/1ljhav8/has_anyone_prepared_for_doordash_ds_interview/",
    "score": 39,
    "num_comments": 33,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ljs8wq",
    "title": "Masters in DS/CS/ML/AI inquiry",
    "content": "For those of you that had a BS in CS then went to pursue a masters degree in CS, Ai, ML or similar how much was the benefit of this masters? \n\nWere there things you learned besides ML theory and application that you could not have learned in the industry?\n\nDid this open additional doors for you versus just working as a data scientist or ML engineer without a masters?\n\nThanks",
    "author": "titiboa",
    "timestamp": "2025-06-24T17:54:04",
    "url": "https://reddit.com/r/datascience/comments/1ljs8wq/masters_in_dscsmlai_inquiry/",
    "score": 9,
    "num_comments": 11,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ljhuda",
    "title": "How to tell the difference between whether managers are embracing reality of AI or buying into hype?",
    "content": "I work in data science with a skillset that comprises of data science, data engineering and analytics. My team seems to want to eventually make my role completely non-technical (I'm not sure what a non-technical role would entail). The reason is because there's a feeling all the technical aspects will be completely eliminated by AI. The rationale, in theory, makes sense - we focus on the human aspects of our work, which is to develop solutions that can clearly be transferred to a fully technical team or AI to do the job for us. \n\nThe reality in my experience is that this makes a strong assumptions data processes have the capacity to fit cleanly and neatly into something like a written prompt that can easily be given to somebody or AI with no 'context' to develop. I don't feel like in my work, our processes are there yet....like at all. Some things, maybe, but most things no. I also feel I'm navigating a lot of ever evolving priorities, stakeholder needs, conflicting advice (do this, no revert this, do this, rinse, repeat). This is making my job honestly frustrating and burning me out FAST. I'm working 12 hour days, sometimes up to 3 AM. My technical skills are deteriorating and I feel like my mind is becoming into a fried egg. Don't have time or energy to do anything to upskill.\n\nOn one hand, I'm not sure if management has a point - if I let go of the 'technical' parts that I like b/c of AI and instead just focus on more of the 'other stuff', would I have more growth, opportunity and salary increase in my career? Or is it better off to have a balance between those skills and the technical aspects? In an ideal world, I want to be able to have a good compromise between subject matter and technical skills and have a job where I get to do a bit of both. I'm not sure if the narrative I'm hearing is one of hype or reality. Would be interested in hearing thoughts. ",
    "author": "thro0away12",
    "timestamp": "2025-06-24T10:47:58",
    "url": "https://reddit.com/r/datascience/comments/1ljhuda/how_to_tell_the_difference_between_whether/",
    "score": 28,
    "num_comments": 24,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lirxxw",
    "title": "Does anybody remember the old Python logo?  Honestly, I've only been using Python since 2018, so I didn't recall that this ever existed.",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-06-23T13:46:06",
    "url": "https://reddit.com/r/datascience/comments/1lirxxw/does_anybody_remember_the_old_python_logo/",
    "score": 210,
    "num_comments": 17,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1libni7",
    "title": "Which workflow to avoid using notebooks?",
    "content": "I have always used notebooks for data science.\nI often do EDA and experiments in notebooks before refactoring it properly to module, api etc.\n\nRecently my manager is pushing the team to move away from notebook because it favor bad code practice and take more time to rewrite the code.\n\nBut I am quite confused how to proceed without using notebook. \n\nHow are you doing a data science project from eda, analysis, data viz etc to final api/reports without using notebook?\n\nThanks a lot for your advice.",
    "author": "Safe_Hope_4617",
    "timestamp": "2025-06-23T01:55:43",
    "url": "https://reddit.com/r/datascience/comments/1libni7/which_workflow_to_avoid_using_notebooks/",
    "score": 93,
    "num_comments": 61,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lhuk01",
    "title": "I have run DS interviews and wow!",
    "content": "Hey all,\nI have been responsible for technical interviews for a Data Scientist position and the experience was quite surprising to me. I thought some of you may appreciate some insights.\n\nA few disclaimers: I have no previous experience running interviews and have had no training at all so I have just gone with my intuition and any input from the hiring manager. As for my own competencies, I do hold a Master’s degree that I only just graduated from and have no full-time work experience, so I went into this with severe imposter syndrome as I do just holding a DS title myself. But after all, as the only data scientist, I was the most qualified for the task.\n\nFor the interviews I was basically just tasked with getting a feeling of the technical skills of the candidates. I decided to write a simple predictive modeling case with no real requirements besides the solution being a notebook. I expected to see some simple solutions that would focus on well-structured modeling and sound generalization. No crazy accuracy or super sophisticated models.\n\nFor all interviews the candidate would run through his/her solution from data being loaded to test accuracy. I would then shoot some questions related to the decisions that were made. This is what stood out to me:\n\n1. Very few candidates really knew of other approaches to sorting out missing values than whatever approach they had taken. They also didn’t really know what the pros/cons are of imputing rather than dropping data. Also, only a single candidate could explain why it is problematic to make the imputation before splitting the data.\n\n2. Very few candidates were familiar with the concept of class imbalance.\n\n3. For encoding of categorical variables, most candidates would either know of label or one-hot and no alternatives, they also didn’t know of any potential drawbacks of either one.\n\n4. Not all candidates were familiar with cross-validation\n\n5. For model training very few candidates could really explain how they made their choice on optimization metric, what exactly it measured, or how different ones could be used for different tasks.\n\nOverall the vast majority of candidates had an extremely superficial understanding of ML fundamentals and didn’t really seem to have any sense for their lack of knowledge.\nI am not entirely sure what went wrong. My guesses are that either the recruiter that sent candidates my way did a poor job with the screening. Perhaps my expectations are just too unrealistic, however I really hope that is not the case. My best guess is that the Data Scientist title is rapidly being diluted to a state where it is perfectly fine to not really know any ML.\nI am not joking - only two candidates could confidently explain all of their decisions to me and demonstrate knowledge of alternative approaches while not leaking data.\n\nWould love to hear some perspectives. Is this a common experience?\n",
    "author": "Fl0wer_Boi",
    "timestamp": "2025-06-22T11:13:21",
    "url": "https://reddit.com/r/datascience/comments/1lhuk01/i_have_run_ds_interviews_and_wow/",
    "score": 830,
    "num_comments": 280,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1li6e4v",
    "title": "Would you do this job if you were rich enough to retire?",
    "content": "Curious your perspective on this. Many of us got into the field because it was lucrative and ensures a stable living,\n\nBut it also is intrinsically interesting to study and challenge yourself. The personalities attracted to tech are often fun and make work not so bad. It’s fun to build, experiment, and be in a role where that is expected!\n\nBut what if you had enough money to retire? What would you do? Quit and do something else? Keep doing it? Consult? Curious your reasons and thoughts here!",
    "author": "Dry-Detective3852",
    "timestamp": "2025-06-22T20:24:36",
    "url": "https://reddit.com/r/datascience/comments/1li6e4v/would_you_do_this_job_if_you_were_rich_enough_to/",
    "score": 96,
    "num_comments": 98,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1li722k",
    "title": "Weekly Entering &amp; Transitioning - Thread 23 Jun, 2025 - 30 Jun, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-06-22T21:01:34",
    "url": "https://reddit.com/r/datascience/comments/1li722k/weekly_entering_transitioning_thread_23_jun_2025/",
    "score": 13,
    "num_comments": 56,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lgvh62",
    "title": "ML case study rounds",
    "content": "I am asking this from context of interview. In almost every company these days, there is an ML case study round where the focus is on solving a real world case study. Idk if this is somewhat similar to ML system design or not (I think ML system design rounds are different or maybe part of case study round). Can anyone help me with resources to prepare from for this round? I am well-versed with ML theories, but never worked on solving an end to end solution from interview context. ",
    "author": "alpha_centauri9889",
    "timestamp": "2025-06-21T05:32:30",
    "url": "https://reddit.com/r/datascience/comments/1lgvh62/ml_case_study_rounds/",
    "score": 56,
    "num_comments": 19,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lhn2sb",
    "title": "I talked to a DS professional who told me Gen AI is going to take up the DE job",
    "content": "",
    "author": "[deleted]",
    "timestamp": "2025-06-22T05:47:54",
    "url": "https://reddit.com/r/datascience/comments/1lhn2sb/i_talked_to_a_ds_professional_who_told_me_gen_ai/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.37,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lgt6nn",
    "title": "Feature Interaction Constraints in GBMs",
    "content": "Hi everyone,\n\n  \nI'm curious if anyone here uses the `interaction_constraints` parameter in [XGBoost](https://xgboost.readthedocs.io/en/stable/tutorials/feature_interaction_constraint.html) or [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html#interaction_constraints). In what scenarios do you find it useful and how do you typically set it up? Any real-world examples or tips would be appreciated, thanks in advance.",
    "author": "silverstone1903",
    "timestamp": "2025-06-21T03:12:57",
    "url": "https://reddit.com/r/datascience/comments/1lgt6nn/feature_interaction_constraints_in_gbms/",
    "score": 20,
    "num_comments": 6,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lg5mrg",
    "title": "Ridiculous offer, how to proceed?",
    "content": "Hello All, after a very long struggle with landing my first data science job, I got a ridiculous offer and would like to know how to proceed. For context, I have 7 years of medtech experience, not specifically in data science but similar and an undergrad in stats and now a masters in data science. I am located in the US.\n\nI've been talking with a company for months now and had several interviews even without a specific position available. Well they finally opened two positions, one associate and one senior with salary ranges of 66-99k and 130k-180k respectively. I applied for both and when HR got involved for the offer they said they could probably just split the difference for 110k. Sure that's fine. However, a couple days later, they called again and offered 60-70k, below even the lower limit of the associate range. So my question is has this happened to anyone else? Is this HR's way of trying to get me to just go away?\n\nMaybe I'm just frustrated since HR said the salary range listed on the job req isn't actually what they are willing to pay",
    "author": "LambdaYeti",
    "timestamp": "2025-06-20T07:38:34",
    "url": "https://reddit.com/r/datascience/comments/1lg5mrg/ridiculous_offer_how_to_proceed/",
    "score": 270,
    "num_comments": 120,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lgfmli",
    "title": "Toolkit to move from junior to senior data analyst (data science track)",
    "content": "I would like to move from data analyst to senior data analyst (SDA) in the next year or so. I have a background in marketing, but pivoted to data science four years ago, and have been learning python since then. Most of my work nowadays is either data wrangling or dashboards, with more senior people doing advanced data science thingies like PCA.\n\nThis is a list of tools I think I would need to move from junior data analyst to senior data analyst. Any feedback on if SDA is the right person for these tools is much appreciated.\n\nExtraction\n- general pandas read (csv, parquet, json)\n- gzip\n- iterating through directories\n- hosting on AWS / Google Cloud\n- various other python packages like sqlite\n\nWrangling\n- cleaning\n- merging\n- regex / search\n- masking\n- dtype conversion\n- bucketing\n- ML preprocessing (hash encoding, standardizing, feature selection)\n\nSegmentation\n- PCA / SVD / ICA\n- k-means / DBSCAN\n- itertools segmentation\n\nStatistics\n- descriptive statistics\n- AB testing: t tests, ANOVAs, chi squared\n- confidence intervals\n\nMachine learning\n- model selection\n- hyperparameter tuning\n- scoring\n- inference\n\nVisualization\n- EDA visualizations in Jupyter Lab / Colab\n- final visualizations in dashboards\n\nDeployment\n- deploy and host on AWS / Google Cloud\n\n———\n\nThings I think are simply out of the realm of any DA, senior or not:\n- recommendation systems\n- neural networks\n- setting up an AB test on the back end\n\nCurious what the community would bucket into data analyst, senior data analyst, or data scientist responsibilities.",
    "author": "SingerEast1469",
    "timestamp": "2025-06-20T14:29:24",
    "url": "https://reddit.com/r/datascience/comments/1lgfmli/toolkit_to_move_from_junior_to_senior_data/",
    "score": 55,
    "num_comments": 24,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lgdg9j",
    "title": "Has anyone seen research or articles proving that code quality matters in data science projects?",
    "content": "Hi all,\n\nI'm looking for articles, studies, or real-world examples backed by data that demonstrate the value of code quality specifically in data science projects.\n\nMost of the literature I’ve found focuses on large-scale software projects, where the codebase is big (tens of thousands of lines), the team is large (10+ developers) the expected lifetime of the product is long (10+ years).\n\nExamples: https://arxiv.org/pdf/2203.04374\n\nIn those cases the long-term ROI of clean code and testing is clearly proven. But data science is often different: small teams, high-level languages like Python or R, and project lifespans that can be quite short.\n\nAlternatively, I found interesting recommandations like https://martinfowler.com/articles/is-quality-worth-cost.html (article is old, but recommandations still apply) but without a lot of data backing up the claims.\n\n\nHas anyone come across evidence (academic or otherwise) showing that investing in code quality, no matter how we define it, pays off in typical data science workflows?\n\n\n\n",
    "author": "MarcDuQuesne",
    "timestamp": "2025-06-20T12:57:40",
    "url": "https://reddit.com/r/datascience/comments/1lgdg9j/has_anyone_seen_research_or_articles_proving_that/",
    "score": 21,
    "num_comments": 53,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lg4t92",
    "title": "How are you making AI applications in settings where no external APIs are allowed?",
    "content": "I've seen a lot of people build plenty of AI applications that interface with a litany of external APIs, but in environments where you can't send data to a third party, what are your biggest challenges of building LLM powered systems and how do you tackle them?\n\nIn my experience LLMs can be complex to serve efficiently, LLM APIs have useful abstractions like output parsing and tool use definitions which on-prem implementations can't use, RAG Processes usually rely on sophisticated embedding models which, when deployed locally, require the creation of hosting, provisioning, scaling, storing and querying vector representations. Then, you have document parsing, which is a whole other can of worms, and is usually critical when interfacing with knowledge bases in a regulated industry.\n\nI'm curious, especially if you're doing On-Prem RAG for applications with large numbers of complex documents, what were the big issues you experienced and how did you solve them?",
    "author": "Daniel-Warfield",
    "timestamp": "2025-06-20T07:04:02",
    "url": "https://reddit.com/r/datascience/comments/1lg4t92/how_are_you_making_ai_applications_in_settings/",
    "score": 33,
    "num_comments": 18,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lggtm2",
    "title": "What is your opinion on Julius and other ai first data science tools?",
    "content": "I’m wondering what people’s opinions are on Julius and similar tools (https://julius.ai/)\n\nHave people tried them? Are they useful or end up causing more work?",
    "author": "alexellman",
    "timestamp": "2025-06-20T15:22:05",
    "url": "https://reddit.com/r/datascience/comments/1lggtm2/what_is_your_opinion_on_julius_and_other_ai_first/",
    "score": 6,
    "num_comments": 46,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lg5043",
    "title": "Problem identification &amp; specification in Data Science (a metacognitive deep dive)",
    "content": "Hey r/datascience,\n\nI've found that one of the impactful parts of our work is the initial phase of **problem identification and specification**. It's crucial for project success, yet often feels more like an art than a structured science.\n\nI've been thinking about the **metacognition** involved: *how* do we find the right problems, and *how* do we translate them into clear, actionable data science objectives? I'd love to kick off a discussion to gain a more structured understanding of this process.\n\n**Problem Identification**\n\n1. What triggers your initial recognition of a problem that wasn't explicitly assigned?\n2. How much is proactive observation versus reacting to a stakeholder's vague need?\n\n**The Interplay of Domain Expertise &amp; Data**\n\nDomain expertise and data go hand-in-hand. Deep domain knowledge can spot issues data alone might miss, while data exploration can reveal patterns demanding domain context.\n\n1. How do these two elements come together in your initial problem framing? Is it sequential or iterative?\n\n**Problem Specification**\n\n1. What critical steps do you take to define a problem clearly?\n2. Who are the key players, and what frameworks or tools do you use for nailing down success metrics and scope?\n\n**The \"Systems Model\" of Problem Formulation (A Conceptual Idea)**\n\nThis is a bit more abstract, but I'm trying to visualize the process itself. I'm thinking about a 'Systems Model' for problem formulation: *how a problem gets identified and specified*.\n\nIf we mapped this process, what would the nodes, edges, and feedback loops look like? Are there common pathways or anti-patterns that lead to poorly defined problems?\n\n\\--\n\nI'm curious in how you navigate this foundational aspect of our work. What are your insights into problem identification and specification in data science?\n\nThank you!",
    "author": "[deleted]",
    "timestamp": "2025-06-20T07:12:10",
    "url": "https://reddit.com/r/datascience/comments/1lg5043/problem_identification_specification_in_data/",
    "score": 11,
    "num_comments": 4,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lg60ju",
    "title": "How to build a usability metric that is \"normalized\" across flows?",
    "content": "Hey all, kind of a specific question here, but I've been trying to research approaches to this question and haven't found a reasonable solution. Basically, I work for a tech company with a user-facing product, and we want to build a metric which measures the usability of all our different flows.\n\nI have a good sense of what metrics might represent usability (funnel conversion rate, time, survey scores, etc) but one request made is that the metric must be \"normalized\" (not sure if that's the right word). In other words, the usability score must be comparable across different flows. For example, conversion rate in an \"add payment\" section is always going to be lower than a \"learn about our features\" section - so to prioritize usability efforts we should have a score which accounts for this difference and measures usability on an \"objective\" scale that accounts for the expected gap between different flows.\n\nDoes anyone have any experience in building this kind of metric? Are there public analyses or papers I can read up on to understand how to approach this problem, or am I doomed? Thanks in advance!",
    "author": "toga287",
    "timestamp": "2025-06-20T07:54:37",
    "url": "https://reddit.com/r/datascience/comments/1lg60ju/how_to_build_a_usability_metric_that_is/",
    "score": 3,
    "num_comments": 8,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lfp3ge",
    "title": "Confidence interval width vs training MAPE",
    "content": "Hi, can anyone with strong background in estimation please help me out here? I am performing price elasticity estimation. I am trying out various levels to calculate elasticities on - calculating elasticity for individual item level, calculating elasticity for each subcategory (after grouping by subcategory) and each category level. The data is very sparse in the lower levels, hence I want to check how reliable the coefficient estimates are at each level, so I am measuring median Confidence interval width and MAPE. at each level. The lower the category, the lower the number of samples in each group for which we are calculating an elasticity. Now, the confidence interval width is decreasing for it as we go for higher grouping level i.e. more number of different types of items in each group, but training mape is increasing with group size/grouping level. So much so, if we compute a single elasticity for all items (containing all sorts of items) without any grouping, I am getting the lowest confidence interval width but high mape.\n\nBut what I am confused by is - shouldn't a lower confidence interval width indicate a more precise fit and hence a better training MAPE? I know that the CI width is decreasing because sample size is increasing for larger group size, but so should the residual variance and balance out the CI width, right (because larger group contains many type of items with high variance in price behaviour)? And if the residual variance due to difference between different type of items within the group is unable to balance out the effect of the increased sample size, doesn't it indicate that the inter item variability within different types of items isn't significant enough for us to benefit from modelling them separately and we should compute a single elasticity for all items (which doesn't make sense from common sense pov)?",
    "author": "dopplegangery",
    "timestamp": "2025-06-19T16:20:01",
    "url": "https://reddit.com/r/datascience/comments/1lfp3ge/confidence_interval_width_vs_training_mape/",
    "score": 10,
    "num_comments": 8,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lfdkws",
    "title": "What are good resources to learn MLE/SWE concepts?",
    "content": "I'm struggling adapting my code and was wondering if there were any (preferably free) resources to further my understanding of the engineering way of creating ML pipelines.",
    "author": "fridchikn24",
    "timestamp": "2025-06-19T08:24:38",
    "url": "https://reddit.com/r/datascience/comments/1lfdkws/what_are_good_resources_to_learn_mleswe_concepts/",
    "score": 26,
    "num_comments": 9,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lenpta",
    "title": "I got ghosted after 8 interviews. Why do companies do this?",
    "content": "I went through 7 rounds of interviews with a company, followed by a month of complete silence. Then the recruiter reached out asking me to do an additional round because of an organizational change — the role now had a new hiring manager. Since I had already invested so much time, I agreed to go through the 8th round.\n\nAfter that, they kept stringing me along and eventually just ghosted me.\n\nNot to make this a therapy session, but this whole experience has left me feeling really sad this past week. I spent months in this process, and they couldn’t even send a simple rejection email? How hard is that? I believe I was one of their top candidates — why else would they \ncircle back a month after the initial rounds? How to get over this?\n\nEdit: One more detail, they have been trying to fill this role for the last 6 months.",
    "author": "Lamp_Shade_Head",
    "timestamp": "2025-06-18T10:51:26",
    "url": "https://reddit.com/r/datascience/comments/1lenpta/i_got_ghosted_after_8_interviews_why_do_companies/",
    "score": 385,
    "num_comments": 122,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1leh4wm",
    "title": "My data science dream is slowly dying",
    "content": "\nI am currently studying Data Science and really fell in love with the field, but the more i progress the more depressed i become.\n\nOver the past year, after watching job postings especially in tech I’ve realized most Data Scientist roles are basically advanced data analysts, focused on dashboards, metrics, A/B tests. (It is not a bad job dont get me wrong, but it is not the direction i want to take)\n\nThe actual ML work seems to be done by ML Engineers, which often requires deep software engineering skills which something I’m not passionate about.\n\nRight now, I feel stuck. I don’t think I’d enjoy spending most of my time on product analytics, but I also don’t see many roles focused on ML unless you’re already a software engineer (not talking about research but training models to solve business problems).\n\nDo you have any advice?                                      \n\n**Also will there ever be more space for Data Scientists to work hands on with ML or is that firmly in the engineer’s domain now? I mean which is your idea about the field?**",
    "author": "FinalRide7181",
    "timestamp": "2025-06-18T06:29:00",
    "url": "https://reddit.com/r/datascience/comments/1leh4wm/my_data_science_dream_is_slowly_dying/",
    "score": 842,
    "num_comments": 229,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lewya2",
    "title": "What tasks don’t you trust zero-shot LLMs to handle reliably?",
    "content": "For some context I’ve been working on a number of NLP projects lately (classifying textual conversation data). Many of our use cases are classification tasks that align with our niche objectives. I’ve found in this setting that structured output from LLMs can often outperform traditional methods.\n\nThat said, my boss is now asking for likelihoods instead of just classifications. I haven’t implemented this yet, but my gut says this could be pushing LLMs into the “lying machine” zone. I mean, how exactly would an LLM independently rank documents and do so accurately and consistently? \n\nSo I’m curious:\n\n* What kinds of tasks have you found to be unreliable or risky for zero-shot LLM use?\n* And on the flip side, what types of tasks have worked surprisingly well for you? ",
    "author": "WristbandYang",
    "timestamp": "2025-06-18T17:18:12",
    "url": "https://reddit.com/r/datascience/comments/1lewya2/what_tasks_dont_you_trust_zeroshot_llms_to_handle/",
    "score": 73,
    "num_comments": 41,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1leyh9g",
    "title": "Splitting Up Modeling in Project Amongst DS Team",
    "content": "Hi! When it comes to modeling portion of a DS project, how does your team divy that part of the project among all the data scientist in your team?\n\nI've been part of different teams and they've each done something different and I'm curious about how other teams have gone about it. I've had a boss who would have us all make one model and we just work off one model together. I've also had other managers who had us all work on our own models and we decide which one to go with based off RMSE.\n\nThanks!",
    "author": "throwaway69xx420",
    "timestamp": "2025-06-18T18:33:41",
    "url": "https://reddit.com/r/datascience/comments/1leyh9g/splitting_up_modeling_in_project_amongst_ds_team/",
    "score": 15,
    "num_comments": 8,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1le3whp",
    "title": "How would you categorize this DS skill?",
    "content": "I am DS with several YOE. My company had a problem with the billing system. Several people tried fixing it for a few months but couldn’t fix it.\n\nI met with a few people and took notes. I wrote a few basic sql queries and threw the data into excel then had the solution after a few hours. This saved the company a lot of money.\n\nI didn’t use ML or AI or any other fancy word that gets you interviews. I just used my brain. Anyone can use their brain but all those other smart people couldn’t figure it out so what is the “thing” I have that I can sell to employers.",
    "author": "Trick-Interaction396",
    "timestamp": "2025-06-17T17:47:49",
    "url": "https://reddit.com/r/datascience/comments/1le3whp/how_would_you_categorize_this_ds_skill/",
    "score": 65,
    "num_comments": 33,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ldqozx",
    "title": "We are back with many Data science jobs in Soccer, NFL, NHL, Formula1 and more sports! 2025-06",
    "content": "Hey guys,\n\nI've been silent here lately but many opportunities keep appearing and being posted.\n\nThese are a few from the last 10 days or so\n\n* [Lead/Senior Quantitative Analyst, Predictive Modeling - Philadelphia Phillies](http://www.sportsjobs.online/jobs/8268-lead-senior-quantitative-analyst-predictive-modeling)\n* [Vice President, Business Strategy &amp; Analytics - Detroit Pistons](http://www.sportsjobs.online/jobs/8294-vice-president-business-strategy-analytics)\n* [Data Scientist - Los Angeles Rams](http://www.sportsjobs.online/jobs/8288-data-scientist)\n* [Data Engineer - Houston Texans](http://www.sportsjobs.online/jobs/8299-data-engineer)\n\nA few Internships (hard to find!)\n\n* [Software Engineer Intern - Dallas Mavericks](https://www.sportsjobs.online/jobs/8107-software-engineer-intern)\n* [Business Strategy Internship - Nashville Predators](https://www.sportsjobs.online/jobs/8212-nashville-predators-business-strategy-internship-fall-2025-nhl)\n* [Business Analytics Intern - Carolina Panthers](http://www.sportsjobs.online/jobs/8197-business-analytics-intern)\n\nNBA Great jobs that were open (and closed applications quickly) but they appear !\n\n* [Data Analyst - Miami Heat](http://www.sportsjobs.online/jobs/8255-data-analyst) \\[Sold out\\]\n* [Applied Scientist, Basketball Analytics - Phoenix Suns](http://www.sportsjobs.online/jobs/8243-applied-scientist-basketball-analytics) \\[Sold out\\]\n\nI run www.sportsjobs(.)online, a job board in that niche. In the last month I added around 300 jobs.\n\nFor the ones that already saw my posts before, I've added more sources of jobs lately. I'm open to suggestions to prioritize the next batch.\n\nIt's a niche, there aren't thousands of jobs as in Software in general but my commitment is to **keep improving a simple metric, jobs per month.** We always need some metric in DS..\n\nI run also a newsletter to receive emails with jobs and interesting content on sports analytics (next edition tomorrow!)  \n[https://sportsjobs-online.beehiiv.com/subscribe](https://sportsjobs-online.beehiiv.com/subscribe)\n\nFinally, I've created also a [reddit community](https://www.reddit.com/r/sports_jobs/) where I post recurrently the openings if that's easier to check for you.\n\nI hope this helps someone!",
    "author": "fark13",
    "timestamp": "2025-06-17T08:50:25",
    "url": "https://reddit.com/r/datascience/comments/1ldqozx/we_are_back_with_many_data_science_jobs_in_soccer/",
    "score": 91,
    "num_comments": 28,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ld1nf6",
    "title": "Just tell them you work with models.  Let them figure out the rest on their own.",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-06-16T12:06:27",
    "url": "https://reddit.com/r/datascience/comments/1ld1nf6/just_tell_them_you_work_with_models_let_them/",
    "score": 662,
    "num_comments": 15,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lcemw6",
    "title": "Don’t be the data scientist who’s in love with models, be the one who solves real problems",
    "content": "work at a company with around 100 data scientists, ML and data engineers.\n\nThe most frustrating part of working with many data scientists and honestly, I see this on this sub all the time too, is how obsessed some folks are with using ML or whatever the latest SoTA causal inference technique is. Earlier in my career plus during my masters, I was exactly the same, so I get it.\n\nBut here’s the best advice I can give you: **don’t be that person.**\n\nUnless you’re literally working on a product where ML is the core feature, **your job is basically being an internal consultant.** That means understanding what stakeholders actually want, challenging their assumptions when needed, and giving them something useful, not just something that will disappear into a slide deck or notebook. \n\nAlways try and make something run in production, don’t do endless proof of concepts. If you’re doing deep dives / analysis, define success criteria of your initiatives, try and measure them (e.g., some of my less technical but awesome DS colleagues made their career of finding drivers of key KPIs, reporting them to key stakeholders and measuring improvement over time). In short, **prove you’re worth it**.\n\nA lot of the time, that means building a dashboard. Or doing proper data/software engineering. Or using GenAI. Or whatever else some of my colleagues (and a loads of people on this sub) roll their eyes at.\n\nSolve the problem. Use whatever gets the job done, not just whatever looks cool on a résumé.",
    "author": "Odd-One8023",
    "timestamp": "2025-06-15T16:52:42",
    "url": "https://reddit.com/r/datascience/comments/1lcemw6/dont_be_the_data_scientist_whos_in_love_with/",
    "score": 845,
    "num_comments": 100,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ld06j0",
    "title": "The Illusion of \"The Illusion of Thinking\"",
    "content": "Recently, Apple released a paper called \"The Illusion of Thinking\", which suggested that LLMs may not be reasoning at all, but rather are pattern matching:\n\n[https://arxiv.org/abs/2506.06941](https://arxiv.org/abs/2506.06941)\n\nA few days later, A paper written by two authors (one of them being the LLM Claude Opus model) released a paper called \"The Illusion of the Illusion of thinking\", which heavily criticised the paper.\n\n[https://arxiv.org/html/2506.09250v1](https://arxiv.org/html/2506.09250v1)\n\nA major issue of \"The Illusion of Thinking\" paper was that the authors asked LLMs to do excessively tedious and sometimes impossible tasks; citing The \"Illusion of the Illusion of thinking\" paper:\n\n&gt;Shojaee et al.’s results demonstrate that models cannot output more tokens than their context limits allow, that programmatic evaluation can miss both model capabilities and puzzle impossibilities, and that solution length poorly predicts problem difficulty. These are valuable engineering insights, but they do not support claims about fundamental reasoning limitations.\n\n&gt;Future work should:\n\n&gt;1. Design evaluations that distinguish between reasoning capability and output constraints\n\n&gt;2. Verify puzzle solvability before evaluating model performance\n\n&gt;3. Use complexity metrics that reflect computational difficulty, not just solution length\n\n&gt;4. Consider multiple solution representations to separate algorithmic understanding from execution\n\n&gt;The question isn’t whether LRMs can reason, but whether our evaluations can distinguish reasoning from typing.\n\nThis might seem like a silly throw away moment in AI research, an off the cuff paper being quickly torn down, but I don't think that's the case. I think what we're seeing is the growing pains of an industry as it begins to define what reasoning actually is.\n\nThis is relevant to application developers, not just researchers. AI powered products are significantly difficult to evaluate, often because it can be very difficult to define what \"performant\" actually means.\n\n(I wrote this, it focuses on RAG but covers evaluation strategies generally. I work for EyeLevel)  \n[https://www.eyelevel.ai/post/how-to-test-rag-and-agents-in-the-real-world](https://www.eyelevel.ai/post/how-to-test-rag-and-agents-in-the-real-world)\n\nI've seen this sentiment time and time again: LLMs, LRMs, and AI in general are more powerful than our ability to test is sophisticated. New testing and validation approaches are required moving forward.",
    "author": "Daniel-Warfield",
    "timestamp": "2025-06-16T11:10:48",
    "url": "https://reddit.com/r/datascience/comments/1ld06j0/the_illusion_of_the_illusion_of_thinking/",
    "score": 24,
    "num_comments": 66,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lcpzzw",
    "title": "\"Yes, I do want to allow this app to make changes to my device!\"",
    "content": "DS's in mid-sized firms: do you have to wrestle with the constant “admin approval required” pop-ups? Is this really best practice?\n\nI'm writing this in anger (sorry if that comes across!) but I feel like every time I stumble on anything remotely  cool or new, *BAM -* admin rights. \n\nI understand the security implication, but surely there's a better way. When I was at a large tech firm, this wasn't a thing - but I'm not sure if my laptop was truly unlocked, or if they had a clever workaround. \n\n1. Is it reasonable/possible to ask IT to carve out an exception for the data science team. If you've manage this, what arguments or evidence actually worked? \n2. Is there a middle ground I don't know about?",
    "author": "Double-Bar-7839",
    "timestamp": "2025-06-16T04:08:55",
    "url": "https://reddit.com/r/datascience/comments/1lcpzzw/yes_i_do_want_to_allow_this_app_to_make_changes/",
    "score": 62,
    "num_comments": 19,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lcjd6h",
    "title": "Weekly Entering &amp; Transitioning - Thread 16 Jun, 2025 - 23 Jun, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-06-15T21:01:18",
    "url": "https://reddit.com/r/datascience/comments/1lcjd6h/weekly_entering_transitioning_thread_16_jun_2025/",
    "score": 4,
    "num_comments": 27,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lccbgj",
    "title": "Books on applied data science for B2B marketing?",
    "content": "There's this thread from 3 years ago: https://www.reddit.com/r/datascience/comments/ram75g/books_on_applied_data_science_for_b2b_marketing/\n\nUnfortunately, it never got any book recommendations - I'm in pretty much the exact same position as the OP of the linked thread and am looking for resources that explain the best methods and provide practical how-tos for marketing science/data science applied to B2B marketing.",
    "author": "PathalogicalObject",
    "timestamp": "2025-06-15T15:01:35",
    "url": "https://reddit.com/r/datascience/comments/1lccbgj/books_on_applied_data_science_for_b2b_marketing/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lare33",
    "title": "\"Data Annotation\" spam",
    "content": "Anyone else's job search site just absolutely spammed by Data Annotation? If I look up Data, ML, AI, or anything similar in my area I get 2-3 pages of there job posting.",
    "author": "MahaloMerky",
    "timestamp": "2025-06-13T14:16:50",
    "url": "https://reddit.com/r/datascience/comments/1lare33/data_annotation_spam/",
    "score": 141,
    "num_comments": 31,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l9w0ln",
    "title": "Significant humor",
    "content": "Saw this and found it hilarious , thought I’d share it here as this is one of the few places this joke might actually land. \n\n\nDatetime.now() + timedelta(days=4) ",
    "author": "MamboAsher",
    "timestamp": "2025-06-12T12:48:52",
    "url": "https://reddit.com/r/datascience/comments/1l9w0ln/significant_humor/",
    "score": 2394,
    "num_comments": 62,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1lbksb6",
    "title": "creating a deepfake identity on Social media ( for good)",
    "content": "To avoid bullying on SM for my ideas, I want to replace my face with a deepfake ( not a real person, but I don t anyone to take it since i ll be using it all the time), what is the best way to do that? I already have ideas. but someone with deep knowledge will help me a lot. My pc also don t have gpu (amd rysen) so advice on that also will be helpful. thanks!",
    "author": "Due-Duty961",
    "timestamp": "2025-06-14T15:22:41",
    "url": "https://reddit.com/r/datascience/comments/1lbksb6/creating_a_deepfake_identity_on_social_media_for/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.32,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l9q78x",
    "title": "Do you say day-tah or dah-tah",
    "content": "Grab the hornets nest, shake it, throw it, run!!!!",
    "author": "No_Length_856",
    "timestamp": "2025-06-12T09:02:08",
    "url": "https://reddit.com/r/datascience/comments/1l9q78x/do_you_say_daytah_or_dahtah/",
    "score": 131,
    "num_comments": 130,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l99bfz",
    "title": "Get dozens of messages from new graduates/ former data scientist  about roles at my organization. Is this a sign?",
    "content": "Everyday I have  been getting more and more LinkedIn messages from people laid off from their analytics roles searching for roles from JPMorgan Chase to CVS, to name a few. Are we in for a downturn? This is making me nervous for my own role. This doesn’t even include all the new students who have just graduated.",
    "author": "Timely_Ad9009",
    "timestamp": "2025-06-11T17:53:52",
    "url": "https://reddit.com/r/datascience/comments/1l99bfz/get_dozens_of_messages_from_new_graduates_former/",
    "score": 222,
    "num_comments": 119,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l8vdvk",
    "title": "What do you hates the most as a data scientist",
    "content": "A bit of a rant here. But sometimes it feels like 90% of the time at my job is not about data science.  \nI wonder if it is just me and my job is special or everyone is like this.\n\nIf I try to add up a project from end to end, may be there is 10-15% of really interesting modeling work.   \nIt looks something like this:  \n- Go after different sources to get the right data - 20% (lot's of meeting)\n- Clean the data - 20% (lot's of meeting to understand the data)\n- Wrestling with some code issue, packages installation, old dependencies - 10%\n- Data exploration, analysis, modeling - 10%\n- validation &amp; documentation - 10%\n- Deployment, debugging deployment issues - 20%\n- Some regular reporting, maintenance - 10%\n\nHow do things look like for you? I wonder if things are different depending on companies, industries etc..",
    "author": "SummerElectrical3642",
    "timestamp": "2025-06-11T08:18:43",
    "url": "https://reddit.com/r/datascience/comments/1l8vdvk/what_do_you_hates_the_most_as_a_data_scientist/",
    "score": 234,
    "num_comments": 130,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l8e4iq",
    "title": "The higher ups asked me for an analysis and it worked.",
    "content": "So I totally mean to brag here. Last week a group of directors said, “We suspect X is happening in the market, do we have data that demonstrates it?”\n\nAnd I thought to myself, here we go again. I’ve got to wade through our data swamp then tell them we don’t have the data that tells the story they want.\n\nWell I waded through the data swamp and the data was there. I made them a graph that definitively demonstrated that yes, X is happening as they suspected. It wasn’t super easy to figure out and it also didn’t require a super complex model to figure out either. ",
    "author": "big_data_mike",
    "timestamp": "2025-06-10T16:44:50",
    "url": "https://reddit.com/r/datascience/comments/1l8e4iq/the_higher_ups_asked_me_for_an_analysis_and_it/",
    "score": 526,
    "num_comments": 45,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l8kf9h",
    "title": "I have a training budget of ~250 USD for my own professional development. What would you recommend I spend it on?",
    "content": "Pretty much the title, but here are some details:\n\n* As far as I know, the budget can be spent on things like books, courses, seminars - things like that (possible also cloud services, haven't found out about that one)\n* As far as the skills I currently have, my educational background is in mathematics (master's degree level) and my work today is mainly in classical ML and NLP. In the past I also did some bio-medical modeling with non-linear ODE systems.\n* However, the scope of both the budget and my interests are pretty much anything to do with data science, so hit me with anything you've got :). Also, whatever it is doesn't have to fit perfectly into the budget - I'm happy to purchase multiple things, not use all of it or dip into my own pocket if needed.\n* I'm based in Melbourne, Australia, in case someone has an in-person thing to recommend\n\nAppreciate all the help!",
    "author": "CantorFunction",
    "timestamp": "2025-06-10T22:14:06",
    "url": "https://reddit.com/r/datascience/comments/1l8kf9h/i_have_a_training_budget_of_250_usd_for_my_own/",
    "score": 49,
    "num_comments": 29,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l8gmy0",
    "title": "Lyft vs Pinterest Data Science",
    "content": "If you have some familiarity with both, how does Lyft compare with Pinterest for career growth both while inside the company and in terms of exit opportunities?",
    "author": "anomnib",
    "timestamp": "2025-06-10T18:48:37",
    "url": "https://reddit.com/r/datascience/comments/1l8gmy0/lyft_vs_pinterest_data_science/",
    "score": 61,
    "num_comments": 41,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l89f9z",
    "title": "no internship as a sophomore",
    "content": "i have sent hundreds of applications, but wasn't able to land an internship this summer. i think it's my experience, i switched from microbiology to stats/ds  a year ago, but was hoping to get something over the summer which would help me recruit in my junior year. genuinely heartbroken.\n\ncan anyone give me advice on what to do in the summer improve my experience? things i can do to add on my cv, i have absolutely no clue.\n\nthank you!\n\n  \nedit: thank you guys so so much - actually - i am so grateful for your ideas! i will work on some projects in the summer, i've reached out to some professors for research opportunities (might be late, but no harm in trying ig!) and i will expand on my knowledge. you guys are awesome :)",
    "author": "Due-Appointment9582",
    "timestamp": "2025-06-10T13:27:21",
    "url": "https://reddit.com/r/datascience/comments/1l89f9z/no_internship_as_a_sophomore/",
    "score": 16,
    "num_comments": 21,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l843cd",
    "title": "Vicious circle of misplaced expectations with PMs and stakeholders",
    "content": "Looking for opinions from experienced folks in DS.\n\nStuck in a vicious circle of misplaced expectations from stakeholders being agreed for delivery by PMs even without consulting DS to begin with. Then, those come to DS team to build because business stakeholders already know that is the solution they need/are missing - not necessarily true. So, that expectation functions like a feature in a front end application in the mind of a Product Manager - deterministic mode (not sure if it is agile or waterfall type of project management or whatever).\n\nDS tries to do what is best possible but it falls short of what stakeholders expect - they literally say we thought some magic would happen through advanced data science!\n\nPM now tries to do RCA to understand where things went wrong while continuing to play gallery to stakeholders unquestioningly. PM has difficulty understanding DS stuff and keeps telling to keep things non-technical while asking questions that are inherently technical! PM is more comfortable looking at data viz, React applications etc.\n\nDS is to blame for not creating magic.\n\nMeanwhile, users have other problems that could be solved by DA or DS but they lie unutilized because they are attached to Excel and Excel Macros. Not willing to share relevant domain inputs.\n\nOn loop.",
    "author": "explorer_seeker",
    "timestamp": "2025-06-10T10:02:52",
    "url": "https://reddit.com/r/datascience/comments/1l843cd/vicious_circle_of_misplaced_expectations_with_pms/",
    "score": 22,
    "num_comments": 23,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l7cbkg",
    "title": "\"What if we inverted that chart?\"",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-06-09T11:24:27",
    "url": "https://reddit.com/r/datascience/comments/1l7cbkg/what_if_we_inverted_that_chart/",
    "score": 978,
    "num_comments": 53,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l8xqgf",
    "title": "Data scientists need to know about data contracts.",
    "content": "Data contracts are these things that data engineers write to set up expectations of what the data looks like.\n\nAnd who understands the expectations better than a data engineer? A data scientist with context about how the business works.\n\n…But, most of us aren’t gonna write YAML files and glue contracts into pipelines.\n\nWe don’t do that kind of dirty job…\n\nStill, if you want to stop data quality issues from showing up and impacting your machine learning models, contracts can still be the way to go.\n\nWhy? Because a good data contract connects two worlds:\n\n• The business context you understand.\n\n• The technical realities your team builds on.\n\nThat’s a perfect match for what great data scientists already do.",
    "author": "santiviquez",
    "timestamp": "2025-06-11T09:52:05",
    "url": "https://reddit.com/r/datascience/comments/1l8xqgf/data_scientists_need_to_know_about_data_contracts/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l7knce",
    "title": "Can someone explain to me the difference between Fitting aggregation functions and regular old linear regression?",
    "content": "They seem like basically the same thing? \nWhen would one prefer to use fitting aggregation functions?",
    "author": "AdventurousAddition",
    "timestamp": "2025-06-09T17:03:49",
    "url": "https://reddit.com/r/datascience/comments/1l7knce/can_someone_explain_to_me_the_difference_between/",
    "score": 13,
    "num_comments": 8,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l77blf",
    "title": "ML monitoring startup NannyML got acquired by Soda Data Quality",
    "content": "",
    "author": "santiviquez",
    "timestamp": "2025-06-09T08:11:14",
    "url": "https://reddit.com/r/datascience/comments/1l77blf/ml_monitoring_startup_nannyml_got_acquired_by/",
    "score": 23,
    "num_comments": 13,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l7spck",
    "title": "What Masters should could be an option after B.Sc Data Science",
    "content": "Hello,\n\nI recently completed B.Sc Data Science in India. Was wondering which M.Sc should I go for after this.\n\nSomeone told me M.Sc Data Science but when I checked the syllabus, a lot of subjects are similar. Would it still be a good option? Or please help with different options as well ",
    "author": "Bulky-Top3782",
    "timestamp": "2025-06-10T00:35:31",
    "url": "https://reddit.com/r/datascience/comments/1l7spck/what_masters_should_could_be_an_option_after_bsc/",
    "score": 0,
    "num_comments": 27,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l6vciq",
    "title": "Weekly Entering &amp; Transitioning - Thread 09 Jun, 2025 - 16 Jun, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-06-08T21:01:44",
    "url": "https://reddit.com/r/datascience/comments/1l6vciq/weekly_entering_transitioning_thread_09_jun_2025/",
    "score": 11,
    "num_comments": 51,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l5t9af",
    "title": "PhD vs Masters prepared data scientist expectations.",
    "content": "Is there anything more that you expect from a data scientist with a PhD versus a data scientist with just a master's degree, given the same level of experience?\n\n For the companies that I've worked with, most data science teams were mixes of folks with master's degrees and folks with PhDs and various disciplines.\n\nThat got me thinking. As a manager or team member, do you expect more from your doctorally prepared data scientist then your data scientist with only Master's degrees? If so, what are you looking for?  \n\nAre there any particular skills that data scientists with phds from a variety of disciplines have across the board that the typical Masters prepare data scientist doesn't have?\n\nIs there something common about the research portion of a doctorate that develops in those with a PhD skills that aren't developed during the master's degree program? If so, how are they applicable to what we do as data scientists?",
    "author": "mcjon77",
    "timestamp": "2025-06-07T12:38:07",
    "url": "https://reddit.com/r/datascience/comments/1l5t9af/phd_vs_masters_prepared_data_scientist/",
    "score": 105,
    "num_comments": 66,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l5tiqg",
    "title": "What is your domain and what are the most important technical skills that help you stand out in your domain?",
    "content": "Aside from soft skills and domain expertise, ofc those are a given.\n\nI'm manufacturing-adjacent (closer to product development and validation). Design of experiments has been my most useful data-related skill. I'm always being asked \"We are doing test X to validate our process. Can you propose how to do it with less runs?\" Most of the other engineers in our team are familiar with the concept of DoE but aren't confident enough to generate or analyze it themselves, which is where my role typically falls into.",
    "author": "corgibestie",
    "timestamp": "2025-06-07T12:49:58",
    "url": "https://reddit.com/r/datascience/comments/1l5tiqg/what_is_your_domain_and_what_are_the_most/",
    "score": 48,
    "num_comments": 37,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l6ac7v",
    "title": "You can now automate deep dives, with clear actionable recommendations based on data.",
    "content": "",
    "author": "phicreative1997",
    "timestamp": "2025-06-08T04:52:12",
    "url": "https://reddit.com/r/datascience/comments/1l6ac7v/you_can_now_automate_deep_dives_with_clear/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.39,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l51ufd",
    "title": "Data analyst vs. engineer? At non-profit",
    "content": "Hi all,\n\nI am the only Data Analyst at a medium-sized company related to shared transportation (adjacent to Lime Scooter/Bike). I'm pretty early in my career (grad from college 3 years ago).\n\nMy role encompasses a LOT of responsibilities that aren't traditionally under \"data analyst\", the biggest of which being that I build and maintain all the data pipelines from our partner companies via API and webhooks to our own SQL database. This feels very much like the role of Data Engineer. From there, I use the SQL data to build dashboards / do analyses, etc, which is what I usually think of as \"Data Analyst\".\n\nI am trying to argue for a raise (since data engineers are usually paid more than analysts), and I am trying to figure out if I should ask for a title change too. I'd like to have engineering somehow in it, but \"Data Engineer and Analyst\" doesn't sound great.\n\nDoes anyone have any experience or advice with this? Thanks!!",
    "author": "oneohsevenam",
    "timestamp": "2025-06-06T12:56:35",
    "url": "https://reddit.com/r/datascience/comments/1l51ufd/data_analyst_vs_engineer_at_nonprofit/",
    "score": 92,
    "num_comments": 26,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l51gfr",
    "title": "Understanding Regression Discontinuity Design",
    "content": "In my latest blog post I break-down **regression discontinuity design** \\- then I build it up again in an intuition-first manner. It will become clear why you really want to understand this technique (but, that there is never really free lunch)\n\n[Here it is](https://towardsdatascience.com/regression-discontinuity-design-how-it-works-and-when-to-use-it/) @ Towards Data Science\n\n**My own takeaways:**\n\n1. Assumptions make it or break it - with RDD more than ever\n2. LATE might be not what we need, but it'll be what we get\n3. RDD and instrumental variables have lots in common. At least both are very \"elegant\".\n4. Sprinkle covariates into your model very, very delicately or you'll do more harm than good\n5. Never lose track of the question you're trying to answer, and never pick it up if it did not matter to begin with\n\nI get it; you really can't imagine how you're going to read straight on for 40 minutes; no worries, you don't have to. Just make sure you don't miss part where I leverage results page cutoff (max. 30 items per page) to recover the causal effect of top-positions on conversion — for them e-commerce / online marketplace DS out there.",
    "author": "chomoloc0",
    "timestamp": "2025-06-06T12:39:44",
    "url": "https://reddit.com/r/datascience/comments/1l51gfr/understanding_regression_discontinuity_design/",
    "score": 16,
    "num_comments": 9,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l4txpv",
    "title": "BI and Predictive Analytics on SaaS Data Sources",
    "content": "Hi guys,\n\nSeeking advice on a best practices in data management using data from SaaS sources (e.g., CRM, accounting software). \n\nThe goal is to establish robust business intelligence (BI) and potentially incorporate predictive analytics while keeping the approach lean, avoiding unnecessary bloating of components.\n\n1. For data integration, would you use tools like Airbyte or Stitch to extract data from SaaS sources and load it into a data warehouse like Google BigQuery? Would you use Looker for BI and EDA, or is there another stack you’d suggest to gather all data in one place?\n\n2. For predictive analytics, would you use BigQuery’s built-in ML modeling features to keep the solution simple or opt for custom modeling in Python? \n\nAppreciate your feedback and recommendations!",
    "author": "petburiraja",
    "timestamp": "2025-06-06T07:33:04",
    "url": "https://reddit.com/r/datascience/comments/1l4txpv/bi_and_predictive_analytics_on_saas_data_sources/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l49208",
    "title": "Humble Bundle: ML, GenAI and more from O'Reilly",
    "content": "This 'pay what you want' [Humble Bundle](https://www.humblebundle.com/books/machine-learning-ai-and-bots-oreilly-2025-books) from O'Reilly is very GenAI leaning",
    "author": "smilodon138",
    "timestamp": "2025-06-05T13:02:09",
    "url": "https://reddit.com/r/datascience/comments/1l49208/humble_bundle_ml_genai_and_more_from_oreilly/",
    "score": 88,
    "num_comments": 16,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l40tho",
    "title": "What is the best IDE for data science in 2025?",
    "content": "Hi all,  \nI am a \"old\" data scientists looking to renew my stacks. Looking for opinions on what is the best IDE in 2025.   \nThe other discussion I found was 1 year ago and some even older. \n\nSo what do you use as IDE for data science (data extraction, cleaning, modeling to deployment)? What do you like and what you don't like about it? \n\nCurrently, I am using JupyterLab:  \n**What I like:**  \n\\- Native compatible with notebook, I still find notebook the right format to explore and share results  \n\\- %magic command  \n\\- Widget and compatible with all sorts of dataviz (plotly, etc)  \n\\- Export in HTML\n\n**What I feel missing (but I wonder whether it is mostly because I don't know how to use it):**  \n\\- Debugging  \n\\- Autocomplete doesn't seems to work most of the time.   \n\\- Tree view of file and folder  \n\\- Comment out block of code ? (I remember it used to work but I don't know why it don't work anymore)  \n\\- Great integration of AI like Github Copilot\n\nThanks in advance and looking forward to read your thoughts.",
    "author": "SummerElectrical3642",
    "timestamp": "2025-06-05T07:37:30",
    "url": "https://reddit.com/r/datascience/comments/1l40tho/what_is_the_best_ide_for_data_science_in_2025/",
    "score": 166,
    "num_comments": 280,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l4b3t7",
    "title": "Need help sorting my thoughts about current \"contract\"",
    "content": "Just reaching out to industry veterans to see if anyone can offer me some level-headed advice. Maybe you've been in a similar situation and can tell me how you approached the issue. Maybe you've been on the other side of my situation and can offer me that perspective.\n\nFor context:  \nI'm a new grad who has been struggling to find work for a while now. My fiancée mentioned my power BI experience to her boss (general manager) at work and that got the ball rolling on a small contract. I was thrilled. I would be reporting to the ops manager and she had plans for a solid 4 month contract. She takes her plan off to the owner who says he wants to start off with 1 BI report done in 35 hours as a test run as a sort of feasibility thing. I do up a solid report in 32 hours. Ops manager loves it. General manager likes it. Owner thinks I missed the mark. Damn. His feedback is that he doesn't like that he has to filter to get some of the information. He'd like pieces of it to be readily available and visible without having to click anything. I take this feedback and quickly add cards with the wanted measures. Not good enough, now he wants to see more without having to filter. Oh also, he wants all the info to be on one page and all viewable without having to scroll. I tried to tell him that's not the best way to use power BI multiple times, but he just kinda brushed me off and kept moving along every time. We get to a point where he's finally happy with this report. Now he wants to see the small approach we agreed upon applied to a new report so he can verify it from scratch without me needing to take more time to implement feedback after. So I get a new report to work on, and only 20 hours this time. It's an easier data set, so I'm able to blast through it pretty quick and I do it up with his own requested measures shown prominently all on one page, with some visuals for some more complex relationships. Nope. Somehow this one isn't good enough either, but now they have this document that they just keep adding little requests to. I've gone at this thing like 4 or 5 times now. It'll be good, so we move on to the next phase, but then I somehow miss the mark on that and have to go back to the first phase and incorporate new measures?!?!?\n\nNow he keeps giving me these tiny 3 hour micro contracts and moving the goal posts while dangling a longer contract in front of me at the end of a long stick. It's gotten to the point that literally everything on the page is being fed by a measure so that he doesn't have to filter. Am I overreacting and is this a normal use of power BI? They're paying me dog shit too (bottom 1% for my area). I feel like telling them to all fuck off, but I need to navigate things appropriately so that it doesn't negatively impact my fiancée. I'm feeling massively disrespected and played, though. I feel like it goes against everything I've learned about the tool. I'm trying to be cooperative so I can land this contract while also trying to avoid being taken advantage of because I'm a new grad. \n\nOh! Also, this dude said to the ops manager that he thought I was going to use up any extra safety time he gives me because I just want the hours. This is after I saved 3 hours on my first sprint and 6 hours on my second sprint. I don't understand what his issue is. Ops manager thinks he should just give me a solid contract but keeps making excuses for why we should just try one more time to meet his unrealistic wants. \n\nTyping all this out has helped me realize just how much I'm being screwed. I'm going to post it anyway cause I still want other people's feedback, but yeah, I see how spineless I'm being. It's just hard to walk away when I could really use the contract that they keep dangling, but I don't think it's ever coming.\n\nSorry if this reads like a scatterbrained mess of words. I'm just kinda shot gunning my thoughts out. Anything constructive you can offer is appreciated. Apologies if this is a topic that has been answered 1000 times.",
    "author": "No_Length_856",
    "timestamp": "2025-06-05T14:25:21",
    "url": "https://reddit.com/r/datascience/comments/1l4b3t7/need_help_sorting_my_thoughts_about_current/",
    "score": 11,
    "num_comments": 10,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l3y3sd",
    "title": "Introducing the MLSYNTH App",
    "content": "Presumably most people here know Python, but either way, [here's an app](https://mlsynthapp.streamlit.app/about) for my mlsynth library. Now, you can run impact analysis models without needing to know Python, all you need to know is econometrics.",
    "author": "turingincarnate",
    "timestamp": "2025-06-05T05:37:23",
    "url": "https://reddit.com/r/datascience/comments/1l3y3sd/introducing_the_mlsynth_app/",
    "score": 7,
    "num_comments": 9,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l2g1rh",
    "title": "Why am I not getting interviews?",
    "content": "",
    "author": "WhiteRaven_M",
    "timestamp": "2025-06-03T09:15:04",
    "url": "https://reddit.com/r/datascience/comments/1l2g1rh/why_am_i_not_getting_interviews/",
    "score": 788,
    "num_comments": 401,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l2i3p2",
    "title": "What projects are in high demand?",
    "content": "I have 15 YOE. Looking for new job after 7 years. I mostly do anomaly detection and data engineering. I have all the normal skills (ML, Spark, etc). All the postings say something like use giant list of tech skills to drive value but they don’t mention the actual projects.\n\nWhat type of projects are you doing which are in high demand?",
    "author": "Trick-Interaction396",
    "timestamp": "2025-06-03T10:34:19",
    "url": "https://reddit.com/r/datascience/comments/1l2i3p2/what_projects_are_in_high_demand/",
    "score": 131,
    "num_comments": 50,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l21w10",
    "title": "Your first job matters more than you know, and sometimes it matters more than an advanced degree",
    "content": "Your first job matters more than you know, and sometimes it matters more than a masters degree.\n\nThis is something myself and a few others have mentioned here however I find that this discussion still doesn't occur enough.\n\nI'm in a position and have been for the last few years where I get to define the hiring pipeline.\n\nGenerally speaking, I pay way more attention to what someone has been doing for the last 4 years than what they have a degree in. If someone studied a BS in geoscience then did predictive analytics for GIS and environmental services and I just happen to be working at a financial firm that's interested in environment / services then when it comes to that person or the guy with a PhD in Industrial Engineering I'm taking the BS in geoscience.\n\nSame thing in a less niche space, if I'm looking for someone who can come up with initiatives and drive them with business leaders then I'm generally looking for someone who did analytics at a supply chain / distribution company because they know how to stand up for themself, they're willing to work more / take ownership, etc.\n\nIt doesn't matter if you got an MS from Stanford if you do compliance analytics or data governance at a bank, you're now less desirable for many applied data science positions. This being said, many smaller companies are now getting to the point where they need data governance and there is a space for you to be a leader there.\n\nSaying this because outside of research positions, the field you work in does impact how easy it is to tranistion to other roles.",
    "author": "Impossible_Notice204",
    "timestamp": "2025-06-02T20:30:27",
    "url": "https://reddit.com/r/datascience/comments/1l21w10/your_first_job_matters_more_than_you_know_and/",
    "score": 329,
    "num_comments": 60,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l2f2ph",
    "title": "DuckLake: This is your Data Lake on ACID",
    "content": "",
    "author": "howMuchCheeseIs2Much",
    "timestamp": "2025-06-03T08:36:50",
    "url": "https://reddit.com/r/datascience/comments/1l2f2ph/ducklake_this_is_your_data_lake_on_acid/",
    "score": 31,
    "num_comments": 9,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l2bmqx",
    "title": "First Hitting Time in ARIMA models",
    "content": "Hi everybody. I am learning about time series, starting from the simple ideas of autoregressive models. I kinda understand, intuitively, how these models define the conditional distribution of the value at the next timestep X\\_t given all previous values, but I'm struggling to understand how can I use these models to estimate the day at which my time series crosses a certain threshold, or in other words the probability distribution of the random variable τ i.e. the first day at which the value X\\_τ exceeds a certain threshold.\n\nSo far I've been following some well known online sources such as [https://otexts.com/fpp3/](https://otexts.com/fpp3/) and lots of google searches but I struggle to find a walkthrough of this specific problem with ARIMA models. Is it that uncommon? Or am I just stupid",
    "author": "vaginedtable",
    "timestamp": "2025-06-03T06:16:25",
    "url": "https://reddit.com/r/datascience/comments/1l2bmqx/first_hitting_time_in_arima_models/",
    "score": 36,
    "num_comments": 8,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l1iud0",
    "title": "Well, that’s one way to waste the budget on tools that nobody will use...",
    "content": "AI Tools Deployed with Purpose = Great  \nAI Tools Deployed without anyone Asking Why or What it's for = Useless",
    "author": "ElectrikMetriks",
    "timestamp": "2025-06-02T07:03:04",
    "url": "https://reddit.com/r/datascience/comments/1l1iud0/well_thats_one_way_to_waste_the_budget_on_tools/",
    "score": 460,
    "num_comments": 28,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l2lqs3",
    "title": "Follow up question to my previous post.",
    "content": "Previous post: https://www.reddit.com/r/datascience/comments/1l1pm5w/am_i_walking_into_a_trap/\n\n\nHello everyone! Thank you so much for the comments on the previous post. It was very helpful to understand your view. I have a follow up question and want to hear your opinion:\n\nI also have an offer to study computer science at University of Bristol. \n\nWould you rather:\n\nTake the data science job with no direct mentoring for £33,000 pay\n\nOR\n\nStudy an MSc for Computer Science (Conversion) at Bristol University\n",
    "author": "marblesandcookies",
    "timestamp": "2025-06-03T13:01:17",
    "url": "https://reddit.com/r/datascience/comments/1l2lqs3/follow_up_question_to_my_previous_post/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l1qvz5",
    "title": "Real or fake pattern?",
    "content": "I am doing some data analysis/engineering to uncover highly pure subnodes in a dataset, but am having trouble understanding something.\n\nIn this graph, each point represents a pandas mask, which is linked to a small subsample of the data. Subsamples range from 30-300 in size (overall dataset was just 2500). The x axis is the size of the sample, and the y axis is %pure, cutoff at 80% and rounded to 4 decimals. Average purity for the overall dataset is just under 29%. There is jitter on the x axis, as it’s an integrated with multiple values per label.\n\nI cannot tell if these “ribbons”relationship is strictly due to integer division (?), as Claude would suggest, or if this is a pattern commonly found in segmentation, and each ribbon is some sub-cohort of a segment.\n\nHas anyone seen these curved ribbons in their data before?",
    "author": "SingerEast1469",
    "timestamp": "2025-06-02T12:15:58",
    "url": "https://reddit.com/r/datascience/comments/1l1qvz5/real_or_fake_pattern/",
    "score": 88,
    "num_comments": 28,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l1uzi1",
    "title": "How do I manage expectations for my career as a prospective data scientist",
    "content": "Hey all,\n\nI'm a recent MS Statistics graduate (Fall '24), who just finished undergrad (Spring '23) with no working and internship experience. Fortunately, I was able to land a data analyst position at a nonprofit company in March this year, but I am kind of missing the hands-on modeling (Bayesian Statistics, Econometrics, ML, Statistical Regression) and theoretical math (stochastic calculus/processes, ML, probability, Real Analysis) during my master's program.\n\nI understand that given my lack of experience and entry level position, I am very luck to have a job, especially in this economy. However, I also do harbor disappointment in my outcomes, as I did apply for \\~1000 jobs, and had more than 40 interviews for all types of positions (quant, data scientist, model validation analyst, data analyst, etc.) this year, but was beat out by people who probably have more relevant experience and technical skills.\n\nI am thinking of applying this Fall/beginning of next year for some more modeling-heavy positions, but I am also wondering whether given the current economy and my unproven track record, I should realistically lower my expectations and evaluate other options (personal projects to sharpen my skills, PhD in a STEM field, working on a research project), and what I should focus on with my projects to improve myself as a candidate (domain knowledge, sound coding skills, implementation of new models). I would like to hear your thoughts and opinions about my future career goals.\n\nThanks",
    "author": "Comfortable-Image850",
    "timestamp": "2025-06-02T14:57:51",
    "url": "https://reddit.com/r/datascience/comments/1l1uzi1/how_do_i_manage_expectations_for_my_career_as_a/",
    "score": 45,
    "num_comments": 32,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l1pm5w",
    "title": "Am I walking into a trap?",
    "content": "I have a job offer from a small company (UK based) under 50 employees. It's a data science job. However there is no direct mentoring involved and I would be the only data scientist in the company. I need a job but don't know if this is safe or not. ",
    "author": "marblesandcookies",
    "timestamp": "2025-06-02T11:27:11",
    "url": "https://reddit.com/r/datascience/comments/1l1pm5w/am_i_walking_into_a_trap/",
    "score": 82,
    "num_comments": 43,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l1nm9m",
    "title": "How do you teach business common sense?",
    "content": "Really not the best way to start the week by finding out a colleague of mine CC'ed our internal-only model run reports to downstream team, which then triggered a chain of ppl requesting to be CC'ed for any future delivery.\n\nWe have an external report for that which said colleague has been sending out for an extended period of time.\n\nSaid colleague would also pull up code base and go line-by-line in a meeting with director-level business people. Different directors had, on multiple occasions, asked to not do that and give an abstraction only. This affects his perception despite the work underneath being solid. We're not toxic but you really can't expect high management to read your SQL code without them feeling like you're wasting their time.\n\nThis person works hard, has good intention, and can deliver if correctly understanding the task (which is in itself another battle). I'm not his manager, but he takes over the processes/pipelines I established so I'm still on the hook if things don't work.\n\nI trust his work on the technical side but this corporate thing is really not clicking for him, and I really have no idea how do you put these \"common sense\" into someone's head.",
    "author": "Outside_Base1722",
    "timestamp": "2025-06-02T10:10:57",
    "url": "https://reddit.com/r/datascience/comments/1l1nm9m/how_do_you_teach_business_common_sense/",
    "score": 57,
    "num_comments": 31,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l10fes",
    "title": "How I scraped 4.1 million jobs with GPT4o-mini",
    "content": "**Background**: During my PhD in Data Science at Stanford, I got sick and tired of ghost jobs &amp; 3rd party offshore agencies on LinkedIn &amp; Indeed. So I wrote a script that fetches jobs from 100k+ company websites' career pages and uses GPT4o-mini to extract relevant information (ex salary, remote, etc.) from job descriptions. I made it publicly available here [https://hiring.cafe](https://hiring.cafe) and you can follow my progress and give me feedback at r/hiringcafe\n\n**Tech details (from a DS perspective)**\n\n1. Verifying legit companies. This I did manually, but it was crucial that I exclude any recruiting firms, 3rd party offshore agencies, etc. I manually sorted through the \\~100,000 company career pages (this took several weeks) and picked the ones that looked legit. At Stanford, we call this technique \"occular regression\" :) \n2. Removing ghost jobs. I discovered that a strong predictor of if a job is a ghost job is that if it keeps being reposted. I was able to identify reposting by doing a embedding text similarity search for jobs from the same company. If 2 job descriptions overlap too much, I only show the date posted for the *earliest* listing. This allowed me to weed out most ghost jobs simply by using a date filter (for example, excluding any jobs posted over a month ago). \n3. Scraping fresh jobs 3x/day. To ensure that my database is reflective of the company career page, I check each company career page 3x/day. To avoid rate-limits, I used a rotating proxy from Oxylabs for now.\n4. Building advanced NLP text filters. After playing with GPT4o-mini API, I realized I could can effectively dump raw job descriptions (in HTML) and ask it to give me back formatted information back in JSON (ex salary, yoe, etc). I used this technique to extract a variety of information, including technical keywords, job industry, required licenses &amp; security clearance, if the company sponsors visa, etc.\n\n**Question for the DS community:** Beyond job search, one thing I'm really excited about this 4.1 million job dataset is to be able to do a yearly or quarterly trend report. For instance, to look at what technical skills are growing in demand. What kinds of cool job trends analyses would you do if you had access to this data.\n\n**Edit:** A few folks DMed asking to explore the data for job searching. I put together a minimal frontend to make the scraped jobs searchable: [https://hiring.cafe](https://hiring.cafe) — note that it's currently non-commercial, unsupported, just a PhD side-project at the moment until I gradute.\n\n**Edit 2::** thank you for all the super positive comments. you can follow my progress on scraping more jobs on r/hiringcafe .Aalso to comments saying this is an ad, my full-time job is my phd, this is just a fun side project beofore I get an actual job haha",
    "author": "hamed_n",
    "timestamp": "2025-06-01T14:27:39",
    "url": "https://reddit.com/r/datascience/comments/1l10fes/how_i_scraped_41_million_jobs_with_gpt4omini/",
    "score": 561,
    "num_comments": 66,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l18ji8",
    "title": "Weekly Entering &amp; Transitioning - Thread 02 Jun, 2025 - 09 Jun, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-06-01T21:02:21",
    "url": "https://reddit.com/r/datascience/comments/1l18ji8/weekly_entering_transitioning_thread_02_jun_2025/",
    "score": 6,
    "num_comments": 23,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l0wx56",
    "title": "Can data science be used in computer networking (if not can it be used in cybersecurity)?",
    "content": "Hi, I’m a high schooler (junior year) who is extremely interested in data science to the point where it is the main career field I want to go into. However, I got enrolled in a program where we train and study for the CCNA and Network+, two prominent computer networking certifications that even adults in the field dont have. I’m taking the certifications next week so hopefully I pass both, but my heart is still in data science although i rlly dont want to waste these newly acquired skills. I know data science is a wide ranging topic that can be extended to multiple different fields, and the use of automation and AI being used in stuff like SDNs are increasing. I guess my question is if theres a solid career in data science with a computer networking background.\n\nAdditional question: I gotta start thinking of college so would I, if there is a possible path, major in data science and minor in computer networking?",
    "author": "Particular_Reality12",
    "timestamp": "2025-06-01T12:01:46",
    "url": "https://reddit.com/r/datascience/comments/1l0wx56/can_data_science_be_used_in_computer_networking/",
    "score": 18,
    "num_comments": 9,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l0y4zo",
    "title": "Advice on processing ~1M jobs/month with LLaMA for cost savings",
    "content": "I'm using GPT-4o-mini to process \\~1 million jobs/month. It's doing things like deduplication, classification, title normalization, and enrichment.\n\nThis setup is fast and easy, but the cost is starting to hurt. I'm considering distilling this pipeline into an open-source LLM, like LLaMA 3 or Mistral, to reduce inference costs, most likely self-hosted on GPU on Google Coud. \n\nQuestions:\n\n\\* Has anyone done a similar migration? What were your real-world cost savings (e.g., from GPT-4o to self-hosted LLaMA/Mistral)\n\n\\* Any recommended distillation workflows? I'd be fine using GPT-4o to fine-tune an open model on our own tasks.\n\n\\* Are there best practices for reducing inference costs even further (e.g., batching, quantization, routing tasks through smaller models first)?\n\n\\* Is anyone running LLM inference on consumer GPUs for light-to-medium workloads successfully?\n\nRight now, our GPT-4o-mini usage is costing me thousands/month (I'm paying for it out of pocket, no investors). Would love to hear what’s worked for others!\n\n\n\n",
    "author": "hamed_n",
    "timestamp": "2025-06-01T12:52:46",
    "url": "https://reddit.com/r/datascience/comments/1l0y4zo/advice_on_processing_1m_jobsmonth_with_llama_for/",
    "score": 13,
    "num_comments": 5,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l0fa7t",
    "title": "What is your functional area?",
    "content": "I don’t mean industry. I mean product, operations, etc. I work in operations. I don’t grow the business. I keep the business alive.",
    "author": "Trick-Interaction396",
    "timestamp": "2025-05-31T20:32:11",
    "url": "https://reddit.com/r/datascience/comments/1l0fa7t/what_is_your_functional_area/",
    "score": 36,
    "num_comments": 54,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l03bjn",
    "title": "Help choosing a book for learning bayesian statistics in python",
    "content": "",
    "author": "guna1o0",
    "timestamp": "2025-05-31T10:59:03",
    "url": "https://reddit.com/r/datascience/comments/1l03bjn/help_choosing_a_book_for_learning_bayesian/",
    "score": 23,
    "num_comments": 15,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kz6tnh",
    "title": "Perfect job for me suffering from Imposter Syndrome",
    "content": "",
    "author": "klaxonlet",
    "timestamp": "2025-05-30T08:16:17",
    "url": "https://reddit.com/r/datascience/comments/1kz6tnh/perfect_job_for_me_suffering_from_imposter/",
    "score": 1750,
    "num_comments": 56,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1l0dsfl",
    "title": "About MCP servers",
    "content": "Do anyone have tried MCP server with llm and rag? If anyone done please share the code ",
    "author": "atharv1525",
    "timestamp": "2025-05-31T19:11:05",
    "url": "https://reddit.com/r/datascience/comments/1l0dsfl/about_mcp_servers/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kz8mmn",
    "title": "President Taps Palantir to Compile Data on Americans",
    "content": "No words",
    "author": "EarthGoddessDude",
    "timestamp": "2025-05-30T09:28:10",
    "url": "https://reddit.com/r/datascience/comments/1kz8mmn/president_taps_palantir_to_compile_data_on/",
    "score": 300,
    "num_comments": 48,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kzpdnv",
    "title": "Infra DA/DS, guidance to ramp up?",
    "content": "Hello!\n\nJust stepped into a new role as Lead DS for a team focused on infra analytics and data science. We'll be analyzing model training jobs/runs (I don't know what the data set is yet but assume it's resource usage, cost, and system logs) to find efficiency wins (think speed, cost, and even sustainability). We'll also explore automation opportunities down the line as subsequent projects.\n\nThis is my first time working at the infrastructure layer, and I’m looking to ramp up fast.\n\nWhat I’m looking for:\n\n- Go-to resources (books, papers, vids) for ML infra analytics\n\n- What data you typically analyze (training logs, GPU usage, queue times, etc.)\n\n- Examples of quick wins, useful dashboards, KPIs?\n\nIf you’ve done this kind of work I’d love to hear what helped you get sharp. Thanks!\n\nPs - I'm a 8 yr DS at this company. Company size, data, number of models, etc, is absolutely massive. Lmk what other info and I can amend this post. Thank you!",
    "author": "unserious1",
    "timestamp": "2025-05-30T22:22:28",
    "url": "https://reddit.com/r/datascience/comments/1kzpdnv/infra_dads_guidance_to_ramp_up/",
    "score": 16,
    "num_comments": 3,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kzgvz0",
    "title": "Bored and underutilized - how to prep for the next gig?",
    "content": "DS/BI team has had 4 different leaders in the past year and our company seems to have lost any sense of analytics strategy. Two years ago we had 16 total, BI devs and data scientists including ML specialists and ML app builders. We are now down to 7 after attrition and I know three more are actively interviewing. Last model put into production was in 2024 and there are no requests for ML work this fiscal year. Our project plans are now less than a sprint ahead and it is not unusual to get an analytical request in the morning only to be told by noon \"that's no longer a priority\".\n\nIt's been this way for long enough that I'm questioning whether I want to continue in DS or move to a related field. I have a background in databases and data engineering. i have done some work in Gen AI with prompt engineering and automation but it for my company because there is a zero trust policy on all Gen AI (thanks to an idiot who loaded the transcript from a VPs disciplinary call to chatGPT to get a summary). I am much more interested in probabilistic modeling and forecasting but again no experience outside of online classes. For all intensive purposes I have been a SQL dev with some Python for the last 4 years. The last model I put into production was an unsupervised model of workers by productivity at different roles, which was in 2022. \n\nWhere should I go next? Seriously thinking about enrolling in a masters just to look fresh again.",
    "author": "Feeling-Carry6446",
    "timestamp": "2025-05-30T15:04:55",
    "url": "https://reddit.com/r/datascience/comments/1kzgvz0/bored_and_underutilized_how_to_prep_for_the_next/",
    "score": 33,
    "num_comments": 20,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kzkwcg",
    "title": "Validation of Statistical Tooling Packages",
    "content": "Hey all,\n\nI was wondering if anyone has any experience on how to properly validating statistical packages for numerical accuracy?\n\nSome context: I've developed a Python package for internal use that can undertake all the statistics we require in our field for our company. The statistics are used to ensure compliance to regulatory guidelines. \n\nThe industry standard is a globally shared maceo-free Excel sheet, that relies heavily on approximations to bypass VBA requirements. Because of this, edge cases will give different reaults. Examples include use of non-central t-distrubtion, MLE, infinite series calcuations, Shapiro-wilk. The sheet is also limited to 50 samples as the approximations end here.\n\nPackages exist in R that do most of it (NADA, EnvStats, STAND, Tolerance). I could (and probably should have) make a package from these, but I'd still need to modify and develop some statistics from scratch, and my R skills are abysmal compared to Python.\n\nFrom a software engineering point, for more math heavy code, is there best practices for validating the outputs? The issue is this Excel sheet is considered the \"gold standard\" and I'll need to justify differences.\n\nI currently have two validation passes, one is a dedicated unit test with a small dataset that I have cross referenced and checked by hand, with exisiting R packages and with the existing notebook. This dataset I've picked tries to cover extremes at either side of the data ranges we get (Geo standard deviations &gt; 5, massive skews, zero range, heavily censored datasets).\n\nThe second is a bulk run of a large datatset to tease out weird edge cases, but I haven't done the cross validations by hand unless I notice weird results.\n\nIs there anything else that I should be doing, or need to consider?",
    "author": "Sebyon",
    "timestamp": "2025-05-30T18:12:57",
    "url": "https://reddit.com/r/datascience/comments/1kzkwcg/validation_of_statistical_tooling_packages/",
    "score": 13,
    "num_comments": 5,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kzjr30",
    "title": "Two‑stage model filter for web‑scale document triage?",
    "content": "I am crawling roughly **20 billion** web pages, and trying to triage for the ones that are only job descriptions. Only about **5%** contain actual job advertisements. Running a Transformer over the whole corpus feels prohibitively expensive, so I am debating whether a **two‑stage pipeline** is the right move:\n\n1. **Stage 1:** ultra‑cheap lexical model (hashing TF‑IDF plus Naive Bayes or logistic regression) on CPUs to toss out the obviously non‑job pages while keeping recall very high.\n2. **Stage 2:** small fine‑tuned Transformer such as DistilBERT on a much smaller candidate pool to recover precision.\n\nMy questions for teams that have done large‑scale extraction or classification:\n\n* Does the two‑stage approach really save enough money and wall‑clock time to justify the engineering complexity compared with just scaling out a single Transformer model on lots of GPUs?\n* Any unexpected pitfalls with maintaining two models in production, feature drift between stages, or tokenization bottlenecks?\n* If you tried both single‑stage and two‑stage setups, how did total cost per billion documents compare?\n* Would you recommend any open‑source libraries or managed services that made the cascade easier?\n\n",
    "author": "hamed_n",
    "timestamp": "2025-05-30T17:15:39",
    "url": "https://reddit.com/r/datascience/comments/1kzjr30/twostage_model_filter_for_webscale_document_triage/",
    "score": 6,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kyr1va",
    "title": "Regularization=magic?",
    "content": "Everyone knows that regularization prevents overfitting when model is over-parametrized and it makes sense. But how is it possible that a regularized model performs better even when the model family is fully specified?\n\nI generated data y=2+5x+eps, eps~N(0, 5) and I fit a model y=mx+b (so I fit the same model family as was used for data generation). Somehow ridge regression still fits better than OLS.\n\nI run 10k experiments with 5 training and 5 testing data points. OLS achieved mean MSE 42.74, median MSE 31.79. Ridge with alpha=5 achieved mean MSE 40.56 and median 31.51.\n\nI cannot comprehend how it's possible - I seemingly introduce bias without an upside because I shouldn't be able to overfit. What is going on? Is it some Stein's paradox type of deal? Is there a counterexample where unregularized model would perform better than model with any ridge_alpha?\n\nEdit: well of course this is due to small sample and large error variance. That's not my question. I'm not looking for a \"this is a bias-variance tradeoff\" answer either. Im asking for intuition (proof?) why would a biased model ever work better in such case. Penalizing high b instead of high m would also introduce a bias but it won't lower the test error. But penalizing high m does lower the error. Why?",
    "author": "Ciasteczi",
    "timestamp": "2025-05-29T17:44:23",
    "url": "https://reddit.com/r/datascience/comments/1kyr1va/regularizationmagic/",
    "score": 49,
    "num_comments": 33,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kyesak",
    "title": "Did any certifications or courses actually make a difference or were great investments financially?",
    "content": "Howdy folks,\n\nLooking for some insights and feedback. Ive been working a new job for the last two months that pays me more than I was previously making, after being out of work for about 8 months. \n\nNonetheless, I feel a bit funky as despite it being the best paying job Ive ever had-I also feel insanely disengaged from my job and not really all that engaged by my manager AT ALL and dont feel secure in it either. Its not nearly as kinetic and innovative of a role as I was sold.\n\nSo I wanted some feedback while I still had money coming in just in case something happens. \n\n**Were there or have there been any particular certifications or courses that you paid for, that REALLY made a difference for you in career opportunities at all? Just trying to make smart investments and money moves now in case anything happens and trying to think ahead.**",
    "author": "WhatsTheAnswerDude",
    "timestamp": "2025-05-29T09:11:39",
    "url": "https://reddit.com/r/datascience/comments/1kyesak/did_any_certifications_or_courses_actually_make_a/",
    "score": 61,
    "num_comments": 46,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kydj2t",
    "title": "I turned a real machine learning project into a children's book",
    "content": "",
    "author": "Clicketrie",
    "timestamp": "2025-05-29T08:21:23",
    "url": "https://reddit.com/r/datascience/comments/1kydj2t/i_turned_a_real_machine_learning_project_into_a/",
    "score": 66,
    "num_comments": 16,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ky9jlz",
    "title": "Seeking help in choosing between two offers.",
    "content": "Hey Y'all,\n\nNeeded some inputs in choosing between two offers. I have tried to read similar thread before. \n\n**Company 1**: Some Fintech\n\n**Position**: Senior Data Scientist\n\n**Role**: Taking care of their models on databricks. Models like ARR modelling. Churn modelling etc.\n\n**Other Important Factors**: Company 1 has 5 days in office. This is a new mandate to prevent previous misuse. You also have to be very social person. They have had rounds of layoffs and had hiring freeze and have started to hiring again. My interview experience was great and I can see myself being successful in this role. However, I havent practiced classic machine learning for a while. I surely can pick it up. I am only worried that this role will have no engineering work at all. No productionsining of models. I am not sure how this will be for my future roles.\n\n\n\n**Company 2**: Some company which is actively using LLMs and Agentic approaches\n\n**Position**: Senior Machine Learning Engineer\n\n**Role**: Work with agentic AI and productionise and update LLMs\n\n**My Preference** \\- Work with a company with stability and in a position where I can grow long term.\n\n\n**Other Important Factors**: This role is in line with my last role, my PhD and LLM experience. I have read tonnes of literature so I sort of feel prepared for this role but I feel worthless when I have to spend weeks to improve latency without touching LLMs. My technical round was also okayish in this company. They are doubling the team. They are a well established company too. \n\n\n-------------------------------\n\nMy last position was of a ML engineer and I think what I disliked is -- the position slowly slipping into too much backend work. I am a stronger data scientist by training but have a PhD in NLP application so know the other bit too. I do struggle a bit when it comes to productinising things but I have improved a lot and in a better place.\n\nI guess what I want to ask is for folks who work at companies that have not yet implemented AI -- do you feel behind the industry or you have satisfied with the current trajectory ?\n\nI honestly don't care about whether I work in NLP / AI or not, All I want is a peaceful job where I can do my best and grow. On one hand the ML engineer position seems to be very on the cutting edge of technology but I know at the end its going to be API call to some LLM with much boiler plate code and many tools. The data scientist position looks like something I have done in the past and now should leave and do progress to ML engineering. \n\nAdvice ?",
    "author": "mlbatman",
    "timestamp": "2025-05-29T05:29:33",
    "url": "https://reddit.com/r/datascience/comments/1ky9jlz/seeking_help_in_choosing_between_two_offers/",
    "score": 20,
    "num_comments": 8,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kymajf",
    "title": "Anyone working for public organizations publish open data?",
    "content": "Hello everyone,\n\nI'm conducting research on how public sector organizations manage and share data with the public. I'm particularly interested in understanding:\n\n* **Which platforms or repositories do you use to publish open data?**\n* **What types of data are you sharing with the public?**\n* **What challenges have you faced in publishing and managing open data?**\n* **Are there specific policies or regulations that guide your open data practices?**\n\nYour insights will be invaluable in understanding the current landscape of open data practices in public organizations. Feel free to share as much or as little as you're comfortable with.\n\nThank you in advance for your contributions!",
    "author": "anuveya",
    "timestamp": "2025-05-29T14:09:13",
    "url": "https://reddit.com/r/datascience/comments/1kymajf/anyone_working_for_public_organizations_publish/",
    "score": 3,
    "num_comments": 17,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kxjfgz",
    "title": "How to stay motivated in a job where my salary has remained flat for last 4 years and there’s no promotion in sight?",
    "content": "I joined my current company 3.5 years ago during a hiring boom. I was excited about the role and contributed heavily, leading process improvements with real financial impact. Despite this, I’ve received 0% raises year after year, which has been discouraging.\n\nI stayed motivated, hoping the role would benefit my long-term career. But since the last performance cycle, my enthusiasm has dropped. I don’t feel appreciated, and it worries me that I could be the first to go if layoffs happen.\n\nI’ve asked for a promotion twice in the past two years, but only received vague feedback like “We haven’t set you up for success yet” or “Promotion isn’t just about performance.”\n\nIt’s frustrating to feel stuck in a job I once loved. I’ve started interviewing, though the market is tough — but I’ll keep at it. In the meantime, I’m not sure what to do next. Any advice?",
    "author": "Substantial_Tank_129",
    "timestamp": "2025-05-28T08:04:52",
    "url": "https://reddit.com/r/datascience/comments/1kxjfgz/how_to_stay_motivated_in_a_job_where_my_salary/",
    "score": 194,
    "num_comments": 77,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kxk5m1",
    "title": "Best youtube playlists for learning causal inference with Python?",
    "content": "Hey folks,\n\nIm starting to learn causal inference and want to understand both the theory and how to apply it using python. I’m comfortable with classical ML, but causal inference is new to me.\n\nLooking for youtube playlists or videos that explain concepts like DAGs, DID, double ML, propensity scores, IPTW, etc., and ideally show practical examples using libraries like DoWhy, EconML, or CausalML.\n\nim not very comfortable with books.\n\nAlso, is it even worth spending time learning causal inference in depth? Im planning to dig into Bayesian inference next, so curious if this is a good path.\n\nWould really appreciate any suggestions. thanks!",
    "author": "guna1o0",
    "timestamp": "2025-05-28T08:33:47",
    "url": "https://reddit.com/r/datascience/comments/1kxk5m1/best_youtube_playlists_for_learning_causal/",
    "score": 76,
    "num_comments": 19,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kxkne5",
    "title": "Does anyone knows a nice course for Streamlit Apps?",
    "content": "What's in the title, I wanna learn how to create a deploy apps using Streamlit and I wanted to know which courses do you suggest for it?",
    "author": "Karl_mstr",
    "timestamp": "2025-05-28T08:53:14",
    "url": "https://reddit.com/r/datascience/comments/1kxkne5/does_anyone_knows_a_nice_course_for_streamlit_apps/",
    "score": 2,
    "num_comments": 27,
    "upvote_ratio": 0.54,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kw1cik",
    "title": "Am i the only one who truly love this field? It sounds like everyone here is in for the money and hate their jobs",
    "content": "it's funny because in real life most of the people i know in the field love it",
    "author": "jinstronda",
    "timestamp": "2025-05-26T11:13:38",
    "url": "https://reddit.com/r/datascience/comments/1kw1cik/am_i_the_only_one_who_truly_love_this_field_it/",
    "score": 1895,
    "num_comments": 143,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kwd8vj",
    "title": "With DS layoffs happening everyday,what’s the future ?",
    "content": "I am a freelancer Data Scientist and finding it extremely hard to get projects. I understand the current environment in DS space with layoffs happening all over the place and even the Director of AI @ Microsoft was laid off. I would love to hear from other Redditors about it. I’m currently extremely scared about my future as I don’t know if I’ll get projects. ",
    "author": "honwave",
    "timestamp": "2025-05-26T20:18:51",
    "url": "https://reddit.com/r/datascience/comments/1kwd8vj/with_ds_layoffs_happening_everydaywhats_the_future/",
    "score": 174,
    "num_comments": 68,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kwh7bw",
    "title": "Seeking Advice: How To Scale AI Models Without Huge Upfront Investment?",
    "content": "Hey folks,  \nOur startup is exploring AI-powered features but building and managing GPU clusters is way beyond our current budget and expertise. Are there good cloud services that provide ready-to-use AI models via API?Anyone here used similar “model APIs” to speed up AI deployment and avoid heavy infrastructure? Insights appreciated!",
    "author": "jameslee2295",
    "timestamp": "2025-05-27T00:25:02",
    "url": "https://reddit.com/r/datascience/comments/1kwh7bw/seeking_advice_how_to_scale_ai_models_without/",
    "score": 10,
    "num_comments": 9,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kvzd73",
    "title": "Thinking of switching from Data Scientist to Data Product Owner — need advice",
    "content": "Hey everyone,\nI’ve been working as a Data Scientist for the past 5 years, currently at a bank. I’ll be honest — this might sound a bit harsh, but it’s just how I personally feel: this job is slowly draining me.\n\nMost of the models I build never make it to production. A big chunk of my time is spent doing analysis that feels more like trying to impress higher-ups than solving real problems. And with AI evolving so rapidly, there’s this growing pressure to “level up” to a senior role — but the bar is so high now, and the opportunities seem fewer and harder to reach. It’s honestly demotivating.\n\nSo, I’m thinking about pivoting into a Data Product Owner (or Product Manager) role. I feel like my experience could bridge the gap between business and technical teams — I can speak the language of data engineers, ML engineers, and data scientists. Plus, I’d love to be in a role that’s more collaborative and human-facing. It also feels like a safer long-term path in this AI-driven world.\n\nHas anyone made a similar transition? Or is anyone here feeling the same way? I’d really appreciate any advice, feedback, or even just hearing your story. Totally open to different perspectives.\n\nThanks!\n",
    "author": "xSicilianDefenderx",
    "timestamp": "2025-05-26T09:55:15",
    "url": "https://reddit.com/r/datascience/comments/1kvzd73/thinking_of_switching_from_data_scientist_to_data/",
    "score": 102,
    "num_comments": 26,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kvqn1s",
    "title": "How can I address wild expectations about Gen AI and Agentic AI?",
    "content": "Following what the title says, people in my company have gone ballistic on Agentic AI and Gen AI more broadly as of late. This sadly includes some of the IT management that should know better/temper out expectations on what these can/cannot do.\n\nTo be clear, I am not a hater either, I see them as useful techonologies that unlock new opportunities within my work. At the same time, I feel like all the non-experts (and in this case even my management which is supposed to be more knowledgeable but has been carried away from the hype and is not hands-on) have completely non-realistic expectations of what these tools can do.\n\nDo any of you have experience with educating people on what is reasonable to expect in this context? I am a bit tired of having to debunk use case by use.",
    "author": "Kellsier",
    "timestamp": "2025-05-26T03:00:26",
    "url": "https://reddit.com/r/datascience/comments/1kvqn1s/how_can_i_address_wild_expectations_about_gen_ai/",
    "score": 101,
    "num_comments": 44,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kwycfm",
    "title": "The DS industry is turning into the investment banking industry",
    "content": "Seems like the DS industry is essentially becoming a reflection of investment banking at places like Goldman Sachs or JP Mo. To get a job in the investment banking world you need to either: know someone high up at the company, have gone to a prestigious school, have experience at a different prestigious institution or transfer into the role internally.\n\nHow is this different from the current state of DS? Sure, it’s still possible to get a job based purely off skills, experience and raw dogging a job application, but it’s unlikely considering you are battling against ~800 resumes filled with exaggerations and lies for each job posting. Some companies don’t even put out job positions and choose to hire from their network instead, similar to IB. Merit based hiring seems like a thing of the past at this point.",
    "author": "Fit-Employee-4393",
    "timestamp": "2025-05-27T13:46:09",
    "url": "https://reddit.com/r/datascience/comments/1kwycfm/the_ds_industry_is_turning_into_the_investment/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kuxcok",
    "title": "2025 stack check: which DS/ML tools am I missing?",
    "content": "**Hi all,**\n\nI work in ad-tech, where my job is to improve the product with data-driven algorithms, mostly on tabular datasets (CTR models, bidding, attribution, the usual).\n\nCurrent work stack (quite classic I guess)\n\n* pandas, numpy, scikit-learn, xgboost, statsmodels \n* PyTorch (light use) \n* JupyterLab &amp; notebooks \n* matplotlib, seaborn, plotly for viz \n* Infra: everything runs on AWS (code is hosted on Github)\n\nThe news cycle is overflowing with LLM tools, I do use ChatGPT / Claude / Aider as helpers, but my main concern right now is the core DS/ML tooling that powers production pipelines.\n\nSo,  \nWhat *genuinely awesome* 2024-25 libraries, frameworks, or services should I try, so I don’t get left behind? :)  \nAny recommendations greatly appreciated, thanks!",
    "author": "meni_s",
    "timestamp": "2025-05-25T01:05:25",
    "url": "https://reddit.com/r/datascience/comments/1kuxcok/2025_stack_check_which_dsml_tools_am_i_missing/",
    "score": 140,
    "num_comments": 53,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kvl43y",
    "title": "Weekly Entering &amp; Transitioning - Thread 26 May, 2025 - 02 Jun, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-05-25T21:01:44",
    "url": "https://reddit.com/r/datascience/comments/1kvl43y/weekly_entering_transitioning_thread_26_may_2025/",
    "score": 3,
    "num_comments": 32,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kv3smi",
    "title": "Can you explain to me the product analytics job?",
    "content": "I ve watched videos about Data Scientist Product Analytics but i still dont understand if the job would excite me. \n\nCan someone explain it more in depth so that i can understand if i like it? I like the data science job (i am pursuing a master in DS) but it seems that product analytics is very different in the sense that it is very focused on SQL.\n\nAlso is it interesting and does it involve a lot of problem solving?\nDoes it have a sort of path to PM?",
    "author": "FinalRide7181",
    "timestamp": "2025-05-25T07:24:48",
    "url": "https://reddit.com/r/datascience/comments/1kv3smi/can_you_explain_to_me_the_product_analytics_job/",
    "score": 11,
    "num_comments": 14,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kulk1b",
    "title": "Found a really amazing video , providing context to the breakthrough as well as the misconceived hype around Alphaevolve",
    "content": "I am sure by now most of us would have seen or atleast heard about AlphaEvolve and it's many breakthroughs including the 4*4 MM improvement. While this was a fantastic step forward in constrained optimisation problems , a lot of the commentary around it in media was absolutely garbage.\n\nThe original paper is an amazing read, however I was scouring the internet to find videos by people who understood it at a better depth than I did. That's where I came across this gem. \n\nIt's long watch at around 40 mins, but is extremely well structured and not too heavy on math ( grad level at best). Would highly recommend watching this!",
    "author": "Much_Discussion1490",
    "timestamp": "2025-05-24T13:52:59",
    "url": "https://reddit.com/r/datascience/comments/1kulk1b/found_a_really_amazing_video_providing_context_to/",
    "score": 21,
    "num_comments": 1,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ku31vc",
    "title": "Is studying Data Science still worth it?",
    "content": "Hi everyone, I’m currently studying data science, but I’ve been hearing that the demand for data scientists is decreasing significantly. I’ve also been told that many data scientists are essentially becoming analysts, while the machine learning side of things is increasingly being handled by engineers.\n\n- Does it still make sense to pursue a career in data science or should i switch to computer science? I mean i dont think i want to do just AB tests for a living\n\n- Also, are machine learning engineers still building models or are they mostly focused on deploying them?\n",
    "author": "FinalRide7181",
    "timestamp": "2025-05-23T21:19:28",
    "url": "https://reddit.com/r/datascience/comments/1ku31vc/is_studying_data_science_still_worth_it/",
    "score": 302,
    "num_comments": 159,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ku5qsq",
    "title": "FOMO at workplace",
    "content": "Hii All. I have joined as a DS and this is my first job. The DS model which I am tasked  to improve and maintain does not adhere to the modern tech stack. It is just old school classical ML in R. It is not in production. We only maintain it in our local and show the stakeholders necessary numbers in quarterly meetings or whenever it is required. My concern is am I falling behind on skills by doing this. Especially seeing all the fancy tools and MLE buzzwords that is being thrown around in almost every DS application ?? If yes how can I develop those skills despite not having opportunities at my workplace. ",
    "author": "NervousVictory1792",
    "timestamp": "2025-05-24T00:15:42",
    "url": "https://reddit.com/r/datascience/comments/1ku5qsq/fomo_at_workplace/",
    "score": 40,
    "num_comments": 21,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kuu5g2",
    "title": "Is it worth to waste a year to do CS?",
    "content": "_(Yesterday i posted “is studying DS worth it” and it seemed that DS nowadays leads to product analytics which i dont enjoy. So i am considering to switch, it is a tough decision that is giving me troubles sleeping and concentrating on other stuff so i’d really like an helping hand from you guys)_\n \nGuys I’m currently doing a 2 years Master in Business Analytics (Management + Data Science), but I’m considering switching to a Master in CS and ML. The downside is that I’d lose a year.\n\nHere are some thoughts I’ve had so far:\nWith Business Analytics, I can access roles like:\n- Data Scientist (but nowadays Data Scientists mostly do Product Analytics rather than ML, which doesn’t excite me)\n- Management roles (but in tech it means mainly Sales, Marketing… less interesting to me. The exception is PM but it is very hard as a graduate)\n\nSo my questions are:\n\n1) Does it make sense to lose a year to switch to CS+ML? My biggest fear is how AI is evolving and impacting the field. **This is the biggest fear i have, should i switch in the era of AI?**\n\n 2) Am I undervaluing the opportunities from the Business Analytics Master? Especially regarding management roles, are there interesting options I’m missing?",
    "author": "FinalRide7181",
    "timestamp": "2025-05-24T21:32:03",
    "url": "https://reddit.com/r/datascience/comments/1kuu5g2/is_it_worth_to_waste_a_year_to_do_cs/",
    "score": 0,
    "num_comments": 34,
    "upvote_ratio": 0.44,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ksz870",
    "title": "The 80/20 Guide to R You Wish You Read Years Ago",
    "content": "After years of R programming, I've noticed most intermediate users get stuck writing code that works but isn't optimal. We learn the basics, get comfortable, but miss the workflow improvements that make the biggest difference.\n\nI just wrote up the handful of changes that transformed my R experience - things like:\n\n* Why DuckDB (and data.table) can handle datasets larger than your RAM\n* How renv solves reproducibility issues\n* When vectorization actually matters (and when it doesn't)\n* The native pipe |&gt; vs %&gt;% debate\n\nThese aren't advanced techniques - they're small workflow improvements that compound over time. The kind of stuff I wish someone had told me sooner.\n\nRead the [full article here.](https://open.substack.com/pub/borkar/p/the-8020-guide-to-r-you-wish-you?r=2qg9ny&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true)\n\nWhat workflow changes made the biggest difference for you?\n\nP.S. Posting to help out a friend",
    "author": "Infinitrix02",
    "timestamp": "2025-05-22T12:14:24",
    "url": "https://reddit.com/r/datascience/comments/1ksz870/the_8020_guide_to_r_you_wish_you_read_years_ago/",
    "score": 293,
    "num_comments": 34,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ktaq9z",
    "title": "How is the market for senior Data Scientists with research experience?",
    "content": "With everything that has going on around deepseek and the memes of US and China competing over the lead on AI, with Europe inventing a new bottle of plastic that is eco friendly, I was wandering how is the ML/AI market for experienced data and research scientists in Europe. Besides Misteral, I don’t think I know much. I guess that all the big companies have sites across the continent, but are there other companies that what are other companies that are worth following? \nAlso, to the European here, do you actually expect a boom in Europe with the shocks the Trump administration gives the system in the US?",
    "author": "[deleted]",
    "timestamp": "2025-05-22T21:21:02",
    "url": "https://reddit.com/r/datascience/comments/1ktaq9z/how_is_the_market_for_senior_data_scientists_with/",
    "score": 11,
    "num_comments": 26,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ksvnsk",
    "title": "\"You will help build and deploy scalable solutions... not just prototypes\"",
    "content": "Hi everyone,\n\nI’m not exactly sure how to frame this, but I’d like to kick off a discussion that’s been on my mind lately.\n\nI keep seeing data science job descriptions *(E2E) data science,* not just prototypes, but scalable, production-ready solutions. At the same time, they’re asking for an overwhelming tech stack: DL, LLMs, computer vision, etc. On top of that, E2E implies a whole software engineering stack too.\n\nSo, what does *E2E* really mean?\n\nFor me, the \"left end\" is talking to stakeholders and/or working with the WH. The \"right end\" is delivering three pickle files: one with the model, one with transformations, and one with feature selection. Sometimes, this turns into an API and gets deployed sometimes not. This assumes the data is already clean and available in a single table. Otherwise, you’ve got another automated ETL step to handle. (Just to note: I’ve never had write access to the warehouse. The best I’ve had is an S3 bucket.)\n\nWhen people say “scalable deployment,” what does that really mean? Let’s say the above API predicts a value based on daily readings. In my view, the model runs daily, stores the outputs in another table in the warehouse, and that gets picked up by the business or an app. Is that considered scalable? If not, what is?\n\nIf the data volume is massive, then you’d need parallelism, Lambdas, or something similar. But is that my job? I could do it if I had to, but in a business setting, I’d expect a software  engineer to handle that.\n\nNow, if the model is deployed on the edge, where exactly is the “end” of E2E then?\n\nSome job descriptions also mention API ingestion, dbt, Airflow, basically full-on data engineering responsibilities.\n\nThe bottom line: Sometimes I read a JD and what it *really* says is:\n\n“We want you to talk to stakeholders, figure out their problem, find and ingest the data, store it in an optimized medallion-model warehouse using dbt for daily ingestion and Airflow for monitoring. Then build a model, deploy it to 10,000 devices, monitor it for drift, and make sure the pipeline never breaks.\n\nMeanwhile, in real life, I spend weeks hand-holding stakeholders, begging data engineers for read access to a table I should already have access to, and struggling to get an EC2 instance when my model takes more than a few hours to run. Eventually, we store the outputs after  more meetings with the DE.\n\nOften, the stakeholder sees the prototype, gets excited, and then has no idea how to use it. The model ends up in limbo between the data team and the business until it’s forgotten. It just  feels like the ego boost of the week for the C guys.\n\nNow, I’m not the fastest or the smartest. But when I try to do all this E2E in personal projects, it takes ages and that’s without micromanagers breathing down my neck. Just setting up ingestion and figuring out how to optimize the WH took me two weeks.\n\nSo... all I am asking am I stupid , am I missing something?  Do you all actually do all of this daily? Is my understanding off?\n\nReally just hoping this kicks off a genuine discussion.\n\nCheers :)",
    "author": "Emergency-Agreeable",
    "timestamp": "2025-05-22T09:51:29",
    "url": "https://reddit.com/r/datascience/comments/1ksvnsk/you_will_help_build_and_deploy_scalable_solutions/",
    "score": 83,
    "num_comments": 48,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ksev5p",
    "title": "Is the traditional Data Scientist role dying out?",
    "content": "I've been casually browsing job postings lately just to stay informed about the market, and honestly, I'm starting to wonder if the classic \"Data Scientist\" position is becoming a thing of the past.\n\nMost of what I'm seeing falls into these categories:\n\n* Data Analyst/BI roles (lots of SQL, dashboards, basic reporting)\n* Data Engineer positions (pipelines, ETL, infrastructure stuff)\n* AI/ML Engineer jobs (but these seem more about LLMs and deploying models than actually building them)\n\nWhat I'm *not* seeing much of anymore is that traditional data scientist role - you know, the one where you actually do statistical modeling, design experiments, and work through complex business problems from start to finish using both programming and solid stats knowledge.\n\nIt makes me wonder: are companies just splitting up what used to be one data scientist job into multiple specialized roles? Or has the market just moved on from needing that \"unicorn\" profile that could do everything?\n\nFor those of you currently working as data scientists - what does your actual day-to-day look like? Are you still doing the traditional DS work, or has your role evolved into something more specialized?\n\nAnd for anyone else who's been keeping an eye on the job market - am I just looking in the wrong places, or are others seeing this same trend?\n\nJust curious about where the field is heading and whether that broad, stats-heavy data scientist role still has a place in today's market.",
    "author": "ImGallo",
    "timestamp": "2025-05-21T18:33:43",
    "url": "https://reddit.com/r/datascience/comments/1ksev5p/is_the_traditional_data_scientist_role_dying_out/",
    "score": 521,
    "num_comments": 175,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ksnwds",
    "title": "Hypothesis Testing and Experimental Design",
    "content": "Sharing my second ever blog post, covering experimental design and Hypothesis testing. \n\nI shared my first blog post here a few months ago and received valuable feedback, sharing it here so I can hopefully share some value and receive some feedback as well.",
    "author": "joshamayo7",
    "timestamp": "2025-05-22T04:04:00",
    "url": "https://reddit.com/r/datascience/comments/1ksnwds/hypothesis_testing_and_experimental_design/",
    "score": 29,
    "num_comments": 4,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ktn5xx",
    "title": "6 degrees of separation",
    "content": "",
    "author": "CapraNorvegese",
    "timestamp": "2025-05-23T08:58:39",
    "url": "https://reddit.com/r/datascience/comments/1ktn5xx/6_degrees_of_separation/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ks5jo6",
    "title": "Those of you who interviewed/working at big tech/finance, how did you prepare for it? Need advice pls.",
    "content": "title. Im a data analyst  with \\~3yoe currently work at a bank. lets say i have this golden time period where my work is low stress/pressure and I can put time into preparing for interviews. My goal is to get into FAANG/finance/similar companies in data science roles. How do I prepare for interviews? Did you follow a specific structure for certain companies? How/what did you allocate time into between analytics/sql/python, ML, GenAI(if at all) or other stuff and how did you prepare? Im good w sql, currently practicing ML and GenAI projects on python. I have very basic understanding of data engg from self projects. What metrics you use to determine where you stand?\n\nI get the job market is shit but Im not ready anyway. My aim is to start interviewing by fall, say august/september. I'd highly appreciate any help i can get. thx. ",
    "author": "potatotacosandwich",
    "timestamp": "2025-05-21T11:40:52",
    "url": "https://reddit.com/r/datascience/comments/1ks5jo6/those_of_you_who_interviewedworking_at_big/",
    "score": 77,
    "num_comments": 42,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1krc89b",
    "title": "No DS job after degree",
    "content": "Hi everyone, \nThis may be a bit of a vent post. I got a few years in DS experience as a data analyst and then got my MSc in well ranked US school. For some reason beyond my knowledge, I’ve never been able to get a DS job after the MS degree. I got a quant job where DS is the furthest thing from it even though some stats is used, and I am now headed to a data engineering fellowship with option to renew for one more year max. I just wonder if any of this effort was worth it sometimes . I’m open to any advice or suggestions because it feels like I can’t get any lower than this.\nThanks everyone \n\nEdit : thank you everyone for all the insights and kind words!!!",
    "author": "Emuthusiast",
    "timestamp": "2025-05-20T11:24:51",
    "url": "https://reddit.com/r/datascience/comments/1krc89b/no_ds_job_after_degree/",
    "score": 266,
    "num_comments": 115,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1krkxl4",
    "title": "Have you ever wondered, what comes next? Once you’ve built the model or finished the analysis, how do you take the next step? Whether it’s turning it into an app, a tool, a product, or something else?",
    "content": "For those of you working on personal data science projects, what comes after the .py script or Jupyter notebook? \n\nI’m trying to move beyond exploratory work into something more usable or shareable. \n\nIs building an app the natural next step? \n\nWhat paths have you taken to evolve your projects once the core analysis or modeling was done?",
    "author": "Proof_Wrap_2150",
    "timestamp": "2025-05-20T17:41:08",
    "url": "https://reddit.com/r/datascience/comments/1krkxl4/have_you_ever_wondered_what_comes_next_once_youve/",
    "score": 27,
    "num_comments": 22,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1krau6q",
    "title": "Are there any math tests that test mathematical skill for data science?",
    "content": "I am looking for a test which can test one’s math skills that are relevant for data science- that way I can understand which areas I’m weak in and how I measure relative to my peers. Is anybody aware of anything like that?",
    "author": "Beginning-Sport9217",
    "timestamp": "2025-05-20T10:30:32",
    "url": "https://reddit.com/r/datascience/comments/1krau6q/are_there_any_math_tests_that_test_mathematical/",
    "score": 47,
    "num_comments": 27,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1krl7kx",
    "title": "Question about using the MLE of a distribution as a loss function",
    "content": "I recently built a model using a Tweedie loss function. It performed really well, but I want to understand it better under the hood. I'd be super grateful if someone could clarify this for me.\n\nI understand that using a \"Tweedie loss\" just means using the negative log likelihood of a Tweedie distribution as the loss function. I also already understand how this works in the simple case of a linear model f(x\\_i) = wx\\_i, with a normal distribution negative log likelihood (i.e., the RMSE) as the loss function. You simply write out the likelihood of observing the data {(x\\_i, y\\_i) | i=1, ..., N}, given that the target variable y\\_i came from a normal distribution with mean f(x\\_i). Then you take the negative log of this, differentiate it with respect to the parameter(s), w in this case, set it equal to zero, and solve for w. This is all basic and makes sense to me; you are finding the w which maximizes the likelihood of observing the data you saw, given the assumption that the data y\\_i was drawn from a normal distribution with mean f(x\\_i) for each i.\n\nWhat gets me confused is using a more complex model and loss function, like LightGBM with a Tweedie loss. I figured the exact same principles would apply, but when I try to wrap my head around it, it seems I'm missing something.\n\nIn the linear regression example, the \"model\" is y\\_i \\~ N(f(x\\_i), sigma\\^2). In other words, you are assuming that the response variable y\\_i is a linear function of the independent variable x\\_i, plus normally distributed errors. But how do you even write this in the case of LightGBM with Tweedie loss? In my head, the analogous \"model\" would be y\\_i \\~ Tw(f(x\\_i), phi, p), where f(x\\_i) is the output of the LightGBM algorithm, and f(x\\_i) takes the place of the mean mu in the Tweedie distribution Tw(u, phi, p). Is this correct? Are we always just treating the prediction f(x\\_i) as the mean of the distribution we've assumed, or is that only coincidentally true in the special case of a linear model with normal distribution NLL?",
    "author": "_hairyberry_",
    "timestamp": "2025-05-20T17:55:46",
    "url": "https://reddit.com/r/datascience/comments/1krl7kx/question_about_using_the_mle_of_a_distribution_as/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kqkszs",
    "title": "\"But, I still put a ton of work into it...\"",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-05-19T12:30:59",
    "url": "https://reddit.com/r/datascience/comments/1kqkszs/but_i_still_put_a_ton_of_work_into_it/",
    "score": 501,
    "num_comments": 8,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kpy8ha",
    "title": "Study looking at AI chatbots in 7,000 workplaces finds ‘no significant impact on earnings or recorded hours in any occupation’",
    "content": "",
    "author": "CanYouPleaseChill",
    "timestamp": "2025-05-18T17:03:15",
    "url": "https://reddit.com/r/datascience/comments/1kpy8ha/study_looking_at_ai_chatbots_in_7000_workplaces/",
    "score": 877,
    "num_comments": 53,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kqgxhb",
    "title": "I’ve modularized my Jupyter pipeline into .py files, now what? Exploring GUI ideas, monthly comparisons, and next steps!",
    "content": "I have a data pipeline that processes spreadsheets and generates outputs.\n\nWhat are smart next steps to take this further without overcomplicating it?\n\nI’m thinking of building a simple GUI or dashboard to make it easier to trigger batch processing or explore outputs.\n\nI want to support month-over-month comparisons e.g. how this month’s data differs from last and then generate diffs or trend insights.\n\nEventually I might want to track changes over time, add basic versioning, or even push summary outputs to a web format or email report.\n\nHave you done something similar? What did you add next that really improved usefulness or usability? And any advice on building GUIs for spreadsheet based workflows?\n\nI’m curious how others have expanded from here",
    "author": "Proof_Wrap_2150",
    "timestamp": "2025-05-19T10:01:43",
    "url": "https://reddit.com/r/datascience/comments/1kqgxhb/ive_modularized_my_jupyter_pipeline_into_py_files/",
    "score": 6,
    "num_comments": 11,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kr41wk",
    "title": "I Scrape FAANG Data Science Jobs from the Last 24h and Email Them to You",
    "content": "I built a tool that scrapes fresh data science, machine learning, and data engineering roles from FAANG and other top tech companies’ official career pages — no LinkedIn noise or recruiter spam — and emails them straight to you.\n\nWhat it does:\n\n* Scrapes jobs directly from sites like Google, Apple, Meta, Amazon, Microsoft, Netflix, Stripe, Uber, TikTok, Airbnb, and more\n* Sends daily emails with newly scraped jobs\n* Helps you find openings faster – before they hit job boards\n* Lets you select different countries like USA, Canada, India, European countries, and more\n\nCheck it out here:  \n[https://topjobstoday.com/data-scientist-jobs](https://topjobstoday.com/data-scientist-jobs)\n\nWould love to hear your thoughts or suggestions!",
    "author": "Flaky_Literature8414",
    "timestamp": "2025-05-20T05:50:06",
    "url": "https://reddit.com/r/datascience/comments/1kr41wk/i_scrape_faang_data_science_jobs_from_the_last/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kpj2cw",
    "title": "Are data science professionals primarily statisticians or computer scientists?",
    "content": "Seems like there's a lot of overlap and maybe different experts do different jobs all within the data science field, but which background would you say is most prevalent in most data science positions?",
    "author": "officialcrimsonchin",
    "timestamp": "2025-05-18T05:41:28",
    "url": "https://reddit.com/r/datascience/comments/1kpj2cw/are_data_science_professionals_primarily/",
    "score": 266,
    "num_comments": 184,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kq2lxu",
    "title": "Weekly Entering &amp; Transitioning - Thread 19 May, 2025 - 26 May, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-05-18T21:01:33",
    "url": "https://reddit.com/r/datascience/comments/1kq2lxu/weekly_entering_transitioning_thread_19_may_2025/",
    "score": 3,
    "num_comments": 62,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kowb8p",
    "title": "Prediction flow with Gaussian distributed features",
    "content": "Hi all,\nJust recently started as a data scientist, so I thought I could use the wisdom of this subreddit before I get up to speed and compare methodologies to see what can help my team better.\n \nSo say I have a dataset for a classification problem with several features (not all) that are normally distributed, and for the sake of numerical stability I’m normalizing those values to their respective Z-values (using the training set’s means and std to prevent leakage).\n\nNow after I train the model and get some results I’m happy with using the test set (that was normalized also with the training’s mean and std), we trigger some of our tests and deploy pipelines (whatever they are) and later on we’ll use that model in production with new unseen data. \n\nMy question is, what is your most popular go to choice to store those mean and std values for when you’ll need to normalize the unseen data’s features prior to the prediction? The same question applies for filling null values.\n\n“Simplest” thing I thought of (with an emphasis on the “”) is a wrapper class that stores all those values as member fields along with the actual model object (or pickle file path) and storing that class also with pickle, but it sounds a bit cumbersome, so maybe you can spread some light with more efficient ideas :)\n\nCheers.",
    "author": "indie-devops",
    "timestamp": "2025-05-17T08:57:36",
    "url": "https://reddit.com/r/datascience/comments/1kowb8p/prediction_flow_with_gaussian_distributed_features/",
    "score": 24,
    "num_comments": 15,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kp0grb",
    "title": "what were your first cloud projects related to DS/ML?",
    "content": "Currently learning GCP. Help me stay motivated by telling me about your first cloud-related DS/ML projects.",
    "author": "corgibestie",
    "timestamp": "2025-05-17T11:59:24",
    "url": "https://reddit.com/r/datascience/comments/1kp0grb/what_were_your_first_cloud_projects_related_to/",
    "score": 6,
    "num_comments": 8,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ko8j3v",
    "title": "Jupyter notebook has grown into a 200+ line pipeline for a pandas heavy, linear logic, processor. What’s the smartest way to refactor without overengineering it or breaking the ‘run all’ simplicity?",
    "content": "I’m building an analysis that processes spreadsheets, transforms the data, and outputs HTML files. \n\nIt works, but it’s hard to maintain. \n\nI’m not sure if I should start modularizing into scripts, introduce config files, or just reorganize inside the notebook. Looking for advice from others who’ve scaled up from this stage. It’s easy to make it work with new files, but I can’t help but wonder what the next stage looks like? \n\nEDIT: Really appreciate all the thoughtful replies so far. I’ve made notes with some great perspectives on refactoring, modularizing, and managing complexity without overengineering.\n\nFollow-up question for those further down the path:\n\nLet’s say I do what many of you have recommended and I refactor my project into clean .py files, introduce config files, and modularize the logic into a more maintainable structure. What comes after that?\n\nI’m self taught and using this passion project as a way to build my skills. Once I’ve got something that “works well” and is well organized… what’s the next stage? \n\nDo I aim for packaging it? Turning it into a product? Adding tests? Making a CLI? \n\nI’d love to hear from others who’ve taken their passion project to the next level! \n\nHow did you keep leveling up?",
    "author": "Proof_Wrap_2150",
    "timestamp": "2025-05-16T11:44:44",
    "url": "https://reddit.com/r/datascience/comments/1ko8j3v/jupyter_notebook_has_grown_into_a_200_line/",
    "score": 138,
    "num_comments": 80,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ko8ngz",
    "title": "When is the right time to move from Jupyter into a full modular pipeline?",
    "content": "I feel stuck in the middle where my notebook works well, but it’s growing, and I know clients will add new requirements. I don’t want to introduce infrastructure I don’t need yet, but I also don’t want to be caught off guard when it’s important. \n\nHow do you know when it’s time to level up, and what lightweight steps help you prepare?\n\nAny books that can help me scale my jupyter notebooks into bigger solutions? ",
    "author": "Proof_Wrap_2150",
    "timestamp": "2025-05-16T11:49:48",
    "url": "https://reddit.com/r/datascience/comments/1ko8ngz/when_is_the_right_time_to_move_from_jupyter_into/",
    "score": 76,
    "num_comments": 44,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kobhx7",
    "title": "Demand forecasting using multiple variables",
    "content": "I am working on a demand forecasting model to accurately predict test slots across different areas. I have been following the Rob Hyndman book. But the book essentially deals with just one feature and predicting its future values. But my model takes into account a lot of variables. How can I deal with that ? What kind of EDA should I perform ?? Is it better to make every feature stationary ? ",
    "author": "NervousVictory1792",
    "timestamp": "2025-05-16T13:50:43",
    "url": "https://reddit.com/r/datascience/comments/1kobhx7/demand_forecasting_using_multiple_variables/",
    "score": 17,
    "num_comments": 41,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ko8lwv",
    "title": "How would you structure a data pipeline project that needs to handle near-identical logic across different input files?",
    "content": "I’m trying to turn a Jupyter notebook that processes 100k rows in a spreadsheet into something that can be reused across multiple datasets. I’ve considered parameterized config files but I want to hear from folks who’ve built reusable pipelines in client facing or consulting setups.",
    "author": "Proof_Wrap_2150",
    "timestamp": "2025-05-16T11:47:59",
    "url": "https://reddit.com/r/datascience/comments/1ko8lwv/how_would_you_structure_a_data_pipeline_project/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kncg7f",
    "title": "Is our job just to P hack for the stakeholders?",
    "content": "Specifically in experimentation and causal inference.\n",
    "author": "darkwhiteinvader",
    "timestamp": "2025-05-15T09:24:08",
    "url": "https://reddit.com/r/datascience/comments/1kncg7f/is_our_job_just_to_p_hack_for_the_stakeholders/",
    "score": 343,
    "num_comments": 107,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ko0frr",
    "title": "Company Data Retention Policies and GDPR",
    "content": "How long are your data retention policies?\n\nHow do you handle GDPR rules?\n\nMy company is instituting a very, very conservative retention policy of &lt;9months of raw event-level data (but storing 15-months worth of aggregated data). Additionally, the only way this company thinks about GDPR compliance is to delete user records instead of anonymizing. \n\nI'm curious how your companies deal with both, and what the risks would be with instituting such policies.",
    "author": "timusw",
    "timestamp": "2025-05-16T06:09:09",
    "url": "https://reddit.com/r/datascience/comments/1ko0frr/company_data_retention_policies_and_gdpr/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kn66el",
    "title": "Federated Platform for Secure Research Data Sharing",
    "content": "",
    "author": "anuveya",
    "timestamp": "2025-05-15T04:49:08",
    "url": "https://reddit.com/r/datascience/comments/1kn66el/federated_platform_for_secure_research_data/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kmv770",
    "title": "Anyone here experimenting with implementing Transformers on tabular data like Strip? Looking for some coding repo to play around and learn.",
    "content": "Here’s the Stripe case: https://techcrunch.com/2025/05/07/stripe-unveils-ai-foundation-model-for-payments-reveals-deeper-partnership-with-nvidia/",
    "author": "Difficult-Big-3890",
    "timestamp": "2025-05-14T17:38:54",
    "url": "https://reddit.com/r/datascience/comments/1kmv770/anyone_here_experimenting_with_implementing/",
    "score": 9,
    "num_comments": 4,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1km3wa1",
    "title": "Is LinkedIn data trust worthy?",
    "content": "Hey all. So I got my month of Linkdin premium and I am pretty shocked to see that for many data science positions it’s saying that more applicants have a masters? Is this actually true? I thought it would be the other way around. This is a job post that was up for 2 hours with over 100 clicks on apply. I know that doesn’t mean they are all real applications but I’m just curious to know what the communities thoughts on this are? ",
    "author": "Suspicious_Coyote_54",
    "timestamp": "2025-05-13T19:06:00",
    "url": "https://reddit.com/r/datascience/comments/1km3wa1/is_linkedin_data_trust_worthy/",
    "score": 146,
    "num_comments": 73,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1klv393",
    "title": "Those in manufacturing and science/engineering, aside from classic DoE (full-fact, CCD, etc.), what other experimental design tools do you use?",
    "content": "Title. My role mostly uses central composite designs and the standard lean six sigma quality tools because those are what management and the engineering teams are used to. Our team is slowly integrating other techniques like Bayesian optimization or interesting ways to analyze data (my new fave is functional data analysis) and I'd love to hear what other tools you guys use and your success/failures with them.",
    "author": "corgibestie",
    "timestamp": "2025-05-13T12:32:51",
    "url": "https://reddit.com/r/datascience/comments/1klv393/those_in_manufacturing_and_scienceengineering/",
    "score": 25,
    "num_comments": 15,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kky19i",
    "title": "Now you're paying an analyst $50/hr to standardize date formats instead of doing actual analysis work.",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-05-12T10:12:00",
    "url": "https://reddit.com/r/datascience/comments/1kky19i/now_youre_paying_an_analyst_50hr_to_standardize/",
    "score": 377,
    "num_comments": 22,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kl55q1",
    "title": "What do you use to build dashboards?",
    "content": "Hi guys, I've been a data scientist for 5 years. I've done lots of different types of work and unfortunately that has included a lot of dashboarding (no offense if you enjoy making dashboards). I'm wondering what tools people here are using and if you like them. In my career I've used mode, looker, streamlit and retool off the top of my head. I think mode was my favorite because you could type sql right into it and get the charts you wanted but still was overall unsatisfied with it.\n\n  \nI'm wondering what tools the people here are using and if you find it meets all your needs? One of my frustrations with these tools is that even platforms like Looker—designed to be self-serve for general staff—end up being confusing for people without a data science background.\n\nAre there any tools (maybe powered my LLMs now) that allow non data science people to write prompts that update production dashboards? A simple example is if you have a revenue dashboard showing net revenue and a PM, director etc wanted you to add an additional gross revenue metric. With the tools I'm aware of I would have to go into the BI tool and update the chart myself to show that metric. Are there any tools that allow you to just type in a prompt and make those kinds of edits?",
    "author": "alexellman",
    "timestamp": "2025-05-12T14:52:24",
    "url": "https://reddit.com/r/datascience/comments/1kl55q1/what_do_you_use_to_build_dashboards/",
    "score": 78,
    "num_comments": 80,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kktdbl",
    "title": "is it necessary to learn some language other than python?",
    "content": "that's pretty much it. i'm proficient in python already, but was wondering if, to be a better DS, i'd need to learn something else, or is it better to focus on studying something else rather than a new language.\n\n  \nedit: yes, SQL is obviously a must. i already know it. sorry for the overlook.",
    "author": "vniversvs_",
    "timestamp": "2025-05-12T07:05:50",
    "url": "https://reddit.com/r/datascience/comments/1kktdbl/is_it_necessary_to_learn_some_language_other_than/",
    "score": 96,
    "num_comments": 75,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kl2ck3",
    "title": "Do open source contributors still need to do coding challenges?",
    "content": "I’ve become an avid open source contributor over the past few years in a few popular ML, Econ, and Jax ecosystem packages.\n\nIn my opinion being able to take someone else’s code and fix bugs or add features is a much better signal than leetcode and hacker rank. I’m really hoping I don’t have to study leetcode/hackerrank for my next job search (DS/MLE roles) and I’d rather just keep doing open source work that’s more relevant.\n\nFor the other open source contributors out there - are you ever able to get out of coding challenges by citing your own pull requests?\n\n\n",
    "author": "James_c7",
    "timestamp": "2025-05-12T13:00:03",
    "url": "https://reddit.com/r/datascience/comments/1kl2ck3/do_open_source_contributors_still_need_to_do/",
    "score": 28,
    "num_comments": 11,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kkwjla",
    "title": "\"Day Since Last X\" feature preprocessing",
    "content": "Hi Everyone! Bit of a technical modeling question here. Apologies if this is very basic preprocessing stuff but I'm a younger data scientist working in industry and I'm still learning.\n\n  \nSay you have a pretty standard binary classification model predicting 1 = we should market to this customer and 0 = we should not market to this customer (the exact labeling scheme is a bit proprietary). \n\nI have a few features that are in the style \"days since last touchpoint\". For example \"days since we last emailed this person\" or \"days since we last sold to this person\". However, a solid percentage of the rows are NULL, meaning we have never emailed or sold to this person. Any thoughts on how should I handle NULLs for this type of column? I've been imputing with MAX(days since we last sold to this person) + 1 but I'm starting to think that could be confusing my model. I think the reality of the situation is that someone with 1 purchase a long time ago is **a lot** more likely to purchase today than someone who has never purchased anything at all. The person with 0 purchases may not even be interested in our product, while we have evidence that the person with 1 purchase a long time ago is at least a fit for our product. Imputing with MAX(days since we last sold to this person) + 1 poses these two cases as very similar to the model.\n\nFor reference I'm testing with several tree-based models (light GBM and random forest) and comparing metrics to pick between the architecture options. So far I've been getting the best results with light GBM.\n\nOne thing I'm thinking about is whether I should just leave the people who have never sold as NULLs and have my model pick the direction to split for missing values. (I believe this would work with LightGBM but not RandomForest).\n\nAnother option is to break down the \"days since last sale\" feature into categories, maybe quantiles with a special category for NULLS, and then dummy encode.\n\nHas anyone else used these types of \"days since last touchpoint\" features in propensity modeling/marketing modeling?",
    "author": "Ok-Needleworker-6122",
    "timestamp": "2025-05-12T09:13:47",
    "url": "https://reddit.com/r/datascience/comments/1kkwjla/day_since_last_x_feature_preprocessing/",
    "score": 29,
    "num_comments": 16,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kl2fq2",
    "title": "[8 YoE] 7 Years Software Engineer Trying to Pivot to Data Analytics/Science/Machine Learning",
    "content": "",
    "author": "PraiseChrist420",
    "timestamp": "2025-05-12T13:03:26",
    "url": "https://reddit.com/r/datascience/comments/1kl2fq2/8_yoe_7_years_software_engineer_trying_to_pivot/",
    "score": 1,
    "num_comments": 12,
    "upvote_ratio": 0.53,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kkjf6g",
    "title": "Weekly Entering &amp; Transitioning - Thread 12 May, 2025 - 19 May, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-05-11T21:01:41",
    "url": "https://reddit.com/r/datascience/comments/1kkjf6g/weekly_entering_transitioning_thread_12_may_2025/",
    "score": 7,
    "num_comments": 20,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kjjb32",
    "title": "I am a staff data scientist at a big tech company -- AMA",
    "content": "**Why I’m doing this**\n\nI am low on karma. Plus, it just feels good to help.\n\n**About me**\n\nI’m currently a staff data scientist at a big tech company in Silicon Valley. I’ve been in the field for about 10 years since earning my PhD in Statistics. I’ve worked at companies of various sizes — from seed-stage startups to pre-IPO unicorns to some of the largest tech companies.\n\n**A few caveats**\n\n* Anything I share reflects my personal experience and may carry some bias.\n* My experience is based in the US, particularly in Silicon Valley.\n* I have some people management experience but have mostly worked as an IC\n* Data science is a broad term. I’m most familiar with machine learning scientist, experimentation/causal inference, and data analyst roles.\n* I may not be able to respond immediately, but I’ll aim to reply within 24 hours.\n\n**Update:**\n\nWow, I didn’t expect this to get so much attention. I’m a bit overwhelmed by the number of comments and DMs, so I may not be able to reply to everyone. That said, I’ll do my best to respond to as many as I can over the next week. Really appreciate all the thoughtful questions and discussions!",
    "author": "Federal_Bus_4543",
    "timestamp": "2025-05-10T13:17:33",
    "url": "https://reddit.com/r/datascience/comments/1kjjb32/i_am_a_staff_data_scientist_at_a_big_tech_company/",
    "score": 1219,
    "num_comments": 437,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kjuwou",
    "title": "Where Can I Find Legit Remote Data Science Jobs That Hire Globally?",
    "content": "Hey folks! I’m on the hunt for trustworthy remote job boards or sites that regularly post real data science and data analyst roles—and more importantly, are open to hiring from anywhere in the world. I’ve noticed sites like Indeed don’t support my country, and while LinkedIn has plenty of remote listings, many seem sketchy or not legit. \n\nSo, what platforms or communities do you recommend for finding genuine remote gigs in this field that are truly global? Any tips on spotting legit postings would also be super helpful! \n\nThanks in advance for sharing your experiences!",
    "author": "Aftabby",
    "timestamp": "2025-05-10T23:57:15",
    "url": "https://reddit.com/r/datascience/comments/1kjuwou/where_can_i_find_legit_remote_data_science_jobs/",
    "score": 39,
    "num_comments": 32,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kjqlcv",
    "title": "New Python Package Feedback - Try in Google Collab",
    "content": "I’ve been occasionally working on this in my spare time and would appreciate feedback.\n\n[Try the package in Colab](https://colab.research.google.com/github/OlivierNDO/framecheck/blob/main/framecheck_quickstart.ipynb)\n\nThe idea for ‘framecheck’ is to catch bad data in a data frame before it flows downstream in *very few* lines of code. \n\nYou’d also easily isolate the records with problematic data. This isn’t revolutionary or new - what I wanted was a way to do this in fewer lines of code than other packages like great expectations and pydantic.\n\nReally I just want honest feedback. If people don’t find it useful, I won’t put more time into it.\n\npip install framecheck\n\nRepo with reproducible examples:\n\nhttps://github.com/OlivierNDO/framecheck",
    "author": "MLEngDelivers",
    "timestamp": "2025-05-10T19:27:05",
    "url": "https://reddit.com/r/datascience/comments/1kjqlcv/new_python_package_feedback_try_in_google_collab/",
    "score": 56,
    "num_comments": 33,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kjvita",
    "title": "rixpress: an R package to set up multi-language reproducible analytics pipelines (2 Minute intro video)",
    "content": "",
    "author": "brodrigues_co",
    "timestamp": "2025-05-11T00:39:05",
    "url": "https://reddit.com/r/datascience/comments/1kjvita/rixpress_an_r_package_to_set_up_multilanguage/",
    "score": 10,
    "num_comments": 3,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kj40s6",
    "title": "How Can Early-Level Data Scientists Get Noticed by Recruiters and Industry Pros?",
    "content": "Hey everyone! \n\nI started my journey in the data science world almost a year ago, and I'm wondering: What’s the best way to market myself so that I actually get noticed by recruiters and industry professionals? How do you build that presence and get on the radar of the right people?  \n  \n**Any tips on networking, personal branding, or strategies that worked for you would be amazing to hear!**",
    "author": "Aftabby",
    "timestamp": "2025-05-09T23:45:43",
    "url": "https://reddit.com/r/datascience/comments/1kj40s6/how_can_earlylevel_data_scientists_get_noticed_by/",
    "score": 201,
    "num_comments": 120,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kjca29",
    "title": "Does your company have a dedicated team/person for MLOps? If not, how do you manage MLOps?",
    "content": "As someone in MLOps, I am curious to hear how other companies and teams manage the MLOps process and workflow. My company (because it's a huge enterprise) has multiple teams doing some type of MLOps or MLOps-adjacent projects. But I know that other companies do this very differently.\n\nSo does your team have a separate dedicated person or a group for MLOps and managing model lifecycle in production? If not, how do you manage it? Is the data scientist / MLE expected to do all?",
    "author": "Illustrious-Pound266",
    "timestamp": "2025-05-10T08:00:28",
    "url": "https://reddit.com/r/datascience/comments/1kjca29/does_your_company_have_a_dedicated_teamperson_for/",
    "score": 28,
    "num_comments": 26,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ki9zo3",
    "title": "Client told me MS Copilot replicated what I built. It didn’t.",
    "content": "I built three MVP models for a client over 12 weeks. Nothing fancy: an LSTM, a prophet model, and XGBoost. The difficulty, as usual, was getting and understanding the data and cleaning it. The company is largely data illiterate. Turned in all 3 models, they loved it then all of a sudden canceled the pending contract to move them to production. Why? They had a devops person do in MS Copilot Analyst (a new specialized version of MS Copilot studio) and it took them 1 week! Would I like to sign a lesser contract to advise this person though? I finally looked at their code and it’s 40 lines of code using a subset of the California housing dataset run using a Random Forest regressor. They had literally nothing. My advice to them: go f*%k yourself. ",
    "author": "melissa_ingle",
    "timestamp": "2025-05-08T21:28:25",
    "url": "https://reddit.com/r/datascience/comments/1ki9zo3/client_told_me_ms_copilot_replicated_what_i_built/",
    "score": 1094,
    "num_comments": 133,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kionyr",
    "title": "I have an in-person interview with the CTO of a company in 2 weeks. I have no industry work experience for data science. Only project based experience. How f*cked am I?",
    "content": "Help",
    "author": "marblesandcookies",
    "timestamp": "2025-05-09T10:47:12",
    "url": "https://reddit.com/r/datascience/comments/1kionyr/i_have_an_inperson_interview_with_the_cto_of_a/",
    "score": 85,
    "num_comments": 41,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kiubls",
    "title": "What are some useful DS/DE projects I can do during slow periods at work?",
    "content": "Things are super slow at work due to economic uncertainty. I'm used to being super busy so I never had to think up my own problems/projects. Any ideas for useful projects I can do or sell to management? Thanks.",
    "author": "Trick-Interaction396",
    "timestamp": "2025-05-09T14:49:35",
    "url": "https://reddit.com/r/datascience/comments/1kiubls/what_are_some_useful_dsde_projects_i_can_do/",
    "score": 20,
    "num_comments": 15,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1khu4f8",
    "title": "This is how I got a (potential) offer revoked: A learning lesson",
    "content": "I’m based in the Bay Area with 5 YOE. A couple of months ago, I interviewed for a role I wasn’t too excited about, but the pay was super compelling. In the first recruiter call, they asked for my salary expectations. I asked for their range, as an example here, let’s say they said $150K–$180K. I said, “That works, I’m looking for something above $150K.” I think this was my first mistake, more on that later.\n\nI am a person with low self esteem(or serious imposter syndrome) and when I say I nailed all 8 rounds, I really must believe that. The recruiter followed up the day after 8th round saying team is interested in extending an offer. Then on compensation expectations the recruiter said, “You mentioned $150K earlier.” I clarified that I was targeting the upper end based on my fit and experience. They responded with, “So $180K?” and I just said yes. It felt a bit like putting words in my mouth.\n\nNext day, I got an email saying that I have to wait for the offer decision as they are interviewing  other candidates. Haven’t heard back since. I don’t think I did anything fundamentally wrong or if I should have regrets but curious what others think.\n\nEdit: Just to clarify, in my mind I thought that’s how negotiations work. They will come back and say can’t do 150 but can do 140. But I guess not.",
    "author": "Lamp_Shade_Head",
    "timestamp": "2025-05-08T09:16:55",
    "url": "https://reddit.com/r/datascience/comments/1khu4f8/this_is_how_i_got_a_potential_offer_revoked_a/",
    "score": 242,
    "num_comments": 124,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1khic8u",
    "title": "The worst thing about being a Data Scientist is that the best you can do you sometimes is not even nearly enough",
    "content": "This specially sucks as a consultant. You get hired because some guy from Sales department of the consulting company convinced the client that they would give them a Data Scientist consultant that would solve all their problems and build perfect Machine Learning models. \n\n\nThen you join the client and quickly realize that is literary impossible to do any meaningful work with the poor data and the unjustified expectations they have. \n\nAs an ethical worker, you work hard and to everything that is possible with the data at hand (and maybe some external data you magically gathered). You use everything that you know and don't know, take some time to study the state of the art, chat with some LLMs on their ideas for the project, run hundreds of different experiments (should I use different sets of features? Should I log transform some numerical features? Should I apply PCA? How many ML algorithms should I try?) \n\nAnd at the end of day... The model still sucks. You overfit the hell of the model, makes a gigantic boosting model with max_depth  set as 1000, and you still don't match the dumb manager expectations. \n\nI don't know how common that it is in other professions, but an intrinsic thing of working in Data Science is that you are never sure that your work will eventually turn out to be something good, no matter how hard you try. ",
    "author": "CadeOCarimbo",
    "timestamp": "2025-05-07T22:17:59",
    "url": "https://reddit.com/r/datascience/comments/1khic8u/the_worst_thing_about_being_a_data_scientist_is/",
    "score": 551,
    "num_comments": 87,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1khkkv8",
    "title": "Code is shit, business wants to scale, what could go wrong?",
    "content": "A bit of context. I have taken charge of a project recently. It's a product in a client facing app. The implementation of the ML system is messy. The data pipelines consists of many sql codes. These codes contain rather complicated business knowledge. There is airflow that schedules them, so there is observability. \n\nThis code has been used to run experiments for the past 2 months. I don't know how much firefighting has been going on. But in the past week that I picked up the project, I spent 3 days on firefighting. \n\nI understand that, at least theoretically, when scaling, everything that could go wrong goes wrong. But I want to hear real life experiences. When facing such issues, what have you done that worked? Could you find a way to fix code while helping with scaling? Did firefightings get in the way? Any past experience would help. Thanks! ",
    "author": "furioncruz",
    "timestamp": "2025-05-08T00:53:48",
    "url": "https://reddit.com/r/datascience/comments/1khkkv8/code_is_shit_business_wants_to_scale_what_could/",
    "score": 32,
    "num_comments": 15,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1ki5bob",
    "title": "May be of interest to anyone looking to learn Python with a stats bias",
    "content": "",
    "author": "bobo-the-merciful",
    "timestamp": "2025-05-08T17:13:09",
    "url": "https://reddit.com/r/datascience/comments/1ki5bob/may_be_of_interest_to_anyone_looking_to_learn/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kgz36l",
    "title": "Anyone else tried of always discussing tech/tools?",
    "content": "Maybe it’s just my company but we spend the majority of our time discussing the pros/cons of new tech. Databricks, Snowflake, various dashboards software. I agree that tech is important but a new tool isn’t going to magically fix everything. We also need communication, documentation, and process. Also, what are we actually trying to accomplish? We can buy a new fancy tool but what’s the end goal? It’s getting worse with AI. Use AI isn’t a goal. How do we solve problem X is a goal. Maybe it’s AI but maybe it’s something else.",
    "author": "Trick-Interaction396",
    "timestamp": "2025-05-07T07:43:14",
    "url": "https://reddit.com/r/datascience/comments/1kgz36l/anyone_else_tried_of_always_discussing_techtools/",
    "score": 121,
    "num_comments": 26,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1khj1wm",
    "title": "Final verdict on LLM generated confidence scores?",
    "content": "",
    "author": "sg6128",
    "timestamp": "2025-05-07T23:05:16",
    "url": "https://reddit.com/r/datascience/comments/1khj1wm/final_verdict_on_llm_generated_confidence_scores/",
    "score": 6,
    "num_comments": 9,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kgzki4",
    "title": "Is HackerRank/LeetCode a valid way to screen candidates?",
    "content": "Reverse questions: is it a red flag if a company is using HackerRank / LeetCode challenges in order to filter candidates?\n\nI am a strong believer in technical expertise, meaning that a DS needs to know what is doing. You cannot improvise ML expertise when it comes to bring stuff into production.\n\nNevertheless, I think those kind of challenges works only if you're a monkey-coder that recently worked on that exact stuff, and specifically practiced for those challenges. No way that I know by heart all the subtle nuances of SQL or edge cases in ML, but on the other hand I'm most certainly able to solve those issues in real life projects.\n\nBottom line: do you think those are legit way of filter candidates (and we should prepare for that when applying to roles) or not?",
    "author": "MorningDarkMountain",
    "timestamp": "2025-05-07T08:03:14",
    "url": "https://reddit.com/r/datascience/comments/1kgzki4/is_hackerrankleetcode_a_valid_way_to_screen/",
    "score": 64,
    "num_comments": 53,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kgsn61",
    "title": "Am I or my PMs crazy? - Unknown unknowns.",
    "content": "My company wants to develop a product that detects \"unknown unknowns\" it a complex system, in an unsupervised manner, in order to identify new issues before they even begin. I think this is an ill-defined task, and I think what they actually want is a supervised, not unsupervised ML pipeline. But they refuse to commit to the idea of a \"loss function\" in the system, because \"anything could be an interesting novelty in our system\". \n\nThe system produces thousands of time series monitoring metrics. They want to stream all these metrics through anomaly detection model. Right now, the model throws thousands of anomalies, almost all of them meaningless. I think this is expected, because statistical anomalies don't have much to do with *actionable events.* Even more broadly **I think unsupervised learning cannot ever produce business value.** You always need some sort of supervised wrapper around it.\n\nWhat PMs want to do: flag all outliers in the system, because they are potential problems\n\nWhat I think we should be doing: (1) define the \"health (loss) function\" in the system (2) whenever the health function degrades look for root causes / predictors / correlates of the issues (3) find patterns in the system degradation - find *unknown* causes of *known* adverse system states \n\nAm I missing something? Are you guys doing something similar or have some interesting reads? Thanks",
    "author": "Ciasteczi",
    "timestamp": "2025-05-07T01:54:28",
    "url": "https://reddit.com/r/datascience/comments/1kgsn61/am_i_or_my_pms_crazy_unknown_unknowns/",
    "score": 98,
    "num_comments": 61,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kgz66a",
    "title": "Grinding through regression discontinuity resulted in this post - feel free to check it out",
    "content": "Title should check out. Been reading on RDD in the spare time I had in the past few months. I put everything together after applying it in my company (#1 online marketplace in the Netherlands) — the result: a few late nights and this [blog post.](https://towardsdatascience.com/regression-discontinuity-design-how-it-works-and-when-to-use-it/)\n\nThanks to the few redditors that shared [their input](https://www.reddit.com/r/CausalInference/comments/1i801e0/call_for_input_regression_discontinuity_design/) on the technique and application. It made me wiser!",
    "author": "chomoloc0",
    "timestamp": "2025-05-07T07:46:52",
    "url": "https://reddit.com/r/datascience/comments/1kgz66a/grinding_through_regression_discontinuity/",
    "score": 7,
    "num_comments": 3,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kgk3hw",
    "title": "A complete guide covering foundational Linux concepts, core tasks, and best practices.",
    "content": "",
    "author": "AhmedOsamaMath",
    "timestamp": "2025-05-06T17:20:50",
    "url": "https://reddit.com/r/datascience/comments/1kgk3hw/a_complete_guide_covering_foundational_linux/",
    "score": 44,
    "num_comments": 5,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kglzv7",
    "title": "I wrote a walkthrough post that covers Shape Constrained P-Splines for fitting monotonic relationships in python. I also showed how you can use general purpose optimizers like JAX and Scipy to fit these terms. Hope some of y'all find it helpful!",
    "content": "",
    "author": "millsGT49",
    "timestamp": "2025-05-06T18:57:15",
    "url": "https://reddit.com/r/datascience/comments/1kglzv7/i_wrote_a_walkthrough_post_that_covers_shape/",
    "score": 20,
    "num_comments": 6,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kgdevk",
    "title": "AWS Batch alternative — deploy to 10,000 VMs with one line of code",
    "content": "I just launched an open-source batch-processing platform that can scale Python to **10,000 VMs in under 2 seconds**, with just **one line of code**.\n\nI've been frustrated by how slow and painful it is to iterate on large batch processing pipelines. Even small changes require rebuilding Docker containers, waiting for AWS Batch or GCP Batch to redeploy, and dealing with cold-start VM delays — a **5+ minute dev cycle per iteration**, just to see what error your code throws *this time*, and then doing it all over again.\n\nMost other tools in this space are too complex, closed-source or fully managed, hard to self-host, or simply too expensive. If you've encountered similar barriers give Burla a try.\n\ndocs: [https://docs.burla.dev/](https://docs.burla.dev/)\n\ngithub: [https://github.com/Burla-Cloud](https://github.com/Burla-Cloud)",
    "author": "Ok_Post_149",
    "timestamp": "2025-05-06T12:31:15",
    "url": "https://reddit.com/r/datascience/comments/1kgdevk/aws_batch_alternative_deploy_to_10000_vms_with/",
    "score": 25,
    "num_comments": 23,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kgkpr5",
    "title": "how does the http:livecode/amazon..... link work for data science technical interview ?",
    "content": "I had a call with the recruiter yesterday and this was for an interview for a DS position at AMZ. \n\nRecruiter told me you can't execute any code on the whiteboard. Then I got another email saying here is the link to \"livecode\" for coding exercise and I can choose the programming language of my choice. \n\nCan someone explain to me what is this whiteboard ? or the livecode ? and how does it work ?",
    "author": "Analytics_Fanatics",
    "timestamp": "2025-05-06T17:51:29",
    "url": "https://reddit.com/r/datascience/comments/1kgkpr5/how_does_the_httplivecodeamazon_link_work_for/",
    "score": 4,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kfbtob",
    "title": "Please, for the love of god ... just give me something!!",
    "content": "",
    "author": "ElectrikMetriks",
    "timestamp": "2025-05-05T06:37:31",
    "url": "https://reddit.com/r/datascience/comments/1kfbtob/please_for_the_love_of_god_just_give_me_something/",
    "score": 748,
    "num_comments": 30,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kfwny7",
    "title": "[Request for feedback] dataframe library",
    "content": "I'm working on a dataframe library and wanted to make sure the API makes sense and is easy to get started with. No official documentation yet but wanted to get a feel of what people think of it so far.\n\nI have some tutorials on the [github repo](https://github.com/mchav/dataframe) and a [jupyter lab environment](https://ihaskell-dataframe-crf7g5fvcpahdegz.westus2-01.azurewebsites.net/) running. Would appreciate some feedback on the API and usability. Functionality is still limited and this site is so far just a sandbox. Thanks so much.",
    "author": "ChavXO",
    "timestamp": "2025-05-05T22:11:25",
    "url": "https://reddit.com/r/datascience/comments/1kfwny7/request_for_feedback_dataframe_library/",
    "score": 13,
    "num_comments": 12,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kf2nlk",
    "title": "Weekly Entering &amp; Transitioning - Thread 05 May, 2025 - 12 May, 2025",
    "content": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).",
    "author": "AutoModerator",
    "timestamp": "2025-05-04T21:01:31",
    "url": "https://reddit.com/r/datascience/comments/1kf2nlk/weekly_entering_transitioning_thread_05_may_2025/",
    "score": 13,
    "num_comments": 51,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kfb10a",
    "title": "Self-Service Open Data Portal: Zero-Ops &amp; Fully Managed for Data Scientists",
    "content": "__Disclaimer: I’m one of the creators of PortalJS.__\n\nHi everyone, I wanted to share this open-source product for data portals with the Data Science community. Appreciate your attention!\n\n**Our mission:**\n\nOpen data publishing shouldn’t be hard. We want local governments, academics, and NGOs to treat publishing their data like any other SaaS subscription: sign up, upload, update, and go.\n\n**Why PortalJS?**\n\n- Small teams need a simple, affordable way to get their data out there.\n- Existing platforms are either extremely expensive or require a technical team to set up and maintain.\n- Scaling an open data portal usually means dedicating an entire engineering department—and we believe that shouldn’t be the case.\n\nHappy to answer any questions!",
    "author": "anuveya",
    "timestamp": "2025-05-05T06:00:46",
    "url": "https://reddit.com/r/datascience/comments/1kfb10a/selfservice_open_data_portal_zeroops_fully/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.58,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kerpax",
    "title": "How would you architect this?",
    "content": "I work for a startup where the main product is a sales meeting analyser. Naturally there are a ton of features that require audio and video processing, like diarization, ASR, video classification, etc…\n\nThe CEO is in cost savings mode and he wants to reduce our compute costs. Currently our ML pipeline is built on top of kubernetes and we always have at least on gpu machine up per task (T4s and L4s) per day and we dont have a lot of clients, meaning most of the time the gpus are idle and we are paying for them. I suggested moving those tasks to cloud functions that use GPUs, since we are using GCP and they have recently came out with that feature, but the CEO wants to use gemini to replace these tasks since we will most likely be on the free tier.\n\nThe problems I see is that once we leave the free tier the costs will be more than 10x our current costs and that there are downstream ML tasks that depend on these, so changing the input distribution is not really a good idea… for example, we have a text classifier that was trained with text from whisper - changing it to gemini does not seem to be a good idea to me…\n\nhe claimed he wants it to be maintainable so an api request makes more sense to him, but the reason why he wants it to be maintainable is because a lot of ML people are leaving (mainly because of his wrong decisions and micro management - is this another of his wrong decisions?)\n\nusing gemini to do asr and diarization, for example, just feels way way wrong",
    "author": "AdministrativeRub484",
    "timestamp": "2025-05-04T12:09:35",
    "url": "https://reddit.com/r/datascience/comments/1kerpax/how_would_you_architect_this/",
    "score": 9,
    "num_comments": 8,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kdzcmj",
    "title": "Gotta love recommender systems 😂",
    "content": "Whippets #1",
    "author": "[deleted]",
    "timestamp": "2025-05-03T11:14:14",
    "url": "https://reddit.com/r/datascience/comments/1kdzcmj/gotta_love_recommender_systems/",
    "score": 76,
    "num_comments": 10,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kcrsyn",
    "title": "Tired of everyone becoming an AI Expert all of a sudden",
    "content": "Literally every person who can type prompts into an LLM is now an AI consultant/expert. I’m sick of it, today a sales manager literally said ‘oh I can get Gemini to make my charts from excel directly with one prompt so ig we no longer require Data Scientists and their support hehe’\n\nThese dumbos think making basic level charts equals DS work. Not even data analytics, literally data science? \n\nI’m sick of it. I hope each one of yall cause a data leak, breach the confidentiality by voluntarily giving private info to Gemini/OpenAi and finally create immense tech debt by developing your vibe coded projects.\n\nRant over ",
    "author": "tiwanaldo5",
    "timestamp": "2025-05-01T20:49:50",
    "url": "https://reddit.com/r/datascience/comments/1kcrsyn/tired_of_everyone_becoming_an_ai_expert_all_of_a/",
    "score": 1556,
    "num_comments": 137,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kdlxel",
    "title": "Wich computer are you using?",
    "content": "Hi guys I'm thinking of buy a new computer, do you have some ideas (no Apple)? Wich computer are you using today? In looking mobility so a laptop is the option.\n\nThanks guys ",
    "author": "SeaSubject9215",
    "timestamp": "2025-05-02T22:54:15",
    "url": "https://reddit.com/r/datascience/comments/1kdlxel/wich_computer_are_you_using/",
    "score": 0,
    "num_comments": 74,
    "upvote_ratio": 0.46,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kcrrzu",
    "title": "Do you have to keep up with the latest research papers if you are working with LLMs as an AI developer?",
    "content": "I've been diving deeper into LLMs these days (especially agentic AI) and I'm slightly surprised that there's a lot of references to various papers when going through what are pretty basic tutorials.\n\nFor example, just on prompt engineering alone, quite a few tutorials referenced the Chain of Thought paper (Wei et al, 2022). When I was looking at intro tutorials on agents, many of them referred to the ICLR ReAct paper (Yao et al, 2023). In regards to finetuning LLMs, many of them referenced the QLoRa paper (Dettmers et al, 2023).\n\nI had assumed that as a developer (not as a researcher), I could just use a lot of these LLM tools out of the box with just documentation but do I have to read the latest ICLR (or other ML journal/conference) papers to interact with them now? Is this common?\n\nAI developers: how often are you browsing through and reading through papers? I just wanted to build stuff and want to minimize academic work...",
    "author": "Illustrious-Pound266",
    "timestamp": "2025-05-01T20:48:16",
    "url": "https://reddit.com/r/datascience/comments/1kcrrzu/do_you_have_to_keep_up_with_the_latest_research/",
    "score": 21,
    "num_comments": 17,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kbps44",
    "title": "Made this meme for a presentation I have to give tomorrow at work",
    "content": "",
    "author": "Smooth_Signal_3423",
    "timestamp": "2025-04-30T12:57:24",
    "url": "https://reddit.com/r/datascience/comments/1kbps44/made_this_meme_for_a_presentation_i_have_to_give/",
    "score": 187,
    "num_comments": 30,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kbfya2",
    "title": "Breaking into DS from academia",
    "content": "Hi everyone,\n\nI need advice from industry DS folks. I'm currently a bioinformatics postdoc in the US, and it seems like our world is collapsing with all the cuts from the current administration. I'm considering moving to industry DS (any field), as I'm essentially doing DS in the biomedical field right now.\n\nI tried making a DS/industry style 1-page resume; could you please advise whether it is good and how to improve? Be harsh, no problemo with that. And a couple of specific questions:\n\n1. A friend told me I should write \"Data Scientist\" as my previous roles, as recruiters will dump my CV after seeing \"Computational Biologist\" or \"Bioinformatics Scientist.\" Is this OK practice? The work I've done, in principle, is data science.\n2. Am I missing any critical skills that every senior-level industry DS should have?\n\nThanks everyone in advance!! \n\nhttps://preview.redd.it/0o0mg29szyxe1.png?width=2550&amp;format=png&amp;auto=webp&amp;s=85d0ec3cdab2e439c42445f90a76f898fa2a3b13\n\n  \n",
    "author": "Training-Screen8223",
    "timestamp": "2025-04-30T06:03:28",
    "url": "https://reddit.com/r/datascience/comments/1kbfya2/breaking_into_ds_from_academia/",
    "score": 118,
    "num_comments": 82,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kanby4",
    "title": "The role of data science in the age of GenAI",
    "content": "I've been working in the space of ML for around 10 years now. I have a stats background, and when I started I was mostly training regression models on tabular data, or the occasional tf-idf + SVM pipeline for text classification. Nowadays, I work mainly with unstructured data and for the majority of problems my company is facing, calling a pre-trained LLM through an API is both sufficient and the most cost-effective solution - even deploying a small BERT-based classifier costs more and requires data labeling. I know this is not the case for all companies, but it's becoming very common.\n\nOver the years, I've developed software engineering skills, and these days my work revolves around infra-as-code, CI/CD pipelines and API integration with ML applications. Although these skills are valuable, it's far away from data science.\n\nFor those who are in the same boat as me (and I know there are many), I'm curious to know how you apply and maintain your data science skills in this age of GenAI? ",
    "author": "Raikoya",
    "timestamp": "2025-04-29T05:59:54",
    "url": "https://reddit.com/r/datascience/comments/1kanby4/the_role_of_data_science_in_the_age_of_genai/",
    "score": 394,
    "num_comments": 95,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kb5xj6",
    "title": "DS in healthcare",
    "content": "So I have a situation.   \nI have a dataset that contains real-world clinical vignettes drawn from frontline healthcare settings. Each sample presents a prompt representing a clinical case scenario, along with the response from a human clinician. The goal is to predict the the phisician's response based on the prompt.\n\nThese vignettes simulate the types of decisions nurses  must make every day, particularly in low-resource environments where access to specialists or diagnostic equipment may be limited.\n\n* These are real clinical scenarios, and the dataset is small because expert-labelled data is difficult and time-consuming to collect.\n* Prompts are diverse across medical specialties, geographic regions, and healthcare facility levels, requiring broad clinical reasoning and adaptability.\n* Responses may include abbreviations, structured reasoning (e.g. \"Summary:\", \"Diagnosis:\", \"Plan:\"), or free text.\n\nmy first go to is to fine tune a small LLM to do this but I have feeling it won't be enough given how diverse the specialties are and the size of the dataset.  \nAnyone has done something like this before? any help or resources would be welcomed.",
    "author": "Aromatic-Fig8733",
    "timestamp": "2025-04-29T19:34:27",
    "url": "https://reddit.com/r/datascience/comments/1kb5xj6/ds_in_healthcare/",
    "score": 12,
    "num_comments": 20,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kayvx4",
    "title": "Putting Forecast model into Production help",
    "content": "I am looking for feedback on deploying a Sarima model. \n\n\nI am using the model to predict sales revenue on a monthly basis. The goal is identifying the trend of our revenue and then making purchasing decisions based on the trend moving up or down. I am currently forecasting 3 months into the future, storing those predictions in a table, and exporting the table onto our SQL server. \n\n\nIt is now time to refresh the forecast. I think that I retrain the model on all of the data, including the last 3 months, and then forecast another 3 months. \n\n\nMy concern is that I will not be able to rollback the model to the original version if I need to do so for whatever reason. Is this a reasonable concern? Also, should I just forecast 1 month in advance instead of 3 if I am retraining the model anyway? \n\n\nThis is my first time deploying a time series model. I am a one person shop, so I don't have anyone with experience to guide me. Please and thank you. ",
    "author": "iwannabeunknown3",
    "timestamp": "2025-04-29T14:02:44",
    "url": "https://reddit.com/r/datascience/comments/1kayvx4/putting_forecast_model_into_production_help/",
    "score": 11,
    "num_comments": 15,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "datascience",
    "post_id": "1kapuvz",
    "title": "Transition to SDE",
    "content": "Is there anyone here who has transitioned to SDE from DS? I have been working as a data scientist for over 2 years now, so my CV comprises of DS related experience only. I want to explore opportunities in SDE (as well as DS/MLE) since I am not enjoying the kind of work I am doing now. My background is CS. \n\nIf someone has done it, can you suggest how to prepare for it given that I have worked as DS? Should I include SDE related self projects? Btw there's no opportunity in my current organization to internally transition to SDE. And I am more inclined towards product related companies.",
    "author": "alpha_centauri9889",
    "timestamp": "2025-04-29T07:52:35",
    "url": "https://reddit.com/r/datascience/comments/1kapuvz/transition_to_sde/",
    "score": 26,
    "num_comments": 11,
    "upvote_ratio": 0.97,
    "is_original_content": false
  }
]