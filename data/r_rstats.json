[
  {
    "subreddit": "rstats",
    "post_id": "1ocgs0e",
    "title": "New release (1.7.6) of the statistics package for GNU Octave",
    "content": "",
    "author": "pr0m1th3as",
    "timestamp": "2025-10-21T08:57:16",
    "url": "https://reddit.com/r/rstats/comments/1ocgs0e/new_release_176_of_the_statistics_package_for_gnu/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1obofly",
    "title": "Analyzing migration flows between EU countries and the rest of the world",
    "content": "As the title says, I'm analyzing migration flows to EU countries (including UK, so 28 countries) from the rest of the world, between 2011 and 2022. EU countries are also origin countries, while outside Europe I have considered macro-areas for various reasons (mainly, aggregates had fewer missing data and there are too many countries in the world). In the end, there are 62 origins.  \nSince I'm working with longitudinal data and count response, I've been using glmmTMB in R with family=nbinom2.  \nMigration flows are something you observe between a pair of countries, so the couples O-D are my units.  \nIn literature I've often seen fixed effects for origin, destination and year being used, but I think there are many things we cannot observe about the pairs, and I find reasonable to think there might be correlation between observations on the same pair.  \nIf I were to use a fixed effect for O-D that would absorb time-constant variables'effect (such as distance). Also, in a decade many things change, the unobserved heterogeneity's sources change, so I wanted to use random effects for O-D, destination and origin (fixed effects for years are fine).\n\nI wanted to ask, what are the proper checks I should make when fitting a GLMM with RE with glmmTMB in R? What should I look for and how should interpret the results?\n\nI know about the correlation between RE and regressors, but apparently I can't perform Hausman's test with a glmmTMB fit. So I grouped the regressors by origin/destination/O-D, averaged them and checked the correlation between the RE for origin/destination/O-D and the mean value of each regressor per country (example, (Germany's average population; Germany's RE as an origin country), (Italy's average population; Italy's RE as an origin country)... I defined these two columns, then checked the correlation. Then, same procedure for destination and O-D RE). If I get it right, I should check the correlation between a certain level RE and the regressors of that level (I shouldn't examine the correlation between destinations RE and origins' control of corruption, for example).  \nIf there is correlation I can apply Mundlak's correction.\n\nAnother thing, using multiple levels of RE it is important that the three levels of RE I'm using should be independent. How do I check this? I have 28 destinations RE, 61 for origins and more than a thousand for O-D pairs.  \nI only checked the correlation between the effects for the EU countries (they have both the destination and origin RE), and between destination and O-D RE, and between origin and O-D RE.  \nWhat should I do were I to find RE not independent?\n\n  \nSummary: fitting a GLMM to study migration flows (modeled as a negative binomial) to EU countries from other EU countries and the rest of the world, from 2011 to 2022. Inserting random effects for origins, destination, and pair of origin-destination countries.  \nWhat should I do to run the diagnostics of the model? How do I validate it? What should I check in order to say the results are fine and can be read, without them being biased by something I did wrong?  \nFeel free to ask me anything, I'm a student trying to make the best I can with only the basic knowledge I received about GLMM.\n\nThanks in advance",
    "author": "1-0-100000",
    "timestamp": "2025-10-20T10:36:17",
    "url": "https://reddit.com/r/rstats/comments/1obofly/analyzing_migration_flows_between_eu_countries/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o9jwov",
    "title": "How is RKWard Compared to RStudio?",
    "content": "I‚Äôve had positive experiences with KDE applications such as Okular. This R IDE seems to be actively maintained, although I haven‚Äôt encountered many users who rely on it. How does it compare with RStudio?\n\n[https://rkward.kde.org/news/](https://rkward.kde.org/news/)",
    "author": "BOBOLIU",
    "timestamp": "2025-10-17T19:13:23",
    "url": "https://reddit.com/r/rstats/comments/1o9jwov/how_is_rkward_compared_to_rstudio/",
    "score": 11,
    "num_comments": 2,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o9fkx3",
    "title": "Poisson GLM aborts session",
    "content": "Hello, I know this will probably come across as extremely vague but I have no idea what might be contributing to this. \n\n\n\nMy glm() calls work with binomial or gaussian family, but whenever I run it with a poisson family, it immediately gives me the \"bomb error message\" that says the session is aborted.\n\n  \nDoes anyone know what might be contributing to this issue, or provide me resources to diagnose where things are going wrong?",
    "author": "AdExotic7198",
    "timestamp": "2025-10-17T15:45:25",
    "url": "https://reddit.com/r/rstats/comments/1o9fkx3/poisson_glm_aborts_session/",
    "score": 6,
    "num_comments": 9,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o9297f",
    "title": "Webinar: A Hybrid SAS/R Submission Story",
    "content": "R Consortium Silver Member Johnson &amp; Johnson will share insights into their work on the successful R submission to the FDA. Three J&amp;J researchers will show how open-source R packages were utilized for statistical analysis and the creation of tables, figures, and listings (TFLs).\n\nFree registration here: [https://r-consortium.org/webinars/jnj-hybrid-sas-r-submission-story.html](https://r-consortium.org/webinars/jnj-hybrid-sas-r-submission-story.html)\n\nAbout the R Submissions Working Group\n\nThe [R Consortium R Submissions Working Group](https://rconsortium.github.io/submissions-wg/) is focused on improving practices for R-based clinical trial regulatory submissions.\n\nHealth authority agencies from different countries require electronic submission of data, computer programs, and relevant documentation to bring an experimental clinical product to market. In the past, submissions have mainly been based on the SAS language.\n\nIn recent years, the use of open source languages, especially the R language, has become very popular in the pharmaceutical industry and research institutions. Although the health authorities accept submissions based on open source programming languages, sponsors may be hesitant to conduct submissions using open source languages due to a lack of working examples.\n\nTherefore, the R Submissions Working Group aims to provide R-based submission examples and identify potential gaps while submitting these example packages. All materials, including submission examples and communications, are publicly available on the R consortium GitHub page.\n\n\nFree webinar registration here: [https://r-consortium.org/webinars/jnj-hybrid-sas-r-submission-story.html](https://r-consortium.org/webinars/jnj-hybrid-sas-r-submission-story.html)",
    "author": "jcasman",
    "timestamp": "2025-10-17T07:05:59",
    "url": "https://reddit.com/r/rstats/comments/1o9297f/webinar_a_hybrid_sasr_submission_story/",
    "score": 20,
    "num_comments": 0,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o9wckn",
    "title": "Doubt in linear regression",
    "content": "I am working on an assignment where I am trying to reduce the mean squared error for unseen test data. Using training data I made a scatter plot for all dependent and independent variable but I see clusters in one of my dependent variable and also four clusters in my independent variable. Since I am bound to use linear regression I am thinking to treat my independent variable as numeric column but for dependent variable I am trying to make it categorical by encoding them as 1 for values above x and 0 for below it basically indicator variables to account for fitting different lines for both clusters. Also this dependent variable was initially numeric so I was looking for if I can also incorporate numerical value of this variable in each model to further reduce my MSE but I am not really able to make out how can I write it in my model matrix probably in R.\n\nCan anyone guide me if what I am doing is right and also how to incorporate numerical value of the column.\nAlso if I can do something about the cluster I see in my dependent variable using only X√ü as the final step for my prediction.\n\nThankyou in advance!",
    "author": "AverageObvious8317",
    "timestamp": "2025-10-18T06:50:09",
    "url": "https://reddit.com/r/rstats/comments/1o9wckn/doubt_in_linear_regression/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o86uig",
    "title": "Erdos: Open-source AI data science IDE",
    "content": "From the two guys who created [Rao](https://www.lotas.ai/rao), we‚Äôre excited to launch [Erdos](https://www.lotas.ai/erdos): a secure, AI-powered data science IDE, all open source! We‚Äôve seen how AI has boosted software development, and we want to bring this speed-up to data science in an IDE that feels like home.\n\nProbably the most relevant topic for the R community is how this is different from Positron. Below we‚Äôll list some similarities and differences, and this should also give a good overview of the features in Erdos.\n\n**Similarities**\n\n* Both are VS Code forks that inherit all VS Code functionality (extensions etc.)\n* Both come with R and Python consoles that can run scripts or code blocks\n* Both include AI capabilities (though with substantial differences below)\n* Both include sections for plots, documentation, database connections, and environmental variable viewing/management (though with many differences throughout)\n* Both can be run as desktop apps or in a browser\n\n**Differences**\n\n|Erdos|Positron|\n|:-|:-|\n|Open source AGPLv3 license|Non-open source Elastic License 2.0|\n|Anthropic and OpenAI models accessible through Lotas‚Äôs secure zero data retention backend, or via bring your own key. Connections for on-premise and private models (e.g. personal AWS) are also available.|Anthropic and GitHub Copilot models accessible via bring your own key.|\n|Read-write data explorer for CSVs and TSVs (with exciting advancements coming soon!)|Read-only data explorer for CSVs, TSVs, and tabular data in memory|\n|In-line code execution for Qmd/Rmd files, similar to RStudio|Source and visual modes with console execution for Qmd/Rmd files|\n|Other miscellanea including a command history tab, a SQL system, and websocket-based console communications|Other miscellanea including reticulate and ZMQ-based console communications|\n\nTry it out at [www.lotas.ai/erdos](https://www.lotas.ai/erdos) \\- we‚Äôd love any feedback or suggestions for future development!",
    "author": "SigSeq",
    "timestamp": "2025-10-16T06:57:04",
    "url": "https://reddit.com/r/rstats/comments/1o86uig/erdos_opensource_ai_data_science_ide/",
    "score": 162,
    "num_comments": 41,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o7lqdx",
    "title": "qol 1.0.2 released on CRAN: More Efficient Bigger Outputs",
    "content": "This package brings powerful SAS inspired concepts for more efficient bigger outputs to R.\n\nThe main goal is to make descriptive evaluations easier to create bigger and more complex outputs in less time with less code. Introducing format containers with multilabels, a more powerful summarise which is capable to output every possible combination of the provided grouping variables in one go, tabulation functions which can create any table in different styles and other more readable functions. The code is optimized to work fast even with datasets of over a million observations.\n\nYou can get an overview here: [https://s3rdia.github.io/qol/](https://s3rdia.github.io/qol/)\n\nThis is the current version released on CRAN: [https://cran.r-project.org/web/packages/qol/index.html](https://cran.r-project.org/web/packages/qol/index.html)\n\nHere you can get the development version: [https://github.com/s3rdia/qol](https://github.com/s3rdia/qol)",
    "author": "qol_package",
    "timestamp": "2025-10-15T13:09:47",
    "url": "https://reddit.com/r/rstats/comments/1o7lqdx/qol_102_released_on_cran_more_efficient_bigger/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o7kj16",
    "title": "Dutch elections - how to do coalition cross-correlation?",
    "content": "*Background*: In about two weeks, the Dutch parliamentary elections will be held to vote for the House of Representative/House of Commons-equivalent (Tweede Kamer). There are &gt;20 parties in this election, with \\~12 of them having a chance of getting into the House. Since no party will ever have the required 76-seat majority, coalitions of parties need to be built to form the government.\n\nBased on some scraping of polls from Wikipedia and brute-force calculations, I can put together a dataframe of possible majority coalitions (&gt;75 seats, 5 or less coalition partners).\n\nThis looks like this:\n\n    &gt; majoritycoalitions\n    # A tibble: 329 √ó 3\n       partylist               numparties seatcount\n       &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;\n     1 PVV, GL/PvdA, CDA                3        79\n     2 PVV, GL/PvdA, CDA, D66           4        93\n     3 PVV, GL/PvdA, CDA, VVD           4        93\n     4 PVV, GL/PvdA, CDA, JA21          4        92\n\nThe first question that comes to mind is determining how many times a party is in all these coalitions, and I figured that out quickly.\n\nThe real question: how often do two parties join in a coalition? Thanks to an AI bot, I got this obvious piece of code involving a triple-nested loop, but it doesn't feel like the \"R way\" of doing things functionally.\n\n    party_matrix &lt;- matrix(0,\n    ¬† ¬† nrow = partycount,\n    ¬† ¬† ncol = partycount,\n    ¬† ¬† dimnames = list(election$parties, election$parties)\n    )\n    \n    \n    for (i in 1:nrow(majoritycoalitions)) { # nolint\n    ¬† ¬† parties &lt;- strsplit(majoritycoalitions$partylist[i], \", \")[[1]]\n    ¬† ¬† for (p1 in parties) {\n    ¬† ¬† ¬† ¬† for (p2 in parties) {\n    ¬† ¬† ¬† ¬† ¬† ¬† if (p1 != p2) {\n    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† party_matrix[p1, p2] &lt;- party_matrix[p1, p2] + 1\n    ¬† ¬† ¬† ¬† ¬† ¬† }\n    ¬† ¬† ¬† ¬† }\n    ¬† ¬† }\n    }\n\nAny ideas for a better and less convoluted way than 3 nested loops?\n\n  \nThe end result looks like this:\n\nhttps://preview.redd.it/t7yo45hfubvf1.png?width=3000&amp;format=png&amp;auto=webp&amp;s=fb0f46a5152fe4ce460e5b3ec8efec2ee0d82cb6\n\n",
    "author": "peperazzi74",
    "timestamp": "2025-10-15T12:23:33",
    "url": "https://reddit.com/r/rstats/comments/1o7kj16/dutch_elections_how_to_do_coalition/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o7bxqd",
    "title": "Anyone Still Using Rmetrics Packages",
    "content": "[https://www.rmetrics.org/about](https://www.rmetrics.org/about)\n\n[https://r-forge.r-project.org/R/?group\\_id=156](https://r-forge.r-project.org/R/?group_id=156)\n\nWhen I first started using R many years ago, Rmetrics packages were widely used within the finance and econometrics community. Recently, however, it seems most of these packages are no longer actively maintained, and many have been delisted from CRAN. Online learning materials rarely use them anymore. Despite this trend, I still use fGarch and find it both straightforward and effective for my work. Is anyone else still using these packages?\n\nPS: I am also surprised that r-forge is still being maintained. ",
    "author": "BOBOLIU",
    "timestamp": "2025-10-15T07:05:33",
    "url": "https://reddit.com/r/rstats/comments/1o7bxqd/anyone_still_using_rmetrics_packages/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o7b1k8",
    "title": "Issue with loading ggtern package",
    "content": "My script was working when using the ggtern package but it seems to be throwing an error anytime I try to load it. I would appreciate any insight folks could provide. I get the following error: \n\nError: package or namespace load failed for ‚Äòggtern‚Äô:  \n .onLoad failed in loadNamespace() for 'ggtern', details:  \n  call: NULL  \n  error: &lt;ggplot2::element\\_line&gt; object properties are invalid:  \n\\- u/lineend must be &lt;character&gt; or &lt;NULL&gt;, not S3&lt;arrow&gt;",
    "author": "accidental_hydronaut",
    "timestamp": "2025-10-15T06:29:48",
    "url": "https://reddit.com/r/rstats/comments/1o7b1k8/issue_with_loading_ggtern_package/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o6bz6p",
    "title": "Advice on analyzing light √ó temperature effects in plant experiments",
    "content": "**Hey everyone,**\n\nI made a similar post here a while ago, but I‚Äôve progressed a bit in my thinking and would love some feedback on my ideas.\n\nFor my PhD, I designed a study to investigate how a plant species responds to changing **light and temperature conditions**. This resulted in **12 experimental treatments**:\n\n* **Temperature:** 5 ¬∞C, 10 ¬∞C, 15 ¬∞C, 20 ¬∞C\n* **Light:** LL (low light), ML (mid light), HL (high light)\n\nWe measured traits such as growth and photosynthetic performance. The data are visualized in the figure with **boxplots** for each treatment, and I added **linear regression lines** to help visualize the direction of the effects.\n\n[LL = blue, ML = orange, HL = red](https://preview.redd.it/mp7dj1ws42vf1.png?width=644&amp;format=png&amp;auto=webp&amp;s=4681ea8be29ef77bb67ae44a01479acdc14298e5)\n\nStatistically, I‚Äôve performed a **two-way permutational ANOVA** using the R package *permuco*. The results (effects and p-values) are printed in the bottom right of each plot.\n\nMy goal is to make statements like:\n\n‚ÄúParameter B is significantly affected by temperature and light, with median values suggesting that Parameter B is higher at higher temperatures and lower light intensity.‚Äù\n\nI‚Äôm wondering: **Is this level of interpretation sufficient**, or should I try to provide stronger statistical support?\n\nSo far, I‚Äôve **avoided pairwise comparisons**, as I don‚Äôt know a good way to perform them after a permutational ANOVA in R. With 12 groups, I also face limitations:\n\n* Many significances disappear after correcting for multiple testing,\n* and with **n = 5 per group**, the statistical power is limited anyway.\n\nI‚Äôm mainly interested in **whether there is an effect and in which direction**, rather than precise pairwise differences.\n\nI‚Äôve now characterized the light conditions more precisely:\n\n* HL = 226.5 ¬µmol photons m‚Åª¬≤ s‚Åª¬π\n* ML = 121.5 ¬µmol photons m‚Åª¬≤ s‚Åª¬π\n* LL = 93 ¬µmol photons m‚Åª¬≤ s‚Åª¬π\n\n(these are median values; the light field isn‚Äôt perfectly uniform).  \nWater temperature, on the other hand, is almost perfectly controlled.\n\nSo I‚Äôm considering whether I could treat **both variables as numeric**.  \nHowever, I‚Äôm unsure if that makes sense with only three light levels ‚Äì or whether an **ANCOVA** would be better (temperature numeric, light as a factor).\n\nThe challenges I see are:\n\n* Light and temperature would then be treated *unequally*,\n* biologically, both are actually continuous,\n* and some parameters (not shown) likely follow **optimum curves** (e.g., increasing from 5 ¬∞C to 15 ¬∞C, then decreasing at 20 ¬∞C). Treating temperature as numeric could then miss these effects and reduce significance, even though that reflects biological reality.\n\nWhat do you think?\n\n* Would you keep light and temperature as factors, or treat temperature (or both) as numeric?\n* How would you justify your choice statistically and biologically?\n* Is it reasonable to focus on the main effects from the permutational ANOVA and the direction of the medians, without pairwise comparisons?\n* **Or would you perhaps take a completely different approach?**\n\nThanks in advance for any input! üôè",
    "author": "diver_0",
    "timestamp": "2025-10-14T03:37:52",
    "url": "https://reddit.com/r/rstats/comments/1o6bz6p/advice_on_analyzing_light_temperature_effects_in/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o6hx2c",
    "title": "Part tolerance from estimated mean and standard deviation (STAN)",
    "content": "So I measured 40 transformers from a batch of 1000 and I wanted to estimate the tolerance of the inductance measurement using the result of my stan model.\n\nThe model is a normal prior for the mean and a halfnormal for the standard deviation. The likelihood function was also a normal.\n\nThe resulting 95% HDI for the mean is \\[53, 58\\] and \\[6, 9.5\\] for the standard deviation. How could I get an estimated % tolerance from these results?",
    "author": "Headshot4985",
    "timestamp": "2025-10-14T08:02:01",
    "url": "https://reddit.com/r/rstats/comments/1o6hx2c/part_tolerance_from_estimated_mean_and_standard/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o5l4kd",
    "title": "RStudio's Future",
    "content": "I‚Äôm not sure about Posit‚Äôs plans for RStudio, but I‚Äôll continue using it as my main R IDE. I‚Äôve tried both Positron and the R extension for VS Code, and each has serious flaws. Positron crashed my computer when I used keyboard shortcuts in its R console, while the VS Code extension relies on a Python-based R console that frequently fails and appears no longer actively maintained. More importantly, their underlying platform, Code OSS, like most Microsoft software, is slow and memory-hungry. Positron feels even slower than VS Code, which already consumes around 500‚ÄØMB of RAM for doing nothing‚Äîthat is insane.\n\n  \n",
    "author": "BOBOLIU",
    "timestamp": "2025-10-13T07:24:16",
    "url": "https://reddit.com/r/rstats/comments/1o5l4kd/rstudios_future/",
    "score": 88,
    "num_comments": 77,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o6d0xs",
    "title": "New R update won't run brms code! Help!",
    "content": "Updated my R program with the latest update and now my model won't run! When I go to run the model I get this show up, \n\nhttps://preview.redd.it/o4w3es8nc2vf1.png?width=491&amp;format=png&amp;auto=webp&amp;s=4937c095af098c38abce28576023c5b1c2f73715\n\nI have downloaded and updated everything possible! I uninstalled everything and tried to reinstall an older version of R but it kept throwing a fit about updating. I'm at a loss of what to do. ",
    "author": "Many_Blueberry6806",
    "timestamp": "2025-10-14T04:34:01",
    "url": "https://reddit.com/r/rstats/comments/1o6d0xs/new_r_update_wont_run_brms_code_help/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o5jhvf",
    "title": "Stan Libraries for R",
    "content": "I recently installed R and Rstan to try out probalistic programing outside of python. Are there any top level stan libraries that have precompiled stan models that you can call instead of having to create them?\n\nI see there are libraries like rstanarm for regression but are there any for more generic situations like estimating a population mean with normal distribution, or proportion using binomial, ect.",
    "author": "Headshot4985",
    "timestamp": "2025-10-13T06:19:01",
    "url": "https://reddit.com/r/rstats/comments/1o5jhvf/stan_libraries_for_r/",
    "score": 23,
    "num_comments": 22,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o5hud0",
    "title": "Multivariate Regression Analysis in SPSS or Jamovi",
    "content": "Hi everyone, we are starting to conduct research, and the multivariate regression fits very well for our analysis. We have 1 IV and 2 DVs. We are having trouble with how to conduct this analysis in SPSS or Jamovi software. Also, on what and how to conduct the assumptions for multivariate. Your input on how we can do it is greatly appreciated. ",
    "author": "Forward-Tip4267",
    "timestamp": "2025-10-13T05:05:33",
    "url": "https://reddit.com/r/rstats/comments/1o5hud0/multivariate_regression_analysis_in_spss_or_jamovi/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.13,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o59jhg",
    "title": "I'm collecting data on student sleep habits for my statistics class! Please fill out this survey, its anonymous and only takes a minute. Every response helps!",
    "content": "[https://www.statcrunch.com/s/48096](https://www.statcrunch.com/s/48096)",
    "author": "Novel-Pea-3371",
    "timestamp": "2025-10-12T20:54:38",
    "url": "https://reddit.com/r/rstats/comments/1o59jhg/im_collecting_data_on_student_sleep_habits_for_my/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o49u6p",
    "title": "Is there a way to generate only specific contrasts with the pairs() function?",
    "content": "I'm using a mixed model to analyze my data that has several interaction variables. My model below...\n\nmodel&lt;-lmer(yield\\~MainGroup\\*Timing\\*Environment + Subgroup:Timing:Environment + (various random variables)\n\nwhere \n\n* MainGroup = 2 levels\n* Subgroup = 10 levels (5 subgroups belong to each MainGroup level, subgroup is nested in maingroup)\n* Timing = 2 levels\n* Environment = 12 levels\n\n  \nI have a significant Subgroup:Timing:Environment interaction. I want to know if there are significant differences in the emmeans values...\n\nyld&lt;-emmeans(model,\\~Subgroup:Timing:Environment,level=0.95)\n\nI want to know if there is a significant difference for each subgroup under different 'Timings' at each environment. I know I can run pairs(yld), but I then end up with SO many extra contrasts that are not important. For example, I want to know if SubgroupA behaved differently between Timing1 and Timing2, at EnvironmentX, but I'm not interested in the difference between Subgroup A and B, or Subgroup A at environment X and Y.\n\nIs there a way to run pairs() so that I only get specific contrasts? Is there another function that would work better for this situation?\n\nIs it okay to subset data from 'yld' for each environment and then run pairs() so there are fewer contrasts to sort through?\n\n  \nWhat do I do?\n\n",
    "author": "In-the-dirt-01",
    "timestamp": "2025-10-11T16:19:57",
    "url": "https://reddit.com/r/rstats/comments/1o49u6p/is_there_a_way_to_generate_only_specific/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o34fry",
    "title": "R+AI 2025 ¬∑ Hosted by R Consortium ¬∑ Nov 12‚Äì13 ¬∑ 100% online",
    "content": "Skip the hype. See how R + AI works in production\n\n‚Ä¢ Keynote: Joe Cheng (CTO, Posit) ‚Äî ‚ÄúKeeping LLMs in Their Lane: Focused AI for Data Science and Research‚Äù\n\n‚Ä¢ Who R+AI is for: hands-on R users, ML beginners, LLM experimenters, and teams shipping AI in finance, healthcare, marketing, and research\n\n‚Ä¢ What you‚Äôll get: practical sessions, reproducible workflows, real examples in R, no noise\n\n‚Ä¢ Pricing: students from $25; affordable tiers for pros\n\n[Register here!](https://rconsortium.github.io/RplusAI_website/)",
    "author": "jcasman",
    "timestamp": "2025-10-10T08:42:30",
    "url": "https://reddit.com/r/rstats/comments/1o34fry/rai_2025_hosted_by_r_consortium_nov_1213_100/",
    "score": 26,
    "num_comments": 0,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o342gk",
    "title": "R Shiny Showcase - SpendDash",
    "content": "Hi, I created [SpendDash](https://spenddash.live/), an app for tracking personal expenses. You can use it to visualise your monthly or daily expenses and see how they fluctuate over time, helping you plan and budget.  \n  \nIf you use Revolut to pay by card, you can directly use your data here! Just export your account statement to Excel and then use it in the app.\nIf you have another way of tracking expenses, the only important thing is that the data is in a tabular format with column names matching the expected ones. Then you can also easily use it with SpendDash.  \n  \nThe application is open source. I hope you find it useful, and I appreciate any feedback and suggestions :)",
    "author": "Deva4eva",
    "timestamp": "2025-10-10T08:28:47",
    "url": "https://reddit.com/r/rstats/comments/1o342gk/r_shiny_showcase_spenddash/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o33n6f",
    "title": "What's wrong with this simple equation?",
    "content": "This is my first day into learning R and I'm unsure what I'm doing wrong. I am unable to calculate this simple equation: 3x^(3) \\+ 2x^(2) \\+ 5x + 1.\n\nThis is how I am writing it in R: 3x\\^3 + 2x\\^2 + 5x + 1\n\nThis is the message I am getting: *Error: unexpected symbol in \"3x\"*\n\nCould somebody please tell me what I am doing wrong?",
    "author": "InspectorRight754",
    "timestamp": "2025-10-10T08:13:24",
    "url": "https://reddit.com/r/rstats/comments/1o33n6f/whats_wrong_with_this_simple_equation/",
    "score": 4,
    "num_comments": 16,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o39oij",
    "title": "Robust multiple regression [Q]",
    "content": "",
    "author": "Funny-Leading-7476",
    "timestamp": "2025-10-10T11:57:44",
    "url": "https://reddit.com/r/rstats/comments/1o39oij/robust_multiple_regression_q/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o2vljt",
    "title": "Metafor forest plot: effect size not visible (mean difference ~21) ‚Äî how to scale x-axis ?",
    "content": "Hi everyone,\n\nI‚Äôm running a meta-analysis using **metafor** with **mean difference** as the effect measure.  \nAll my study-level mean differences and CIs are within the range **0‚Äì22**, and the pooled mean difference is around **21**.\n\nHowever, when I plot the results using `forest()`, the effect sizes don‚Äôt appear at all ‚Äî they‚Äôre *outside the visible plotting area*. It seems like the default x-axis scaling assumes log-transformed data (e.g., for odds ratios), so my mean difference of 21 is way off the visible range.\n\nHow can I properly rescale or center the x-axis so that all mean differences (0‚Äì22) are visible and the plot resembles a RevMan-style forest plot on the extreme right side with a linear scale, a null line at 0, and clearly visible confidence intervals ?\n\nThe data looks like this :\n\nn.e= number of patients in Experimental arm, n.c= number of patients in the Control arm\n\nhttps://preview.redd.it/73i1brzazhuf1.png?width=707&amp;format=png&amp;auto=webp&amp;s=ff67b2fb03bd970fc241771f18b4b49aad277099\n\n",
    "author": "1SageK1",
    "timestamp": "2025-10-10T01:47:37",
    "url": "https://reddit.com/r/rstats/comments/1o2vljt/metafor_forest_plot_effect_size_not_visible_mean/",
    "score": 5,
    "num_comments": 8,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o2h2ac",
    "title": "Interesting Blog and Discussion",
    "content": "[https://jtibs.substack.com/p/if-all-the-world-were-a-monorepo](https://jtibs.substack.com/p/if-all-the-world-were-a-monorepo)\n\n[https://news.ycombinator.com/item?id=45259623](https://news.ycombinator.com/item?id=45259623)",
    "author": "BOBOLIU",
    "timestamp": "2025-10-09T13:33:54",
    "url": "https://reddit.com/r/rstats/comments/1o2h2ac/interesting_blog_and_discussion/",
    "score": 8,
    "num_comments": 7,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o2fy2f",
    "title": "Confusing CRAN Info",
    "content": "I am interested in using this R package called gpuR. I noticed that the maintainer is one guy, the documentation is written by another guy, and the URL leads to a third guy's GitHub account. I have never seen anything like this before. Any clarification? [https://cran.r-project.org/web/packages/gpuR/index.html](https://cran.r-project.org/web/packages/gpuR/index.html)",
    "author": "BOBOLIU",
    "timestamp": "2025-10-09T12:51:15",
    "url": "https://reddit.com/r/rstats/comments/1o2fy2f/confusing_cran_info/",
    "score": 8,
    "num_comments": 8,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o30o6e",
    "title": "how could i do this",
    "content": "https://preview.redd.it/kefkh5hpbauf1.png?width=772&amp;format=png&amp;auto=webp&amp;s=388b7a39f41c5a787efb131a84fbd9d7f46ac949\n\neven chatgpt gets it wrong   \n",
    "author": "Ancient_Safe4932",
    "timestamp": "2025-10-10T06:17:56",
    "url": "https://reddit.com/r/rstats/comments/1o30o6e/how_could_i_do_this/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.23,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o220mv",
    "title": "Can someone explain me the process of analysing data and using it to predict future?",
    "content": "I am searching it online but it's feels too complicated \n\n\nI have the marketing campaign data  stored and accessible via querying in mySQL.  I know python more than basics and can understand a code by looking at it \n\nMy question is how can I use python to analyse the data and find some existing bottlenecks so the marketing campaigns can be optimised further \n\nDo I have to build a predictive model or I can adapt an existing one?",
    "author": "Top-Run-21",
    "timestamp": "2025-10-09T03:09:38",
    "url": "https://reddit.com/r/rstats/comments/1o220mv/can_someone_explain_me_the_process_of_analysing/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.13,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o0uvge",
    "title": "Page Breaks in Word Markdown",
    "content": "For a school project, I created a table one using the table1 package. However, I have to have my R Markdown output be a word document, so the formatting did not stay. I used the flextable package around my saved object and it visually looks good in word, but I cannot prevent a page break in the middle of my table.\n\nI tried paginate, set_table_properties and the chunk option of ft.keepnext.\n\nI'd prefer not to start over on table one using a different package, but will if I have to. Am I missing some way to prevent a page break in my table with the current setup?",
    "author": "bailesbells",
    "timestamp": "2025-10-07T16:31:59",
    "url": "https://reddit.com/r/rstats/comments/1o0uvge/page_breaks_in_word_markdown/",
    "score": 3,
    "num_comments": 6,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o0as4x",
    "title": "Questions for terminal based IDE users",
    "content": "This is coming from a person who uses terminal based IDEs for other languages and is trying to do the same for R development.\n\nI mostly use Helix for package development at work, but there are still somethings I'm forcing myself to not miss about RStudio.\n\nI mostly have one helix terminal tab and another R console terminal tab open when I,m working. Do you use tmux or any other tool for workflow?\n\n1. browser() - It works fine on other terminal tab I guess, but how do you do it? Not highlighting which code is running I miss that a lot.\n\n2. REPL style development. I guess I dont really do this much anymore, but how do you deal with not having ctrl+return to execute code? I think nvim r has this, not sure\n\n3. Markdown and vignettes. For render do you just knitr:: whatever_command_it_is() on console tab to render Rmd files? Or you dont do it at all?\n\n4. This maybe a helix specific question, but for air users, on save do you make your ide run \"air format .\"? If so, what command do you use on your config.toml or config.lua?\n\n**NOTE**: I never tried NVIM R, but I know it has a REPL style console? What do you use?\n\nI want to see how you guys do it.",
    "author": "1k5slgewxqu5yyp",
    "timestamp": "2025-10-07T03:15:34",
    "url": "https://reddit.com/r/rstats/comments/1o0as4x/questions_for_terminal_based_ide_users/",
    "score": 7,
    "num_comments": 7,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1o08wqn",
    "title": "Fake positives from Malwarebites?",
    "content": "As the tile says, MB flagged these as malwares. I suppose they are false positive, given also the results of Virtustotal (at most 1/72, always SecureAge, flag it as malware).\n\n\n\nWondering why they get targeted though. They are in my system since 2023.",
    "author": "MaDeVi55",
    "timestamp": "2025-10-07T01:17:06",
    "url": "https://reddit.com/r/rstats/comments/1o08wqn/fake_positives_from_malwarebites/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nz9dx0",
    "title": "How Many Spaces for Indentation?",
    "content": "Using 4 spaces for indentation is common across many modern programming languages, such as Python and C++. How come most R users appear to use 2 spaces? \n\nPS: I use 4 spaces for my R, C++, and LaTeX codes to maintain consistency.  ",
    "author": "BOBOLIU",
    "timestamp": "2025-10-05T21:15:39",
    "url": "https://reddit.com/r/rstats/comments/1nz9dx0/how_many_spaces_for_indentation/",
    "score": 3,
    "num_comments": 11,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nyv0yb",
    "title": "Systematic Correctness Bugs",
    "content": "Some programming languages, such as Julia, have been found to suffer from systematic correctness bugs. In contrast, I have not encountered similar concerns with languages like R, Python, or C/C++. Most of us are statisticians, engineers, or scientists, and we typically do not have the time to worry about the fundamental correctness of the underlying language or widely used packages. Kudos to the R developers for sparing us these unnecessary headaches.\n\nCheck out this horrifying post: [https://news.ycombinator.com/item?id=45427021](https://news.ycombinator.com/item?id=45427021)",
    "author": "BOBOLIU",
    "timestamp": "2025-10-05T10:53:24",
    "url": "https://reddit.com/r/rstats/comments/1nyv0yb/systematic_correctness_bugs/",
    "score": 2,
    "num_comments": 11,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ny4hnt",
    "title": "R session aborted",
    "content": "I am a student in a stats class which is learning to use R however I keep getting ‚ÄúR session aborted R encountered a fatal error The session was terminated‚Äù \n\nI don‚Äôt know anything about coding as I‚Äôm a a beginner and my professor has no experience with Macs. \nI've tried the basics with restarting, deleting and redownloading both R and Rstudio (although I‚Äôm pretty sure my R is working since I was able to type there etc. but theirs an issue with Rstudio)\nDetails: \nI have an Intel-based MacBook Air (2017) running macOS Monterey (version 12.7.4).\nThe R I have installed is version 4.5.1 GUI 1.82 Big Sur intel build and the version of R studio I have installed is: 2024.09.1+394\n- according to the posit or whatever these were supposed to be the compatible versions for my device \n\nAny help is greatly appreciated as I have a test in a couple days on \n",
    "author": "Affectionate_Monk502",
    "timestamp": "2025-10-04T13:24:15",
    "url": "https://reddit.com/r/rstats/comments/1ny4hnt/r_session_aborted/",
    "score": 16,
    "num_comments": 16,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nyd81h",
    "title": "Ways to forecast [Help]",
    "content": "Hello everyone, I just wanted to come on here and ask what are some ways to forecast data. I'm currently working on some homework that requires to forecast the future of a company quarterly revenue (it's next quarter Q3).\n\nI'm already familiar with the forecast option on Excel, but is there any other formulas tools that can help?",
    "author": "No-Science-8489",
    "timestamp": "2025-10-04T20:04:36",
    "url": "https://reddit.com/r/rstats/comments/1nyd81h/ways_to_forecast_help/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nxb644",
    "title": "Rao 0.4.0 (BYOK, auto-accept tracking, etc.) + alpha test new IDE",
    "content": "Wanted to keep folks updated on Rao and mention our upcoming IDE!\n\nWe've made a number of updates in Rao since version 0.3. Folks with the previous version will get auto-updates, and anyone can [download the latest version here](https://www.lotas.ai/rao).\n\n* **Bring your own key option.**¬†Users can input their own Anthropic and OpenAI API keys to code in Rao without ever signing in to Lotas. We've also added functionality to connect to on-premise models for sensitive data analysis (contact us if interested). One-click Lotas sign-in still works as before with 50 free queries per month.\n* **New models.** Claude Sonnet 4.5 and GPT 5 are now available.\n* **New context.** Users can attach previous chats and function documentation as context, and models can now choose to retrieve documentation and plots from the plots pane as needed.\n* **Auto-accept.** All model actions can now be put on auto-accept mode for faster editing or code running. Each type of action can have its automation configured individually, and users can specify lists of functions to allow (or deny) the model to run automatically.\n* **In-line change tracking.** On auto-accept mode, changes made by the model are tracked in-line for individual acceptance or rejection. Chat checkpoints allow users to undo any undesired changes or change and resend queries.\n* **Dark mode.** Dark mode and standard themes all work, providing an easy transition from other IDEs.\n* Code searching, file editing, and overall speed have been improved.\n\nAs always, we'd love any feedback and thoughts on what you want to see in the next version!\n\nWe're currently inviting users to alpha test our new secure, AI-centric data science IDE based on VS Code. It handles R, Python, and SQL with extensive data science and AI features. If you'd like to alpha test it, [add your email here](https://www.lotas.ai/erdos) and we'll send you the link!",
    "author": "SigSeq",
    "timestamp": "2025-10-03T14:05:05",
    "url": "https://reddit.com/r/rstats/comments/1nxb644/rao_040_byok_autoaccept_tracking_etc_alpha_test/",
    "score": 4,
    "num_comments": 6,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nw9c19",
    "title": "Sovereign Tech Fund has invested $450,000 in the R Foundation to enhance the sustainability, security, and modernization of R‚Äôs core infrastructure",
    "content": "üö® Big news for the R community!\n\nThe Sovereign Tech Fund has invested $450,000 in the R Foundation to enhance the sustainability, security, and modernization of R‚Äôs core infrastructure.\n\nThis 18-month initiative will:\n\n ‚úîÔ∏è Remove legacy and unmaintained code\n ‚úîÔ∏è Improve portability (including Windows ARM support)\n ‚úîÔ∏è Strengthen supply chain trust with code and binary signing\n ‚úîÔ∏è Enhance developer tooling, documentation, and contributor experience\n ‚úîÔ∏è Build a more resilient foundation for R‚Äôs future\n\nThis initiative is championed by the R Foundation and the R Core Team. The R Foundation is a not-for-profit organization providing crucial financial and logistical support for the R project. Established by the members of the R Core Team, its primary mission is to ensure the continued success and stability of R for the global community. The Foundation, based in Vienna, Austria, holds the copyright for R software and uses its resources to support development and foster innovation in statistical computing. The R Core Team is the dedicated group of developers with write access to the R source code, who volunteer their time to guide the technical evolution of the language.\n\nLed by R Core contributor Tomas Kalibera, with support from the R Foundation and the wider community, this project is a major step forward in keeping R strong for decades to come!\n\nüëâ Read the full announcement:\n\n[https://r-consortium.org/posts/sovereign-tech-fund-invests-450000-in-r-foundation-to-enhance-r-sustainability-and-security/](https://r-consortium.org/posts/sovereign-tech-fund-invests-450000-in-r-foundation-to-enhance-r-sustainability-and-security/)\n",
    "author": "jcasman",
    "timestamp": "2025-10-02T09:58:09",
    "url": "https://reddit.com/r/rstats/comments/1nw9c19/sovereign_tech_fund_has_invested_450000_in_the_r/",
    "score": 164,
    "num_comments": 3,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nx493y",
    "title": "Quarto book with stats on chapters",
    "content": "",
    "author": "lipflip",
    "timestamp": "2025-10-03T09:41:43",
    "url": "https://reddit.com/r/rstats/comments/1nx493y/quarto_book_with_stats_on_chapters/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nvsfzw",
    "title": "extracting  factors after by()",
    "content": "I am doing paired t-tests on subgroups of subgroups of groups by using by:  \n\n\n    result&lt;-by(data,list(data$f1,data$f2),function(x)\n      t.test(x$val ~ x$f3)[c(1:9)]\n    \n\nIf I print(result), I see the values of the factors,  f1: f2: and the t.test result.\n\nI would like to extract the values of f1, f2, and the t.test p.value from the result, but I do not see where the values of f1 and f2 are kept in \"result\".",
    "author": "fasta_guy88",
    "timestamp": "2025-10-01T19:54:59",
    "url": "https://reddit.com/r/rstats/comments/1nvsfzw/extracting_factors_after_by/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nvi9xt",
    "title": "Run R snippets/functions/scripts in a Docker container",
    "content": "I somewhat recently wrote a package that's a little in the same spirit as `callr`. Basically it lets the user run arbitrary R code snippets and functions in a separate R process that is running within a Docker container and returns the output to the user's current local R session. I'm obviously quite biased, but I actually think it's a pretty neat little package‚Äîthough maybe it's a little bit more fun than useful. I'd be curious to get any thoughts, particularly on if others find this concept to be potentially useful? Currently the most helpful use case (at least that I have used it for) is making scientific research scripts easily replicable without requiring the replicator to locally install the full set of packages you used, etc.\n\n[https://github.com/dmolitor/jetty](https://github.com/dmolitor/jetty)",
    "author": "Ok_Yesterday_1386",
    "timestamp": "2025-10-01T12:41:08",
    "url": "https://reddit.com/r/rstats/comments/1nvi9xt/run_r_snippetsfunctionsscripts_in_a_docker/",
    "score": 10,
    "num_comments": 4,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nvaz4o",
    "title": "Suggestions for a typed version of R",
    "content": "Hi everyoneüëã, \n\nI am currently working on a typed version of the R programming language and wanted your advices/suggestions about it's composition (syntax and functioning and functionalities)üöÄ\n\nMy goal is to help package developers and R users in general to build more maintanable/safer R code. \n\nI already have a prototype of the project on github with it's documentation here:\n\nhttps://fabricehategekimana.github.io/typr.github.io/build/\n\nThe work is still in progress and your feedback would be helpful to build this project and make it useful for the community. Thanks in advance!ü§©",
    "author": "Artistic_Speech_1965",
    "timestamp": "2025-10-01T08:15:14",
    "url": "https://reddit.com/r/rstats/comments/1nvaz4o/suggestions_for_a_typed_version_of_r/",
    "score": 24,
    "num_comments": 13,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nv8j60",
    "title": "CFA: standard, scaled or robust",
    "content": "Hi guys,\nI'm afraid to ask this question it might is a stupid question.\nI am using lavaan, for cfa using wlsmv for likert type items.in the output I find for Cfi, tli and rsmea the standard value, scaled and robust. What do I report? \n\nThank you so much ",
    "author": "RedPanda_CGN",
    "timestamp": "2025-10-01T06:41:33",
    "url": "https://reddit.com/r/rstats/comments/1nv8j60/cfa_standard_scaled_or_robust/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nu9j8z",
    "title": "ANOVA or t-tests?",
    "content": "Hi everyone, I came across a recent *Nature Communications* paper (https://www.nature.com/articles/s41467-024-49745-5/figures/6). In Figure 6h, the authors quantified the percentage of dead senescent cells (n = 3 biological replicates per group). They reported P values using a two-tailed Student‚Äôs t-test.\n\nHowever, the figure shows multiple treatment groups compared with the control (shControl). It looks like they ran several pairwise t-tests rather than an ANOVA.\n\nMy question is:\n\n* Is it statistically acceptable to only use multiple t-tests in this situation, assuming the authors only care about treatment vs control and not treatment vs treatment?\n* Or should they have used a one-way ANOVA with Dunnett‚Äôs post hoc test (which is designed for multiple vs control comparisons)?\n* More broadly, how do you balance biological conventions (t-tests are commonly used in papers with small n) with statistical rigor (avoiding inflated Type I error from multiple comparisons)?\n\nCurious to hear what others think ‚Äî is the original analysis fine, or would reviewers/editors expect ANOVA in this case?",
    "author": "NoAttention_younglee",
    "timestamp": "2025-09-30T03:26:47",
    "url": "https://reddit.com/r/rstats/comments/1nu9j8z/anova_or_ttests/",
    "score": 50,
    "num_comments": 24,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nuoiso",
    "title": "options for propensity score matching that's not MatchIt?",
    "content": "I was using MatchIt but there's a conflict with the new version of RStudio. I rolled back to the prior version and it works. I submitted an issue to the package repo and they know of the problem but may not be able to fix it any time soon. \n\nSo looking for recs for other packages people like for propensity score",
    "author": "_fake_empire",
    "timestamp": "2025-09-30T13:37:28",
    "url": "https://reddit.com/r/rstats/comments/1nuoiso/options_for_propensity_score_matching_thats_not/",
    "score": 6,
    "num_comments": 9,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nto2ir",
    "title": "Plotly is retiring its R documentation",
    "content": "See below quote and link. \n\nThis gives me a lump in my stomach. I use the R plotly package every day. I have multiple apps in production within my company using plotly. \n\nWhat exactly does this mean for the continued support? I gather that the R plotly package will continue to work. We can no longer get good help with ChatGPT? What else?\n\n‚Äú‚Ä¶we have decided to take two steps. The first is to retire the documentation for R, MATLAB, Julia, and F#, which will give our team the time to focus on continuing to actively develop and maintain the JavaScript and Python documentation. We haven‚Äôt maintained these languages or their documentation for several years, and rather than keeping out-of-date material online to confuse both people and LLMs, we will take it down at the beginning of November 2025. All of the sources will remain in those languages‚Äô repositories on GitHub for reference, and will always be under an open license so that community members can look at it and/or look after it.‚Äù\n\nhttps://community.plotly.com/t/retire-the-documentation-for-r-matlab-julia-and-f/94147?_gl=1*dc6kjz*_gcl_au*MTA3NDgxODg4Ni4xNzU5MTY1OTQw*_ga*MTk4Mjg4MTQ1Ni4xNzU5MTY1OTQw*_ga_6G7EE0JNSC*czE3NTkxNjU5MzkkbzEkZzEkdDE3NTkxNjU5NTckajQyJGwwJGgw",
    "author": "gyp_casino",
    "timestamp": "2025-09-29T10:20:13",
    "url": "https://reddit.com/r/rstats/comments/1nto2ir/plotly_is_retiring_its_r_documentation/",
    "score": 134,
    "num_comments": 43,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nu47q6",
    "title": "Convenience package for ffts",
    "content": "If you often use ffts in R, you might like [fftab](https://thk686.github.io/fftab/). It stores results in a tibble along with frequency information, which helps when trying to pick out the components of interest. ",
    "author": "Prof_T_Keitt",
    "timestamp": "2025-09-29T21:51:02",
    "url": "https://reddit.com/r/rstats/comments/1nu47q6/convenience_package_for_ffts/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ntpe42",
    "title": "GGPlot error bars are very slightly off and it's driving me nuts",
    "content": "",
    "author": "trevorefg",
    "timestamp": "2025-09-29T11:09:13",
    "url": "https://reddit.com/r/rstats/comments/1ntpe42/ggplot_error_bars_are_very_slightly_off_and_its/",
    "score": 31,
    "num_comments": 14,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ntmik7",
    "title": "Have a Bad Feeling About Positron",
    "content": "I completely understand why RStudio (now Posit) wants to expand into Python and VS Code. As a long-time R user who has greatly benefited from their contributions to the R ecosystem, I sincerely wish them success. That said, I struggle to see how Positron will gain significant traction. VS Code already provides excellent extensions for both R and Python, and my own experience using R in VS Code has been largely positive. This raises the question: why would users like me switch to Positron? Perhaps it will offer stronger enterprise-level support tailored to corporate environments, but I cannot shake the feeling that this initiative may face serious challenges.\n\n[https://code.visualstudio.com/docs/languages/r](https://code.visualstudio.com/docs/languages/r)",
    "author": "BOBOLIU",
    "timestamp": "2025-09-29T09:22:31",
    "url": "https://reddit.com/r/rstats/comments/1ntmik7/have_a_bad_feeling_about_positron/",
    "score": 42,
    "num_comments": 38,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ntpykk",
    "title": "Am I clustering appropriately? Using LMER in R  with multiple groupings",
    "content": "I am examining the impact of the food environment and the economic environment on participants' diets before and after a program.  \n\nThe levels include:   \nLevel 3: MSA (metro area / economic environ. var) \n\n¬†‚îî‚îÄ Level 2: Block Group (food environ. var)\n\n‚îî‚îÄ Level 1: Individual (Participant)\n\n‚îî‚îÄ Repeated measures (pre/post test)\n\n\n\nCurrent model:\n\nlmer(score \\~ test\\_type + foodenvironment\\_Var + Economicenvironment\\_Var +(1| individual) +(1| MSA\\_ ID/BlockGroup\\_ID) ,data = .x)\n\n  \nI'm trying to understand better how to measure these clusters using the accurate writing elements for the model.  I'm also curious to know if clustering at the MSA and Blockgroup is advised.",
    "author": "AmazingChipmunk526",
    "timestamp": "2025-09-29T11:31:03",
    "url": "https://reddit.com/r/rstats/comments/1ntpykk/am_i_clustering_appropriately_using_lmer_in_r/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ntdux8",
    "title": "Update for Cookbook polars for R !",
    "content": "üí° Cookbook polars for R users has been updated with syntax from the tidypolars package. Have a look ! It's becoming easier to use the Polars API.\n\n[https://ddotta.github.io/cookbook-rpolars/](https://ddotta.github.io/cookbook-rpolars/)",
    "author": "damiendotta",
    "timestamp": "2025-09-29T02:53:17",
    "url": "https://reddit.com/r/rstats/comments/1ntdux8/update_for_cookbook_polars_for_r/",
    "score": 14,
    "num_comments": 1,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ntf9w7",
    "title": "Organize R Markdown/Flexdashboard",
    "content": "I have a R project folder with the subfolders 'data', 'script', and 'output'. I have various excel files in my data folder, create my dashboards (rmd) in the script folder, and knit them into output. That works fine for me. But: what is the best practice to organize code to create the dashboards. After yaml and some css i load all my needed libraries in one chunk. Then do you load all your data in one chunk or do you do it right where you create plots and tables? Do you have extra script for your datawrangling and plots and load it then to your markdown? Everything works fine but i want to know what is good practice to organize a longer markdown with multiple datasources and many plots.",
    "author": "Sem0815",
    "timestamp": "2025-09-29T04:18:27",
    "url": "https://reddit.com/r/rstats/comments/1ntf9w7/organize_r_markdownflexdashboard/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nsvose",
    "title": "This Package Need to Be In Every R Tutorial",
    "content": "I have been teaching R for several years, and the first major challenge beginners face is setting the working directory to the script‚Äôs location. After trying many different approaches, I have found the package`this.path`¬†to be the most reliable solution. Now, I always use it at the start of my R scripts, and I strongly believe that every R tutorial should adopt this package. [https://github.com/ArcadeAntics/this.path](https://github.com/ArcadeAntics/this.path)\n\n    this.path::this.dir() |&gt; setwd()\n\nEdit: I didn't know that so many R users only have experience with RStudio. Guys, it is time to open your eyes and see the world!",
    "author": "BOBOLIU",
    "timestamp": "2025-09-28T11:27:47",
    "url": "https://reddit.com/r/rstats/comments/1nsvose/this_package_need_to_be_in_every_r_tutorial/",
    "score": 56,
    "num_comments": 89,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nt7fps",
    "title": "Wanted to try Positron but reticulate isn't working",
    "content": "Hi everyone, I'm experiencing a very frustrating problem. I wanted to try Positron because I need to work with R and Python for a project and it seems to be quiet interesting (I always worked with Rstudio before). So, I created a quarto file and I installed the reticulate package in order to run R and Python chunk in the same script. The problem is that when I run R chunk everything works as it should but when I run Python chunk the interpret goes automatically back to R (even if I forcefully select Python) and of course the Python code isn't loaded, it just shows up in the R console without doing anything (and no error messages). I searched online but I couldn't find a solution for this, I tried the same code in Rstudio and reticulate works as it should. Thank you for the help!",
    "author": "SaPpHiReFlAmEs99",
    "timestamp": "2025-09-28T20:11:35",
    "url": "https://reddit.com/r/rstats/comments/1nt7fps/wanted_to_try_positron_but_reticulate_isnt_working/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nsnqmo",
    "title": "plume 0.3.0",
    "content": "I'm very excited to announce the release of plume 0.3.0. plume makes it very easy to handle author information in scientific writing in R Markdown and Quarto. The package greatly reduces the hassle of dealing with author lists, authors' contributions and more. plume also provides a simple solution to add or update author data in YAML for Quarto when using journal templates.\n\n[Documentation](https://arnaudgallou.github.io/plume/) | [GitHub](https://github.com/arnaudgallou/plume)",
    "author": "arangaca",
    "timestamp": "2025-09-28T06:00:13",
    "url": "https://reddit.com/r/rstats/comments/1nsnqmo/plume_030/",
    "score": 19,
    "num_comments": 2,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nt9rn4",
    "title": "Copilot/Chatgpt 5 for big data set analysis",
    "content": "",
    "author": "CloudsInCoffee123",
    "timestamp": "2025-09-28T22:22:44",
    "url": "https://reddit.com/r/rstats/comments/1nt9rn4/copilotchatgpt_5_for_big_data_set_analysis/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ns1qwz",
    "title": "nesstar explorer alternative for mac",
    "content": "Need nesstar explorer to extract data from nss survey, what should I go with instead of nesstar explorer? ",
    "author": "Turbulent_Push_338",
    "timestamp": "2025-09-27T10:56:59",
    "url": "https://reddit.com/r/rstats/comments/1ns1qwz/nesstar_explorer_alternative_for_mac/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nrb3nx",
    "title": "Blueycolors 0.1.0",
    "content": "Hey all! I just updated my package providing Bluey-themed colors and ggplot scales. Check it out if you also 1) enjoy data analysis and 2) have young kids who watch Bluey.\n\nhttps://ekholme.github.io/blueycolors/",
    "author": "BurtFrart2",
    "timestamp": "2025-09-26T12:50:23",
    "url": "https://reddit.com/r/rstats/comments/1nrb3nx/blueycolors_010/",
    "score": 48,
    "num_comments": 7,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nqs40w",
    "title": "ggsci 4.0.0: 400+ new color palettes",
    "content": "",
    "author": "nanxstats",
    "timestamp": "2025-09-25T21:32:41",
    "url": "https://reddit.com/r/rstats/comments/1nqs40w/ggsci_400_400_new_color_palettes/",
    "score": 42,
    "num_comments": 3,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nr6unm",
    "title": "RED-S Calculator for Risk Assessment and Evaluation",
    "content": "Hello Stats Community,\n\nWe're looking for feedback on our RED-S and Performance Weight risk assessment for athletes. We tried to build this within the guidelines of the IOC, NCAA, using Cunningham Equation, and the others listed below. [https://beastfingersclimbing.com/grippul/weight-calculator](https://beastfingersclimbing.com/grippul/weight-calculator)\n\n1. **Lean Body Mass (LBM)**  \nIf entered manually ‚Üí use input.  \nElse ‚Üí LBM = Weight √ó (1 ‚àí BodyFat%).  \n\n2. **Thresholds (sex-specific)**  \nEssential Fat: ~12% (F), ~5% (M).  \nRED-S caution line: ~16% (F), ~8% (M).  \nPerformance Zone: 17‚Äì20% (F), 10‚Äì12% (M).  \n\n3. **Target Weights from LBM**  \nPhysiological Floor = LBM √∑ (1 ‚àí Essential).  \nRED-S Weight = LBM √∑ (1 ‚àí REDS line).  \nPerformance Zone = LBM √∑ (1 ‚àí PerfLow) ‚Üí LBM √∑ (1 ‚àí PerfHigh).  \n\n4. **Baseline Energy (Rest-Day Maintenance)**  \nCunningham RMR: 500 + 22 √ó LBM(kg).  \nBaseline kcal = RMR √ó 1.3 (activity factor).  \n\n5. **Training Energy Add-On**  \nBase ref kcal/hr (light = 250 ‚Ä¶ elite = 1000 at 150 lb).  \nScaled by weight: kcal/hr √ó (weight / 150).  \nTraining add-on = scaled kcal/hr √ó training hours.  \n\n6. **Energy Availability (EA)**  \nEA = (Daily Intake ‚àí Training kcals) √∑ LBM(kg).  \nClassified as:  \n&lt;30 ‚Üí Low (RED-S risk).  \n30‚Äì45 ‚Üí Marginal.  \n‚â•45 ‚Üí Adequate.  \n\n",
    "author": "beastfingersclimbing",
    "timestamp": "2025-09-26T10:04:02",
    "url": "https://reddit.com/r/rstats/comments/1nr6unm/reds_calculator_for_risk_assessment_and_evaluation/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nqylf6",
    "title": "Behavioural data (Scan sampling) analysis using R and GLMMs.",
    "content": "Hello. I have scan sampling data in the form of counts/zone/duration (or day) of Individuals  visible (i know the total number of individuals; but have only taken count of those visible in each zone in the same area). I saw that repeated measures anova (for zone preference) using average values per day will not give the right information and identifying need to go for GLMMs. Im a novice in that but am eager to learn more and get the right analysis. So, it would be helpful for me if you could provide insight into this kind of analysis and any scientific papers that provide information and data on the same. ",
    "author": "Easy_Masterpiece5705",
    "timestamp": "2025-09-26T04:15:25",
    "url": "https://reddit.com/r/rstats/comments/1nqylf6/behavioural_data_scan_sampling_analysis_using_r/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nqqfkr",
    "title": "question about set.seed, train and test",
    "content": "I am not really sure how to form this question, I am relatively new to working with other models for my project other than step wise regression. I could only post one photo here but anyway, for the purpose of my project I am creating a stepwise. Plastic counts with 5 factors, identifying if any are significant to abundances. We wanted to identify the limitations to using stepwise but also run other models to run alongside to present with or strengthen the idea of our results. \nSo anyway, the question. The way I am comparing these models results it through set.seed. I was confused about what exactly that did but I think I get it now. My question is, is this a statistically correct way to present results? I have the lasso, elastic, and stepwise results by themselves without the test sets too but I am curious if the test set the way R has it set up is a valid way in also showing results. had a difficult time reading about it online.",
    "author": "Swagmoneysad3",
    "timestamp": "2025-09-25T20:03:09",
    "url": "https://reddit.com/r/rstats/comments/1nqqfkr/question_about_setseed_train_and_test/",
    "score": 5,
    "num_comments": 17,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nq2wtz",
    "title": "geom_point with position_dodge command tilts for some reason",
    "content": "Hello, I have an issue with the position\\_dodge command in a geom\\_point function:   \nmy x-axis is discrete, the y-axis is continuous.   \nOn the left is the data set and the code I used with one variable, no tilt, just a dodge along the x-axis.  \nOn the right, the same data set and the same code, just with a different variable, produce a tilt. \n\nIs there a way to get rid of that tilt? \n\n  \nThis is the code I used, variable names are replaced by generics.\n\n  ggplot() +\n\n  geom\\_point(position = position\\_dodge(width = 0.6)) + \n\n(aes(x = group,\n\ny = value,\n\ncol = season,\n\nsize = n,\n\nalpha = 0.3))",
    "author": "dumpster_scuba",
    "timestamp": "2025-09-25T03:14:47",
    "url": "https://reddit.com/r/rstats/comments/1nq2wtz/geom_point_with_position_dodge_command_tilts_for/",
    "score": 1,
    "num_comments": 7,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1npg0rm",
    "title": "Ungrouping grouped bar plot in ggplot2",
    "content": "Hello!\n\nI'm looking to ungroup Letters A and D below so that the data is in ascending order per group (color) like the dataset is ordered in. I can't seem to figure it out and always appreciate the help on this thread! Thanks in advance!\n\n`mydata &lt;- data.frame(group = c(\"group1\", \"group1\", \"group1\", \"group2\", \"group2\", \"group3\", \"group3\", \"group3\", \"group3\", \"group4\", \"group5\",`\n\n`\"group5\", \"group5\", \"group5\", \"group5\", \"group5\", \"group5\", \"group6\", \"group6\"),`\n\n`Letter = c(\"A\", \"P\", \"G\", \"D\", \"H\", \"F\", \"A\", \"D\", \"B\", \"C\", \"E\", \"I\", \"O\",`\n\n`\"N\", \"D\", \"J\", \"K\", \"M\", \"L\"),`\n\n`depvar = c(19.18, 53.15, 54.51, 34.40, 51.61, 43.78, 47.71, 54.87, 62.77, 43.22, 38.78, 42.22, 48.15, 49.04, 56.32,`\n\n`56.08, 67.35, 34.28, 63.53))`\n\n`mydata$group &lt;- factor(mydata$group, levels = unique(mydata$group))`\n\n`mydata$Letter &lt;- factor(mydata$Letter, levels = unique(mydata$Letter))`\n\n`ggplot(mydata, aes(x = Letter, fill = group, y = depvar)) +`\n\n`geom_col(position = position_dodge2(width = 0.8, preserve = \"single\"), width = 1) +`\n\n`scale_fill_manual(values = c(\"#62C7FF\", \"#FFCC00\", \"#6AD051\", \"#DB1B43\", \"#F380FE\", \"#FD762B\") ) +`\n\n`geom_text(aes(label = depvar), position = position_dodge(width = 1), vjust = -0.25, size = 3) +`\n\n`xlab(\"Letter\") + ylab(\"Variable\") +`\n\n`theme(plot.margin = unit(c(1,0.5,0.5,0.5), 'cm')) +`\n\n`ylim(0, 70) +`\n\n`guides(fill = guide_legend(title = \"Group\"))`\n\nhttps://preview.redd.it/guogl1udx4rf1.png?width=1400&amp;format=png&amp;auto=webp&amp;s=7734b54f71f803ac42f0e969faecb5756e969255",
    "author": "IndividualPiece2359",
    "timestamp": "2025-09-24T08:59:17",
    "url": "https://reddit.com/r/rstats/comments/1npg0rm/ungrouping_grouped_bar_plot_in_ggplot2/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nppv8k",
    "title": "GPU parallel processing options?",
    "content": "I using the simr package to run power analyses for a study preregistration (analyses will use LME modeling). It's taking forever to run the simulations. What recommendations do people have for incorporating parallel processing into this? I've seen some options that use CPU cores, but before I try to figure them out, I'd love to know if there are any options that use GPU cores. I did some experimenting with a Python package a couple years ago (can't recall the name) that used GPU cores (using a 4070 GPU) and it was incredible how much faster it ran. \n\nI'd appreciate any recs people have! I can run these sims the old-fashioned way, but it would be better for my mental health if I could figure out something to make the process a little faster. Thanks!",
    "author": "diediedie_mydarling",
    "timestamp": "2025-09-24T15:20:11",
    "url": "https://reddit.com/r/rstats/comments/1nppv8k/gpu_parallel_processing_options/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nnuicz",
    "title": "R-package broadcast: Broadcasted Array Operations like NumPy",
    "content": "Hello R-users!\n\nI‚Äôm pleased to announce that the 'broadcast' R-package has been published on [CRAN](https://cran.r-project.org/package=broadcast).\n\n‚Äòbroadcast‚Äô is an efficient ‚ÄòC‚Äô/‚ÄòC++‚Äô - based ‚ÄòR‚Äô package that performs ‚Äúbroadcasting‚Äù - similar to broadcasting in the ‚ÄòNumpy‚Äô module for ‚ÄòPython‚Äô.\n\nIn the context of operations involving 2 (or more) arrays, ‚Äúbroadcasting‚Äù refers to efficiently recycling array dimensions without allocating additional memory.\n\nA Quick-Start guide can be found [here](https://tony-aw.github.io/broadcast/vignettes/b_quickstart.html).\n\nThe implementations available in 'broadcast' include, but are not limited to, the following:\n\n* Broadcasted element-wise operations on any 2 arrays; they support a large set of relational, arithmetic, Boolean, string, and bit-wise operations.\n* A faster, more memory efficient, and broadcasted abind()-like function, for binding arrays along an arbitrary dimension.\n* Broadcasted ifelse- and apply-like functions.\n* Casting functions that cast subset-groups of an array to a new dimension, or cast a nested list to a dimensional list ‚Äì and vice-versa.\n* A few linear algebra functions for statistics.  \n  \nBesides linking to ‚ÄòRcpp‚Äô, ‚Äòbroadcast‚Äô was developed from scratch and has no other dependencies nor does it use any other external library.\n\n[Benchmarks](https://tony-aw.github.io/broadcast/about/e_benchmarks_numpy.html) show that ‚Äòbroadcast‚Äô is about as fast as, and sometimes even faster than, ‚ÄòNumPy‚Äô.\n\nIf you appreciate ‚Äòbroadcast‚Äô, consider giving a star to its [GitHub](https://github.com/tony-aw/broadcast) page.",
    "author": "tony_aw",
    "timestamp": "2025-09-22T11:30:05",
    "url": "https://reddit.com/r/rstats/comments/1nnuicz/rpackage_broadcast_broadcasted_array_operations/",
    "score": 24,
    "num_comments": 5,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nnp3tt",
    "title": "TypR: a statically typed version of R",
    "content": "Hi everyone,\n\nI am working on TypR and integrated your feedbacks about its design. I feel it's getting to the right direction.\n\nI mainly simplified the syntax and the type system to make it easier to work with. If you can put a star on github it would be helpfulüôè\n\n[Github link](https://github.com/we-data-ch/typr)\n\n[Documentation link](https://fabricehategekimana.github.io/typr.github.io/build/)\n\n[Presentation video](https://youtu.be/xLoFlHbDQfc)\n\nMy Goal is to make it useful for the R community. Especially for package creators so I am open to your feedbacks\n\nThanks in advance!",
    "author": "Artistic_Speech_1965",
    "timestamp": "2025-09-22T08:09:54",
    "url": "https://reddit.com/r/rstats/comments/1nnp3tt/typr_a_statically_typed_version_of_r/",
    "score": 36,
    "num_comments": 7,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nnt7mg",
    "title": "rOpenSci Community Call - R-multiverse: a new way to publish R packages",
    "content": "Save the date!!\n\n* Next Community Call,¬†**R-multiverse: a new way to publish R packages**¬†with¬†[Will Landau](https://ropensci.org/author/will-landau/)\n* ¬†Monday,¬†**29 September 2025 14:00 UTC**¬†([find your local time](https://www.timeanddate.com/worldclock/fixedtime.html?msg=R-multiverse%3A+dual+publishing+for+R+packages&amp;iso=20250929T14&amp;p1=1440&amp;ah=1))\n* Information + How to join:¬†[rOpenSci | R-multiverse: a new way to publish R packages ¬∑ Community Call](https://ropensci.org/commcalls/r-multiverse/)\n\nPlease share this event with anyone who may be interested in the topic.  \nWe look forward to seeing you!",
    "author": "Glittering-Summer869",
    "timestamp": "2025-09-22T10:42:41",
    "url": "https://reddit.com/r/rstats/comments/1nnt7mg/ropensci_community_call_rmultiverse_a_new_way_to/",
    "score": 14,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nnpwz6",
    "title": "New R Consortium webinar: Modular, Interoperable, Extensible Topological Data Analysis in R",
    "content": "This R Consortium webinar will cover work from an R Consortium ISC grant project called ‚ÄúModular, interoperable, and extensible topological data analysis in R‚Äù starting in early 2024.\n\nThe goal of the project is to seamlessly integrate popular techniques from topological data analysis (TDA) into common statistical workflows in R. The expected benefit is that these extensions will be more widely used by non-specialist researchers and analysts, which will create sufficient awareness and interest in the community to extend the individual packages and the collection.\n\nAgenda\n* Introductions \n* What is topological data analysis? \n* How can R users do TDA? \n* Engines: {TDA} and {ripserr} \n* Utilities: {TDA} and {phutil} \n* Recipes: {TDAvec} and {tdarec} \n* Inference: {fdatest} and {inphr} \n* Invitations (an open invitation to the community to raise issues, contribute code) \n\nSpeakers\n\nJason Cory Brunson\nResearch Assistant Professor, University of Florida\nLaboratory for Systems Medicine, Division of Pulmonary, Critical Care, and Sleep Medicine\n\nAymeric Stamm\nResearch Engineer in Statistics, French National Centre for Scientific Research (CNRS), Nantes University\n\n---\nThis work with TDA for R is a prime example of how R Consortium‚Äôs technical grants don‚Äôt just fund projects ‚Äî they help integrate advanced methods into everyday workflows, make open-source tools more accessible, and support a stronger, more capable R ecosystem.\n\nüìÖ When: October 7, 2025\nüéØ What: Techniques like TDA, inference, and more, via packages like {TDA}, {ripserr}, {phutil}, {TDAvec}, {tdarec}, {fdatest}, {inphr} \nüë• Speakers: Jason Cory Brunson and Aymeric Stamm \n\nüîó Read more &amp; register: [https://r-consortium.org/webinars/modular-interoperable-extensible-topological-data-analysis-in-r.html](https://r-consortium.org/webinars/modular-interoperable-extensible-topological-data-analysis-in-r.html)",
    "author": "jcasman",
    "timestamp": "2025-09-22T08:40:19",
    "url": "https://reddit.com/r/rstats/comments/1nnpwz6/new_r_consortium_webinar_modular_interoperable/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1no2uh8",
    "title": "Issue opening/running R commander",
    "content": "I had trouble installing R-commander at first, so I downloaded R tools 45 and that seemed to work, but now I'm having trouble opening R commander itself\n\nLoading required package: splines  \nLoading required package: RcmdrMisc  \nLoading required package: car  \nLoading required package: carData  \nLoading required package: sandwich  \nLoading required package: effects  \nlattice theme set by effectsTheme()  \nSee ?effectsTheme for details.\n\nIdk how to fix the issue so if anyone's got any idea then lmk... btw im running the program from a windows device if that helps at all",
    "author": "LeftTable1551",
    "timestamp": "2025-09-22T17:10:02",
    "url": "https://reddit.com/r/rstats/comments/1no2uh8/issue_openingrunning_r_commander/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nmh6v4",
    "title": "ggplot2: Can you combine a table and a plot?",
    "content": "I want to create a figure that looks like this. Is this possible or do I have to do some Photoshopping?",
    "author": "Aryore",
    "timestamp": "2025-09-20T20:08:50",
    "url": "https://reddit.com/r/rstats/comments/1nmh6v4/ggplot2_can_you_combine_a_table_and_a_plot/",
    "score": 82,
    "num_comments": 32,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nlf1q6",
    "title": "[E] Roof renewal - effect on attic temperature",
    "content": "",
    "author": "peperazzi74",
    "timestamp": "2025-09-19T13:48:32",
    "url": "https://reddit.com/r/rstats/comments/1nlf1q6/e_roof_renewal_effect_on_attic_temperature/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nlai5k",
    "title": "Where to focus efforts when improving stats and coding",
    "content": "21M\n\nSenior in college\n\nBS in neuroscience\n\nRealize quite late I am good at math, stats, and decent at coding\n\nThink: perhaps should have focused more energy there, perhaps a math major? \nToo late to worry about such shoulda coulda wouldas \n\nCurrently: Applying to jobs in LifeSci consulting to jump start career\n\nWondering: If I want to boost my employability in the future and move into data science, stats, ML, and AI, where should I focus my efforts once I‚Äôm settled at an entry level job to make my next moves? MS? PhD? Self Learning? Horizontal moves?\n\nRelevant Courses:\nCalc 1\nCalc 2 \nMulti Var Calc\nLinear Algebra\nStats 1\nEconometrics\nMaker Electronics in Python\nExperimental statistic in R\n\nGoal? Be a math wiz and use skills to boost career prospects in data science üòé\n\nAny advice would beüî• ",
    "author": "g-Eagle45",
    "timestamp": "2025-09-19T10:52:31",
    "url": "https://reddit.com/r/rstats/comments/1nlai5k/where_to_focus_efforts_when_improving_stats_and/",
    "score": 7,
    "num_comments": 4,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nln3us",
    "title": "Trouble with summarize() function",
    "content": "",
    "author": "EFB102404",
    "timestamp": "2025-09-19T19:58:54",
    "url": "https://reddit.com/r/rstats/comments/1nln3us/trouble_with_summarize_function/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nl2m0i",
    "title": "Question about assignment by reference (data.table)",
    "content": "I've just had some of my code exhibit behavior I was not expecting. I knew I was probably flying too close to the sun by using assignment by reference within some custom functions, without fully understanding all its vagaries. But, I want to understand what is going on here for future reference. I've spent some time with the relevant documentation, but don't have a background in comp sci, so some of it is going over my head.\n\n`func &lt;- function(x){`\n\n  `y &lt;- x`\n\n  `y[, a := a + 1]`\n\n`}`\n\n`x &lt;- data.table(a = c(1, 2, 3))`\n\n`x`\n\n`func(x)`\n\n`x`\n\nWhy does x get updated to c(2, 3, 4) here? I assumed I would avoid this by copying it as y, and running the assignment on y. But, that is not what happened.",
    "author": "Black_Bear_US",
    "timestamp": "2025-09-19T05:46:48",
    "url": "https://reddit.com/r/rstats/comments/1nl2m0i/question_about_assignment_by_reference_datatable/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nl78pe",
    "title": "A new interpretable clinical model. Tell me what you think",
    "content": "Hello everyone, I wrote an article about how an XGBoost can lead to clinically interpretable models like mine. Shap is used to make statistical and mathematical interpretation viewable",
    "author": "ksrio64",
    "timestamp": "2025-09-19T08:50:18",
    "url": "https://reddit.com/r/rstats/comments/1nl78pe/a_new_interpretable_clinical_model_tell_me_what/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nke54n",
    "title": "R6 Questions - DRY principle? Sourcing functions? Unit tests?",
    "content": "Hey everyone,\n\nI am new to R6 and I was wondering how to do a few things as I begin to develop a little package for myself. The extent of my R6 knowledge comes from the *Object-Oriented Programming with R6 and S3 in R* course on DataCamp. \n\nMy first question is about adherence to the DRY principle. In the DataCamp course, they demonstrated some getter/setter functions in the `active` binding section of an R6 class, wherein each private field was given its own function. This seems to be unnecessarily repetitive as shown in this code block:\n\n    MyClient &lt;- R6::R6Class(\n      \"MyClient\",\n      private = list(\n        ..field_a = \"A\",\n          ...\n        ..field_z = \"Z\"\n      )\n    \n      active = list(\n        field_a = function(value) {\n          if (!missing(value)) {\n            private$..field_a\n           } else {\n            private$..field_a &lt;- value\n           }\n        },\n          ...\n        field_z = function(value) {\n          if (!missing(value)) {\n            private$..field_z\n           } else {\n            private$..field_z &lt;- value\n           }\n        },\n      )\n    )\n\nIs it possible (recommended?) to make one general function which takes the field's name and the value? I imagine that you might not want to expose all fields to the user, but could this not be restricted by a conditional (e.g. `if (name %in% private_fields) message(\"This is a private field\")`) ?\n\nSecond question: I imagine that when my class gets larger and larger, I will want to break up my script into multiple files. Is it possible (or recommended?, again) to source functions into the class definition? I don't expect, with this particular package, to have a need for inheritance. \n\nFinal question: Is there anything I should be aware of when it comes to unit tests with `testthat`? I asked Google's LLM about it and it gave me a code snippet where the class was initialized and then the methods tested from there. For example, \n\n    testthat(\"MyClient initializes correctly\", {\n      my_client &lt;- MyClient$new()\n      my_client$field_a &lt;- \"AAA\"\n      expect_equal(my_client$field_a, \"AAA\")\n    })\n\nThis looks fine to me but I was wondering, related to the sourcing question above, whether the functions themselves can or should be tested directly and in isolation, rather than part of the class. \n\nAny wisdom you can share with R6 development would be appreciated!\n\nThanks for your time,\n\nAGranFalloon",
    "author": "AGranfalloon",
    "timestamp": "2025-09-18T10:10:17",
    "url": "https://reddit.com/r/rstats/comments/1nke54n/r6_questions_dry_principle_sourcing_functions/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nkgd5b",
    "title": "R for medical statistics",
    "content": "Hi everyone! \n\nI am a medical resident and working on a project where I need to develop a predictive clinical score. This involves handling patient-level data and running regression analyses.\nI‚Äôm a complete beginner in R, but I‚Äôd like to learn it specifically from the perspective of medical statistics and clinical research ‚Äî not just generic coding.\n\nCould anyone recommend good resources, online courses, or YouTube playlists that are geared toward clinicians/biostatistics in medicine using R? \n\nThanks in advance! \n\n",
    "author": "pilot_v7",
    "timestamp": "2025-09-18T11:32:58",
    "url": "https://reddit.com/r/rstats/comments/1nkgd5b/r_for_medical_statistics/",
    "score": 2,
    "num_comments": 12,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nkdjvx",
    "title": "Interview Help - R focused Role",
    "content": "",
    "author": "bass581",
    "timestamp": "2025-09-18T09:48:28",
    "url": "https://reddit.com/r/rstats/comments/1nkdjvx/interview_help_r_focused_role/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nk9p4p",
    "title": "DHARMa Plots - Element Blood Concentration Data",
    "content": "I've had trouble finding examples of this in the vignettes and faq, so I'm hoping someone might help clarify things for me.  The model is running a GLMM.  The response variable is blood concentration (ppm; ex: 0.005 - 0.03) and the two predictor variables are counts of different groups of food (ex: 0 - 12 items for group A).  The concentration data is right skewed.  The counts of food groups among subjects are also right skewed though closer to a normal dist. than the concentration data.\n\n1. Is it correct to say in the first pair of diagnostic plots, (QQ plot) the residuals deviate from the Normal family distribution used (KS test is significant) and (Qu Dev. plot) that the residuals have less variation than would be expected from the quantile simulation (the clustering of points between the 0.25 and 0.5, or even between 0.25 and 0.75)?\n2. Does anyone know of a good resource that discusses the limitations that are imposed on a glmm (ex: where assumptions are violated, etc.) when the response variable shows 'minimal' variation?  I log-transformed the response, the plots look good and I intuitively understand the issue with a response that may have little variation but am having trouble solidifying the idea conceptually.\n\nhttps://preview.redd.it/bn6ug4fknxpf1.jpg?width=522&amp;format=pjpg&amp;auto=webp&amp;s=042e845a5e2d105f5e779c2700bc158b5e184b4e\n\n",
    "author": "LanternBugz",
    "timestamp": "2025-09-18T07:23:56",
    "url": "https://reddit.com/r/rstats/comments/1nk9p4p/dharma_plots_element_blood_concentration_data/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nk1l9g",
    "title": "MCPR: How to talk with your data",
    "content": "A few people asked me how MCPR works and what it looks like to use it, so I made a short demo video. This is what conversational data analysis feels like: I connect Claude to my live R session and just talk to the data. I ask it to load, transform, filter, and plot‚Äîand watch my requests become reality. It‚Äôs like having a junior analyst embedded directly in your console, turning natural language intent into executed code. Instead of copy-pasting or re-running scripts, I stay focused on the analytical questions while the agent handles the mechanics.\n\nThe 3.5-minute video is sped up 10x to show just how much you can get done (I can share the full version if you request).\n\nPlease, let me know what do you think. Do you see yourself interacting with data like this? Do you think it will speed you up? I look forward to your thoughts!\n\n  \nIf you do data analysis and would like to give it a try, here is the repo:¬†[https://github.com/phisanti/MCPR](https://github.com/phisanti/MCPR)\n\nSince this sub-reddit does not allow the use videos, I have placed the video in the MCP community: [https://www.reddit.com/r/mcp/comments/1nk1ggp/mcpr\\_how\\_to\\_talk\\_with\\_your\\_data/](https://www.reddit.com/r/mcp/comments/1nk1ggp/mcpr_how_to_talk_with_your_data/)\n\nu/AI_Tonic  \nu/[techlatest\\_net](https://www.reddit.com/user/techlatest_net/)  \n\n\n",
    "author": "Unable_Huckleberry75",
    "timestamp": "2025-09-18T00:12:42",
    "url": "https://reddit.com/r/rstats/comments/1nk1l9g/mcpr_how_to_talk_with_your_data/",
    "score": 3,
    "num_comments": 10,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nk3cs0",
    "title": "How to handle noisy data in timeseries analysis",
    "content": "",
    "author": "constantLearner247",
    "timestamp": "2025-09-18T02:10:00",
    "url": "https://reddit.com/r/rstats/comments/1nk3cs0/how_to_handle_noisy_data_in_timeseries_analysis/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1njd6pi",
    "title": "Github rcode/data repository question",
    "content": "I guess this isnt an R question per se, but I work almost exclusively in R so figured I might get some quality feedback here. For people who put their code and data on github as a way to make your research more open science, are you just posting it via the webpage as one time upload, or are you pushing it from folders on your computer to github. Im not totally sure what the best practice is here or if this question is even framed correctly. ",
    "author": "[deleted]",
    "timestamp": "2025-09-17T06:28:47",
    "url": "https://reddit.com/r/rstats/comments/1njd6pi/github_rcodedata_repository_question/",
    "score": 9,
    "num_comments": 16,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1njejd1",
    "title": "Cross-level interaction in hierarchical linear model: significant despite overlapping CIs?",
    "content": "Hey community,\n\nI am a social sciences student and am conducting a statistical analysis for my term paper. The technical details are not that important, so I will try to explain all the important technical aspects quickly:\n\nI am conducting a hierarchical linear regression (HLM) with three levels. Individuals (level 1) are nested in country-years (level 2), which are nested in countries (level 3). Almost all of my predictors are at level 1, except for the variable `wgi_mwz`, which is at the country level. In my most complex model, I perform a cross-level interaction between a Level 1 variable and `wgi\\_mwz`. This is the code for the model:\n\n    hlm3 &lt;- lmer(ati ~ 1 + class_low + class_midlow + class_mid + class_midhigh + \n    wgi_mwz + \n    educ_low + educ_high + \n    lrscale_mwz + \n    res_mig + m_mig + f_mig + \n    trust_mwz + \n    age_mwz + \n    male + \n    wgi_mwz*class_low + wgi_mwz*class_midlow + wgi_mwz*class_mid + wgi_mwz*class_midhigh + \n    (1 | iso/cntryyr), data)\n\nThe result of `summary(hlm3)` ishows that the interactions are significant (p&lt;0.01). Since I always find it a bit counterintuitive to interpret interaction effects from the regression table, I plotted the interactions and attached one of those plots.\n\nMy statistical knowledge is not the best (I am studying social sciences at bachelor's level), but since the confidence intervals overlap, it cannot be said with 95% certainty that the slopes differ significantly from each other, which would mean that the `class_low` variable has no influence on the effect of `wgi\\_mwz` on `ati`. But the Regression output suggests that the Interaction is in fact significant, so I really dont know how to interpret this.\n\nIf anyone can help me, that would be great! I appreciate any help.",
    "author": "KokainKevin",
    "timestamp": "2025-09-17T07:23:08",
    "url": "https://reddit.com/r/rstats/comments/1njejd1/crosslevel_interaction_in_hierarchical_linear/",
    "score": 7,
    "num_comments": 11,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1njqoux",
    "title": "Looking for 1 minute intraday OHLC data",
    "content": "Hi everyone, I need 1minute OHLC data for the following indices DJIA, Nasdaq, FTSE, Nifty50 and DAX. I tried MT5, TradingView, Yahoo Finance but it‚Äôs insufficient. I searched Google, and FirstRate data seems to be selling what I‚Äôm looking for. However, they would only provide 10-15 years of data, not exceeding 2009. So, that option‚Äôs ruled out. Can anyone suggest a good data source I can use? Free or paid. Thanks.",
    "author": "dukelynus",
    "timestamp": "2025-09-17T15:02:58",
    "url": "https://reddit.com/r/rstats/comments/1njqoux/looking_for_1_minute_intraday_ohlc_data/",
    "score": 1,
    "num_comments": 8,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1njbg5g",
    "title": "Data repository suggestions for newbie",
    "content": "Hello kind folk. I'm submitting a manuscript for publication soon and wanted to upload all the data and code to go with it on an open source repository. This is my first time doing so and I wanted to know what is the best format to 1) upload my data (eg, .xlsx, .csv, others?) and 2), to which repository (eg, Github)? Ideally, I would like it to be accessible in a format that is not restricted to R, if possible. Thank you in advance.   ",
    "author": "traditional_genius",
    "timestamp": "2025-09-17T05:13:54",
    "url": "https://reddit.com/r/rstats/comments/1njbg5g/data_repository_suggestions_for_newbie/",
    "score": 6,
    "num_comments": 16,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1njerdm",
    "title": "Dusting off an old distill blog, worth porting over to Quarto?",
    "content": "I have a personal distill blog that I haven‚Äôt touched in a few years. Is it worth porting it over to Quarto? Interested in people‚Äôs experiences and any ‚Äòbetter‚Äô options. ",
    "author": "dovertoo",
    "timestamp": "2025-09-17T07:31:42",
    "url": "https://reddit.com/r/rstats/comments/1njerdm/dusting_off_an_old_distill_blog_worth_porting/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nit2ht",
    "title": "R Template Ideas",
    "content": "Hey All,\n\n\n\nI'm new to data analytics and R. I'm trying to create a template for R scripts to help organize code and standardize processes.\n\nAny feedback or suggestions would be highly appreciated.\n\n\n\nHere's what I've got so far.\n\n\n\n\\# &lt;Title&gt;\n\n\n\n\\## Install &amp; Load Packages\n\n\n\ninstall.packages(&lt;package name here&gt;)\n\n.\n\n.\n\n\n\nlibrary(&lt;package name here&gt;)\n\n.\n\n.\n\n\n\n\n\n\\## Import Data\n\n\n\nlibrary or read.&lt;file type&gt;\n\n\n\n\n\n\\## Review Data\n\n¬†¬†\n\nView(&lt;insert data base here&gt;)\n\n\n\nglimpse(&lt;insert data base here&gt;)\n\ncolnames(&lt;insert data base here&gt;)\n\n\n\n\n\n\\## Manipulate Data? Plot Data? Steps? (I'm not sure what would make sense here and beyond)",
    "author": "amp_one",
    "timestamp": "2025-09-16T13:38:05",
    "url": "https://reddit.com/r/rstats/comments/1nit2ht/r_template_ideas/",
    "score": 4,
    "num_comments": 21,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1niv1iy",
    "title": "üëâ R Consortium webinar: How to Use pointblank to Understand, Validate, and Document Your Data",
    "content": "",
    "author": "jcasman",
    "timestamp": "2025-09-16T14:54:43",
    "url": "https://reddit.com/r/rstats/comments/1niv1iy/r_consortium_webinar_how_to_use_pointblank_to/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nivp70",
    "title": "Issues Formatting Axes Text Size in Likert bar Plot (likert) package.. Help?",
    "content": "Hi All!\n\n  \nI'm plotting some of my likert data (descriptive percentages) using the likert package in r. I would consider myself a beginner with R, having learned a little in undergrad and stumbling my way through code I find online when I need to run a specific analysis. I have a few graphs (centered stacked bar charts) I've made using the likert package but I can't seem to change the text size from my values outside of the graph (x-axis, y-axis, and legend). I followed a tutorial online for the workaround using fake data because the likert package is really picky about each column having the same number of levels/values, so if a question never got a 1 on a likert scale it wouldn't run it. \n\nI've tried structuring it or changing it like you would ggplot but it only changes the percentages within the graph (showing percentage negative, neutral and positive responses). So my y-axis labels are quite small and I know I'll get asked to increase their text size for readability. Would anyone be willing to help me figure out how I can adjust the text using the likert bar plot? TIA!\n\n  \nHere's the code I'm using.\n\n    support &lt;- Full_Survey1 %&gt;%\n    select(How_likely_Pre_message, How_likely_post_message)\n    \n    support &lt;- support %&gt;%\n    mutate(ResponseID = row_number())\n    \n    support_df &lt;- as.data.frame(support)\n    \n    ResponseID &lt;- c(\"1138\", \"1139\", \"1140\", \"1141\", \"1142\")\n    How_likely_Pre_message &lt;- c(1, 2, 3, 4, 5)\n    How_likely_post_message &lt;- c(1, 2, 3, 4, 5)\n    fake_support &lt;- data.frame(ResponseID, How_likely_Pre_message, How_likely_post_message)\n    \n    support2 &lt;- rbind(support_df, fake_support)\n    support2$How_likely_Pre_message_f &lt;- as.factor(support2$How_likely_Pre_message)\n    support2$How_likely_post_message_f &lt;- as.factor(support2$How_likely_post_message)\n    \n    factor_levels &lt;- c(\"Extremely unlikely\", \"Somewhat unlikely\", \"Neither unlikely nor likely\", \"Somewhat likely\", \"Extremely likely\")\n    levels(support2$How_likely_Pre_message_f) &lt;- factor_levels\n    levels(support2$How_likely_post_message_f) &lt;- factor_levels\n    \n    support2$ResponseID &lt;- as.numeric(support2$ResponseID) #Issue here with values being chr\n    \n    #Removes the fake data \n    nrow(support2)\n    support3 &lt;- subset(support2, ResponseID &lt; 1138)\n    nrow(support3)\n    \n    #Removes the original columns and pulls out those converted to factor above\n    colnames(support3)\n    support4 &lt;- support3[,4:5]\n    colnames(support4)\n    \n    \n    VarHeadings &lt;- c(\"Support pre-message\", \"Support post-message\")\n    names(support4) &lt;- VarHeadings\n    colnames(support4)\n    \n    library(likert)\n    library(gridExtra) #Needed to use gridExtra to add a title. Normal ggplot title coldn't be centered at all and it annoyed me\n    library(grid)\n    \n    p &lt;- likert(support4)\n    a &lt;- likert.bar.plot(\n    ¬†¬†p,\n    ¬†¬†legend.position = \"right\",\n    ¬†¬†text.size = 4\n    ) +\n    ¬†¬†theme_classic()\n    \n    # Centered title with grid.arrange\n    grid.arrange(\n    ¬†¬†a,\n    ¬†¬†top = textGrob(\n    ¬†¬†¬†¬†\"Support Pre- and Post- Message Exposure\",\n    ¬†¬†¬†¬†gp = gpar(fontsize = 16, fontface = \"bold\"),\n    ¬†¬†¬†¬†hjust = 0.5, ¬† ¬† ¬† # horizontal centering\n    ¬†¬†¬†¬†x = 0.5¬† ¬† ¬† ¬† ¬† ¬† # place at center of page\n    ¬†¬†)\n    )\n\n\n\n\n\n",
    "author": "Miserable_Amoeba8766",
    "timestamp": "2025-09-16T15:21:12",
    "url": "https://reddit.com/r/rstats/comments/1nivp70/issues_formatting_axes_text_size_in_likert_bar/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ni9mza",
    "title": "New R package: kerasnip (tidymodels + Keras bridge)",
    "content": "I found a new package called [**kerasnip**](https://github.com/davidrsch/kerasnip) that connects **Keras models** with the **tidymodels/parsnip** framework in R.\n\nIt lets you define Keras layer ‚Äúblocks,‚Äù build sequential or functional models, and then tune/train them just like any other tidymodels model. Docs here: [davidrsch.github.io/kerasnip](https://davidrsch.github.io/kerasnip/).\n\nLooks promising for integrating deep learning into tidy workflows. Curious what others think!",
    "author": "FriendlyAd5913",
    "timestamp": "2025-09-15T23:07:27",
    "url": "https://reddit.com/r/rstats/comments/1ni9mza/new_r_package_kerasnip_tidymodels_keras_bridge/",
    "score": 14,
    "num_comments": 1,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nif0zx",
    "title": "Wrong Likert Scale- Thesis Research",
    "content": "I am currently conducting data analysis for my honours thesis. I just realised I made a horribly stupid mistake. One of the scales I'm using is typically rated on a 7-point or 4-point Likert scale. I remember following the format of the 7-point Likert scale (Strongly Disagree, Disagree, Somewhat Disagree, Neither Agree nor Disagree, Somewhat Agree, Agree, Strongly Agree), but instead I input a 5-point Likert scale (Strongly Disagree, Somewhat Disagree, Neither Agree nor Disagree, Somewhat Agree, Strongly Agree).\n\nThis was a stupid mistake on my part that I completely overlooked. I was so preoccupied with assignments and other things that I just assumed it was correct.\n\nI have no idea how I can fix this. I can recode the scales, but I'm assuming that will just ruin my data. My supervisor asked if I could recode it on a 4-point Likert scale and suggested that I shouldn't recode it to a 7-point scale.\n\nHow do I go about this? How do I explain and justify this in my thesis? I would greatly appreciate any advice!",
    "author": "Adorable-Lie1355",
    "timestamp": "2025-09-16T04:39:15",
    "url": "https://reddit.com/r/rstats/comments/1nif0zx/wrong_likert_scale_thesis_research/",
    "score": 3,
    "num_comments": 8,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nhn67s",
    "title": "GGplot2 4.0.0",
    "content": "",
    "author": "Tarqon",
    "timestamp": "2025-09-15T07:20:24",
    "url": "https://reddit.com/r/rstats/comments/1nhn67s/ggplot2_400/",
    "score": 138,
    "num_comments": 4,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ni70a0",
    "title": "Emacs Treesitter for R",
    "content": "I am developing an Emacs Major Mode to use treesitter with R and ESS. I've been using it for over 2 weeks now and it is looking good, but it would greatly benefit from feedback to solve bugs and add features faster. So, if you would like to try it and help it grow, leave me a message or feel free to grab it directly and open issues in the git repository:\n\nhttps://codeberg.org/teoten/esr",
    "author": "teobin",
    "timestamp": "2025-09-15T20:40:20",
    "url": "https://reddit.com/r/rstats/comments/1ni70a0/emacs_treesitter_for_r/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nhds1u",
    "title": "I got feed up of AI agents running RScript for every command, so I built: MCPR",
    "content": "**TL;DR:** AI agents for R are stateless and force you to re-run your whole script for a tiny change. I built an R package called MCPR that lets an AI agent connect to your live, persistent R session, so it can work with you without destroying your workspace. [GitHub Repo](https://github.com/phisanti/MCPR)\n\nHey everyone,\n\nLike many of you, I've been trying to integrate tools like Claude and Copilot into my R workflow. And honestly, it's been maddening.\n\nYou've got two terrible options:\n\n**The Copy-Paste Hell:** You ask a chatbot a question, it gives you a code snippet, you paste it into RStudio, run it, copy the result/error, paste it back into the chat, and repeat. It's slow and you're constantly managing context yourself.\n\n**The \"Stateless\" Agent:** You use a more advanced agent, but it just calls `Rscript` for every. single. command. Need to change a ggplot color theme? Great, the agent will now re-run the entire 20-minute data loading and modeling pipeline just for that one `theme()` call.\n\nI got so fed up with this broken workflow that I spent the last few months building a solution.\n\n# The Solution: MCPR (Model Context Protocol for R)\n\nMCPR is a practical framework that enables AI agents to establish persistent, interactive sessions within a live R environment. It exposes the R session as a service that agents can connect to, discover, and interact with.\n\nThe core of MCPR is a minimal, robust toolset exposed to the agent via a clear protocol.\n\n    # 1. Install from GitHub\n    remotes::install_github(\"phisanti/MCPR\")\n    MCPR::install_mcpr('your_agent')\n    # 2. Start a listener in your R console\n    library(MCPR)\n    mcpr_session_start()\n\nNow you can say things like:\n\n* \"Filter the results\\_df dataframe for values greater than 50 and show me a summary.\"\n* \"Take the final\\_model object and create a residual plot.\"\n* \"What packages do I have loaded right now?\"\n\nThe agent executes the code in your session, using the objects you've already created. No more re-running everything from scratch.\n\nThe project is still experimental, but the core functionality is solid. I believe this model of treating the IDE session as a long-lived server for an AI client is a much more effective paradigm for collaborative coding.\n\nI'm looking for feedback, especially on the protocol design and tool interface. Pull requests and issues are very welcome.\n\nGitHub (Source &amp; README): [https://github.com/phisanti/MCPR](https://github.com/phisanti/MCPR)\n\nThanks for checking it out. I'll be in the comments to discuss the implementation details.\n\nhttps://i.redd.it/flj8a4s5p9pf1.gif\n\n",
    "author": "Unable_Huckleberry75",
    "timestamp": "2025-09-14T22:49:15",
    "url": "https://reddit.com/r/rstats/comments/1nhds1u/i_got_feed_up_of_ai_agents_running_rscript_for/",
    "score": 26,
    "num_comments": 18,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nhh2lc",
    "title": "No interaction in Lmer model on R, what should I do ?",
    "content": "I am using an lmer model to calculate interactions between factor A (before-after) and factor B (3 groups). When I find no interaction (which is the case for one of my very important dependent variables), what should I do? Is it possible to perform emmeans-type contrast calculations, or is this considered inappropriate in scientific literature? ",
    "author": "Dear_Wall_229",
    "timestamp": "2025-09-15T02:21:55",
    "url": "https://reddit.com/r/rstats/comments/1nhh2lc/no_interaction_in_lmer_model_on_r_what_should_i_do/",
    "score": 3,
    "num_comments": 14,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nh5fhl",
    "title": "Time series",
    "content": "https://preview.redd.it/wur4weoam7pf1.png?width=1138&amp;format=png&amp;auto=webp&amp;s=25629806f47fcadd244f2da47ec76521831e409a\n\nHi!! I'm trying to get a time series investigation done, and I'm a little bit confused by this number, representing the seasonal value. What does this mean, and I have I likely done something wrong?  \n",
    "author": "SandwichOpposite3274",
    "timestamp": "2025-09-14T15:51:15",
    "url": "https://reddit.com/r/rstats/comments/1nh5fhl/time_series/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nh319g",
    "title": "best AI for writing R code (if you can‚Äôt code at all)",
    "content": "I took a coding class last semester and basically learnt nothing! And anything I did learn has completely disappeared from my mind over the last few months. \n\nI am currently faced with the issue of needing to complete an assignment based around coding and data analysis and I don‚Äôt have a clue. \n\nDue to my own personal stupidity I have around 10 days to write the code and the accompanying 6000 word report. \n\nI currently have a subscription to Claude, but is it worth my while getting another one for a month to more coding focused AI? Is there a specific Claude model I should be using? \n\nAny help is much appreciated!! \n\nTIA",
    "author": "Hairy_Turnip719",
    "timestamp": "2025-09-14T14:11:15",
    "url": "https://reddit.com/r/rstats/comments/1nh319g/best_ai_for_writing_r_code_if_you_cant_code_at_all/",
    "score": 0,
    "num_comments": 18,
    "upvote_ratio": 0.19,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ng6fl2",
    "title": "A rather unusual question - Recovering lost images‚Ä¶",
    "content": "Hello, everyone,\n\nI recently lost my laptop and some important data, which has left me using a very slow, ancient one.\n\nThe problem is: I created high-resolution figures in the TIFF format using R for a manuscript. Unfortunately, these files were on my old laptop and are now gone. However, I have a Word document where I pasted these figures for documentation. When I tried to save the images from the Word file, their resolution was significantly reduced, making them unusable for publication.\n\nSo‚Ä¶ My questions:\n\nIs there any method to recover these figures from the Word document in their original high-resolution quality and TIFF format?\n\nI still have my R script and .Rhistory files. Is there any way that the figures might be saved internally within R or an associated directory?\nThese might be a stupid questions, but I'm in a desperate situation with a tight deadline and would greatly appreciate any feedback, even if the answer is a simple \"no.‚Äú , then, I will accept my fate, haha.\n\nThank you for your time in advance!",
    "author": "grimfandangolupe",
    "timestamp": "2025-09-13T12:27:16",
    "url": "https://reddit.com/r/rstats/comments/1ng6fl2/a_rather_unusual_question_recovering_lost_images/",
    "score": 5,
    "num_comments": 11,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nglzvt",
    "title": "School Help",
    "content": "I'm sure the solution to this is simple, but I'm all the way lost.\n\nI am meant to provide the mean, sds, min, and max of lifeexp for all the countries listed in the gapminder\\_df. However, no matter what I adjust, when I run the code, they are still grouped by continent. \n\nSorry for the shady Reddit account... I never use Reddit on my desktop.",
    "author": "Delicious_Gap2302",
    "timestamp": "2025-09-14T01:37:01",
    "url": "https://reddit.com/r/rstats/comments/1nglzvt/school_help/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ngh4ub",
    "title": "RStudio AI Assistant - Clean, Code, Analyze, Debug with RgentAI",
    "content": "RgentAI is an AI assistant, powered by Claude, that integrates directly into RStudio to provide AI assistance with coding, data cleaning, modelling and analysis, interpretation, bug checking, and more. In this video I test a range of features and was impressed by the outcomes. ",
    "author": "MagicandHearthstone",
    "timestamp": "2025-09-13T20:49:33",
    "url": "https://reddit.com/r/rstats/comments/1ngh4ub/rstudio_ai_assistant_clean_code_analyze_debug/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.17,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nf4cgc",
    "title": "A Good Read",
    "content": "[https://eddelbuettel.github.io/ldlasb2/benchmarks.html](https://eddelbuettel.github.io/ldlasb2/benchmarks.html)",
    "author": "BOBOLIU",
    "timestamp": "2025-09-12T07:12:50",
    "url": "https://reddit.com/r/rstats/comments/1nf4cgc/a_good_read/",
    "score": 10,
    "num_comments": 4,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nexsqu",
    "title": "ANOVA confusion: numeric vs factor in R",
    "content": "Hi everyone, thanks in advance for any hints!\n\nI‚Äôm analyzing an experiment where I test measurements in relation to temperature and light. I just want to know if there‚Äôs any effect at all.\n\n* **Light** is clearly a factor (HL, ML, ...). (called groupL)\n* **Temperature** is technically numeric (5, 10, ... ¬∞C), but in a two-way ANOVA it should probably be treated as a factor. (called temp)\n\nI noticed that using R, `anova_test()` and `aovperm()` give different results depending on whether I treat temperature as numeric or factor. From what I‚Äôve read, when temperature is numeric, R seems to test for a linear increase/decrease ‚Äî but that‚Äôs not really ANOVA, is it? More like ANCOVA?\n\nHere are example outputs from `aovperm()` with temperature as numeric vs factor. In both cases, the output is labeled ‚ÄúANOVA.‚Äù\n\nTemperature numeric\n\n    Anova Table\n    Resampling test using freedman_lane to handle nuisance variables and 1e+06 permutations.\n                      SS df      F parametric P(&gt;F) resampled P(&gt;F)\n    temp         0.35266  1 1.6946           0.1976          0.1979\n    groupL       0.09831  2 0.2362           0.7903          0.7902\n    temp:groupL  0.37523  2 0.9015           0.4110          0.4121\n    Residuals   13.52697 65\n\nTemperature faktor\n\n    Anova Table\n    Resampling test using freedman_lane to handle nuisance variables and 1e+06 permutations.\n                     SS df      F parametric P(&gt;F) resampled P(&gt;F)\n    temp         0.4733  3 0.7109         0.549344        0.552214\n    groupL       3.2963  2 7.4267         0.001328        0.000959\n    temp:groupL  0.6860  6 0.5152         0.794456        0.797242\n    Residuals   13.0932 59\n\nAs a beginner in statistics, can someone explain this ‚Äúchaos‚Äù in simple terms and confirm that using `as.factor()` for temperature is the safe approach when performing a two-way ANOVA?",
    "author": "diver_0",
    "timestamp": "2025-09-12T01:36:05",
    "url": "https://reddit.com/r/rstats/comments/1nexsqu/anova_confusion_numeric_vs_factor_in_r/",
    "score": 9,
    "num_comments": 10,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ndegps",
    "title": "R Markdown (beginner) question",
    "content": "https://preview.redd.it/f30ibwambcof1.png?width=1706&amp;format=png&amp;auto=webp&amp;s=e7510a7a5c29423f61c321bc790505679af7ec4f\n\nHi! I‚Äôm trying to create a regression line/linear model(?) in this scatterplot, but I can‚Äôt get it to work. When I use the lm function, I get 5 ‚Äúplots.‚Äù I‚Äôm working on a MacBook.  \nDoes anyone know why 5 plots are showing up and not a linear model? Thanks for any help and tips :)",
    "author": "the_ain",
    "timestamp": "2025-09-10T06:38:37",
    "url": "https://reddit.com/r/rstats/comments/1ndegps/r_markdown_beginner_question/",
    "score": 8,
    "num_comments": 13,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nckyv8",
    "title": "I made an R package to query data in Microsoft Fabric",
    "content": "https://github.com/KennispuntTwente/fabricQueryR",
    "author": "Ok_Sell_4717",
    "timestamp": "2025-09-09T07:48:42",
    "url": "https://reddit.com/r/rstats/comments/1nckyv8/i_made_an_r_package_to_query_data_in_microsoft/",
    "score": 28,
    "num_comments": 7,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ncj5jp",
    "title": "Package that tells you the outcome of a join (and other functions)",
    "content": "I used to use a helper package that would tell you the outcome of certain dplyr functions in red text in the console. It was particularly useful for joins - it would tell you how many records from each data frame had been joined/not joined. I‚Äôve moved jobs and had a bit of a break from writing code. I now cannot for the life of me remember the name of said package, and I‚Äôve had no joy with Google either. \n\nDoes anyone know the one I‚Äôm looking for? ",
    "author": "Slight-Elderberry421",
    "timestamp": "2025-09-09T06:37:24",
    "url": "https://reddit.com/r/rstats/comments/1ncj5jp/package_that_tells_you_the_outcome_of_a_join_and/",
    "score": 26,
    "num_comments": 3,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nbpe3m",
    "title": "Agents in RStudio",
    "content": "Hey everyone! Over the past month, I‚Äôve built five specialized agents in RStudio that run directly in the Viewer pane. These agents are contextually aware, equipped with multiple tools, and can edit code until it works correctly. The agents cover data cleaning, transformation, visualization, modeling, and statistics.\n\nI‚Äôve been using them for my PhD research, and I can‚Äôt emphasize enough how much time they save. They don‚Äôt replace the user; instead, they speed up tedious tasks and provide a solid starting framework.\n\nI have used Ellmer, ChatGPT, and Copilot, but this blows them away. None of those tools have both context and tools to execute code/solve their own errors while being fully integrated into RStudio. It is also just a package installation once you get an access code from my website. I would love for you to check it out and see how much it boosts your productivity!¬†The website is in the comments below",
    "author": "Puzzleheaded_Bid1535",
    "timestamp": "2025-09-08T07:34:45",
    "url": "https://reddit.com/r/rstats/comments/1nbpe3m/agents_in_rstudio/",
    "score": 79,
    "num_comments": 19,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nbijyg",
    "title": "A bare-bones TVM calculator in R",
    "content": "",
    "author": "DracoMilfoy69",
    "timestamp": "2025-09-08T01:52:10",
    "url": "https://reddit.com/r/rstats/comments/1nbijyg/a_barebones_tvm_calculator_in_r/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1nb1ncs",
    "title": "Bioinformatics Help",
    "content": "I'm desperate for help since my lab has no one familiar with GO enrichment.\n\nI am currently trying to do the GO Enrichment Analysis. I key getting this message, \"--&gt; No gene can be mapped....\n\n--&gt; Expected input gene ID: ENSG00000161800,ENSG00000168298,ENSG00000164256,ENSG00000187166,ENSG00000113460,ENSG00000067369\n\n--&gt; return NULL...\"\n\nI don't possibly know what I am doing wrong. I have watched all types of GO videos, looked at different webpages. ",
    "author": "1D-Lover-2001",
    "timestamp": "2025-09-07T12:00:15",
    "url": "https://reddit.com/r/rstats/comments/1nb1ncs/bioinformatics_help/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1narn4z",
    "title": "How to Get Started With R - Beginner Roadmap",
    "content": "Hey everyone!\n\nI know a lot of people come here wanting to get into R for the first time, so I thought I‚Äôd share a quick roadmap. When I first started, I was totally lost with all the packages and weird syntax, but once things clicked, R became one of my favorite tools.\n\n1. Get Set Up\n\t‚Ä¢\tInstall R and RStudio (most popular IDE).\n\t‚Ä¢\tLearn the basics: variables, data types, vectors, data frames, and functions.\n\t‚Ä¢\tGreat free book: R for Data Science\n\t‚Ä¢\tAlso check out DataDucky ‚Äì super beginner-friendly and interactive.\n\n‚∏ª\n\n2. Work With Real Data\n\t‚Ä¢\tImport CSVs, Excel files, etc.\n\t‚Ä¢\tLearn data wrangling with tidyverse (especially dplyr and tidyr).\n\t‚Ä¢\tPractice using free datasets from Kaggle.\n\n‚∏ª\n\n3. Visualize Your Data\n\t‚Ä¢\tggplot2 is a must ‚Äì start with bar charts and scatter plots.\n\t‚Ä¢\tSeeing your data come to life makes learning way more fun.\n\n‚∏ª\n\n4. Build Small Projects\n\t‚Ä¢\tAnalyze data you care about ‚Äì sports, games, whatever keeps you interested.\n\t‚Ä¢\tShare your work to stay motivated and get feedback.\n\n‚∏ª\n\nLearning R can feel overwhelming at first, but once you get past the basics, it‚Äôs incredibly rewarding. Stick with it, and don‚Äôt be afraid to ask questions here ‚Äì this community is awesome.",
    "author": "JDD17",
    "timestamp": "2025-09-07T05:15:10",
    "url": "https://reddit.com/r/rstats/comments/1narn4z/how_to_get_started_with_r_beginner_roadmap/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1na32in",
    "title": "ggplot2 - Combining italic with plain font in factor legend",
    "content": "How can I combine a string in italics with a string in normal font in the legend for factors in a ggplot?",
    "author": "fasta_guy88",
    "timestamp": "2025-09-06T08:55:05",
    "url": "https://reddit.com/r/rstats/comments/1na32in/ggplot2_combining_italic_with_plain_font_in/",
    "score": 1,
    "num_comments": 8,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n9aapn",
    "title": "oRm: an Object Relational Model framework for R update",
    "content": "straight to it: https://kent-orr.github.io/oRm/\n\nI submitted my package to CRAN this morning and felt inclined to share my progress here since my last post. If you didn't catch that last post `oRm` is my answer to the google search query \"sqlalchemy equivalent for R.\" If you're still not quite sure what that means I'll give it a shot in ~~a few sentences~~ the overlong but still incomplete introduction below, but I'd recommend you check the vignette [Why oRm](https://kent-orr.github.io/oRm/articles/why_oRm.html). \n\nThis list is quick updates for those following along since the last post. if you're curious about the package from the start, skip down a paragraph. \n\n- transaction state has been implemented in Engine to allow for sessions  \n- you can flush a record before commit within a transaction to retrieve the db generated defaults (i.e. serial numbers, timestamps, etc.)  \n- schema setting in the postgres dialect  \n- extra args like `mode` or `limit` were changed to use '.' prefix to avoid column name collisions, i.e. `.mode=` and `.limit=`  \n- `.mode` has been expanded to incldue `tbl` and `data.frame` so you can user `oRm` to retrieve tabular data in standardized way.  \n- `.offset` included in Read methods now makes pagination of records easy, great for server side paginated tables  \n- `.order_by` argument now in Read methods which allows for supplying arguments to a `dplyr::order_by` call (also helpful when needing reliable pagination or repeatable display)\n\n## So What's this `oRm` thing?\n\nIn a nutshell, `oRm` is an object oriented abstraction away from writing raw SQL to work with records. While tools like `dbplyr` are incredible for reading tabular data, they are not designed for manipulating said data. And while joins are standard for navigating relationships between databases, they can become repetitive and applying operations on joined data can feel... Well, I know I have spent a lot of time checking and double checking that my statement was right before hitting enter. For example:\n\n    delete from table where id = 'this_id';\n\nThose operations can be kind of scary to write at times. Even worse is pasting that together via R\n\n    paste0(\"delete from \", table, \" where id = '\" this_id, \"';\")\n\nThat example is very [where did the soda go](https://old.reddit.com/r/wheredidthesodago/), but it illustrates my point. What `oRm` does is makes such operations cleaner and more repeatable. Imagine we have a TableModel object (`Table`) which is an R6 object mapped to a live database table. We want to delete the record where id is `this_id`. In `oRm` this would look like:\n\n    record = Table$read(id == 'this_id', .mode='get')\n    record$delete()\n\nThe Table$Read method passes the `...` args to a `tbl` built from the TableModel definition, which means you can use native dplyr syntax for your queries because it *is* calling `dplyr::filter()` under the hood to read records. \n\nLet's take it one level deeper to where `oRm` really shines: relationships. Let's say we have a table of users and users can have valuable treasures. We get a request to delete a user's treasure. If we get the treaure's ID, all hunky dory, we can blip that out of existence. But what if we want to be a bit more explicit and double check that we arent' accidentally deleting another user's precious, unrecoverable treasures?\n\n    user_treasures = Users |&gt;\n        filter(id == expected_user) |&gt;\n        left_join(Treasures, by = c(treasure_id = 'id'))\n        filter(treasure_id == target_treasure_id)\n\n    if (nrow(user_treasures)) &gt; 0 {\n        paste0('delete from treasures where id = \"', target_treasure_id \"';\")\n    }\n\nIn the magical land of `oRm` where everything is easier:\n\n    user = Users$read(id == exepcted_user, .mode='get')\n\n    treasure = user$relationship('treasure', id == target_treasure_id, .mode='get')\n\n    treasure$delete()\n\nSome other things to note:\n\nEvery `Record` (row) belongs to a `TableModel` (db table) and tables are mapped to an `Engine` the connection. The Engine is a wrapper on a `DBI::dbConnect` connection, and it's initialization arguments are the same with some bonus options. So the same db connection args you would normally use get applied to the `Engine$new()` arguments. \n\n    conn = DBI::dbConnect(drv = RSQLite::SQLite(), dbname = 'file.sqlite')\n\n    # can convert to an Engine via \n    engine = Engine$new(drv = RSQLite::SQLite(), dbname = 'file.sqlite')\n\nTableModels are defined by you, the user. You can create your own tables from scratch this way, or you can model an existing table to use. \n\n    Users = TableModel$new(\n        engine = engine,\n        'users',\n        id = Column('VARCHAR', primary_key = TRUE, default = uuid::UUIDgenerate),\n        timestamp = Column('DATETIME', default = Sys.time)\n        name = Column('VARCHAR')\n    )\n\n    Treasures = TableModel$new(\n        engine = engine,\n        'treasures',\n        id = Column('VARCHAR', primary_key = TRUE, default = uuid::UUIDgenerate),\n        user_id = ForeignKey('VARCHAR', 'users', 'id'),\n        name = Column('VARCHAR'),\n        value = COLUMN('NUMERIC')\n    )\n\n    Users$create_table()\n    Treasures$create_table()\n\n    define_relationship(\n        local_model    = Users,\n        local_key      = 'id',\n        type           = 'one_to_many',\n        related_model  = Treasures,\n        related_key    = 'user_id',\n        ref            = 'treasures',\n        backref        = 'users'\n    )\n\nAnd if you made it this far: There is a `with.Engine` method that handles transaction state and automatic rollback. Not at all unlike a `with Sesssion()` block in sqlalchemy. \n\n    with(engine, {\n        users = Users$read()\n        for (user in users) {\n            treasures = user$relationship('treasures')\n            for (treasure in treasures) {\n                if (treasures$data$value &gt; 1000) {\n                    user$update(name = paste(user$data$name, 'Musk'))\n                }\n            }\n        }\n    })\n\nwhich will open a transaction, process the expression, and if successful commit to the db, if fail roll back the changes and throw the original error.",
    "author": "binarypinkerton",
    "timestamp": "2025-09-05T09:43:37",
    "url": "https://reddit.com/r/rstats/comments/1n9aapn/orm_an_object_relational_model_framework_for_r/",
    "score": 23,
    "num_comments": 2,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n8afbh",
    "title": "Mixed-effects multinomial logistic regression",
    "content": "Hey everyone! I've been trying to run a mixed effect multinomial logistic regression but every package i've tried to use doesn't seem to work out. Do you have any suggestion of which package is the best for this type of analysis? I would really appreciate it. Thanks",
    "author": "Sicalis",
    "timestamp": "2025-09-04T06:40:41",
    "url": "https://reddit.com/r/rstats/comments/1n8afbh/mixedeffects_multinomial_logistic_regression/",
    "score": 9,
    "num_comments": 9,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n8k46q",
    "title": "Covariance matrix pattern, level-1 residuals, MLM in Mplus",
    "content": "In Mplus, for a 2-level multilevel model, is there a way to specify the pattern of the R matrix (the covariance matrix of the level-1 residuals) with the data in long, not wide, format?",
    "author": "rj565",
    "timestamp": "2025-09-04T12:44:56",
    "url": "https://reddit.com/r/rstats/comments/1n8k46q/covariance_matrix_pattern_level1_residuals_mlm_in/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n8d87n",
    "title": "Benford Analysis Tool For Statistic Verification",
    "content": "My father has been working on a tool that I thought some might find interesting regarding the Benford Analysis.  I'm sure he would appreciate if anyone would be interested in learning more.  A little over a 6 minute video and the tool is listed in the description.  Thanks in advance!  [https://www.youtube.com/watch?v=B7kvjhQxxfM](https://www.youtube.com/watch?v=B7kvjhQxxfM)",
    "author": "DG-Nerd-652",
    "timestamp": "2025-09-04T08:25:06",
    "url": "https://reddit.com/r/rstats/comments/1n8d87n/benford_analysis_tool_for_statistic_verification/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n7zo0d",
    "title": "Help with R code for curve fitting",
    "content": "",
    "author": "LolaRey1",
    "timestamp": "2025-09-03T20:33:52",
    "url": "https://reddit.com/r/rstats/comments/1n7zo0d/help_with_r_code_for_curve_fitting/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n7p6ue",
    "title": "ggplot2/patchwork ensuring identical panel width",
    "content": "I have a plot with 5 panels in two columns, where I only want to put the color/shape legend to the right of the bottom panel (because there is no panel to the right).  Using patchwork, I can make the 5 panels be the same width, through a process of trial and error setting  p5 + plot\\_void + plot\\_layout(width=c(3,0.8)) for the last row.\n\nBut I would like to be able to tell exactly how much wider the bottom panel with the legend should be by learning the width of the no-legend panels and the legend panel, so that I can calculate the relative widths algebraically.\n\nIs there a way to learn the sizes of the panels for this calculation?",
    "author": "fasta_guy88",
    "timestamp": "2025-09-03T12:57:47",
    "url": "https://reddit.com/r/rstats/comments/1n7p6ue/ggplot2patchwork_ensuring_identical_panel_width/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n7gxm8",
    "title": "I need some help grouping or recoding data in R",
    "content": "I am working on some football data, and I am trying to recode my yards column into 4 groups and assign a number to them, as follows. 0-999 yds = 1 , 1000-1999 = 2 , 2000-2999 = 3, 3000 - and Beyond = 4. I have been stumped on this problem for days.",
    "author": "Mountain-Evening-557",
    "timestamp": "2025-09-03T07:52:12",
    "url": "https://reddit.com/r/rstats/comments/1n7gxm8/i_need_some_help_grouping_or_recoding_data_in_r/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.44,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n6vuu8",
    "title": "Apply now for R Consortium Technical Grants!",
    "content": "The R Consortium ISC just opened the second technical grant cycle of 2025!\n\nüëâ Deadline: Oct 1, 2025\n üëâ Results: Nov 1, 2025\n üëâ Contracts: Dec 1, 2025\n\nWe‚Äôre looking for proposals that move the R ecosystem forward‚Äînew packages, teaching resources, infrastructure, documentation, and more.\n\nThis is your chance to get funded, gain visibility, and make a lasting impact for R users worldwide.\n\nüìÑ Details + apply here: [https://r-consortium.org/posts/r-consortium-technical-grant-cycle-opens-today/](https://r-consortium.org/posts/r-consortium-technical-grant-cycle-opens-today/)",
    "author": "jcasman",
    "timestamp": "2025-09-02T14:15:22",
    "url": "https://reddit.com/r/rstats/comments/1n6vuu8/apply_now_for_r_consortium_technical_grants/",
    "score": 20,
    "num_comments": 0,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n62yc4",
    "title": "New R package for change-point detection",
    "content": "üöÄ Excited to share our new R package for high-performance change-point detection, rupturesRcpp, developed as part of Google Summer of Code 2025 for The R Foundation for Statistical Computing. \n\nKey features:\n- Robust, modern OOP design based on R6¬†for modularity and maintainability\n- High-performance C++ backend¬†using Armadillo for fast linear algebra\n- Multivariate cost functions ‚Äî many supporting¬†O(1) segment queries\n- Implements several¬†segmentation algorithms: Pruned Exact Linear Time, Binary Segmentation, and Window-based Slicing\n- Rigorously tested for robustness and mathematical correctness\n\nThe package is in¬†beta¬†but nearly ready for CRAN. It enables¬†efficient, high-performance change-point detection, especially for multivariate data, outperforming traditional packages like¬†changepoint, which are slower and lack multivariate support. Empirical evaluations also demonstrate that it substantially outperforms ruptures, which is implemented entirely in Python.\n\nIf you work with time series or signal processing in R, this package is ready to use ‚Äî and feel free to ‚≠ê it on GitHub! If you‚Äôre interested in contributing to the project (we have several ideas for new features) or using the package for practical problems, don‚Äôt hesitate to reach out.\n\nhttps://github.com/edelweiss611428/rupturesRcpp",
    "author": "noisyminer61",
    "timestamp": "2025-09-01T15:37:16",
    "url": "https://reddit.com/r/rstats/comments/1n62yc4/new_r_package_for_changepoint_detection/",
    "score": 96,
    "num_comments": 10,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n6ipoy",
    "title": "Timeseries affected by one-time expense",
    "content": "Our HOA keeps and publishes pretty extensive financial records that I can use to practice some data analysis. One of those is the cash position of the town homes section. \n\nRecently they did some big remodeling (new roofs) that depleted some of that cash, however this is going to be a one-time event with no changes in income expected over the next years. \n\nFor the timeseries, this has a big effect. Models are flopping all over the place with the lowest outcome being a steady decline, the highest model show an overshoot and the median being steady. Needless to say, none of these would be correct. \n\nAny idea how long it takes for these models to get back on track? My expectation is that the rate of increase should be similar to before the big expense.\n\n\n\nhttps://preview.redd.it/j1u1nv1i0rmf1.png?width=1242&amp;format=png&amp;auto=webp&amp;s=aaf1327b66ab1ab52246ba7d94436764323ae782\n\n  \n(time series modeled via different methods, showing max, min and medium lines)",
    "author": "peperazzi74",
    "timestamp": "2025-09-02T05:55:59",
    "url": "https://reddit.com/r/rstats/comments/1n6ipoy/timeseries_affected_by_onetime_expense/",
    "score": 7,
    "num_comments": 9,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n72fdk",
    "title": "Quick Tutorial using melt()",
    "content": "",
    "author": "Rare-Teacher-4328",
    "timestamp": "2025-09-02T19:00:31",
    "url": "https://reddit.com/r/rstats/comments/1n72fdk/quick_tutorial_using_melt/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.22,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n6jar2",
    "title": "Display data on the axes - ggplot",
    "content": "Hi all, I am having trouble coming up with an elegant solution to a problem I‚Äôm having. \n\nI have a simple plot using geom_line() to show growth curves with age on the x-axis and mass on the y-axis. I would like that the Y axis line be used to display a density curve of the average adult mass. \n\nSo far, I have used geom_density with no fill and removed the Y axis line but it doesn‚Äôt look too great. The density curve doesn‚Äôt extend to 0, the x axis extends beyond 0 on the left, etc. \n\nAre there any resources that discuss how to do this?",
    "author": "LaridaeLover",
    "timestamp": "2025-09-02T06:20:59",
    "url": "https://reddit.com/r/rstats/comments/1n6jar2/display_data_on_the_axes_ggplot/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n6bio5",
    "title": "Positron - .Rprofile not sourced when working in subdirectory of root",
    "content": "Hi all,\n\nNew user of Positron here, coming from RStudio. I have a codebase that looks like:\n\n    &gt; data_extraction\n      &gt; extract_1.R\n      &gt; extract_2.R\n    &gt; data_prep\n      &gt; prep_1.R\n      &gt; prep_2.R\n    &gt; modelling\n      &gt; ...\n    &gt; my_codebase.Rproj\n    &gt;.Rprofile\n\nEach script requires that its immediate parent directory be the working directory when running the script. Maybe not best practise but I'm working with what I have.\n\nThis is fairly easy to run in RStudio. I can run each script, and hit Set Working Directory when moving from one subdirectory to the next. After each script I can restart R to clear the global environment. Upon restarting R, I guess RStudio looks to the project root (as determined by the Rproj file) and finds/sources the .Rprofile.\n\nThis is not the case in Positron. If my active directory is `data_prep`, then when restarting the R session, .Rprofile will not be sourced. This is an issue when working with `renv`, and leads to an annoying workflow requiring me to run `setwd()` far more often.\n\n**Does anybody know a nice way around this? To get Positron to recognise a project root separate from the current active directory?**\n\nThe settings have a project option: `terminal.integrated.cwd`, which (re-)starts the terminal at the root directory only. This doesn't seem to apply to the R session, however.\n\nOptions I've considered are:\n\n* .Rprofile in every subdirectory - seems nasty\n* Write a VSCode extension to do this - I don't really want to maintain something like this, and I'm not very good at JS.\n* File Github issue, wait - I'll do this if nobody can help here\n* Rewrite the code so all file paths are relative to the project root - lots of work across multiple codebases but probably a good idea",
    "author": "HeartDistinct888",
    "timestamp": "2025-09-01T22:48:38",
    "url": "https://reddit.com/r/rstats/comments/1n6bio5/positron_rprofile_not_sourced_when_working_in/",
    "score": 2,
    "num_comments": 11,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n6pfl6",
    "title": "Colour Prediction Website Need A Partner",
    "content": "",
    "author": "MominulIslam12",
    "timestamp": "2025-09-02T10:13:55",
    "url": "https://reddit.com/r/rstats/comments/1n6pfl6/colour_prediction_website_need_a_partner/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n6peev",
    "title": "Colour Prediction Website Need Partnership",
    "content": "",
    "author": "MominulIslam12",
    "timestamp": "2025-09-02T10:12:40",
    "url": "https://reddit.com/r/rstats/comments/1n6peev/colour_prediction_website_need_partnership/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n5vp2m",
    "title": "Built-In Skewness and Kurtosis Functions",
    "content": "I often need to load the R package moments to use its skewness and kurtosis functions. Why they are not available in the fundamental R package stats? ",
    "author": "BOBOLIU",
    "timestamp": "2025-09-01T10:54:12",
    "url": "https://reddit.com/r/rstats/comments/1n5vp2m/builtin_skewness_and_kurtosis_functions/",
    "score": 8,
    "num_comments": 7,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n4pblh",
    "title": "Running AI-generated ggplot2: why we moved from WebR to cloud computing?",
    "content": "WebR (R in the browser with Web Assembly) is awesome and works like a charm. So, why moved from it to boring AWS Lambda? \n\nIf you want to play with it, though - [ggplot2 and dplyr in WebR](https://quesmaorg.github.io/demo-webr-ggplot/).",
    "author": "pmigdal",
    "timestamp": "2025-08-31T01:20:15",
    "url": "https://reddit.com/r/rstats/comments/1n4pblh/running_aigenerated_ggplot2_why_we_moved_from/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n47016",
    "title": "Turning Support Chaos into Actionable Insights: A Data-Driven Approach to Customer Incident Management",
    "content": "",
    "author": "afaqbabar",
    "timestamp": "2025-08-30T10:11:57",
    "url": "https://reddit.com/r/rstats/comments/1n47016/turning_support_chaos_into_actionable_insights_a/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n391mu",
    "title": "Rstan takes forever to install ?",
    "content": "I am trying to install rstan but one of the required packages (RcppEigen) takes a lot of time that I force the installation to stop, is it normal or am I having problems in my computer ?",
    "author": "al3arabcoreleone",
    "timestamp": "2025-08-29T07:32:11",
    "url": "https://reddit.com/r/rstats/comments/1n391mu/rstan_takes_forever_to_install/",
    "score": 3,
    "num_comments": 9,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n3bfaw",
    "title": "Labelling a dendrogram",
    "content": "I have a CSV file, the first few lines of which are:\n\n`Distillery,Body,Sweetness,Smoky,Medicinal,Tobacco,Honey,Spicy,Winey,Nutty,Malty,Fruity,Floral`\n\n`Aberfeldy,2,2,2,0,0,3,2,2,1,2,2,2`\n\n`Aberlour,3,3,1,0,0,3,2,2,3,3,3,2`\n\n`Alt-A-Bhaine,1,3,1,0,0,1,2,0,1,2,2,2`\n\nI read this in using read.csv, setting header to TRUE.\n\nI then calculate a distance matrix, and perform hierarchical clustering. To plot the dendrogram I use:\n\n`fviz_dend(hcr, cex = 0.5, horiz = TRUE, main = \"Dendrogram - ward.D2\")`\n\nThis gives me the dendrogram, but labelled with the line number in the file, rather than the distillery name.\n\nHow do I make the dendrogram use the distillery name?\n\nHappy to provide the full CSV file if this helps.",
    "author": "Bright_Flan4481",
    "timestamp": "2025-08-29T09:02:47",
    "url": "https://reddit.com/r/rstats/comments/1n3bfaw/labelling_a_dendrogram/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n3ague",
    "title": "Creating an DF of events in one DF that happened within a certain range of another DF",
    "content": "Hey y‚Äôall, I‚Äôm working a in a large database. I have two data frames. One with events and their date (we can call date_1) that I am primarily concerned about. The second is a large DF with other events and their dates (date_2). I am interested in creating a third DF of the events in DF2 that happened within 7 days of DF1‚Äôs events. Both DFs have person IDs and DF1 is the primary analytic file, I‚Äôm building. \n\nI tried a fuzzy join but from a memory standpoint this isn‚Äôt feasible. I know there‚Äôs data.table approaches (or think there may be), but primarily learned R with base R + tidyverse so am less certain about that. I‚Äôve chatted with the LLMs, would prefer to not just vibe code my way out. I am a late in life coder as my primary work is in medicine, so I‚Äôm learning as I go. Any tips?",
    "author": "southbysoutheast94",
    "timestamp": "2025-08-29T08:26:36",
    "url": "https://reddit.com/r/rstats/comments/1n3ague/creating_an_df_of_events_in_one_df_that_happened/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n37swm",
    "title": "New trouble with creating variables that include a summary statistic",
    "content": "(SECOND EDIT WITH RESOLUTION)\n\nTurns out my original source dataframe was actually grouped rowwise for some reason, so the function was essentially trying to take the mean and standard deviation within each row, resulting in NA values for every row in the dataframe.  Now that I've removed the grouping, everything's working as expected.  \n\nThanks for the troubleshooting help!\n\n(EDITED BECAUSE ENTERED TOO SOON)\n\nI built a workflow for cleaning some data that included a couple of functions designed to standardize and reverse score variables.  Yesterday, when I was cleaning up my script to get it ready to share, I realized the functions were no longer working and were returning NAs for all cases.  I haven't been able to effectively figure out what's going wrong, but they have worked great in the past and I didn't change anything else that I know of.\n\nIdeas for troubleshooting what might have caused these functions to stop working and/or to fix them?  I tried troubleshooting with AI, but didn't get anything particularly helpful, so I figured humans might be the better avenue for help.\n\nFor context, I'm working in RStudio (2025-05-01, Build 513)\n\n\\## Example function:\n\n    z_standardize &lt;- function(x) {\n      var_mean &lt;- mean(x, na.rm = TRUE)\n      std_dev &lt;- sd(x, na.rm = TRUE)\n      return((x - var_mean) / std_dev)   # EDITED AS I WAS MISSING PARENTHESES\n      }\n\n\\## Properties of a variable it is broken for:\n\n    &gt; str(df$wage)\n     num [1:4650] 5.92 8 5.62 25 9.5 ...\n     - attr(*, \"value.labels\")= Named num(0) \n      ..- attr(*, \"names\")= chr(0) \n    \n    &gt; summary(wage)\n     wage   \n     Min.   :  1.286  \n     1st Qu.: 10.000  \n     Median : 12.821  \n     Mean   : 15.319  \n     3rd Qu.: 16.500  \n     Max.   :107.500  \n     NA's   :405\n\n\\## It's broken when I try this:\n\n`df_test &lt;- df %&gt;% mutate(z_wage = z_standardize(wage))`\n\n    &gt; summary(df_test$z_wage)\n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n         NA      NA      NA     NaN      NA      NA    4650 \n\n\\## It works when I try this:\n\n    &gt; df_test$z_wage &lt;- z_standardize(df_test$wage)    #EDITED DF NAME FOR CONSISTENCY\n    &gt; summary(df_test$z_wage)\n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n     -0.153   8.561  11.382  13.880  15.061 106.061     405 \n\nI couldn't get the error to replicate with this sample dataframe, ruining my idea that there was something about NA values that were breaking the function:\n\n    df_sample &lt;- tibble(a = c(1, 2, 4, 11), b = c(9, 18, 6, 1), c = c(3, 4, 5, NA))\n    \n    df_sample_z &lt;- df_sample %&gt;% \n      mutate(z_a = z_standardize(a),\n             z_b = z_standardize(b),\n             z_c = z_standardize(c)) \n    \n    &gt; df_sample_z\n    # A tibble: 4 x 6\n          a     b     c    z_a     z_b   z_c\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n    1     1     9     3 -0.776  0.0700    -1\n    2     2    18     4 -0.554  1.33       0\n    3     4     6     5 -0.111 -0.350      1\n    4    11     1    NA  1.44  -1.05      NA",
    "author": "ohbonobo",
    "timestamp": "2025-08-29T06:42:45",
    "url": "https://reddit.com/r/rstats/comments/1n37swm/new_trouble_with_creating_variables_that_include/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n2x0qr",
    "title": "ggplot's geom_label() plotting in the wrong spot when adding \"fill = [color]\"",
    "content": "https://preview.redd.it/ufj7b3axpvlf1.png?width=800&amp;format=png&amp;auto=webp&amp;s=fab2624c77c72989cec90f581e3dc161457a0c7c\n\nHello,\n\nI'm working on putting together a grouped bar chart with labels above each bar. The code below is an example of what I'm working on.\n\nIf I don't add a `fill` color to `geom_label()`, then the labels are plotted correctly with each bar.\n\nHowever, when I add the line `fill = \"white\"` to `geom_label()`, the labels revert back to the position they would be in with a stacked bar chart.\n\nThe image in this post shows what I get when I add that white fill.\n\nDoes anybody know a way to keep those labels positioned above each bar?\n\nThank you!\n\n    # Data\n    data &lt;- data.frame(\n          category = rep(c(\"A\", \"B\", \"C\"), each = 2),\n          group = rep(c(\"X\", \"Y\"), 3),\n          value = c(10, 15, 8, 12, 14, 9)\n          )\n        \n    # Create the grouped bar chart with white-filled labels\n    ggplot(data, aes(x = category, y = value, fill = group)) +\n          geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +\n          geom_label(aes(label = value), \n                     position = position_dodge(width = 0.9), \n                     fill = \"white\") +\n          labs(title = \"Grouped Bar Chart with White Labels\",\n          x = \"Category\",\n          y = \"Value\") +\n          theme_minimal()",
    "author": "djn24",
    "timestamp": "2025-08-28T20:39:52",
    "url": "https://reddit.com/r/rstats/comments/1n2x0qr/ggplots_geom_label_plotting_in_the_wrong_spot/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n2mio9",
    "title": "Replicability of Random Forests",
    "content": "I use the R package ranger for random forests modeling, but I am unsure how to maintain replicability. I can use the base function set.seed(), but the function ranger() also has an argument seed. The function importance\\_pvalues() also needs to set seed when the Altmann method is used. Any suggestions?",
    "author": "BOBOLIU",
    "timestamp": "2025-08-28T12:57:40",
    "url": "https://reddit.com/r/rstats/comments/1n2mio9/replicability_of_random_forests/",
    "score": 6,
    "num_comments": 2,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n2k575",
    "title": "I'm new and I need some help step-by-step if possible",
    "content": "Hello all,\n\nI posted a few days ago before I left to do field work. I am now going back to my data analysis for the project that I posted about. I do not think that the codes are working as they should, leading to errors. My coworker created this code. I wanted someone to coach me step-by-step because my coworker is still out on vacation. As of right now this is my code for the uploading of packages, data, directory, and cleaning data. This is the beginning of the code.\n\n    ### Load Packages ###\n    \n    library(tidyverse)\n    library(readr)\n    library(dplyr)\n    \n    ### Directory to File Location ###\n    dataAll &lt;- read_csv(\"T:/HSC/Marsh_Fiddler/Analysis/All_Blocks_All_Data.csv\")\n    dataSites &lt;- read_csv(\"T:/HSC/Marsh_Fiddler/Analysis/tbl_MarshSurvey.csv\")\n    dataBlocks &lt;- read_csv(\"T:/HSC/Marsh_Fiddler/Analysis/tbl_BlocksAnna.csv\")\n    \n    indata &lt;- read_excel(\"T:/HSC/Marsh_Fiddler/Analysis/All_Blocks_All_Data.xlsx\", sheet = \"Bay\", col_types = c(\"date\",\"text\", \"text\", \"numeric\", \"numeric\", \"numeric\", \"numeric\", \"numeric\", \"numeric\", \"numeric\", \"numeric\"))\n    \n    head(indata)\n    \n    str(indata)\n    \n    #---- Clean and prep data ----\n    \n    # unfortunately, not all the CSV files come in with the same variables in the same format\n    # make any adjustments and add any additional columns that you need/want\n    str(\"dataBlocks\")\n    dataBlocks2 &lt;- dataBlocks %&gt;%\n      mutate(SurveyID = as.factor(SurveyID),\n             Year = as.factor(year(SurveyDate)),\n             Month = as.factor(month(SurveyDate))) #%&gt;%\n    #select(!c(BlockID))\n    \n    dataSites2 &lt;- dataSites %&gt;%\n      mutate(SurveyDate = mdy(SurveyDate),\n             Location = as.factor(Location),\n             TideCode = as.factor(TideCode),\n             Year = as.factor(year(SurveyDate)),\n             Month = as.factor(month(SurveyDate)),\n             State =  \"DE\") %&gt;%\n      select(!c(Crew))\n    \n    str(dataSites2) \n    \n    # select(!c(SurveyID))\n\nThe first `str()` command appears to go through. However, the code below goes to error.\n\n    dataBlocks2 &lt;- dataBlocks %&gt;%\n      mutate(SurveyID = as.factor(SurveyID),\n             Year = as.factor(year(SurveyDate)),\n             Month = as.factor(month(SurveyDate)))\n\nThe error for the code is\n\n    Error in `mutate()`:\n    ‚Ñπ In argument: `Year = as.factor(year(SurveyDate))`.\n    Caused by error in `as.POSIXlt.character()`:\n    ! character string is not in a standard unambiguous format\n    Run `` to see where the error occurred.rlang::last_trace()\n\nI believe that dataBlocks2 was supposed to be created by that command, but it isn't and when I run the next `str()` command it says that dataBlocks2 cannot be found. I also assume that this is happening with dataSites as well.",
    "author": "unceasingfish",
    "timestamp": "2025-08-28T11:26:49",
    "url": "https://reddit.com/r/rstats/comments/1n2k575/im_new_and_i_need_some_help_stepbystep_if_possible/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n1tspt",
    "title": "25 Things You Didn‚Äôt Know You Could Do with R (CascadiaRConf2025)",
    "content": "I used to think R was pretty much just for stats and data analysis, but David Keyes' keynote at Cascadia R this year totally changed my perspective.\n\nHe walked through 25 different things you can do with R that go way beyond your typical regression models and ggplot charts with some creative, some practical, and honestly some that caught me completely off guard.\n\nDefinitely worth watching if you're stuck in a rut with your usual R workflow or just want some fresh inspiration for projects.\n\nüé• Video here: [https://youtu.be/wrPrIRcOVr0](https://youtu.be/wrPrIRcOVr0)",
    "author": "mulderc",
    "timestamp": "2025-08-27T14:31:05",
    "url": "https://reddit.com/r/rstats/comments/1n1tspt/25_things_you_didnt_know_you_could_do_with_r/",
    "score": 79,
    "num_comments": 5,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n2lq9k",
    "title": "ggplot2() using short lines (and line types) to distinguish points",
    "content": "Would like to plot 5 y-values for 20 categories, where I am using combinations of colors and symbols to distinguish the 20 categories in other plots.  So I am considering drawing short lines through the 20 color/symbol combinations, and using different line types (dotted, short-dashed, etc) to distinguish the 5 values.\n\nIs there a geom\\_??? that would allow me to draw a short line through a symbol that has been placed by its y-value and category?",
    "author": "fasta_guy88",
    "timestamp": "2025-08-28T12:27:14",
    "url": "https://reddit.com/r/rstats/comments/1n2lq9k/ggplot2_using_short_lines_and_line_types_to/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n23pdw",
    "title": "Claude Code for R/RStudio with (almost) zero setup for Mac.",
    "content": "Hi all,   \n  \nI'm quite fascinated by the Claude Code functionalities so I've implemented a : [https://github.com/thomasxiaoxiao/rstudio-cc](https://github.com/thomasxiaoxiao/rstudio-cc) \n\nAfter installing the basics such as brew, npm, claude code, R..., you should then be able to interact with r/RStudio natively with CC, exposing the R execution logs so that CC has the visibility into the context. This should be quite helpful for debugging and more. \n\nAlso, since I'm not really a heavy R user I'm also curious about the following from the community: what R/RStudio can provide that is still essential that prevent you from migrating to other languages and IDEs, such as Python +VScode? where the AI integrations are usually much better.  \n\nAppreciate any feedback on the repo and discussions.",
    "author": "AdSpecialist666",
    "timestamp": "2025-08-27T22:18:22",
    "url": "https://reddit.com/r/rstats/comments/1n23pdw/claude_code_for_rrstudio_with_almost_zero_setup/",
    "score": 8,
    "num_comments": 15,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n2h0a5",
    "title": "Does anyone know how to divide the columns?",
    "content": "https://preview.redd.it/uic5tt4oeslf1.png?width=3024&amp;format=png&amp;auto=webp&amp;s=5c3764adf455bc65d5cf27f60023e007f95e100c\n\nI have to divide 2015Q2 by 2015pop and I'm not sure why it keeps saying that there's an unknown symbol in 2015Q2\n\n  \nedit: i figured it out it was just gdp$'2015Q2' /  gdp$'2015pop'",
    "author": "Royal-Shop1400",
    "timestamp": "2025-08-28T09:29:24",
    "url": "https://reddit.com/r/rstats/comments/1n2h0a5/does_anyone_know_how_to_divide_the_columns/",
    "score": 0,
    "num_comments": 14,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n1pzks",
    "title": "Rcpp Organization Logo",
    "content": "The logo for the Rcpp GitHub organization features a clock pointing to 11. What does it mean? The C++11 standard, the package being created in 2011, or the package existing for 11 years, etc?  \n\n[https://github.com/RcppCore](https://github.com/RcppCore)¬†",
    "author": "BOBOLIU",
    "timestamp": "2025-08-27T12:04:12",
    "url": "https://reddit.com/r/rstats/comments/1n1pzks/rcpp_organization_logo/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n108b4",
    "title": "Addicted to Pipes",
    "content": "I can't help but use |&gt; everywhere possible. Any similar experiences?",
    "author": "BOBOLIU",
    "timestamp": "2025-08-26T15:39:55",
    "url": "https://reddit.com/r/rstats/comments/1n108b4/addicted_to_pipes/",
    "score": 79,
    "num_comments": 40,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n1r22d",
    "title": "Postdoc data science uk- help I'm poor",
    "content": "",
    "author": "Tunashadow",
    "timestamp": "2025-08-27T12:45:20",
    "url": "https://reddit.com/r/rstats/comments/1n1r22d/postdoc_data_science_uk_help_im_poor/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.22,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n1b985",
    "title": "Title: Request for arXiv cs.LG Endorsement ‚Äì First-Time Submitter Body",
    "content": "\n[R]Hi everyone,\n\nI‚Äôm a 4th-year CS student at SRM Institute of Science and Technology, Chennai, India, and I‚Äôm preparing to submit my first paper to cs.LG (Machine Learning) on arXiv.\n\nMy paper is titled:\n‚ÄúA Comprehensive Analysis of Optimized Machine Learning Models for Predicting Parkinson‚Äôs Disease‚Äù\n\nSince I don‚Äôt have a personal endorser yet, I would greatly appreciate it if a qualified arXiv author in cs.LG could provide an endorsement.\n\nMy unique arXiv endorsement code is: YV8C4C\n\nThank you so much for your time and help! I‚Äôd be happy to provide a short summary or draft if needed. [R]",
    "author": "Significant-Ice-7926",
    "timestamp": "2025-08-27T01:16:52",
    "url": "https://reddit.com/r/rstats/comments/1n1b985/title_request_for_arxiv_cslg_endorsement/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n0qbr5",
    "title": "Does pseudo-R2 represent an appropriate measure of goodness-of-fit for Conway-Maxwell Possion?",
    "content": "Good morning,\n\nI have a question regarding Conway-Maxwell Poisson and pseduo-R2.\n\nIn R, I have fitted a model using glmmTMB as such:\n\n`richness_glmer_Full &lt;- glmmTMB(richness ~ vl100m_cs + roads100m_cs + (1 | neighbourhood/site), data = df_Bird, family = \"compois\", na.action = \"na.fail\")`\n\nI elected to use a COMPOIS due to evidence of underdispersion. COMPOIS mitigates the issue of underdispersion well, but my concern lies in the subsequent calculation of pseudo-R2:\n\n`r.squaredGLMM(richness_glmer_Full)`\n\n`R2m R2c`\n\n`[1,] 0.06240816 0.08230917`\n\nI'm skeptical that the model has such low explanatory power (models fit with different error structures show much higher marginal R2). Am I correct in assuming that using a COMPOIS error structure leads to these low pseudo-R2 values (i.e., something related to the computation of pseudo-R2 with COMPOIS leads to deflated values).\n\nAny insight for this humble ecologist would be greatly appreciated. Thank you in advance.",
    "author": "Pseudachristopher",
    "timestamp": "2025-08-26T09:20:43",
    "url": "https://reddit.com/r/rstats/comments/1n0qbr5/does_pseudor2_represent_an_appropriate_measure_of/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n02y6u",
    "title": "Shiny app to merge PDF files with page removal options",
    "content": "Hi r/rstats,  \n\nJust want to give back to the community on something I've worked on. I always get frustrated when I have the occasional need to merge PDF files and/or remove or rotate certain pages. Like most others, our corporate-default Acrobat Reader does not have these built-in features (why?), and we cannot use external websites to handle any sensitive info.  \n\nCollectively, the world must've wasted many, many hours on this issue trying to find an acceptable workaround (e.g. finding a colleague that has the professional Adobe Acrobat, or wait for IT to install it on their own laptop).  \n\nIt's 2025 and no one else should suffer any more.  \n\nSo I've created an app called PDF Combiner that does exactly that. It is **fast, free, and secure**. Anyone with access to R can load this up locally in less than a minute, and no installation is required (other than a few common packages). Until Adobe decides to step up their game, this does the job.  \n\nüåê [Online demo](https://lagom.shinyapps.io/PDF_Combiner/)  \n\nüíª [GitHub](https://github.com/stevechoy/pdfcombiner)",
    "author": "pmxthrowaway",
    "timestamp": "2025-08-25T14:16:17",
    "url": "https://reddit.com/r/rstats/comments/1n02y6u/shiny_app_to_merge_pdf_files_with_page_removal/",
    "score": 31,
    "num_comments": 9,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n0bn5o",
    "title": "R-studio/Python with a BA",
    "content": "I am a senior majoring in Political Science (BA) at a DC school. My school is somewhat unique in the land of theoretical-based Political Science degrees and I have taken 6 econ classes as well as a TA position with a micro class (earning a minor), a introductory statistics course, as well as having learned SPSS through a quantitative-based research class. However, I feel this is still not enough to justify a valuable, competitive skill set as SPSS is not widely used anymore it seems and other than that, what can I say... I can read and analyze well?\n\nSo this is my dilemma and I find myself wanting to add another semester (I was supposed to graduate early this December so this wont really delay my plans, just my wallet) and take both an R-studio class and Python class. I would also add a data analytics class that develops a research paper with multiple coding programs.\n\nIs it a good idea to pursue a more statistical route? Any advice about this area helps. I loved my research class and messing with datasets and SPSS even tho it's a piece of shit on my computer. I want to be competitive for graduate schools and the job market and my career advisors have told me that polisci and policy analysis is going down a more quantitative route.",
    "author": "Crafty-Fisherman-241",
    "timestamp": "2025-08-25T20:45:11",
    "url": "https://reddit.com/r/rstats/comments/1n0bn5o/rstudiopython_with_a_ba/",
    "score": 3,
    "num_comments": 10,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mzu4bu",
    "title": "üéØ Reviving R Communities Through Practical Projects: Meet R User Group Finland",
    "content": "Vicent Boned and Marc Eixarch transformed an R user group into a thriving community by focusing on real-world applications.\n\nFrom custom Spotify music reports to Helsinki real estate analysis, they've created engaging meetups that go beyond traditional data science workflows.\n\nTheir approach shows how practical, fun projects can breathe new life into local R communities.\n\nRead more: [https://r-consortium.org/posts/spotify-stats-and-real-estate-insights-r-user-group-finland-builds-practical-projects/](https://r-consortium.org/posts/spotify-stats-and-real-estate-insights-r-user-group-finland-builds-practical-projects/)",
    "author": "jcasman",
    "timestamp": "2025-08-25T08:47:17",
    "url": "https://reddit.com/r/rstats/comments/1mzu4bu/reviving_r_communities_through_practical_projects/",
    "score": 14,
    "num_comments": 1,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n09fbu",
    "title": "R course certification",
    "content": "Hello all, I am completely new to R, with absolutely 0 experience in it. I wanted to complete a certification or just be in the process of one for upcoming masters applications for biotech. I wanted an actual certification to show credentials as opposed to learning it myself through books. I saw a few on coursera but I wanted to know if anyone had any recommendations? Any help would be MUCH appreciated ",
    "author": "New_Dragonfruit_350",
    "timestamp": "2025-08-25T18:55:04",
    "url": "https://reddit.com/r/rstats/comments/1n09fbu/r_course_certification/",
    "score": 1,
    "num_comments": 11,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mzx47x",
    "title": "I keep getting an Error and \"Object Not Found\"",
    "content": "Hello all,\n\nI just started learning R last week and I have had a bit of a rocky start, but I am getting the hang of it (very slowly). Anyways, I am a scientist who needs help figuring out what's wrong with this code. I did not make this code, another scientist made it and gave it to me to experiment with. If information is needed, this is for an experiment fiddler crabs in quadrats and soil cores. (BTW Clusters are multiple crabs)\n\nI believe this code is supposed to lead up to the creation of an Excel file (an explanation of `str()` would be helpful as well).\n\nI have mixed and matched things that I think could be wrong with it, but it still goes to an error. Please let me know if it there isn't enough information, I really don't know why it isn't working.\n\nMy errors include this:\n\n    Error: object 'BlockswithClustersTop' not found\n    \n    Error: object 'CrabsTop' not found\n    \n    Error: object 'HowManyCrabs' not found\n\nHere is the current code:\n\n    str(\"dataBlocks\")\n    HowManyCrabs &lt;- dataBlocks%&gt;%\n      group_by(SurveyID)%&gt;%\n      summarize(blocks=n(),\n                CrabsTopTotal = sum(CrabsTop),\n                CrabsBottomTotal = sum(CrabsBottom),\n                BlocksWithCrabsTop = sum(CrabsTop&gt;0),\n                BlocksWithCrabsBottom = sum(CrabsBottom&gt;0),\n                BlocksWithCrabs = sum(CrabsTop + CrabsBottom &gt;0),\n                BlocksWithCrabsTop = sum(CrabsTop&gt;0),\n                BlockswithClustersTop = sum(CrabsTop &gt;1.5),\n                BlockswithClustersBottom = sum(CrabsBottom &gt;1.5),\n                BlockswithClusters = sum(CrabsTop &gt;1.5|CrabsBottom &gt;1.5),\n                MinVegetationClass = as.factor(min(VegetationClass)),\n                MaxVegetationClass = as.factor(max(VegetationClass)),\n                AvgVegetationClass = as.factor(floor(mean(VegetationClass))),\n                MinHardness = min(Hardness,na.rm = TRUE),\n                MaxHardness = max(Hardness, na.rm = TRUE),\n                AvgHardness = mean(Hardness, na.rm = TRUE),\n                MinHardFloor = floor(MinHardness),\n                MaxHardFloor = floor(MaxHardness),\n                AvgHardFloor = floor(AvgHardness)) +\n      mutate(BlockswithClusters = BlockswithClustersTop + BlockswithClustersBottom,\n              Crabs = as.factor(ifelse(BlocksWithCrabs &gt;0,\"YES\", \"NO\")),\n              Clusters = as.factor(ifelse(BlockswithClusters &gt;0, \"YES\", \"NO\")),\n              TypeofCrabs = as.factor (ifelse(BlockswithClusters &gt;0, \"CLUSTERS\",                 ifelse(BlocksWithCrabs &gt;0,\"SINGLESONLY\",\"NOTHING\"))))\n    \n    str(HowManyCrabs)\n    \n    write_csv(HowManyCrabs, \"HowManyCrabs.csv\")",
    "author": "unceasingfish",
    "timestamp": "2025-08-25T10:36:56",
    "url": "https://reddit.com/r/rstats/comments/1mzx47x/i_keep_getting_an_error_and_object_not_found/",
    "score": 0,
    "num_comments": 19,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1n00owc",
    "title": "Flextable said no",
    "content": "So I have been using the same flextable for two weeks now with no issues. Today, all kinds of issues popped up. \nThe error is (function(nrow, keys, vertical.align = \"top\", text.direction = \"lrtb\", : argument \"keys\" is missing,  with no default. \n\nI searched the error and addressed everything it could be (even just a glitch) and even restarted. My code is in the picture (too hard to type that on my phone).... help or the Dell gets it!! Lol",
    "author": "jaimers215",
    "timestamp": "2025-08-25T12:50:54",
    "url": "https://reddit.com/r/rstats/comments/1n00owc/flextable_said_no/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mzcwiy",
    "title": "Uncertainty measures for net sentiment",
    "content": "Hi experts,\n\nI have aggregated survey results which I have transformed into net sentiment by taking the proportion disagree from the proportion agree. The groups vary in order of magnitude between 10 respondents up to 4000 respondents. How do I sensibly provide a measure of uncertainty so my audience gets a clear understanding of the variability associated with each score? \n\nInitial research suggested that parametric measures of uncertainty would not be appropriate given the groups can be so small. Over half of all responses come from groups that have less than 25 respondents. So the approach would need to be robust for small groups. Open to bayesian approaches.\n\nThanks in advance!",
    "author": "Salty_Interest_7275",
    "timestamp": "2025-08-24T17:58:56",
    "url": "https://reddit.com/r/rstats/comments/1mzcwiy/uncertainty_measures_for_net_sentiment/",
    "score": 5,
    "num_comments": 5,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1my63be",
    "title": "Fast Rolling Statistics",
    "content": "I work with large time series data on a daily basis, which is computationally intensive. After trying so many different approaches, this is what I end up with. First, use the package roll, which is fast and convenient. Second, if a more customized function is needed, code it up in C++ using Rcpp (and RcppEigen if regressions are needed). [https://jasonjfoster.r-universe.dev/roll](https://jasonjfoster.r-universe.dev/roll)\n\n  \nI have spent countless hours on this type of work. Hopefully, this post can save you some time when encountering similar issues. ",
    "author": "BOBOLIU",
    "timestamp": "2025-08-23T09:33:55",
    "url": "https://reddit.com/r/rstats/comments/1my63be/fast_rolling_statistics/",
    "score": 14,
    "num_comments": 3,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1myadle",
    "title": "Need help interpreting a significant interaction with phia package",
    "content": "Hello.  I'm running several logistic regression mixed effect models, and I'm trying to interpret the simple effects of the significant interaction terms.  I have tried several methods, all of which yield different outcomes, and I do not know how to interpret any of them or which to rely on.  Hoping someone here has some experience with this and can point me in the right direction.\n\nFirst, I fit a model that looks like this:\n\nmodel &lt;- glmer(DV \\~ F1\\*F2 + (1|random01) + (1|random02)\n\nThe dependent variable is binomial.\n\nF1 has two levels: A and B.\n\nF2 has three levels: C, P, and N.\n\nI've specified contrast codes for F2: Contrast 1: (C = 0.5; P = 0.5; N = -1) and Contrast 2 (C = -1; P = 1; N = 0).\n\nThe summary of the model reveals a significant interaction between F1 and F2 (Contrast 2).  I want to understand the simple effects of this interaction, but I am stuck on how to proceed.  I've tried a few things, but mainly these two approaches:\n\n1. I created two data sets (one for each level of F1) and then fit a new model for each: glmer(DV \\~ F2 + (1|random01) + (1|random02).  Then I exponentiated the estimated term to determine the odds ratio.  My issue here is that I can't find any support for this approach, and I was unclear whether I should include the random effects or not.\n\n2. Online searches recommend using the \"phia\" package, and the \"testInteractions\" function, but the output gives me only a single value for the desired contrast when I'm trying to understand how to compare this contrast across the levels of F1.  I also don't know how to interpret the value or what units its in.\n\nAny suggestions are greatly appreciated!  Thank you",
    "author": "AAnxiousCynic",
    "timestamp": "2025-08-23T12:18:45",
    "url": "https://reddit.com/r/rstats/comments/1myadle/need_help_interpreting_a_significant_interaction/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mwvfux",
    "title": "SEM with R",
    "content": "Hi all! \n\nI'm doing my doctoral thesis, and haven't done any quantitative analysis since 2019. I need to do an SEM analysis, using R if possible. I'm looking for tutorials or classes to learn how to do the analysis myself, and there's not many people around me who can help (very small university, not much available time for the professors, and my supervisor can't help). \n\nDoes anyone have suggestions on a textbook I could read or a tutorial I could watch to familiarize myself with it?",
    "author": "Alexndrine",
    "timestamp": "2025-08-21T20:09:31",
    "url": "https://reddit.com/r/rstats/comments/1mwvfux/sem_with_r/",
    "score": 21,
    "num_comments": 19,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mwjw5w",
    "title": "Assistance with mixed-effects modelling in glmmTMB",
    "content": "Good afternoon,\n\nI am using R to run mixed-effects models on a rather... complex dataset.\n\nSpecifically, I have an outcome \"Score\", and I would like to explore the association between score and a number of variables, including \"avgAMP\", \"L10AMP\", and \"Richness\". Scores were generated using the BirdNET algorithm across 9 different thresholds: 0.1,0.2,0.3,0.4 \\[...\\] 0.9.\n\nI have converted the original dataset into a long format that looks like this:\n\n      Site year Richness vehicular avgAMP L10AMP neigh Thrsh  Variable Score\n    1 BRY0 2022       10        22   0.89   0.88   BRY   0.1 Precision     0\n    2 BRY0 2022       10        22   0.89   0.88   BRY   0.2 Precision     0\n    3 BRY0 2022       10        22   0.89   0.88   BRY   0.3 Precision     0\n    4 BRY0 2022       10        22   0.89   0.88   BRY   0.4 Precision     0\n    5 BRY0 2022       10        22   0.89   0.88   BRY   0.5 Precision     0\n    6 BRY0 2022       10        22   0.89   0.88   BRY   0.6 Precision     0\n\nSo, there are 110 Sites across 3 years (2021,2022,2023). Each site has a value for Richness, avgAMP, L10AMP (ignore vehicular). At each site we get a different \"Score\" based on different thresholds.\n\nThe problem I have is that fitting a model like this:\n\n    Precision_mod &lt;- glmmTMB(Score ~ avgAMP + Richness * Thrsh + (1 | Site), family = \"ordbeta\", na.action = \"na.fail\", REML = F, data = BirdNET_combined)\n\nwould bias the model by introducing pseudoreplication, since Richness, avgAMP, and L10AMP are the same at each site-year combination.\n\nI'm at a bit of a slump in trying to model this appropriately, so any insights would be greatly appreciated.\n\nThis humble ecologist thanks you for your time and support!",
    "author": "Pseudachristopher",
    "timestamp": "2025-08-21T11:58:27",
    "url": "https://reddit.com/r/rstats/comments/1mwjw5w/assistance_with_mixedeffects_modelling_in_glmmtmb/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mvj1wz",
    "title": "How Is Collapse?",
    "content": "I‚Äôve been following collapse for a while, but as a diehard data.table user I‚Äôve never seriously considered switching. Has anyone here used collapse extensively for data wrangling? How does it compare with data.table in terms of runtime speed, memory efficiency, and overall workflow smoothness?\n\n[https://cran.r-project.org/web/packages/collapse/index.html](https://cran.r-project.org/web/packages/collapse/index.html)\n\n\n\n",
    "author": "BOBOLIU",
    "timestamp": "2025-08-20T09:07:01",
    "url": "https://reddit.com/r/rstats/comments/1mvj1wz/how_is_collapse/",
    "score": 27,
    "num_comments": 15,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mvifsg",
    "title": "Offtopic: Study on AI Perception published with lots of R and ggplot for analysis and data visualization",
    "content": "I would like to share a research article we have published with the help of R+Quarto+`tidyverse`\\+`ggplot` on the public perception of AI in terms of expectancy, perceived risks and benefits, and overall attributed value.\n\nI don't want to go too much into the details, but people (N=1100, survey from Germany) tend to expect that AI is here to stay, but they see risks, limited benefits and low value. However, in the formation of value judgements, benefits are more important than the risks. User diversity influences the evaluations but age and gender effects are mitigated by data and AI literacy. If you‚Äôre interested, here‚Äôs the full article:  \nMapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance, Technological Forecasting and Social Change (2025), [doi.org/10.1016/j.techfore.2025.124304](http://doi.org/10.1016/j.techfore.2025.124304)\n\nIf you want to push the use of R to other science domains, you can also give us an upvote here: [https://www.reddit.com/r/science/comments/1mvd1q0/public\\_perception\\_of\\_artificial\\_intelligence/](https://www.reddit.com/r/science/comments/1mvd1q0/public_perception_of_artificial_intelligence/) üôèüôà\n\nWe used `tidyverse` a lot for data cleaning and transforming the data into different formats. We study two perspectives: 1) Individual differences in form of a regular data matrix and 2) a rotated, topic-centric perspective with topic evaluations). These topic evaluations are spatially mapped as a scatter plot (e.g., *x*\\-axis for risk and *y*\\-axis for benefit) with `ggplot` and `ggrepel` to display the topics' labels on each point. We also used `geom_boxplot()` and `geom_violin()` plots to display the data. Technically, we munged through 300k data points for the analysis. \n\nI find the scatterplots a bit hard to read owing to the small font size but we couldn't come up with an alternative solution given the huge number of 71 different topics. While this article is published, we appreciate feedback or suggestions on how to improve the legibility of the diagrams (besides querying fewer topics:) The data and analyses are available on osf.\n\nI really enjoy these scatterplots, as they can be interpreted in numerous ways. Besides studying the correlation, e.g. between risks and benefits, one can meaningfully interpret the breadths and intercept of the data.\n\n[Scatterplot of the average risk \\(x\\) and benefit \\(y\\) attributions across the 71 different AI-related topics. There is a strong correlation between both variables. A linear regression lm\\(value\\~risk+benefit\\) explains roughly 95&amp;#37; of the variance in overall value attributed to AI.](https://preview.redd.it/8oas3cd6p6kf1.png?width=850&amp;format=png&amp;auto=webp&amp;s=9f077cfab171a6ab9624c3aa6bf630cf959f52e5)",
    "author": "lipflip",
    "timestamp": "2025-08-20T08:45:20",
    "url": "https://reddit.com/r/rstats/comments/1mvifsg/offtopic_study_on_ai_perception_published_with/",
    "score": 25,
    "num_comments": 1,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mvbk3j",
    "title": "Looking to learn R from practically scratch",
    "content": "like the title says I want to learn to code and graph in R for biology projects and have some experience with it but it was very much copy and paste and I am looking for courses or ideally free resources i can use to really sink my teeth and learn to use it on my own",
    "author": "18if",
    "timestamp": "2025-08-20T03:58:07",
    "url": "https://reddit.com/r/rstats/comments/1mvbk3j/looking_to_learn_r_from_practically_scratch/",
    "score": 35,
    "num_comments": 16,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1muqc1o",
    "title": "RandomWalker Update",
    "content": "My friend and I have updated our RandomWalker package to version 1.0.0\n\nPost: https://www.spsanderson.com/steveondata/posts/2025-08-19/",
    "author": "spsanderson",
    "timestamp": "2025-08-19T11:14:51",
    "url": "https://reddit.com/r/rstats/comments/1muqc1o/randomwalker_update/",
    "score": 29,
    "num_comments": 4,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mulwgs",
    "title": "Adding text to a .png file and then saving it as a new .png file without border",
    "content": "Hi, \n\nI am looking to load in a .png image with readPNG() and then add text using text() but I am struggling with a white border when I resave the image as a new file. My script it essentially:\n\nlibrary(png)  \nblankimg &lt;- readPNG('file.png') #this object has dimensions that suggest it is 1494x790 px\n\npng('newfile.png', width=1494, height=790)  \npar(mar=c(0,0,0,0))  \nplot(0, xlim=c(1,1494), ylim=c(1,790), type='n')  \nrasterImage(blankimg,1,1,1494,790)  \ntext(340,185,'Example Text', adj=0.5, cex=2.5)  \ndev.off()\n\nI don't need to get rid of the axes in the original plotting due to the margin changes but I still get a bit of a white border around the image in the new .png file.\n\nDoes anyone have any ideas? I'd appreciate it :)\n\nThanks!",
    "author": "Takeo7789",
    "timestamp": "2025-08-19T08:36:35",
    "url": "https://reddit.com/r/rstats/comments/1mulwgs/adding_text_to_a_png_file_and_then_saving_it_as_a/",
    "score": 5,
    "num_comments": 7,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mv85kr",
    "title": "PW skills Data analyst is good",
    "content": "",
    "author": "Equivalent_Sir5230",
    "timestamp": "2025-08-20T00:30:18",
    "url": "https://reddit.com/r/rstats/comments/1mv85kr/pw_skills_data_analyst_is_good/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.22,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mth696",
    "title": "Sample size in Gpower: equal groups allocation?",
    "content": "Hello everyone, I hope you are doing well. \nI have a (perhaps simple) question.\n\nI‚Äôm calculating an a priori sample size in G*Power for an F-test. My study is a 3 (Group; between) √ó 3 (Phase/Measurement; within) √ó 2 (Order of phase presentation; between) mixed design. \n\nI initially tried an R simulation, as I know that GPower is not very precise for mixed repeated-measures ANOVAs. However, my supervisors feel it is too complex and that we might be underpowered anyway, so, under the suggestion of our uni statistician, I am using a mixed ANOVA (repeated measures with a between-subjects factor) in GPower instead. We don't account for the within factor as he said it is implied in the repeated measure design.\nI‚Äôve entered all the values (alpha, effect size, power) and specified 6 groups to reflect the Group √ó Order cells. \n\nMy question is: does the total sample size that G*Power returns assume equal allocation of participants across the 6 groups, or not?\nFrom what I understand, in G*Power‚Äôs repeated-measures ANOVA modules you cannot enter unequal cell sizes, so the reported total N should correspond to equal n per group. However, I‚Äôm not entirely sure. \nDoes anyone know of an explicit source or documentation that confirms this?\n\nThank you very much in advance ‚ò∫Ô∏è",
    "author": "Lorsleafs",
    "timestamp": "2025-08-18T02:25:12",
    "url": "https://reddit.com/r/rstats/comments/1mth696/sample_size_in_gpower_equal_groups_allocation/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mt2drh",
    "title": "Positron IDE under 'free &amp; open source' on their website, but has Elastic License 2.0 -- misleading?",
    "content": "The definition of open source, according to [OSD](https://opensource.org/osd), would imply that Positron's Elastic License 2.0 would is not considered 'open source' but 'source available' ought to be the correct term. Further, 'free' means *libre* as in freedom, not free beer.\n\nHowever, when you visit [Posit's website](https://posit.co) and check under 'free &amp; open source' tab, it doubles down by mentioning 'open source' again, and Positron is listed under that section.\n\nCan I get some clarification on this?\n\nEDIT: It seems that on GitHub README, it does indeed say 'source available' so I don't know why this is the case. And there are 109 forks...",
    "author": "jinnyjuice",
    "timestamp": "2025-08-17T13:56:08",
    "url": "https://reddit.com/r/rstats/comments/1mt2drh/positron_ide_under_free_open_source_on_their/",
    "score": 16,
    "num_comments": 9,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mtgh8e",
    "title": "Feedback needed for surveyüôè",
    "content": "",
    "author": "Ok-Energy7962",
    "timestamp": "2025-08-18T01:42:21",
    "url": "https://reddit.com/r/rstats/comments/1mtgh8e/feedback_needed_for_survey/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.22,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mssh3w",
    "title": "Rgent - AI for Rstudio",
    "content": "I was tired of the lack of AI in Rstudio, so I built it.\n\nRgent is an AI assistant that runs inside the RStudio viewer panel and actually understands your R session. It can see your code, errors, data, plots, and packages, so it feels much more ‚Äúaware‚Äù than a generic LLM.\n\nRight now it can:\n\n‚Ä¢ Help debug errors in one click with targeted suggestions\n\n‚Ä¢ Analyze plots in context\n\n‚Ä¢ Suggest code based on your actual project environment\n\nI‚Äôd love feedback from folks who live in RStudio daily. Would this help in your workflow, need different features, etc? I have a free trial at my website and go in-depth there on the security measures. I‚Äôll put it in the comments :)",
    "author": "Puzzleheaded_Bid1535",
    "timestamp": "2025-08-17T07:33:16",
    "url": "https://reddit.com/r/rstats/comments/1mssh3w/rgent_ai_for_rstudio/",
    "score": 3,
    "num_comments": 8,
    "upvote_ratio": 0.52,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ms1idt",
    "title": "Lessons to Learn from Julia",
    "content": "When Julia was first introduced in 2012, it generated considerable excitement and attracted widespread interest within the data science and programming communities. Today, however, its relevance appears to be gradually waning. What lessons can R developers draw from Julia‚Äôs trajectory? I propose two key points:\n\nFirst, build on established foundations by deeply integrating with C and C++, rather than relying heavily on elaborate just-in-time (JIT) compilation strategies. Leveraging robust, time-tested technologies can enhance functionality and reliability without introducing unnecessary technical complications.\n\nSecond, acknowledge and embrace R‚Äôs role as a specialized programming language tailored for statistical computing and data analysis. Exercise caution when considering additions intended to make R more general-purpose; such complexities risk diluting its core strengths and compromising the simplicity that users value.",
    "author": "BOBOLIU",
    "timestamp": "2025-08-16T10:18:12",
    "url": "https://reddit.com/r/rstats/comments/1ms1idt/lessons_to_learn_from_julia/",
    "score": 36,
    "num_comments": 40,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1msj5bx",
    "title": "Undergrad Stats Student Looking For Advice",
    "content": "I‚Äôm currently an undergraduate Statistics student at a university in the Bay Area. I‚Äôll be graduating next year with minors in Data Science and Marketing. What areas would you recommend I focus on for the future of statistics, considering long-term career and financial stability as well as a good work-life balance? I‚Äôm open to all suggestions.",
    "author": "BeginningNo6",
    "timestamp": "2025-08-16T23:04:07",
    "url": "https://reddit.com/r/rstats/comments/1msj5bx/undergrad_stats_student_looking_for_advice/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.46,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mr841z",
    "title": "Make This Program Faster",
    "content": "Any suggestions?\n\n    library(data.table)\n    library(fixest)\n    x &lt;- data.table(\n    ret = rnorm(1e5),\n    mktrf = rnorm(1e5),\n    smb = rnorm(1e5),\n    hml = rnorm(1e5),\n    umd = rnorm(1e5)\n    )\n    carhart4_car &lt;- function(x, n = 252, k = 5) {\n    # x (data.table .SD): c(ret, mktrf, smb, hml, umd)\n    # n (int): estimation window size (1 year)\n    # k (int): event window size (1 week | month | quarter)\n    # res (double): cumulative abnormal return\n    res &lt;- as.double(NA) |&gt; rep(times = x[, .N])\n    for (i in (n + 1):x[, .N]) {\n    mdl &lt;- feols(ret ~ mktrf + smb + hml + umd, data = x[(i - n):(i - 1)])\n    res[i] &lt;- (predict(mdl, newdata = x[i:(i + k - 1)]) - x[i:(i + k - 1)]) |&gt;\n    sum(na.rm = TRUE) |&gt;\n    tryCatch(\n    error = function(e) {\n    return(as.double(NA))\n    }\n    )\n    }\n    return(res)\n    }\n    Sys.time()\n    x[, car := carhart4_car(.SD)]\n    Sys.time()",
    "author": "BOBOLIU",
    "timestamp": "2025-08-15T12:42:22",
    "url": "https://reddit.com/r/rstats/comments/1mr841z/make_this_program_faster/",
    "score": 11,
    "num_comments": 29,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mqvyqx",
    "title": "Struggling with finding a purpose to learn",
    "content": "I have been trying to learn statistical analysis with R (tidyverse) but I have no ultimate goal, and this leads me to questioning all the matter, I see people doing some cool stuff with their programming skills but I rarely see an actual use-case of those projects.\n\n  \nHow did you find a purpose to learn whatever you learned ? I mean aside from work/study requirements how did you manage to keep learning skills that aren't directly going to benefit you ?",
    "author": "al3arabcoreleone",
    "timestamp": "2025-08-15T05:10:08",
    "url": "https://reddit.com/r/rstats/comments/1mqvyqx/struggling_with_finding_a_purpose_to_learn/",
    "score": 10,
    "num_comments": 19,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mrb913",
    "title": "Counting (and ordering) client encounters",
    "content": "I'm working with a dataframe where each row is an instance of a service rendered to a particular client. What I'd like to do is:\n\n1) iterate over the rows in order of date (an existing column)  \n2) look at the name of the client in each row (another existing column), and  \n3) add a number to a new column (let's call it \"Encounter\") that indicates whether that row corresponds to the first, second, third, etc. time that person has received services.\n\nI am certain this can be done, but a little at a loss in terms of how to actually do it. Any help or advice is much appreciated!",
    "author": "looksmall",
    "timestamp": "2025-08-15T14:38:54",
    "url": "https://reddit.com/r/rstats/comments/1mrb913/counting_and_ordering_client_encounters/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mr8cqf",
    "title": "Setting hatch bars to custom color using ggplot2/ggpattern?",
    "content": "I have a data set I would like to plot a bar chart for with summary stats (mean value for 4 variables with error bars). I am trying to have the first 2 bars solid, and the second two bars with hatching on white with the hatching and border in the same color as the first two bars. This is to act as an inset for another chart so I need to keep the color scheme as is, since adding 2 additional colors would make the chart too difficult to follow. (Hence the manual assigning of individual bars) I've been back and forth between my R coding skills (mediocre) and copilot.\n\nI'm 90% there but the hatching inside the bars continues to be black despite multiple rounds of troubleshooting through copilot and on my own. I'm sure the fix is pretty straightforward, but I can't figure it out.\n\nUsing ggplot2 and ggpattern\n\nThanks!\n\n    # aggregate data\n    data1 &lt;- data.frame(\n      Variable = c(\"var1\", \"var2\", \"var3\", \"var4\"),\n      Mean = c(mean(var1), mean(var2), mean(var3), mean(var4)),\n      SEM = c(sd(var1) / sqrt(length(var1)),\n              sd(var2) / sqrt(length(var2)),\n              sd(var3) / sqrt(length(var3)),\n              sd(var4) / sqrt(length(var4))\n    ))\n    \n    # Define custom aesthetics\n    data1$fill_color &lt;- with(data1, ifelse(\n      Variable %in% c(\"var1\", \"var2\"),\n      \"white\",\n      ifelse(Variable == \"var1\", \"#9C4143\", \"#4040A5\")\n    ))\n    \n    data1$pattern_type &lt;- with(data1, ifelse(\n      Variable %in% c(\"var3\", \"var4\"),\n      \"stripe\", \"none\"\n    ))\n    \n    # Set pattern and border colors manually\n    pattern_colors &lt;- c(\n      \"var1\" = \"transparent\",\n      \"var2\" = \"transparent\",\n      \"var3\" = \"#9C4143\",\n      \"var4\" = \"#4040A5\"\n    )\n    \n    border_colors &lt;- pattern_colors\n    \n    ggplot(data1, aes(x = Variable, y = Mean)) +\n      geom_bar_pattern(\n        stat = \"identity\",\n        width = 0.6,\n        fill = data1$fill_color,\n        pattern = data1$pattern_type,\n        pattern_fill = pattern_colors[data1$Variable],\n        color = border_colors[data1$Variable],\n        pattern_angle = 45,\n        pattern_density = 0.1,\n        pattern_spacing = 0.02,\n        pattern_key_scale_factor = 0.6,\n        size = 0.5\n      ) +\n      geom_errorbar(aes(ymin = Mean - SEM, ymax = Mean + SEM),\n                    width = 0.2, color = \"black\") +\n      scale_x_discrete(limits = unique(data1$Variable)) +\n      scale_y_continuous(\n        limits = c(-14000, 0),\n        breaks = seq(-14000, 0, by = 2000),\n        expand = c(0, 0)\n      ) +\n      coord_cartesian(ylim = c(-14000, 0)) +\n      labs(x = NULL, y = NULL) +\n      theme(\n        panel.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        #legend.position = \"none\",\n        panel.border = element_rect(color = \"black\", fill = NA, size = 0.5),\n        axis.line.x = element_line(color = \"black\", size = 0.5)\n      )",
    "author": "pagingbaby123",
    "timestamp": "2025-08-15T12:51:16",
    "url": "https://reddit.com/r/rstats/comments/1mr8cqf/setting_hatch_bars_to_custom_color_using/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mpl8jq",
    "title": "Better Way to Calculate Target Inventory?",
    "content": "Update: Sorry, I did not realize that this subreddit was focused on R. Any help you can offer is likely beyond me, unfortunately.\n\nI am going to do my best to describe what my situation, but I am not much of a stats guy, so please bear with me and I will do my best to clarify whatever I can.\n\nI have been tasked with finding a better way to determine my company's monthly target inventory across all product lines (for what it's worth, we produce to stock, not to order) and to do it in Excel in such a way that it was fairly automatic. Apparently, target inventory was determined using mostly guesswork based on historical trends up until now.\n\nFrom my initial research, the basic formula I settled on was: T*arget Inventory = Avg Period Demand(Review Period + Lead time) + Safety Stock*\n\nMy supervisor and I went back and forth on refining the formula to fit our needs, and it was decided that for our *Average Period Demand* (which we are basing on monthly sales forecast numbers), would need to be weighted. Since we are looking at a year out for targeting, outlier months could throw off our EOY inventory. So the further away an individual month's forecasted sales are from the year's average, the lower its weight is. My supervisor also asked that months with 0 forecasted sales actually be weighted the same as months that are close to the average to ensure that we do not overproduce (we make perishable food products, so overproduction leads to waste quickly).\n\nThere are some more details I can fill in if need be, but in short my current problem is this:\n\nTo keep things consistent with our other reports, my supervisor stipulated that the sum of the Product Weighted Averages be equal to the weighted average of the Product Group (PG being the sum of each product therein). The problem is that when you total the weighted averages, they sometimes don't equal the weighted average of the Product Group. In my original spreadsheet, I speculate that this had to do with the weighted 0s, as groups without 0s DO total out properly. Unfortunately, I cannot seem to replicate this effect in an example sheet.\n\nEssentially, I need either a) a better way to take into account months with 0 forecasted sales that allows for my supervisor's stipulations, or b) an entirely different way to determine target inventory. Option A is preferred at this point, but I'll take what I can get.\n\nAny input is welcome!\n\nhttps://preview.redd.it/b8i8ydmffvif1.png?width=1448&amp;format=png&amp;auto=webp&amp;s=ea2d04d9e530051b39c95f0be9584211c766ce07",
    "author": "northoberlin16",
    "timestamp": "2025-08-13T17:11:09",
    "url": "https://reddit.com/r/rstats/comments/1mpl8jq/better_way_to_calculate_target_inventory/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mpb5ge",
    "title": "Naming Column the Same as Function",
    "content": "It is strongly discouraged to name a variable the same as the function that creates it. How about data.frame or data.table columns? Is it OK to name a column the same as the function that creates it? I have been doing this for a while, and it saves me the trouble of thinking of another name. ",
    "author": "BOBOLIU",
    "timestamp": "2025-08-13T10:41:10",
    "url": "https://reddit.com/r/rstats/comments/1mpb5ge/naming_column_the_same_as_function/",
    "score": 2,
    "num_comments": 9,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mogvq6",
    "title": "Best intro stats textbook for undergrads (with R)?",
    "content": "I‚Äôll be teaching applied statistics to undergrads (200-level) and want to introduce them to R from the start. This will be an introductory course, so they will have no prior experience with stats at the college level. \n\nI‚Äôm deciding between three books and would love your thoughts on which works best:\n\n1. *An Introduction to Statistical Learning: with Applications in R* (ISLR)\n\n2. Field‚Äôs *Discovering Statistics Using R*\n\n3. Agresti‚Äôs *Statistical Methods for the Social Sciences* \n\nWould you recommend one over the others? Thoughts on this welcome!",
    "author": "Safe-Antelope-7775",
    "timestamp": "2025-08-12T11:33:22",
    "url": "https://reddit.com/r/rstats/comments/1mogvq6/best_intro_stats_textbook_for_undergrads_with_r/",
    "score": 47,
    "num_comments": 27,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mov455",
    "title": "How to set working directory (and change permissions) (mac)",
    "content": "I am *very* new to R and RStudio and I'm attempting to change the working directory.. I've tried everything and it's simply not allowing me to open files. There's a good likelihood that I'm missing something easy.. Does someone know how to help?\n\nIn the bar at the top of my mac, when i go: session &gt; set working directory &gt; choose directory, it isn't allowing me to select files. I assume it's something to do with permissions but I can't figure out how to change it.\n\nIn the code, I've gone:\n\nbase\\_directory &lt;- \"\\~/Desktop/filename.csv\" (as directed in the instructions I'm using). That's worked fine (I think).\n\nThen:\n\nsetwd(base\\_directory)\n\nIt comes up: Error in setwd(base\\_directory) : cannot change working directory\n\nDoes anyone have any advice?",
    "author": "glorious_papaya",
    "timestamp": "2025-08-12T21:42:29",
    "url": "https://reddit.com/r/rstats/comments/1mov455/how_to_set_working_directory_and_change/",
    "score": 1,
    "num_comments": 9,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1moflsb",
    "title": "A Series of Box Plot Tutorials I Made",
    "content": "Several weeks ago I made a tutorial series about scatter plots, and it seemed to help a lot of people. So, I wanted to make an additional series about box plots. Does anyone have any requests for what type of plotting tutorials to make next? \n\n",
    "author": "Amber32K",
    "timestamp": "2025-08-12T10:47:23",
    "url": "https://reddit.com/r/rstats/comments/1moflsb/a_series_of_box_plot_tutorials_i_made/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1moduoc",
    "title": "applying an inflator vector to a data matrix",
    "content": "i have a matrix m with various econ measures grouped by year (i.e. columns = year, x1, x2...\n\nI want to convert them to net present value to i have another data matrix (year, inflator).\n\nwhat is the best was to apply the transformation? ",
    "author": "m0grady",
    "timestamp": "2025-08-12T09:43:30",
    "url": "https://reddit.com/r/rstats/comments/1moduoc/applying_an_inflator_vector_to_a_data_matrix/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mo432n",
    "title": "RStudio on macOS Tahoe",
    "content": "Has anyone tried it and have you seen any issues? I don't recall many issues in new macOS versions in the past, but this is a major UI redesign and given RStudio's current wonky window behaviour I am wondering if this has got worse. (not expecting it to get better....)",
    "author": "SarkyBot",
    "timestamp": "2025-08-12T02:31:58",
    "url": "https://reddit.com/r/rstats/comments/1mo432n/rstudio_on_macos_tahoe/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mnemxr",
    "title": "Recommendations for Dashboard Tools with Client-Side Hosting and CSV Upload Functionality",
    "content": "I am working on creating a dashboard for a client that will primarily include bar charts, pie charts, pyramid charts, and some geospatial maps. I would like to use a template-based approach to speed up the development process.\n\nMy requirements are as follows:\n\n1. The dashboard will be hosted on the client‚Äôs side.\n2. The client should be able to log in with an email and password, and when they upload their own CSV file, the data should automatically update and be reflected on the frontend.\n3.  I need to submit my shiny project to the client once it gets completed. \n\nCan I do these things by using Shiny App in R ?  Need help and suggestions.Recommendations for Dashboard Tools with Client-Side Hosting and CSV Upload Functionality",
    "author": "CalendarOk67",
    "timestamp": "2025-08-11T07:33:59",
    "url": "https://reddit.com/r/rstats/comments/1mnemxr/recommendations_for_dashboard_tools_with/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mn8luh",
    "title": "Missing data pattern plot using ggplot2",
    "content": "Is anybody aware of a function that can produce a plot like mice::md.pattern but as a ggplot?  md.pattern is great but with big datasets and complex patterns it gets unreadable quickly.  The ability to resize, flip coordinates etc would be really helpful.\n\nEdit: the function I wanted was ggmice::plot\\_pattern()\n\n",
    "author": "Misfire6",
    "timestamp": "2025-08-11T02:44:55",
    "url": "https://reddit.com/r/rstats/comments/1mn8luh/missing_data_pattern_plot_using_ggplot2/",
    "score": 7,
    "num_comments": 7,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mms925",
    "title": "For anyone curious about the Positron IDE:  I found a neat guide on using it with Dev Containers",
    "content": "I‚Äôve been exploring **Positron IDE** lately and stumbled across a nice little guide that shows how to combine it with:\n\n* **Dev Containers** for reproducible setups\n* **DevPod** to run them anywhere\n* **Docker** for local or remote execution\n\nIt‚Äôs a simple, step-by-step walkthrough that makes it much easier to get Positron up and running in a portable dev environment.\n\nRepo &amp; guide here:  \nüëâ [https://github.com/davidrsch/devcontainer\\_devpod\\_positron](https://github.com/davidrsch/devcontainer_devpod_positron)",
    "author": "FriendlyAd5913",
    "timestamp": "2025-08-10T12:51:57",
    "url": "https://reddit.com/r/rstats/comments/1mms925/for_anyone_curious_about_the_positron_ide_i_found/",
    "score": 40,
    "num_comments": 9,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mngrxi",
    "title": "LoadLibrary failure",
    "content": "Ayuda, por favor. Trato de instalar \"permute\" pero me dice que tengo un problema con stats. Me sugirieron desinstalar e instalar nuevamente R, pero el problema contin√∫a. ¬øAlguien sabe como resolverlo?",
    "author": "Comfortable-Agent397",
    "timestamp": "2025-08-11T08:54:38",
    "url": "https://reddit.com/r/rstats/comments/1mngrxi/loadlibrary_failure/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.17,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mm1rxf",
    "title": "Experience with Databricks as an R user?",
    "content": "I‚Äôm interested in R users‚Äô opinions of Databricks. My work is really trying to push its use and I think they‚Äôll eventually disallow running local R sessions entirely ",
    "author": "One-Plastic6501",
    "timestamp": "2025-08-09T15:10:54",
    "url": "https://reddit.com/r/rstats/comments/1mm1rxf/experience_with_databricks_as_an_r_user/",
    "score": 39,
    "num_comments": 24,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mlyk9u",
    "title": "Need help figuring out how to find state changes",
    "content": "Hello all, hopefully someone has experience with this and knows how to accomplish it. The background is that I‚Äôm trying to figure out when something is and isn‚Äôt moving based off the change in signal strength between its transmitter and a static receiver. I have a time series of detection data that I‚Äôve trended so that points where the object is sitting still have a negative value and when it‚Äôs moving the points have positive values. I‚Äôve graphed the cumulative sum of these points for easier visualization, and added notation where the thing is still or ‚Äòon‚Äô and moving or ‚Äòoff.‚Äô \n\nWhat I‚Äôd like to do, and am seeking help to do so, is to figure out a way to make something akin to a rolling window that samples 20 points of data at a time, moving forward thru the data one point at a time. As it ‚Äòcrawls,‚Äô I want it to track the up/down trend of points. If, after tracking negative values it comes across a positive one, I want it to track the next 10 points and if they are all positive, I want it to record the time of that first positive point and assign a value indicating a state change for the thing. \n\nI‚Äôd also like it to do the opposite and identify when the thing goes from moving (positive values) to sitting still (negative values). \n\nThis is all pretty complicated, definitely out of my wheelhouse but I need to get it done and could really use some help. If anyone has an idea of how to accomplish this or can point me towards a guide that does exactly this, I‚Äôd appreciate it! ",
    "author": "KipKebal",
    "timestamp": "2025-08-09T12:53:04",
    "url": "https://reddit.com/r/rstats/comments/1mlyk9u/need_help_figuring_out_how_to_find_state_changes/",
    "score": 16,
    "num_comments": 9,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mm5r8l",
    "title": "Generating random smooth surfaces",
    "content": "Hello everyone,\n\nI‚Äôm a graduate student in aerospace engineering currently working on a research project involving sensitivity analysis of the buckling load of cylindrical shells with random geometric imperfections. Specifically, I want to generate random but smooth surface imperfections on cylindrical shells for use in numerical simulations.\n\nMy advisor has recommended that I look into Gaussian random fields (GRFs) and the Karhunen‚ÄìLo√®ve (K‚ÄìL) expansion as potential tools for modeling these imperfections.\n\nAlthough I have some background in probability and statistics (an undergraduate course taken about 8 years ago), I would still consider myself a novice in this area. I recently watched a YouTube video titled *\"Implementing Random Fields in MATLAB: A Step-by-Step Guide\"*, but I found myself struggling to understand the theory behind the implementation, particularly how the correlation structure and smoothness are controlled.\n\nI‚Äôd really appreciate it if someone could help me with the following:\n\n* What are the main methods for generating smooth random fields, especially in 2D for curved geometries?\n* What basic probability/statistics and stochastic process concepts should I learn or revisit to understand these methods properly?\n* Are there any recommended resources (books, papers, tutorials) for learning GRFs and the Karhunen‚ÄìLo√®ve expansion with applications in structural mechanics?\n\nThank you in advance for any guidance or resources you can share!",
    "author": "[deleted]",
    "timestamp": "2025-08-09T18:22:02",
    "url": "https://reddit.com/r/rstats/comments/1mm5r8l/generating_random_smooth_surfaces/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mlutxm",
    "title": "Do rows not re-index? And I get a FALSE when I check that a condition matches, though I can see that specific item in my data frame",
    "content": "I had a data frame with over three thousand rows. Then I filtered some data out, selected what I wanted, and built a new data frame. This new data frame has 738 rows. However, when I view the data frame, the rows are numbered with their original indices. It makes it confusing. For example, row 1 in the new data frame is row 4 from the original (see [here](https://imgur.com/a/K4Lxs4A)).\n\nAnother issue. I'm trying to find the index of two specific rows that are the beginning and end of the time series. For the first, I do `which(ts_data_cleaned$datetime == \"2024-07-24 18:00:00\")` and I get row 471. I do the same for the end date: `which(ts_data_cleaned$datetime == \"2024-09-06 15:00:05\")` and the result is `integer(0)`. \n\nThen I tried `any(ts_data_cleaned$datetime == \"2024-09-06 15:00:05\")` and the result was `FALSE`. How is that possible when I can see it in the data frame?\n\nI've tried troubleshooting based on what I know and with AI, but can't crack it.\n\nTLDR:\n\n* A created a new data frame from a subset of a larger one. When I view the data frame, the row numbers came from the original data frame, so row numbers go into the thousands, despite it having only 738 rows.\n\n* I'm trying to get the row number for a datetime. Apparently my datetime doesn't exist, though I can see it in the data frame?",
    "author": "EngineEngine",
    "timestamp": "2025-08-09T10:17:28",
    "url": "https://reddit.com/r/rstats/comments/1mlutxm/do_rows_not_reindex_and_i_get_a_false_when_i/",
    "score": 0,
    "num_comments": 15,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mlbzwl",
    "title": "patchwork plot_spacer() (and other solutions) does not take enough space",
    "content": "I have 5 panels that I would like to arrange in a 3 row/2 column configuration, so I use the patchwork layout:\n\n`(r1_left + r1_right)/(r2_left + r2_right) / (r3_left + r3_void)`  \n  \nBut no matter how I try (`r3_void` can be `ggplot() + geom_void()+ theme.void()` or `plot_spacer()`, or several other things I have tried), the `r3_left` panel is always  a bit too wide (even with `plot_layout(widths=c(5,5))`).  Putting the y-axis on the left in the left panels helped a lot, but it's still not perfect.  Suggestions?",
    "author": "fasta_guy88",
    "timestamp": "2025-08-08T17:41:03",
    "url": "https://reddit.com/r/rstats/comments/1mlbzwl/patchwork_plot_spacer_and_other_solutions_does/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mk53s6",
    "title": "üöÄ Upcoming R Consortium Webinar ‚Äî SAS to R in Pharma: Creating Custom Solutions for Closed-Source Code üöÄ",
    "content": "üìÖ September 9\n9 AM PT / 12 PM ET\n\nüëâ Save your spot: [https://r-consortium.org/webinars/sas-to-r-in-pharma-creating-custom-solutions-for-closed-source-code.html](https://r-consortium.org/webinars/sas-to-r-in-pharma-creating-custom-solutions-for-closed-source-code.html)\n\nWhen a heavily regulated pharmaceutical client needed to migrate a complex, proprietary SAS pipeline to R, ProCogia‚Äôs team built a high-fidelity replacement that plugged straight into the existing workflow.\n\nJoin Gabriel Martins Brock, R Developer at ProCogia, to learn:\n\nüîß How to replicate closed-source SAS functionality in open-source R  \nüìê Why a structured evaluation process is mission-critical for compliance  \nüöÄ Lessons learned delivering production-ready code in high-stakes environments  \n\nGabriel brings experience across pharma, finance, healthcare, and consumer analytics, leveraging R, Python, SAS, and SQL on AWS &amp; Google Cloud to solve real-world challenges.\n\nüëâ Save your spot: [https://r-consortium.org/webinars/sas-to-r-in-pharma-creating-custom-solutions-for-closed-source-code.html](https://r-consortium.org/webinars/sas-to-r-in-pharma-creating-custom-solutions-for-closed-source-code.html)",
    "author": "jcasman",
    "timestamp": "2025-08-07T09:42:19",
    "url": "https://reddit.com/r/rstats/comments/1mk53s6/upcoming_r_consortium_webinar_sas_to_r_in_pharma/",
    "score": 17,
    "num_comments": 4,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mjwxcq",
    "title": "R course similar to laerd statistics",
    "content": "I‚Äôm currently analysing my data and so far I‚Äôve done it all with SPSS and laerd statistics was a great help. I also want to analyse the data in R, I have basic R skills but not enough to do it without a guideline. Does anyone have a recommendation for a pretty straightforward R guide/ course similar to laerd statistics for SPSS but for R ? Thank you very much !!",
    "author": "Good_Research8368",
    "timestamp": "2025-08-07T03:56:09",
    "url": "https://reddit.com/r/rstats/comments/1mjwxcq/r_course_similar_to_laerd_statistics/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mjm5im",
    "title": "I built a data flow to track personal finances with Google Sheets, Colab &amp; Looker Studio",
    "content": "**TL;DR:**\n\nI built a **free personal finance data flow** using **R** in the Google architecture (Google Sheets + Colab + Looker Studio). It lets you:\n\n* Track **how much money you have**\n* See **income vs expenses** over time\n* **Project end‚Äëof‚Äëyear balance**\n* Visualize **where your money comes from and goes**\n\nSetup: Copy the files ‚Üí connect them ‚Üí run the Colab script ‚Üí enjoy your dashboard.\n\n\\[**Demo + files here** ‚Üí [*https://drive.google.com/drive/folders/1Vmf662pa7qaVa\\_9h-vThF6j9CwsKYVeu?usp=sharing*](https://drive.google.com/drive/folders/1Vmf662pa7qaVa_9h-vThF6j9CwsKYVeu?usp=sharing) \\]\n\n============================\n\nHi everyone I‚Äôve been tinkering with **R** in the architecture of Google: **Google Sheets + Google Colab + Looker Studio,** and ended up building a **personal finance data flow** that I‚Äôve been using monthly. It‚Äôs still a **minimum viable product** (not polished), but it works ‚Äî and I want to share it.\n\nI call it **EnergyBank**.\n\n# What it does\n\n* Shows **how much money you have** right now\n* Tracks **income vs expenses** over time\n* Projects **end‚Äëof‚Äëyear balance**\n* Visualizes **where money comes from and where it goes**\n\n# What‚Äôs inside\n\n**Folders:**\n\n* `01_data` ‚Üí the main Google Sheet\n* `02_docs` ‚Üí (documentation to be done)\n* `03_scripts` ‚Üí an **R script** (runs on Colab) for data consolidation\n\n**Google Sheet includes:**\n\n* Categories (activities)\n* Annual planning (planned income/expenses)\n* Execution (actual transactions)\n* Automatic reconciliation (planned vs executed)\n* Combined transaction log\n\n**Dashboard in Looker Studio:**\n\n* Planned vs spent (timeline)\n* Income and expense distribution\n* Money flow: where it comes from and where it goes\n\n# How to use it\n\n1. **Make copies** of the Google Sheet, Colab script, and dashboard.\n2. Rename your copies and connect them (**update the script to point to your Sheet**).\n3. Define your activities and enter your annual plan in the ‚Äúplanning‚Äù sheet.\n4. Log real transactions in the ‚Äúexecution‚Äù sheet (only amount/date).\n5. Run the Colab script ‚Üí it updates reconciliation &amp; transaction log.\n6. Refresh the dashboard ‚Üí see your updated cash flow visuals.\n\nIt‚Äôs not plug‚Äëand‚Äëplay (you‚Äôll need to configure a few things), but once set up, it‚Äôs powerful.\n\nWould love **feedback, suggestions, or collaborators** (documentation, UX improvements, new visualizations).\n\n\\[**Demo (with dummy data) + files here ‚Üí** [*https://drive.google.com/drive/folders/1Vmf662pa7qaVa\\_9h-vThF6j9CwsKYVeu?usp=sharing*](https://drive.google.com/drive/folders/1Vmf662pa7qaVa_9h-vThF6j9CwsKYVeu?usp=sharing) \\]\n\n# ",
    "author": "cruzjulian",
    "timestamp": "2025-08-06T17:53:38",
    "url": "https://reddit.com/r/rstats/comments/1mjm5im/i_built_a_data_flow_to_track_personal_finances/",
    "score": 7,
    "num_comments": 2,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mj2skx",
    "title": "Copy the Pros: Recreate a Viral NYTimes Chart in R",
    "content": "I've been waiting for a chart to go at least semi-viral for the past few weeks so I could make a video like this.",
    "author": "Pecners",
    "timestamp": "2025-08-06T05:04:13",
    "url": "https://reddit.com/r/rstats/comments/1mj2skx/copy_the_pros_recreate_a_viral_nytimes_chart_in_r/",
    "score": 19,
    "num_comments": 0,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mizbpe",
    "title": "EMM differences after ANOVA",
    "content": "Hello! I am currently working with carbon flux data and I performed a two-way ANOVA using the two factors \"year type\" (dry years and reference years) and \"ecosystem\" (9 different ecosystems). I found significant interactions between the two factors. Then, I computed estimated marginal means (EMM) and their differences within each ecosystem. The sample sizes vary across the groups.\n\nanova &lt;- aov(z\\_score \\~ year\\_type + ecosystem + year\\_type \\* ecosystem, data = carbon\\_flux)  \nem &lt;- emmeans(anova, \\~ year\\_type | ecosystem)  \npairs(em)\n\nMy questions now are: Why are the EMM in my case identical to the mean of the corresponding group? How are the confidence intervals computed?\n\nMy understanding is that a significant p-value (p&lt;alpha) indicates a significant difference between dry years and reference years in the corresponding ecosystem.\n\nThank you for any help, I really appreciate it! Since this is my first reddit-post, I hope I have explained my problem clearly.",
    "author": "Dangerous_Chance2248",
    "timestamp": "2025-08-06T01:43:25",
    "url": "https://reddit.com/r/rstats/comments/1mizbpe/emm_differences_after_anova/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mid9zh",
    "title": "How do you share Quarto notebooks readably on OSF ‚Äî without spinning up a separate website?",
    "content": "As a researcher, I try to increase the transparency of my work and now publish not only the manuscripts, but also the data, materials, and the R-based analysis. I conduct the analysis in Quarto using R. The data are hosted on [osf.io](https://osf.io). However, I‚Äôm not satisfied with how the components are integrated.\n\nWhile it‚Äôs  possible for interested readers or other researchers to download the notebook and the data, render them locally, and then verify the results (or take a different path in the data analysis), I‚Äôm looking for a better way to present a rendered Quarto notebook in a readable format *directly* on the OSF website.\n\nI explicitly do *not* want to create a separate website. Of course, this would be easy to do with Quarto, but it would go against my goal of keeping data, materials, and analyses hosted with an independent provider of scientific data.\n\nAny idea how I can realize this?",
    "author": "lipflip",
    "timestamp": "2025-08-05T09:06:22",
    "url": "https://reddit.com/r/rstats/comments/1mid9zh/how_do_you_share_quarto_notebooks_readably_on_osf/",
    "score": 25,
    "num_comments": 23,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1miphj1",
    "title": "Aggregated data across years analysis",
    "content": "Hi! I have doubt what would be the best solution to a simple research problem. I have data across 15 years and counts of admitted patients with certain symptoms for each year. The counts go from around 40 to around 100. That is 15 rows of data (15 rows, 2 columns). The plot shows a slight u-shaped relation between years (on x-axis) and counts on y-axis. Due to overdispersion I fitted a negative binomial model to model the count data, instead of poisson. I also included the quadratic year^2, so the model is count ~ year_centered +I( year_centered^2). And it fits better than the model with only year. The quadratic term is statistically significant and positive while the linear is not, although it's close. I have tried glmmTMB tom account for autocorrelation, but the models are virtually the same. My question is, can I trust the results from a negative binomial regression given my number of observations 15, and small degrees of freedom? Is this worth modeling or just showing the plot? Is there any other model that would be better suited for this scenario? \n\nHere is the output:\n\nCoefficients:\nEstimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   3.847625   0.094680  40.638   &lt;2e-16 ***\nYear_c      0.025171   0.014041   1.793   0.0730 .\nI(Year_c^2) 0.009391   0.003686   2.548   0.0108 *\n\nSignif. codes:  0 ‚Äò‚Äô 0.001 ‚Äò‚Äô 0.01 ‚Äò‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n(Dispersion parameter for Negative Binomial(25.3826) family taken to be 1)\n\nNull deviance: 26.561  on 14  degrees of freedom\n\nResidual deviance: 15.789  on 12  degrees of freedom\nAIC: 128.65\n\nNumber of Fisher Scoring iterations: 1\n\nTheta:  25.4   \n      Std. Err.:  14.2\n\n2 x log-likelihood:  -120.645\n\n\nThank you in advance!",
    "author": "eyesenck93",
    "timestamp": "2025-08-05T16:54:14",
    "url": "https://reddit.com/r/rstats/comments/1miphj1/aggregated_data_across_years_analysis/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mi7sap",
    "title": "Rao (Cursor for RStudio) v0.3 out!",
    "content": "It was great to see so much interest in Rao a few weeks ago, so we're posting an update that [Rao v0.3.0 is out](https://www.lotas.ai/download)! Here's what's new since last time:\n\n* **Free tier at 50 queries per month.**¬†Intended for people who might use Rao occasionally but found the single week trial too short.\n* **Zero data retention policies with Anthropic and OpenAI.**¬†With the fact that we don‚Äôt store any data either, this means that¬†*no user data whatsoever (code, data files, etc.) is stored or used to train models.*\n   * We continue to collect user analytics data by default, but we‚Äôve added a¬†`Secure mode`¬†toggle to turn this off. We also have a¬†`Web search`¬†toggle that determines whether the assistant can search the internet.\n   * Our [security policy is here](https://www.lotas.ai/security).\n* **Single-click sign-in.**¬†A¬†`Sign in/Sign up`¬†button will automatically sign you in on the app (no more manual API keys). Your key will be securely stored locally to keep you logged in between sessions.\n* **Rao rules.**¬†You can specify a set of instructions the model should follow. This will always be provided to the model when you make queries.\n* **Automatic app updates.**¬†The app will fetch new updates when available and install them automatically so you stay up to date with our latest features and bug fixes.\n* **Demo datasets.**¬†We‚Äôve included¬†[6 demo datasets](https://github.com/lotas-ai/rao/tree/main/demos)¬†in the Rao GitHub that you can try out to get started. Topics range from a metagenomic analysis of Crohn‚Äôs Disease and Ulcerative Colitis to comparing energy access across African countries. Each demo only takes 1-2 queries.\n* **Search/replace.**¬†We‚Äôve provided the models with a search and replace function for more precise code edits that also allows users in Secure mode to use the app without calling our third party provider for fuzzy edits.\n* **Merged RStudio updates**¬†All updates made to RStudio through late July are merged in, so anything you do in RStudio should work in Rao.\n* Other robustness and speed updates‚Ä¶\n\nWould love any feedback and thoughts on what you want to see in the next version!",
    "author": "SigSeq",
    "timestamp": "2025-08-05T05:26:35",
    "url": "https://reddit.com/r/rstats/comments/1mi7sap/rao_cursor_for_rstudio_v03_out/",
    "score": 28,
    "num_comments": 3,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1micha6",
    "title": "Interpreting PERMANOVA results",
    "content": "Hi all,  \nI‚Äôm working on a microbiome beta diversity analysis using Bray-Curtis distances calculated from a `phyloseq` object in R. I have 2 groups (treatment vs c) (n=16). I‚Äôm using the `adonis2()` function from the **vegan** package to test whether diet groups have significantly different microbial communities. Each time I run the code, the **p-value (**`Pr(&gt;F)`**) is slightly different** ‚Äî sometimes below 0.05, sometimes not (`Pr(&gt;F)` = 0.046, 0.043, 0.052, 0.056, 0.05). I understand it‚Äôs a permutation test, but now I‚Äôm unsure how to **report significance**.\n\nHere‚Äôs a simplified version of my code:\n\nmetadata &lt;- as(sample\\_data(ps\\_b\\_diversity), \"data.frame\")\n\n\\#recalculate the Bray-Curtis distance matrix\n\nbray\\_dist &lt;- phyloseq::distance(ps\\_b\\_diversity, method = \"bray\")\n\nadonis\\_result &lt;- adonis2(bray\\_dist \\~ Diet, data = metadata)\n\nadonis\\_result",
    "author": "Top_Substance_8659",
    "timestamp": "2025-08-05T08:35:25",
    "url": "https://reddit.com/r/rstats/comments/1micha6/interpreting_permanova_results/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mi27yt",
    "title": "Analysis help",
    "content": "Hi r/rstats \nI've been asked by a friend to help with some analysis and I really want to but my issue is I don't really know complex stats and they can't afford an actual statistican. I haven't done anything really since leaving college and I think my comfort using r is mistaken for statistical prowess. \n\nI need to analyse the data to see if the number of observations per minute surveying (OPUE) is influenced by factors such as month, season and site. Normally I'd use a glm in this case but the data is skewed due to lots of surveys where nothing was seen. \nThe data has:\n- right skew \n- lots of 0 values \n- uneven sampling effort by month, site \n\nHonestly any advice on where to go would be great I'm just stuck ATM. \nSorry if the answer is super obvious. ",
    "author": "Silly-Web-1008",
    "timestamp": "2025-08-04T23:59:10",
    "url": "https://reddit.com/r/rstats/comments/1mi27yt/analysis_help/",
    "score": 8,
    "num_comments": 9,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mgdlsh",
    "title": "uv for R",
    "content": "Someone really should build a similar tool for R as uv for Python. Conda does manage R versions and packages in a severely limited way. The whole Rstat users need a uv like tool asap.",
    "author": "Lanky-Introduction-9",
    "timestamp": "2025-08-03T00:54:03",
    "url": "https://reddit.com/r/rstats/comments/1mgdlsh/uv_for_r/",
    "score": 40,
    "num_comments": 51,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mgw5g6",
    "title": "Dynamic date-based table and colored rows",
    "content": "Hey everyone,\n\nI‚Äôm trying to create a table with three columns: movie titles, release dates, and a status column that categorizes each movie as ‚Äúpast‚Äù, ‚Äúin cinema‚Äù, ‚Äúnow in cinema‚Äù, ‚Äúsoon in cinema‚Äù, ‚Äúthis year in cinema‚Äù, or ‚Äúnext year in cinema‚Äù.\n\nI want the rows to be automatically colored based on the status, and the table to update dynamically depending on today‚Äôs date.\n\nI‚Äôve tried several approaches but haven‚Äôt been able to get it working correctly yet. Is it even possible to implement this in R? I‚Äôd really appreciate any help or pointers!\n\nHere‚Äôs my current code (please excuse the German variable names, as I‚Äôm German):\n\n    ---\n    title: \"Kinotabelle mit Farben\"\n    output:\n    ¬† pdf_document:\n    ¬† ¬† latex_engine: xelatex\n    ¬† ¬† keep_tex: false\n    ¬† ¬† toc: false\n    ¬† ¬† number_sections: false\n    header-includes:\n    ¬† - \\usepackage[table]{xcolor}\n    ¬† - \\definecolor{past}{HTML}{D3D3D3}\n    ¬† - \\definecolor{in_cinema}{HTML}{FFFACD}\n    ¬† - \\definecolor{now}{HTML}{98FB98}\n    ¬† - \\definecolor{soon}{HTML}{ADD8E6}\n    ¬† - \\definecolor{this_year}{HTML}{D8BFD8}\n    ¬† - \\definecolor{next_year}{HTML}{FFB6C1}\n    ---\n    \n    ```{r setup, include=FALSE}\n    knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)\n    library(dplyr)\n    library(lubridate)\n    library(readr)\n    library(kableExtra)\n    ```\n    \n    ```{r kinotabelle}\n    kino &lt;- read_csv(\"kino.csv\", show_col_types = FALSE)\n    \n    \n    kino &lt;- kino %&gt;%\n    ¬† mutate(Releasedatum = as.Date(Releasedatum, format = \"%Y %m %d\"))\n    \n    heute &lt;- Sys.Date()\n    \n    kino &lt;- kino %&gt;%\n    ¬† mutate(Status = case_when(\n    ¬† ¬† Releasedatum == heute ~ \"Jetzt im Kino\",\n    ¬† ¬† Releasedatum &lt; heute &amp; (heute - Releasedatum) &lt;= 30 ~ \"Im Kino\",\n    ¬† ¬† Releasedatum &lt; heute ~ \"Vergangen\",\n    ¬† ¬† Releasedatum &gt; heute &amp; (Releasedatum - heute) &lt;= 7 ~ \"Demn√§chst im Kino\",\n    ¬† ¬† year(Releasedatum) == year(heute) ~ \"Dieses Jahr im Kino\",\n    ¬† ¬† year(Releasedatum) &gt; year(heute) ~ \"N√§chstes Jahr im Kino\"\n    ¬† ))\n    \n    farben &lt;- c(\n    ¬† \"Vergangen\" = \"past\",\n    ¬† \"Im Kino\" = \"in_cinema\",\n    ¬† \"Jetzt im Kino\" = \"now\",\n    ¬† \"Demn√§chst im Kino\" = \"soon\",\n    ¬† \"Dieses Jahr im Kino\" = \"this_year\",\n    ¬† \"N√§chstes Jahr im Kino\" = \"next_year\"\n    )\n    \n    kino %&gt;%\n    ¬† arrange(Releasedatum) %&gt;%\n    ¬† select(Titel, Releasedatum, Status) %&gt;%\n    ¬† kbl(booktabs = TRUE, col.names = c(\"Titel\", \"Releasedatum\", \"Status\")) %&gt;%\n    ¬† kable_styling(latex_options = c(\"striped\", \"hold_position\", \"scale_down\")) %&gt;%\n    ¬† row_spec(0, bold = TRUE) %&gt;%\n    ¬† row_spec(1:nrow(kino), background = farben[kino$Status])\n    ```\n    \n\n  \n",
    "author": "No-King-8224",
    "timestamp": "2025-08-03T15:10:53",
    "url": "https://reddit.com/r/rstats/comments/1mgw5g6/dynamic_datebased_table_and_colored_rows/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mgcjoh",
    "title": "Losing my mind over output sign reversal",
    "content": "I am trying to do a meta-analysis with the help of metafor and escalc. I am extremely stuck on the first study out of 150 and losing my mind.\n\nI am simply trying to correctly quantify the effect size of their manipulation check, which they gave summary stats of as a within-subjects variable. I am therefor assuming r = 0.5 since it is not reported and using SMCC to calculate Gz and Gz\\_variance (please god tell me if this is wrong!).\n\nMy code:\n\n\\&gt; es\\_within &lt;- escalc(  \n\\+     measure = \"SMCC\",  \n\\+     m1i = 4.38, sd1i = 1.56, # Pre-test stats  \n\\+     m2i = 5.92, sd2i = 1.55, # Post-test stats  \n\\+     ni = 25, ri = 0.5,       # N and correlation  \n\\+ )  \n\\&gt;  \n\\&gt; print(es\\_within)\n\nyi     vi  \n1 -0.9590 0.0584\n\nObviously, the pre &gt; post change was an increase from 4.38 to 5.92, so the effect size should be positive, no? Yet it is reported as -0.959\n\nThe documentation for SMCC specifically says\n\nm1i = vector with the means (first group or time point).\n\nm2i = vector with the means (second group or time point).\n\nwhich is what I have done. However when I ask AI for suggestions on why it is nonetheless returning a negative sign it tells me the first part of the SMCC formula is just m1i - m2i, so to fix this I should just put the higher value in m1i if I want the sign to be correctly positive. I ask it why the documentation would say the opposite and it says the documentation is wrong. I don't dare trust AI over the actual documentation, just wanted it to give some suggestions, and it literally just suggests the documentation is misleading/ wrong. What is going on here? As a PhD student I have booked a consultation with the staff statistics support team but that won't happen for another week, I don't really have that time to spare. Please, if you have any advice...",
    "author": "madcatte",
    "timestamp": "2025-08-02T23:45:58",
    "url": "https://reddit.com/r/rstats/comments/1mgcjoh/losing_my_mind_over_output_sign_reversal/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mfo122",
    "title": "Beginner to statistics, I can't figure out if I should use dharma for lmer model, please help",
    "content": "I have to do an analysis using mixed effect model for the volumes of some regions of human brain. In my model i've included the information about the regions (5), gender, hemispere and age. At firts I used the lmer model and checked the assumptions for normal distribution of residuals and heteroskedasticity using xyplots and qq norm. The results showed some heavy tails, and some pattern in heteroskedasticity. I've tried  transforming the volumetric values using log - it helped a bit but not enough, then i tried adding weights, also not helpful. Then i used glmmTMB model, and for that on I've found that dharma function is better to check residuals - the results are fine. But then when doing research I've found that you can also use dharma on lmer model, i did, and the results are also fine. Now I'm just so confused what I should do. I'm a beginner to statistics, and the only help I have is the internet and ai, which kinda sucks. I would really appreciate if anyone would be available to discuss the problem.",
    "author": "PatternMysterious550",
    "timestamp": "2025-08-02T04:16:08",
    "url": "https://reddit.com/r/rstats/comments/1mfo122/beginner_to_statistics_i_cant_figure_out_if_i/",
    "score": 9,
    "num_comments": 6,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mfx6zf",
    "title": "Want More Visibility for Your SaaS? I Can Help.",
    "content": "",
    "author": "The_TechGuy_",
    "timestamp": "2025-08-02T11:09:55",
    "url": "https://reddit.com/r/rstats/comments/1mfx6zf/want_more_visibility_for_your_saas_i_can_help/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.11,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1me1feq",
    "title": "Copy the Pros: Recreate a NYTimes Chart in R",
    "content": "What can I say, I enjoy making these videos. ü§∑‚Äç‚ôÇÔ∏è",
    "author": "Pecners",
    "timestamp": "2025-07-31T06:21:55",
    "url": "https://reddit.com/r/rstats/comments/1me1feq/copy_the_pros_recreate_a_nytimes_chart_in_r/",
    "score": 72,
    "num_comments": 7,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1me3f44",
    "title": "oRm: An object relational model framework for R",
    "content": "`oRm` is inspired by sqlalchemy. I kept wanting to reach for an ORM solution to provide a backend for things like interactive shiny tables or reproducible data entry. So, as they say \"be the change you want to see in the world.\" For those not previously introduced to ORM, it's an object oriented approach to CRUD operations via objects (rows) and their related data (foreign keys).\n\nYou can think of `oRm` like a wrapper that takes your tried and true `DBI` connection methods and `dbplyr` filtering syntax to make `R6` mutable objects. And once you have your objects, the real magic happens in the relationships. \n  \nyou can jump straight to the pkdown site [here](https://kent-orr.github.io/oRm/).\n\nA couple of points to get out of the way before I give an example:\n\n- This package is not for analysis and statistical work, it's not for reading large tables (though it can), and it doesn't seek to improve on or compete with `dbplyr,` in fact I use `dbplyr` under the hood so I can rely on their dialect agnostic syntax as much as possible.  \n- Yes, `reticulate` does make sqlalchemy very easy to port into any R work. But what if you just don't know python very well, and / or don't want a `.Renviron` and a `.env`, and `.renv/` and a `.venv/` in your project?  \n\nAnd a couple of features that I'm not going to get to in this post, but are likely to interest some people:\n\n- with.Engine allows for a managed transaction state with automatic rollback in case of failure.  \n- on delete and on update support for related objects.  \n- Some dialect specific support, for example making use of a flush() method and `RETURNING` for postgres backends.  \n\n## Okay, now show me what it looks like\n\nSure thing. `oRm` uses a few key objects:\n\n- Engine: your db connection\n- TableModel: a model representing a sql table  \n- Record: an object that represents a row in a table\n- Relationships: mappings between TableModels that define how observations are linked together. \n\nThe example below is based on the idea of having a data team entering measurements of plant heights during the course of an experiment.\n\n## Engine\n\nThe engine uses `DBI` under the hood. So the syntax should be very familiar, some might even say the exact same to what you're used to. This example uses SQLite, but you should be able to plop whatever driver you want in there. \n\n    library(oRm)\n    \n    engine &lt;- Engine$new(\n      drv = RSQLite::SQLite(),\n      dbname = \":memory:\",\n      persist = TRUE # this arg is sqlite memory specific, not always needed\n    )\n\nYour engine will manage opening and closing connections for you. You can also implicitly create a managed pool with the argument `use_pool=TRUE`. There are a few methods that you might find useful from your engine itself, but for the most part you just define it and leave it be. \n\n## TableModel\n\nYou can use the `TableModel$new()` method, but I like the hierarchical structure of building my table model off the `engine` it relies on. Defining a TableModel you give a table name and a list of Columns. \n\n    Measurements &lt;- engine$model(\n      tablename = \"measurements\",\n      id = Column(\"INTEGER\", primary_key = TRUE),\n      observer_id = Column(\"INTEGER\"),\n      plant_id = ForeignKey(\"INTEGER\", references = 'plants.id'),\n      measurement_date = Column(\"DATE\"),\n      measurement_value = Column(\"REAL\")\n    )\n    Measurements$create_table()\n\n## Records\n\nAgain, you can define a `Record$new()` but I like to make my records from the TableModel they came from. \n\n    m1 = Measurements$record(\n      observer_id = 1,\n      plant_id = 101,\n      measurement_date = as.Date(\"2025-07-30\"),\n      measurement_value = 14.2\n    )\n    # and after we have m1, we need to explicitly create it in the db\n    m1$create()\n\nAt this point, we have our object representing a single row. If you go no further, this will give you CRUD functionality at the row level. The methods assigned to a Record are named to align with CRUD:\n\n\n    m1$create()\n    m1$update(measurement_value = 15)\n    # m1$delete()\n\nThe 'R' belongs to the table, since you're reading from there. Here's an example to get our m1 object from the table itself. You can use dbplyr filter syntax here. \n\n    m1_read = Measurements$read(observer_id == 1, mode = 'get')\n    m1_read\n\nIf you've gotten this far, I'm going to consider you formally interested and refer you to the pkdown site for seeing the [Relationships](https://kent-orr.github.io/oRm/articles/why_oRm.html#relationships) in action. This post mirrors that documenation, so you'll pick up right where you left off here.",
    "author": "binarypinkerton",
    "timestamp": "2025-07-31T07:41:52",
    "url": "https://reddit.com/r/rstats/comments/1me3f44/orm_an_object_relational_model_framework_for_r/",
    "score": 31,
    "num_comments": 10,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mds0zi",
    "title": "R vs Python",
    "content": "Is becoming a data scientist doable with only R proficiency (tidyverse,ggplot2, ML models, shiny...) and no python knowledge \n(Problems of a degree in probability and statistics)",
    "author": "Dillon_37",
    "timestamp": "2025-07-30T21:21:07",
    "url": "https://reddit.com/r/rstats/comments/1mds0zi/r_vs_python/",
    "score": 59,
    "num_comments": 88,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mdvg36",
    "title": "Help with tidying data (updated)",
    "content": "I wasn‚Äôt able to upload a screenshot to my previous post so here is an updated post with a screenshot. \n\nI‚Äôm learning about tidying data. I have a dataset where each Row is a different climate measurement. The columns are initially months, then number of years, start date, end year.\n\nWhat‚Äôs confusing me about getting this into tidy format is that some of the rows are values (eg. temperature), while others are dates in DD-MM-YYYY form. I thought of having a value and a date column but not all of the measurements have dates.\n\nAny advice would be appreciated - I am new to this!",
    "author": "RepresentativeTwo852",
    "timestamp": "2025-07-31T00:49:19",
    "url": "https://reddit.com/r/rstats/comments/1mdvg36/help_with_tidying_data_updated/",
    "score": 14,
    "num_comments": 8,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1me2678",
    "title": "Help with small dataset and large feature space",
    "content": "Hiya, \n\nI have a spectral library with 56 observations and about 2000 features (full spectral range). I use Pearson correlation between each spectral feature and my target variable (biochemical variable) to reduce the feature count, so I end up with about 100/150 features. It is a longitudinal study where same individuals were sampled at multiple time points.\n\nI'm trying to use PLSR to predict the biochemical variable from the spectra. There's a few things I'm unsure about, hoping someone here has some valuable insight:\n\n1) does my approach sound reasonable?\n2) with such a smal dataset, im unsure how to deal with the data split and cross validation. Seems that nested CV is recommended in cases of small datasets. Any suggestions on how to implement that with PLSR? \n3) related to point above: a few models I've already built (using LOOCV and training/test 70/30) achieve higher R^2 in the test set than in the training set. How can that be explained?\n\ncheers",
    "author": "Strange-Equipment400",
    "timestamp": "2025-07-31T06:52:28",
    "url": "https://reddit.com/r/rstats/comments/1me2678/help_with_small_dataset_and_large_feature_space/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mejman",
    "title": "How will AI impact R programmers in the near future?",
    "content": "With the rise of tools like ChatGPT and other generative models, how do you think AI will impact our work?\nFor those of us who program in R, is there a real risk?\nI wonder if the demand for R programmers ‚Äî in analysis, data science, or statistics ‚Äî will decrease in the future.\nDo you see a real threat of being replaced?\n",
    "author": "International_Mud141",
    "timestamp": "2025-07-31T18:33:59",
    "url": "https://reddit.com/r/rstats/comments/1mejman/how_will_ai_impact_r_programmers_in_the_near/",
    "score": 0,
    "num_comments": 41,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mdgwfy",
    "title": "[Q] Linear Regression &amp; P-values (of regressors)",
    "content": "Is it possible for a small sample size to have a large p-value?\n\nFor example, say I'm collecting data on conductivity and chloride (Cl-) concentrations (both in the field and in the lab) and making a linear regression model to find if there is correlation (model: Cl = Œ≤1EC + u). Let's say that the actual relationship between Cl- and conductivity is a prefect correlation.\n\nWhen the sample size is small, I would imagine that the data in the field will a much larger p-value, as though the 2 are actually perfectly correlated, the residuals from field data will be a lot larger (due to omitted variables\\*), so the p-value of the coefficient will be a lot smaller.  However, as the sample size increases, the difference in residual coefficient from the lab model and the field model should decrease, I think.\n\nIs my understanding correct? If not, what have I misunderstood?\n\nAlso, the smaller the p-value, the smaller the residuals, so the smaller the R^(2) value, right?\n\n\\* Omitted variables could (from what I understand) lead to omitted variable bias (so the coefficients will be inaccurate). But (to my understanding), that is a slightly different topic.",
    "author": "Good-Breakfast-5585",
    "timestamp": "2025-07-30T13:03:16",
    "url": "https://reddit.com/r/rstats/comments/1mdgwfy/q_linear_regression_pvalues_of_regressors/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1md58ei",
    "title": "\"collapse\" in r",
    "content": "stata user here:\n\nis there an equivalent to the collapse command in r? i have budget data by line item and department is a categorical variable. i want to sum at the department level. \n\n",
    "author": "m0grady",
    "timestamp": "2025-07-30T05:26:49",
    "url": "https://reddit.com/r/rstats/comments/1md58ei/collapse_in_r/",
    "score": 6,
    "num_comments": 28,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mdfs8i",
    "title": "Help me with my design please",
    "content": "",
    "author": "xBliss_",
    "timestamp": "2025-07-30T12:20:16",
    "url": "https://reddit.com/r/rstats/comments/1mdfs8i/help_me_with_my_design_please/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mbzqnq",
    "title": "How do to this kind of plot",
    "content": "is a representation where the proximity of the points implies a relationship or similarity.",
    "author": "International_Mud141",
    "timestamp": "2025-07-28T19:28:29",
    "url": "https://reddit.com/r/rstats/comments/1mbzqnq/how_do_to_this_kind_of_plot/",
    "score": 256,
    "num_comments": 45,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mcen92",
    "title": "How to build a thriving R community: Lessons from Salt Lake City",
    "content": "Julia Silge shares insights on growing an inclusive and technically rich R user group in Salt Lake City. From solo consultants to PhDs, the group brings together a wide range of backgrounds with a focus on community, consistency, and connection to the broader #rstats ecosystem.\n\nIf you're running a local meetup‚Äîor thinking about starting one‚Äîthis post is worth a read.\n\nüîó [https://r-consortium.org/posts/julia-silge-on-fostering-a-technical-inclusive-r-community-in-salt-lake-city/](https://r-consortium.org/posts/julia-silge-on-fostering-a-technical-inclusive-r-community-in-salt-lake-city/)\n\nWhat‚Äôs worked (or not worked) in your local R/data science community? Would love to hear other experiences.",
    "author": "jcasman",
    "timestamp": "2025-07-29T08:34:29",
    "url": "https://reddit.com/r/rstats/comments/1mcen92/how_to_build_a_thriving_r_community_lessons_from/",
    "score": 22,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mcgunp",
    "title": "need help with some correlations im trying to do",
    "content": "Hi everyone! I'm rather new to R and trying to work with this proteomics data set I have. I want to correlate my protein of interest with all others in the dataset. when I first tried, I was getting warnings about the SD being 0 for many of my proteins and I was confused why when I already did quality control when tidying my data. Either way, I think i fixed it and went through with the correlations but now it's just showing me correlations for the proteins against themselves. Can someone tell me what I'm doing wrong or how I can fix this?\n\n    # transpose dataset to make proteins columns and samples rows\n    cea_t &lt;- t(cea_norm_abund)\n    \n    # identify target protein\n    target_protein &lt;- \"Q6DUV1\"\n    \n    # Check if your protein of interest exists \n    if (!\"Q6DUV1\" %in% colnames(cea_t)) {\n      stop(\"Protein Q6DUV1 not found in data.\")\n    }\n    \n    # Define a function that handles missing values safely\n    safe_cor &lt;- function(x, y) {\n      valid &lt;- complete.cases(x, y) \n      if (sum(valid) &lt; 2) return(NA)  # Need at least 2 points \n      return(cor(x[valid], y[valid], method = \"spearman\"))\n    }\n    \n    # get expression values for target protein\n    target_vec &lt;- cea_t[, 'Q6DUV1']\n    \n    # run corrs\n    cor_vals &lt;- apply(cea_t, 2, function(x) safe_cor(x, target_vec))\n    \n    # got an error above so filtering out warning proteins\n    sd(target_vector, na.rm = TRUE)\n    zero_sd_proteins &lt;- apply(cea_t, 2, function(x) sd(x, na.rm = TRUE) == 0)\n    sum(zero_sd_proteins)  # How many proteins have zero variance?\n    \n    # I got 288 so let's remove proteins with zero variance\n    cea_t_filtered &lt;- cea_t[, apply(cea_t, 2, function(x) sd(x, na.rm = TRUE) != 0)]\n    \n    # Then run correlations again\n    correlations &lt;- apply(cea_t_filtered, 2, function(x) cor(x, target_vector, use =   \n    \"pairwise.complete.obs\", method = \"spearman\"))\n    \n    # Sort in descending order\n    cor_sorted &lt;- sort(correlations, decreasing = TRUE)\n    \n    # Remove NA values (from zero-variance proteins)\n    cor_sorted &lt;- cor_sorted[!is.na(cor_sorted)]\n    \n    # Get top 20 correlated proteins\n    top_n &lt;- 20\n    top_proteins &lt;- names(cor_sorted)[1:top_n]\n    \n    # create corr table\n    top_table &lt;- data.frame(Protein = top_proteins, Correlation = cor_sorted[1:top_n])\n    \n    # View and save \n    print(top_table)\n    write.csv(top_table, \"top_correlated_proteins.csv\", row.names = FALSE)\n\n\n\n\n\n",
    "author": "Southern-War-8915",
    "timestamp": "2025-07-29T09:56:33",
    "url": "https://reddit.com/r/rstats/comments/1mcgunp/need_help_with_some_correlations_im_trying_to_do/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mcj8fb",
    "title": "replacing non-numeric with 0s",
    "content": "i have a 10x77 table/data frame with missing values randomly throughout. they are either coded as \"NA\" or \".\"\n\nHow do i replace them with zeros without having to go line by line in each row/column?  \n\nedit 1: the reason for this is i have two sets of budget data, adopted and actual, and i need to create a third set that is the difference. the NAs/. represent years when particular line items werent funded.\n\nedit 2: i dont need peoples opinions on potential bias, ive already done an MCAR analysis. ",
    "author": "m0grady",
    "timestamp": "2025-07-29T11:23:09",
    "url": "https://reddit.com/r/rstats/comments/1mcj8fb/replacing_nonnumeric_with_0s/",
    "score": 1,
    "num_comments": 12,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1maxjxt",
    "title": "Rcpp is Highly Underrated",
    "content": "Whenever I need a faster function, I can write it in C++ and call it from R via Rcpp. To my best knowledge, Python still does not have something that can compile C++ codes on the fly as seamless as Rcpp. The closest one is cppyy, but it is not as good and lacks adoption. ",
    "author": "BOBOLIU",
    "timestamp": "2025-07-27T14:04:58",
    "url": "https://reddit.com/r/rstats/comments/1maxjxt/rcpp_is_highly_underrated/",
    "score": 67,
    "num_comments": 27,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1mb1ypr",
    "title": "[OC] The rise of HIV research compared to tuberculosis over time (PubMed data, 1980‚Äì2023)",
    "content": "",
    "author": "madkeepz",
    "timestamp": "2025-07-27T17:21:05",
    "url": "https://reddit.com/r/rstats/comments/1mb1ypr/oc_the_rise_of_hiv_research_compared_to/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m9ri2g",
    "title": "I'm making some ggplot tutorials for beginners",
    "content": "Hey everyone, I've been using R for several years, but I don't really feel like I've done much to give back to the community. So I decided to start making a series of tutorials about ggplot. The goal is to create a comprehensive playlist that covers the basics but also scales up to more advanced topics. \n\nPlease let me know if anyone has any suggestions or potential topics to cover in future episodes.",
    "author": "Amber32K",
    "timestamp": "2025-07-26T04:56:39",
    "url": "https://reddit.com/r/rstats/comments/1m9ri2g/im_making_some_ggplot_tutorials_for_beginners/",
    "score": 110,
    "num_comments": 9,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m9xl9u",
    "title": "I often see people in this subreddit using three backticks for code blocks or wrong format for tables on reddit, presuming it's identical to Markdown. So I made a Markdown to reddit converter!",
    "content": "",
    "author": "BIOffense",
    "timestamp": "2025-07-26T09:26:34",
    "url": "https://reddit.com/r/rstats/comments/1m9xl9u/i_often_see_people_in_this_subreddit_using_three/",
    "score": 12,
    "num_comments": 1,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ma8ffe",
    "title": "Extracting point values from a raster and the objects are not quite overlapping",
    "content": "I am trying to do a point value extraction of some sampling sites on a raster of oceanic net primary productivity and having a hard time getting the points and the raster to overlap exactly despite having the same crs. The extraction generates some values but also a bunch of NAs. When mapped, you can see the points don't seem to quite overlap the Aleutian Islands like they're supposed to. I'd appreciate any help I can get. My R code is below and you can get an example raster here:¬†[https://orca.science.oregonstate.edu/.../eppley.2012183..](https://orca.science.oregonstate.edu/.../eppley.2012183..).\n\n    library(sf)\n    library(raster)\n    library(terra)\n    library(dplyr)\n    \n    df &lt;- df &lt;- data.frame(\n      Latitude =  c(53.95563333,  53.65600833, 53.855755,  53.93453667,  54.0081),\n      Longitude = c(-166.058595, -167.46038,-167.3238867, -167.1091167, -166.9350567)\n    )\n    \n    df &lt;- df %&gt;% select(-Depth)\n    prod_rast &lt;- raster(file.choose())\n    crs(prod_rast) &lt;- st_crs(4326)\n    df_sf &lt;- st_as_sf(x =df,\n                      coords = c(\"Latitude\", \"Longitude\"),\n                      crs = 4326)\n    df_sf &lt;- st_cast(df_sf, 'POINT')\n    values &lt;-as.data.frame(\n      raster::extract(prod_rast, df_sf))\n    #map check\n    plot(prod_rast)\n    plot(st_geometry(df_sf), add=T, pch=19, col=\"red\")\n     \n\nhttps://preview.redd.it/qq9ujovd7bff1.png?width=920&amp;format=png&amp;auto=webp&amp;s=8a15c591cdb7fbde751b0876e7f11882649197ac\n\n",
    "author": "accidental_hydronaut",
    "timestamp": "2025-07-26T17:13:31",
    "url": "https://reddit.com/r/rstats/comments/1ma8ffe/extracting_point_values_from_a_raster_and_the/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ma03f8",
    "title": "What encoding to choose when I save? (RStudio)",
    "content": "I've used RStudio for a few years at this point. Today is the first time that it asked me to choose my encoding when I tried to save. A quick search makes it seem like it's related to symbols in my code: I used the degree symbol to indicate temperature. So what encoding do I use (UTF-8 (system default), ASCII, BIG5, etc...)?",
    "author": "EngineEngine",
    "timestamp": "2025-07-26T11:08:28",
    "url": "https://reddit.com/r/rstats/comments/1ma03f8/what_encoding_to_choose_when_i_save_rstudio/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m92ne6",
    "title": "Switch from RStudio to Positron",
    "content": "Howdy friends,\n\nI am trying to switch from RStudio to the Positron IDE. I am fairly well stuck on stupid with this transition. Do any of you have any good video recommendations to orient me to Positron better?\n\nThank you!",
    "author": "jaimers215",
    "timestamp": "2025-07-25T08:45:08",
    "url": "https://reddit.com/r/rstats/comments/1m92ne6/switch_from_rstudio_to_positron/",
    "score": 52,
    "num_comments": 24,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m96t0d",
    "title": "Interaction in R",
    "content": "Hello,\n\nI am trying to do a repeated measures analysis with the codes below. However, I'd like to incorporate an interaction term to see if the changes in \"luckmas\" is different by Age within \"Group\". How can I do this? \n\n  \nwtlfu is the dataset\n\nvisit identifies which visit the data point is from\n\nGroup identifies the 2 different groups of interest. \n\n  \n**curepmeas(wtlfu, \"luckmas\", \"visit\", \"Group\", interact=T)**\n\n  \n",
    "author": "Snoo-25191",
    "timestamp": "2025-07-25T11:22:37",
    "url": "https://reddit.com/r/rstats/comments/1m96t0d/interaction_in_r/",
    "score": 10,
    "num_comments": 2,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m90h4a",
    "title": "IIT JAM and GATE preparation",
    "content": "I'm currently in my third year of B.Sc. (Hons.) in Statistics and I'm interested in pursuing an M.Sc. in Data Science from an IIT. I'm planning to appear for IIT JAM and GATE exams, but I'm unsure how to start my preparation. With all the changes under NEP, I‚Äôm a bit confused‚Äîwill my honors degree still make me eligible for a master‚Äôs at IIT?\n\nCan someone guide me on how to begin, what resources to use, and how much time to dedicate?\nMy qualifications: B.Sc. Statistics (Hons.), currently in 3rd year.\n",
    "author": "sleekcinch",
    "timestamp": "2025-07-25T07:21:47",
    "url": "https://reddit.com/r/rstats/comments/1m90h4a/iit_jam_and_gate_preparation/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m8va47",
    "title": "Is there a way to order data according to two factors on the x-axis?",
    "content": "Hi!\n\nIs there a way to order data according to two factors on the x-axis?\n\nI have a dataset of temperature data over several years. I want to plot the means per season for each year in a geom\\_point(). I have Year and Season as two factors, and mean Temperature as my dependent variable. Is there a way to plot this so i have the seasons in order over the years (so 2005: spring, summer, autumn, winter; 2006: spring, summer, autumn, winter; etc)?\n\nI have tried making a combined Year\\_Season factor, but then it just keeps ordering itself by season, so i get all the springs of every year first, etc...",
    "author": "Frosty_Lawfulness_24",
    "timestamp": "2025-07-25T03:08:50",
    "url": "https://reddit.com/r/rstats/comments/1m8va47/is_there_a_way_to_order_data_according_to_two/",
    "score": 4,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m94fsz",
    "title": "Quantmod package errors out while requesting FRED data?",
    "content": "I use the quantmod package to download economic data from various sources. In the last couple days, the FRED (Federal Reserve data source) has been wonky. \n\nAs example:\n\n    &gt; quantmod::getSymbols(\"GDP\", src = \"FRED\") \n    Error in getSymbols.FRED(Symbols = \"GDP\", env = &lt;environment&gt;, verbose = FALSE,  : \n      Unable to import \"GDP\".\n    cannot open the connection\n    In addition: Warning message:\n    Failed to open 'https://fred.stlouisfed.org/series/GDP/downloaddata/GDP.csv': Could not resolve host: https\n\n  \nI haven't updated the package or R version, and last week it worked fine. Any idea what could be going on?\n\n  \nFor counter-example: stock data from Yahoo seems to be working without issue.\n\n    &gt; quantmod::getSymbols(\"AAPL\", src = \"yahoo\")\n    [1] \"AAPL\"",
    "author": "peperazzi74",
    "timestamp": "2025-07-25T09:52:41",
    "url": "https://reddit.com/r/rstats/comments/1m94fsz/quantmod_package_errors_out_while_requesting_fred/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m85ghv",
    "title": "Recreating a New York Times Chart in R  | Line-by-line Coding Tutorial",
    "content": "Yu",
    "author": "Pecners",
    "timestamp": "2025-07-24T07:10:42",
    "url": "https://reddit.com/r/rstats/comments/1m85ghv/recreating_a_new_york_times_chart_in_r_linebyline/",
    "score": 36,
    "num_comments": 0,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m6bzng",
    "title": "Show me beautiful R code",
    "content": "I really love seeing beautiful code (as in aesthetically pleasing).\n\nI don't think there is just one way of making code beautiful though. With Python I like one line does one thing code even if you end up with lots of intermediate variables. With (Frontend) Javascript (React), I love the way they define functions within functions and use lambdas literally everywhere.\n\nI'd like to see examples of R code that you think is beautiful to look at. I know that R is extremely flexible, and that base, data.table and tidyverse are basically different dialects of R. But I love the diversity and I want to see whatever so long as it looks beautiful. Pipes, brackets, even right-assign arrows... throw 'em at me.\n",
    "author": "Top_Lime1820",
    "timestamp": "2025-07-22T04:50:57",
    "url": "https://reddit.com/r/rstats/comments/1m6bzng/show_me_beautiful_r_code/",
    "score": 96,
    "num_comments": 64,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m6z9rv",
    "title": "Results of My First R Project",
    "content": "",
    "author": "TurboBlackpillYT",
    "timestamp": "2025-07-22T20:57:16",
    "url": "https://reddit.com/r/rstats/comments/1m6z9rv/results_of_my_first_r_project/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m5u33k",
    "title": "If you use R, you need to know R Markdown ‚Äî it‚Äôs a must-have tool",
    "content": "Whether you're doing data analysis, writing reports, or preparing presentations, R Markdown lets you combine code, text, and output in a clean, reproducible format ‚Äî all inside one document. It can even replace tools like Word, PowerPoint, and Excel for many workflows.\n\nI've just released a video walking through the basics, and I‚Äôll be sharing some lesser-known tricks that even experienced users might not know.\n\nHope you like it.",
    "author": "OnlyDemor",
    "timestamp": "2025-07-21T13:30:29",
    "url": "https://reddit.com/r/rstats/comments/1m5u33k/if_you_use_r_you_need_to_know_r_markdown_its_a/",
    "score": 81,
    "num_comments": 41,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m6ntyr",
    "title": "Random Intercept Cross Lag Panel model with hierarchical correlation structure. Need help",
    "content": "Hi all, I'm working on my masters project currently and hitting a road block that no one around me seems to know how to solve. I'm using a cross lag panel model to model the relationships between daily movement and sleep. Participants were measured for a full week at 4 different time points, so my model needs to account for the covariance within participant, and within the week of measurement. I'm using the 'lavaan' package, but right now my models are treating each participant x week as an independent observation. Does anyone know how to get lavaan to do the more complex correlation structure, or could you recommend other packages that might be more suited to this problem? Thanks in advance for any help.",
    "author": "SirWallaceIIofReddit",
    "timestamp": "2025-07-22T12:36:52",
    "url": "https://reddit.com/r/rstats/comments/1m6ntyr/random_intercept_cross_lag_panel_model_with/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m6f628",
    "title": "Visualizing hierarchical data",
    "content": "I have data where I am dealing with subsubsubsections. I basically want a stacked bar chart where each stack is further sliced (vertically).\n\nMy best attempt so far is using treemapify and wrap plots, but I can‚Äôt get my tree map to not look box-y (i.e., I can‚Äôt get my tree map to create bars).\n\nDoes anyone know a solution to this? I‚Äôm stuck.\n\nEdit: clarified wording",
    "author": "_MidnightMeatTrain_",
    "timestamp": "2025-07-22T07:12:16",
    "url": "https://reddit.com/r/rstats/comments/1m6f628/visualizing_hierarchical_data/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m67bcy",
    "title": "Structural equation modeling - mediation comparison of indirect effect between age groups",
    "content": "My model is a mediation model with a binary independent x-variable (coded 0 and 1), two parallel numeric mediators and one numeric dependent y-variable (latent variable). Since I want to compare whether the indirect effect differs across age groups, I first ran an unconstrained model in which I allow that paths and effects to vary. Then, I ran a second model, a constrained one, in which I fixed the¬†**indirect effects**¬†across the age groups. Last, I run a Likelihood Ratio (LRT) to test whether the constrained model is a better fit, and the answer is no.\n\nI extensively wrote up the statistical results of the unconstrained model, then shortly the model fit indices of the constrained one, to later compare them with the LRT.\n\nAre these steps appropriate for my research question?\n\nSo the first model was a good fit, the second as well, and the LRT revealed that the model did not improve, so there is no difference in indirect effects when comparing the age groups.",
    "author": "conversation_14",
    "timestamp": "2025-07-22T00:05:01",
    "url": "https://reddit.com/r/rstats/comments/1m67bcy/structural_equation_modeling_mediation_comparison/",
    "score": 6,
    "num_comments": 1,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m5jkmk",
    "title": "Fantasy Basketball Lineup Tool",
    "content": "If anyone here is interested in fantasy basketball, I just uploaded my R code for fantasy basketball to help prepare rosters for the playoffs throughout the season. The full description of the code and the github link are below:\n\nThe purpose of this code is to help show the impacts of adding/subtracting players on the fantasy basketball playoffs. This can used be throughout the entire season to help keep an eye on the layout of the different schedules your players have during the playoffs to help with decisions involving player aquisitions. The idea is that you want to minimize the number of times you have to leave a player on the bench because your lineup is full. If you can start up to 8 people per day, then every time you have more than 8 players with a game in one day, you're essentially wasting the points for all the extra players you have to put on your bench. It would be optimal to instead have the starts spread out as much as possible (given that the total number of starts remains the same). This code shows, in a number of different ways, which team's schedule would best fit the schedules of the players currently on your team, as well as which players on your team have schedules that are not optimal compared to the rest of your team.\n\nThis code is specifically designed for the format of the league that I'm in, which is a points league with 8 lineup spots (5 pos 3 flex), but the code could be adjusted for cat leagues and/or different lineup settings as well. The league I'm in also has contracts that are bid on, rookie drafts, etc., so player additionals/subtractions are less frequent than in a regular redraft league (making this code more necessary), but that doesn't impact how the code is used.\n\n  \n[https://github.com/kevinwaite45/fantasy-basketball](https://github.com/kevinwaite45/fantasy-basketball)",
    "author": "Adventurous-Boot6681",
    "timestamp": "2025-07-21T06:55:56",
    "url": "https://reddit.com/r/rstats/comments/1m5jkmk/fantasy_basketball_lineup_tool/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m5i2e3",
    "title": "üöÄ R Consortium Webinar Alert: Unlocking Collaborative Power with Git, GitHub CI/CD &amp; LLMs in Pharma üöÄ",
    "content": "üóì August‚ÄØ28,‚ÄØ2025\nüïô 10‚ÄØAM‚ÄØPT‚ÄØ/‚ÄØ1‚ÄØPM‚ÄØET\n\n[https://r-consortium.org/webinars/unlocking-collaborative-power-with-git-github-ci-cd-and-llms-in-pharma.html](https://r-consortium.org/webinars/unlocking-collaborative-power-with-git-github-ci-cd-and-llms-in-pharma.html)\n\nReady to see how 15+ programmers from across the pharma industry turned Git &amp; GitHub into a force‚Äëmultiplier for clinical‚Äëtrial workflows? We‚Äôll break down:\n\nProven branching &amp; review tactics that kept a multi‚Äëcompany codebase humming.\n\nHow GitHub Actions + CI/CD slashed QC time and killed tedious manual checks.\n\nA sneak peek at harnessing LLMs for those tricky QC cases that rules can‚Äôt catch.\n\nYou‚Äôll walk away with concrete steps to level‚Äëup your own projects‚Äîand a clear path to sharpen your skills through open‚Äësource contributions.\n\nFeatured speakers\n\nNing‚ÄØLeng ‚Äì Global Head (ad‚ÄØinterim), Data Science Acceleration, Roche\n\nEli‚ÄØMiller ‚Äì Senior Manager, Cloud Solutions, Atorus Research\n\nBen‚ÄØStraub ‚Äì Principal Programmer, GSK\n\nüëâ Save your spot now! [https://r-consortium.org/webinars/unlocking-collaborative-power-with-git-github-ci-cd-and-llms-in-pharma.html](https://r-consortium.org/webinars/unlocking-collaborative-power-with-git-github-ci-cd-and-llms-in-pharma.html)",
    "author": "jcasman",
    "timestamp": "2025-07-21T05:51:22",
    "url": "https://reddit.com/r/rstats/comments/1m5i2e3/r_consortium_webinar_alert_unlocking/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m4gidi",
    "title": "A Great Package to Make R Quicker",
    "content": "I resort to Rcpp for speed and am happy with this approach. Recently, I found a package that transpiles R codes into Fortran codes. If you want speed but dislike C/C++/Fortran, this package is a great solution!\n\n[https://github.com/t-kalinowski/quickr](https://github.com/t-kalinowski/quickr)",
    "author": "BOBOLIU",
    "timestamp": "2025-07-19T21:59:47",
    "url": "https://reddit.com/r/rstats/comments/1m4gidi/a_great_package_to_make_r_quicker/",
    "score": 41,
    "num_comments": 2,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m4htf3",
    "title": "What are the use cases of R arrays?",
    "content": "I have worked with many different R object types such vectors, lists, data frames, nibbles and the like but not R arrays and I can't find good resources giving details on applications of arrays.   If anyone has worked with arrays I would like to hear you use cases and their advantages of the other R objects. Also if you can point me to a good resource where I can learn more that will be appreciated ",
    "author": "66alpha",
    "timestamp": "2025-07-19T23:18:42",
    "url": "https://reddit.com/r/rstats/comments/1m4htf3/what_are_the_use_cases_of_r_arrays/",
    "score": 25,
    "num_comments": 26,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m341b6",
    "title": "R Consortium webinar: Open Source Software Adoption in Japan's Pharma Industry",
    "content": "NEXT WEEK! R Consortium webinar\n\nOpen Source Software Adoption in Japan's Pharma Industry: Key Findings from the 2024 Japan Pharmaceutical Manufacturers Association (JPMA) R Usage Survey\n\nFree registration: [https://r-consortium.org/webinars/open-source-adoption-in-japans-pharma-industry.html](https://r-consortium.org/webinars/open-source-adoption-in-japans-pharma-industry.html)\n\nJoin us for a special webinar hosted by R consortium and the Japan Pharmaceutical Manufacturers Association (JPMA) to explore the results of the \"2024 OSS Usage Status Questionnaire Report.\" This report captures how pharmaceutical companies in Japan are adopting open-source software ‚Äî particularly R ‚Äî and how trends have evolved since our last survey in 2022.\n\nKey highlights include:\n\n-- Over 60% of companies have adopted R; 16 have used or plan to use R for regulatory submissions (e.g., FDA, PMDA).\n\n-- 25% are actively using pharmaverse packages such as Admiral, rtables, and pkglite.\n\n-- More than 80% expressed interest in submitting R Shiny applications, similar to the R Consortium‚Äôs Pilot 4.\n\nDuring the session, JPMA members will walk through the key insights and host a Q&amp;A discussion to address your questions and perspectives.\n\nSpeakers\n\nShinichi Hotta, Sumitomo Pharma Co., Ltd.\n\nShinichi Hotta is the Statistical Programmer at Sumitomo Pharma Co., Ltd. He has 22 years of experience in pharmaceutical companies and CROs (Contract Research Organizations) in Japan as a statistical programmer, working for clinical trials and submissions at Japan, US and China, etc. Through his career, he given presentations about data analyses, SAS, R and CDISC. From 2020, he joined the open source software task force in Japan Pharmaceutical Manufacturers Association (JPMA) as its leader.\n\nYuki Matsunaga, Novartis Pharma K.K. \n\nYuki Matsunaga has worked as a Clinical Development Director Japan, a Statistical Programmer, a Medical Scientific Expert, and a Medical Science Liaison for Novartis Pharma K.K. since April 2017. Recently, he is working on new drug development and retrospective studies using medical real-world data such as electronic healthcare record and health claims data. Also, he is a member of the {admiralophtha} development team, and a start-up member of the open source software task force in Japan Pharmaceutical Manufacturers Association.",
    "author": "jcasman",
    "timestamp": "2025-07-18T07:37:16",
    "url": "https://reddit.com/r/rstats/comments/1m341b6/r_consortium_webinar_open_source_software/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m0s6g5",
    "title": "R Package for Polymarket data",
    "content": "Hello! I put together a simple package together to query event and price data from Polymarket.   \n[https://github.com/clintmckenna/polymarketR](https://github.com/clintmckenna/polymarketR)  \n  \nIt would be great if anyone could give some initial suggestions or feedback. Thanks!  \n",
    "author": "WolverinePsych",
    "timestamp": "2025-07-15T13:03:12",
    "url": "https://reddit.com/r/rstats/comments/1m0s6g5/r_package_for_polymarket_data/",
    "score": 22,
    "num_comments": 9,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m0l5sc",
    "title": "muttest: mutation testing for R",
    "content": "Coverage tools like `{covr}` show **how much of your code is executed by tests**, but reveal nothing about the quality of those tests.\n\nYou can actually have tests with **zero assertions** and still get 100% coverage. That creates a false sense of security.\n\nRecently, I discovered **mutation testing** as a practical way to address this gap, and that's how muttest was created.\n\nHow {muttest} works:\n\n1. **Define a set of code changes (mutations).**\n2. **Run your test suite against mutated versions of your source code.**\n3. **Measure how often the mutations are caught** (i.e., cause test failures).\n\nWhat mutation testing reveals:\n\n* **0% score:** Your tests pass no matter what changes - your assertions are weak.\n* **100% score:** Every mutation triggers a test failure - your tests are robust.\n\n`{muttest}` provides not just a mutation score, but **identifies which files have tests needing stronger assertions**.\n\nCurrently only binary operator mutations are implemented, but more are on their way!\n\nI‚Äôve already used it in my projects and it helped me improve my tests, maybe it‚Äôll help you too?",
    "author": "jakubsobolewski",
    "timestamp": "2025-07-15T08:42:05",
    "url": "https://reddit.com/r/rstats/comments/1m0l5sc/muttest_mutation_testing_for_r/",
    "score": 8,
    "num_comments": 6,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lzvz60",
    "title": "Rao: Cursor for RStudio",
    "content": "Been working on this for a few months: [Rao is Cursor for RStudio](https://www.lotas.ai/). It's a coding assistant in RStudio that reads/writes/edits files, searches for context, runs code/commands, etc. Should make R programming a lot faster. Would love any feedback!",
    "author": "SigSeq",
    "timestamp": "2025-07-14T12:28:25",
    "url": "https://reddit.com/r/rstats/comments/1lzvz60/rao_cursor_for_rstudio/",
    "score": 102,
    "num_comments": 24,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1m0hq0c",
    "title": "Using LSM values in a meta-analysis",
    "content": "I'm trying to conduct a meta-analysis in R. One of my studies only provides Least Squares Mean values and Standard error, while the other studies provide raw values and adjusted means/ mean differences. What meta-analyses could I do? How would you best suggest to go about this? ",
    "author": "Internal_Dog6143",
    "timestamp": "2025-07-15T06:25:58",
    "url": "https://reddit.com/r/rstats/comments/1m0hq0c/using_lsm_values_in_a_metaanalysis/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lzw4j5",
    "title": "[Q] How to get marginal effects for ordered probit with survey design in R?",
    "content": "",
    "author": "ThrowRA_dianesita",
    "timestamp": "2025-07-14T12:34:08",
    "url": "https://reddit.com/r/rstats/comments/1lzw4j5/q_how_to_get_marginal_effects_for_ordered_probit/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lzqpzf",
    "title": "Is there a way to find missing date values in a data frame if the rows are simply missing?",
    "content": "I have a data frame with dates and associated temperatures. Now, there are some dates missing, but I would like to know which ones. These arent NAs in the data frame, they are simply missing rows. The data frame is too large to just go through it to find the missing dates. Is there a way for R to tell me which ones are missing? compare it to a calendar or something?",
    "author": "Frosty_Lawfulness_24",
    "timestamp": "2025-07-14T09:16:06",
    "url": "https://reddit.com/r/rstats/comments/1lzqpzf/is_there_a_way_to_find_missing_date_values_in_a/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lyz0yt",
    "title": "data.table is a NumFOCUS project!",
    "content": "check out this blog: [https://www.r-bloggers.com/2025/07/data-table-is-a-numfocus-project/](https://www.r-bloggers.com/2025/07/data-table-is-a-numfocus-project/)",
    "author": "BOBOLIU",
    "timestamp": "2025-07-13T10:48:54",
    "url": "https://reddit.com/r/rstats/comments/1lyz0yt/datatable_is_a_numfocus_project/",
    "score": 28,
    "num_comments": 4,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ly39lx",
    "title": "üåü Anyone here preparing for ISI, CMI, or IIT JAM MSc Data Science/Statistics? Or has already cracked them?",
    "content": "",
    "author": "Dapper-Wall312",
    "timestamp": "2025-07-12T08:32:09",
    "url": "https://reddit.com/r/rstats/comments/1ly39lx/anyone_here_preparing_for_isi_cmi_or_iit_jam_msc/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ly2mt1",
    "title": "Is R better? Convince me to learn the language.",
    "content": "I work in a data heavy field and it's split pretty evenly between R, and Power Bi/Tableau.  Personally, I use Power Bi for all my visuals and analysis.  I haven't yet seen a reason to learn R that I can't do (and usually quicker) in Power Bi. \n\nHelp me see what I'm not seeing.  Those of you who have used both, what benefit does R provide that you just can't get from Power Bi?",
    "author": "CatsOfDeath",
    "timestamp": "2025-07-12T08:04:54",
    "url": "https://reddit.com/r/rstats/comments/1ly2mt1/is_r_better_convince_me_to_learn_the_language/",
    "score": 0,
    "num_comments": 25,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lwy1dt",
    "title": "Best code editor or IDE to start with Python for an R programmer?",
    "content": "Hi, I have experience programming in R (I mainly use RStudio) and I'm starting to work with Python. Which code editor or development environment would you recommend for Python? I'm considering VS Code, JupyterLab, or Spyder.",
    "author": "Interesting_Fee_5265",
    "timestamp": "2025-07-10T21:46:22",
    "url": "https://reddit.com/r/rstats/comments/1lwy1dt/best_code_editor_or_ide_to_start_with_python_for/",
    "score": 46,
    "num_comments": 44,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lwi54q",
    "title": "We created an open course called \"R for Excel Users\" ‚Äî all materials available",
    "content": "To make it easier for people to learn R at my university, we designed an open course called **‚ÄúR for Excel Users.‚Äù** The idea was simple: take advantage of what people already know‚Äîspreadsheets, rows, columns, formulas, filters‚Äîand use that shared language to bridge into R programming.\n\nThe course has been very well received. All participants were professionals, teachers, or postgraduates, and the feedback has been overwhelmingly positive. What‚Äôs most interesting is that in just **12 hours**, we covered the kind of content usually delivered over 36‚Äì40 hours. This shows the power of building from what learners already know.\n\n[In this link](https://github.com/CruzJulian/Course-R-for-Excel-Users), we‚Äôre sharing the full repository with all course materials for anyone interested.",
    "author": "cruzjulian",
    "timestamp": "2025-07-10T10:07:07",
    "url": "https://reddit.com/r/rstats/comments/1lwi54q/we_created_an_open_course_called_r_for_excel/",
    "score": 129,
    "num_comments": 3,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lwhjkz",
    "title": "Full screen ggplot on mobile",
    "content": "I am trying to adapt a shiny app to be more mobile friendly. My biggest issue are ggplot charts that are squished on a small screen, becoming unreadable.   \n  \nI tried using shinyfullscreen to enable fullscreen mode for relevant charts which should solve the issue by going full screen in landscape mode. This however is not working at all when testing on mobile while working perfectly on pc. \n\nI would appreciate any guidance or suggestions on how to best display a ggplot chart on a small mobile screen. ",
    "author": "Eyhrion",
    "timestamp": "2025-07-10T09:43:58",
    "url": "https://reddit.com/r/rstats/comments/1lwhjkz/full_screen_ggplot_on_mobile/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lwfhom",
    "title": "Can't find a form with Rvest",
    "content": "I'm trying to scrape a [website](https://sitem.herts.ac.uk/aeru/ppdb/en/search.htm), but I'm unable to find the form in R. The following code is not working:  \n  \n    link &lt;- \"http://sitem.herts.ac.uk/aeru/ppdb/en/index.htm\"\n    \n    ppdb &lt;- read_html(link)\n    \n    search &lt;- ppdb |&gt; \n      html_element(\"#maincontent\") |&gt; \n      html_element(\".innertube\") |&gt; \n      html_form()  \n  \n  \nWhat am I missing?",
    "author": "Maunoir",
    "timestamp": "2025-07-10T08:22:17",
    "url": "https://reddit.com/r/rstats/comments/1lwfhom/cant_find_a_form_with_rvest/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lw9gx6",
    "title": "Formatting x-axis with scale_x_break() language acquisition study in R",
    "content": "Hey all! R beginner here!\n\nI would like to ask you for recommendations on how to fix the plot I show below.\n\n\\# What I'm trying to do:   \nI want to compare compare language production data from children and adults. I want to compare children and adults and older and younger children (I don't expect age related variation within the groups of adults, but I want to show their age for clarity). To do this, I want to create two plots, one with child data and one with the adults.\n\n\\# My problems:\n\n1. adult data are not evenly distributed across age, so the bar plots have huge gaps, making it almost impossible to read the bars (I have a cluster of people from 19 to 32 years, one individual around 37 years, and then two adults around 60).\n2. In a first attempt to solve this I tried using scale\\_x\\_break(breaks = c(448, 680),   scales = 1) for a break on the x-axis between 37;4 and 56;8 months, but you see the result in the picture below.\n3. A colleague also suggested scale\\_x\\_log10() or binning the adult data because I'm not interested much in the exact age of adults anyway. However, I use a custom function to show age on the x-axis as \"year;month\" because this is standard in my field. I don't know how to combine this custom function with scale\\_x\\_log10() or binning.\n\n\\# Code I used and additional context: \n\nIf you want to run all of my code and see an example of how it should look like, check out the link. I also provided the code for the picture below if you just want to look at this part of my code: All materials: [https://drive.google.com/drive/folders/1dGZNDb-m37\\_7vftfXSTPD4Wj5FfvO-AZ?usp=sharing](https://drive.google.com/drive/folders/1dGZNDb-m37_7vftfXSTPD4Wj5FfvO-AZ?usp=sharing) \n\nCode for the picture I uploaded:\n\n# Custom formatter to convert months to Jahre;Monate format\n\n# I need this formatter because age is usually reported this way in my field\n\nformat\\_age\\_labels &lt;- function(months) { years &lt;- floor(months / 12) rem\\_months &lt;- round(months %% 12) paste0(years, \";\", rem\\_months) }\n\n# Adult data second trial: plot with the data breaks\n\nlibrary(dplyr) library(ggplot2) library(ggbreak)\n\n# ‚úÖ Fixed plotting function\n\nbase\\_plot\\_percent &lt;- function(data) {\n\n# 1. Group and summarize to get percentages\n\ndf\\_summary &lt;- data %&gt;% group\\_by(Alter, Belebtheitsstatus, Genus.definit, Genus.Mischung.benannt) %&gt;% summarise(n = n(), .groups = \"drop\") %&gt;% group\\_by(Alter, Belebtheitsstatus, Genus.definit) %&gt;% mutate(prozent = n / sum(n) \\* 100)\n\n# 2. Define custom x-ticks\n\nyear\\_ticks &lt;- unique(df\\_summary$Alter\\[df\\_summary$Alter %% 12 == 0\\]) %&gt;% sort() year\\_ticks\\_24 &lt;- year\\_ticks\\[seq(1, length(year\\_ticks), by = 2)\\]\n\n# 3. Build plot\n\np &lt;- ggplot(df\\_summary, aes(x = Alter, y = prozent, fill = Genus.Mischung.benannt)) + geom\\_col(position = \"stack\") + facet\\_grid(rows = vars(Genus.definit), cols = vars(Belebtheitsstatus)) +\n\n    # ‚úÖ Add scale break \n    scale_x_break(\n      breaks = c(448, 680),  # Between 37;4 and 56;8 months\n      scales = 1\n    ) +\n    \n    # ‚úÖ Control tick positions and labels cleanly\n    scale_x_continuous(\n      breaks = year_ticks_24,\n      labels = format_age_labels(year_ticks_24)\n    ) +\n    \n    scale_y_continuous(\n      limits = c(0, 100),\n      breaks = seq(0, 100, by = 20),\n      labels = function(x) paste0(x, \"%\")\n    ) +\n    \n    labs(\n      x = \"Alter (Jahre;Monate)\",\n      y = \"Antworten in %\",\n      title = \" trying to format plot with scale_x_break() around 37 years and 60 years\",\n      fill = \"gender form pronoun\"\n    ) +\n    \n    theme_minimal(base_size = 13) +\n    theme(\n      legend.text = element_text(size = 9),\n      legend.title = element_text(size = 10),\n      legend.key.size = unit(0.5, \"lines\"),\n      axis.text.x = element_text(size = 6, angle = 45, hjust = 1),\n      strip.text = element_text(size = 13),\n      strip.text.y = element_text(size = 7),\n      strip.text.x = element_text(size = 10),\n      plot.title = element_text(size = 16, face = \"bold\")\n    )\n\nreturn(p) }\n\n# ‚úÖ Create and save the plot for adults\n\nplot\\_erw\\_percent &lt;- base\\_plot\\_percent(df\\_pronomen %&gt;% filter(Altersklasse == \"erwachsen\"))\n\nggsave(\"100\\_Konsistenz\\_erw\\_percent\\_Reddit.jpeg\", plot = plot\\_erw\\_percent, width = 10, height = 6, dpi = 300)\n\nThank you so much in advance!\n\nPS: First time poster - feel free to tell me whether I should move this post to another forum!",
    "author": "Strange-Block-5879",
    "timestamp": "2025-07-10T03:39:33",
    "url": "https://reddit.com/r/rstats/comments/1lw9gx6/formatting_xaxis_with_scale_x_break_language/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lvs7tm",
    "title": "Hosting knitted htmls online but not publicly",
    "content": "Im trying to find a way to share stats output to my research advisor using a knitted HTML as I really enjoy how it looks compared to the pdf or word documents.\n\nIs there any way to host knitted HTMLs without using GitHub or RPubs? I‚Äôm trying to keep my stats output somewhat private so I don‚Äôt want to just publish it for anyone to see. Any help would be appreciated!",
    "author": "TheDopamineDaddy",
    "timestamp": "2025-07-09T12:41:47",
    "url": "https://reddit.com/r/rstats/comments/1lvs7tm/hosting_knitted_htmls_online_but_not_publicly/",
    "score": 2,
    "num_comments": 10,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lvlsdl",
    "title": "PLEASE HELP: Error in matrix and vector multiplication: Error in listw %*%x: non-conformable arguments",
    "content": "Hi, I am using splm::spgm() for a research. I prepared my custom weight matrix, which is normalized according to a theoretic ground. Also, I have a panel data. When I use spgm() as below, it gave an error:\n\n\n\n\\&gt; sdm\\_model &lt;- spgm(\n\n\\+ formula = Y \\~ X1 + X2 + X3 + X4 + X5,\n\n\\+ data = balanced\\_panel,\n\n\\+ index = c(\"firmid\", \"year\"),\n\n\\+ listw = W\\_final,\n\n\\+ lag = TRUE,\n\n\\+ spatial.error = FALSE,\n\n\\+ model = \"within\",\n\n\\+ Durbin = TRUE,\n\n\\+ endog = \\~ X1,\n\n\\+ instruments = \\~ X2 + X3 + X4 + X5,\n\n\\+ method = \"w2sls\"\n\n\\+ )\n\n\\&gt; Error in listw %\\*%x: non-conformable arguments\n\n\n\nI have to say row names of the matrix and firm IDs at the panel data matching perfectly, there is no dimensional difference. Also, my panel data is balanced and there is no NA values. I am sharing the code for the weight matrix preparation process. firm\\_pairs is for the firm level distance data, and fdat is for the firm level data which contains firm specific characteristics.\n\n\n\n\\# Load necessary libraries\n\nlibrary(fst)\n\nlibrary(data.table)\n\nlibrary(Matrix)\n\nlibrary(RSpectra)\n\nlibrary(SDPDmod)\n\nlibrary(splm)\n\nlibrary(plm)\n\n\n\n\\# Step 1: Load spatial pairs and firm-level panel data -----------------------\n\nfirm\\_pairs &lt;- read.fst(\"./firm\\_pairs\") |&gt; as.data.table()\n\nfdat &lt;- read.fst(\"./panel\") |&gt; as.data.table()\n\n\n\n\\# Step 2: Create sparse spatial weight matrix -------------------------------\n\nfirm\\_pairs &lt;- unique(firm\\_pairs\\[firm\\_i != firm\\_j\\])\n\nfirm\\_pairs\\[, weight := 1 / (distance\\^2)\\]\n\nfirm\\_ids &lt;- sort(unique(c(firm\\_pairs$firm\\_i, firm\\_pairs$firm\\_j)))\n\nid\\_map &lt;- setNames(seq\\_along(firm\\_ids), firm\\_ids)\n\n\n\nW0 &lt;- sparseMatrix(\n\ni = id\\_map\\[as.character(firm\\_pairs$firm\\_i)\\],\n\nj = id\\_map\\[as.character(firm\\_pairs$firm\\_j)\\],\n\nx = firm\\_pairs$weight,\n\ndims = c(length(firm\\_ids), length(firm\\_ids)),\n\ndimnames = list(firm\\_ids, firm\\_ids)\n\n)\n\n\n\n\\# Step 3: Normalize matrix by spectral radius -------------------------------\n\neig\\_result &lt;- RSpectra::eigs(W0, k = 1, which = \"LR\")\n\nif (eig\\_result$nconv == 0) stop(\"Eigenvalue computation did not converge\")\n\ntau\\_n &lt;- Re(eig\\_result$values\\[1\\])\n\nW\\_scaled &lt;- W0 / (tau\\_n \\* 1.01) # Slightly below 1 for stability\n\n\n\n\\# Step 4: Transform variables -----------------------------------------------\n\nfdat\\[, X1 := asinh(X1)\\]\n\nfdat\\[, X2 := asinh(X2)\\]\n\n\n\n\\# Step 5: Align data and matrix to common firms -----------------------------\n\ncommon\\_firms &lt;- intersect(fdat$firmid, rownames(W\\_scaled))\n\nfdat\\_aligned &lt;- fdat\\[firmid %in% common\\_firms\\]\n\nW\\_aligned &lt;- W\\_scaled\\[as.character(common\\_firms), as.character(common\\_firms)\\]\n\n\n\n\\# Step 6: Keep only balanced firms ------------------------------------------\n\nbalanced\\_check &lt;- fdat\\_aligned\\[, .N, by = firmid\\]\n\nbalanced\\_firms &lt;- balanced\\_check\\[N == max(N), firmid\\]\n\nbalanced\\_panel &lt;- fdat\\_aligned\\[firmid %in% balanced\\_firms\\]\n\n\n\nsetorder(fdat\\_balanced, firmid, year)\n\nW\\_final &lt;- W\\_aligned\\[as.character(sort(unique(fdat\\_balanced$firmid))),\n\nas.character(sort(unique(fdat\\_balanced$firmid)))\\]\n\n\n\nAdditionally, I am preparing codes with a mock data, but using them at a secure data center, where everything is offline. The point I confused is when I use the code with my mock data, everything goes well, but with the real data at the data center I face with the error I shared. Can anyone help me, please?",
    "author": "InternationalTwo6104",
    "timestamp": "2025-07-09T08:31:18",
    "url": "https://reddit.com/r/rstats/comments/1lvlsdl/please_help_error_in_matrix_and_vector/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lvhblf",
    "title": "Hosting access controled quarto docs",
    "content": "I need to publish some result documents to a web hosting site.\n\nThe documents could be from quarto and probably need to contain interactive graphics, so I'm thinking plotly, but maybe shinylive.\n\nI need to have some kind of access control though with different people being able to see different sets of results.\n\nI think the later points me towards a CMS like WordPress, but I'm not finding any articles about how to publish pages from eg quarto to wordpress apart from static pages, which apparently don't get any access control.\n\nIs there any solution to my problem?",
    "author": "Sancho_Panzas_Donkey",
    "timestamp": "2025-07-09T05:19:21",
    "url": "https://reddit.com/r/rstats/comments/1lvhblf/hosting_access_controled_quarto_docs/",
    "score": 4,
    "num_comments": 6,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ltzudl",
    "title": "Problem to install package",
    "content": "Hi, im trying to install in R the package 'tabulizer' but R gaves me back this error:\n\n`&gt; install.packages('tabulizer')` \n\n`WARNING: Rtools is required to build R packages but is not currently installed. Please download and install the appropriate version of Rtools before proceeding: https://cran.rstudio.com/bin/windows/Rtools/ Installing package into ‚ÄòC:/Users/Juan/AppData/Local/R/win-library/4.4‚Äô (as ‚Äòlib‚Äô is unspecified) Warning in install.packages : package ‚Äòtabulizer‚Äô is not available for this version of R A version of this package for your version of R might be available elsewhere, see the ideas at https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages`",
    "author": "International_Mud141",
    "timestamp": "2025-07-07T10:10:15",
    "url": "https://reddit.com/r/rstats/comments/1ltzudl/problem_to_install_package/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ltcrvf",
    "title": "TypR on RStudio",
    "content": "Hi,\n\nThis post is a follow-up to last time. I made a short video about using the [TypR](https://github.com/fabriceHategekimana/typr) language (a statically typed version of R for package development) in RStudio using the [typr\\_runner](https://github.com/fabriceHategekimana/typr_runner) package.\n\nThe video link is [here](https://youtu.be/GMo20g__nOc).\n\nThank you for your support and feedback!\n\nhttps://preview.redd.it/9bjmc3g1pbbf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=84a8934048ebeeaa545d96ba62ef2fdae81a8d1d\n\n",
    "author": "Artistic_Speech_1965",
    "timestamp": "2025-07-06T14:35:58",
    "url": "https://reddit.com/r/rstats/comments/1ltcrvf/typr_on_rstudio/",
    "score": 9,
    "num_comments": 11,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ltax85",
    "title": "Analysing factors contributing to disease risk",
    "content": "What is the best way to analyse a dataset to uncover disease risk factors e.g smoking, alcohol etc. All the attributes (columns) are categorical except one, BMI. The target has 3 variables, it can either be Yes (the disease), No, or Early signs. Is JASP contigency tables applicable here or what is the best way to analyse?",
    "author": "Top_Berry_8589",
    "timestamp": "2025-07-06T13:17:44",
    "url": "https://reddit.com/r/rstats/comments/1ltax85/analysing_factors_contributing_to_disease_risk/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lsgtte",
    "title": "Statically typed R runner for RStudio",
    "content": "Hi everyone, \n\nA new update about typr, a typed version of R that transpile to R:\n\nhttps://github.com/fabriceHategekimana/typr\n\nBuilt to make package development easier, it help R users and developers from other programming languages to apply software development principles to the development of tools for the R community\n\nI have just launched a package that implement typr named typr_runner that is just a playground for RStudio for windows and linux (Ubuntu). I will try to put the most recent version of the project at each release",
    "author": "Artistic_Speech_1965",
    "timestamp": "2025-07-05T11:39:06",
    "url": "https://reddit.com/r/rstats/comments/1lsgtte/statically_typed_r_runner_for_rstudio/",
    "score": 11,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lsbf1f",
    "title": "Example repos that use both R and Python.",
    "content": "Does anyone have examples of repos that use both R and Python for data science? I use each separately for their own strengths, but am starting to mix both languages together in single workflows and projects. \n\nI'm curious to see examples on GitHub of how people who use both in a single project structure their code. I'm literally looking for repos with at least one .py and at least one .R file. I haven't found many examples. ",
    "author": "[deleted]",
    "timestamp": "2025-07-05T07:41:01",
    "url": "https://reddit.com/r/rstats/comments/1lsbf1f/example_repos_that_use_both_r_and_python/",
    "score": 13,
    "num_comments": 9,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lroint",
    "title": "Simple slopes in moderation",
    "content": "Hi everybody, \n\nI am doing moderation with simple slopes in lavaan and have hard time to be at least in some way \"confiden\" in what I am doing :D... I found this paper:\nTests of Moderation Effects: \r\nDifference in Simple Slopes versus the Interaction Term (Cecil D. Robinson, Sara Tomek, Randall E. Schumacker) (please, google it, as it is only as pdf link and I don't want to share download link here)\n\nAnd I am not sure how valid it is, does anybody know it? What do you think about doing simple slopes analysis even if interaction term is non significant? Thank you for answers and discussion:) \n\n---- \nOf course I am asking because I got nonsignificant interaction and significant slopes - but I would not take them serious if standardized effect size was not statistically different - and even practically (0.4 Vs 015 for +1SD vs -1SD...) I have some understanding why to not use/use simple slopes in this case, but I am not sure how to read this paper and how to look at information/results there...",
    "author": "BirdAticus",
    "timestamp": "2025-07-04T10:33:01",
    "url": "https://reddit.com/r/rstats/comments/1lroint/simple_slopes_in_moderation/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lqhy4k",
    "title": "After a year in beta, Positron IDE reaches stable release (R + Python IDE from Posit)",
    "content": "Positron IDE from Posit just hit its first stable release! For those who haven't tried it yet, it's essentially a modern IDE that handles both R and Python in a unified environment.\n\nBeen using it during the beta and it's been pretty solid for mixed R/Python workflows. Nice to see it's now considered production-ready.\n\nDownload link: [https://positron.posit.co/download.html](https://positron.posit.co/download.html)",
    "author": "coatless",
    "timestamp": "2025-07-02T22:51:22",
    "url": "https://reddit.com/r/rstats/comments/1lqhy4k/after_a_year_in_beta_positron_ide_reaches_stable/",
    "score": 221,
    "num_comments": 54,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lqsmh3",
    "title": "I developed an open-source app (with R, Shiny) for automatic qualitative text analysis (e.g., thematic analysis) with large language models",
    "content": "https://github.com/KennispuntTwente/tekstanalyse_met_llm",
    "author": "Ok_Sell_4717",
    "timestamp": "2025-07-03T08:33:51",
    "url": "https://reddit.com/r/rstats/comments/1lqsmh3/i_developed_an_opensource_app_with_r_shiny_for/",
    "score": 32,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lqvonb",
    "title": "GLMM with zero-inflation: Help interpreting results and bettering my model",
    "content": "Hello everyone. I am very new to reddit so sorry for any formatting mistakes etc. I am trying to model my variable (which is a count with mostly 0s) and assess if my treatments have some effect on it. The tank of the animals is used here as a fixed factor to ensure any differences are not due to tank variations.\n\nAfter some help from colleagues (and ChatGPT), this is the model I ended up with, which has better BIC and AIC than other things I've tried: \n\n    model_variable &lt;- glmmTMB(variable ~ treatment + (1|tank), \n    +                         family = tweedie(link = \"log\"), \n    +                         zi = ~treatment + (1|tank), \n    +                         dispformula = ~1,\n    +                         data = Comp1) \n\nWhen I do a summary of the model, this is what I get:\n\n    Random effects:\n    Conditional model:\n     Groups   Name        Variance  Std.Dev.\n     tank  (Intercept) 5.016e-10 2.24e-05\n    Number of obs: 255, groups:  tank, 16\n    \n    Zero-inflation model:\n     Groups   Name        Variance Std.Dev.\n     tank     (Intercept) 2.529    1.59    \n    Number of obs: 255, groups:  tank, 16\n    \n    Dispersion parameter for tweedie family (): 1.06 \n    \n    Conditional model:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n    (Intercept)    1.2889     0.2539   5.076 3.85e-07 ***\n    treatmentA  -0.3432     0.2885  -1.190   0.2342    \n    treatmentB  -1.9137     0.4899  -3.906 9.37e-05 ***\n    treatmentC  -1.6138     0.7580  -2.129   0.0333 *  \n    ---\n    Zero-inflation model:\n                 Estimate Std. Error z value Pr(&gt;|z|)   \n    (Intercept)     3.625      1.244   2.913  0.00358 **\n    treatmentA   -3.340      1.552  -2.152  0.03138 * \n    treatmentB   -3.281      1.754  -1.870  0.06142 . \n    treatmentC   -1.483      1.708  -0.868  0.38533 \n\nMy colleagues then told me I should follow with this:\n\n    Anova(model_variable, test.statisic=\"Chisq\", type=\"III\")\n    Response: variable\n                 Chisq Df Pr(&gt;Chisq)    \n    (Intercept) 25.768  1  3.849e-07 ***\n    treatment   18.480  3  0.0003502 ***\n\n    MV &lt;- emmeans(model_variable, ~ treatment, adjust = \"bonferroni\", type = \"response\")\n    &gt; pairs(MV)\n     contrast  ratio    SE  df null z.ratio p.value\n     CTR / A   1.409 0.407 Inf    1   1.190  0.6356\n     CTR / B   6.778 3.320 Inf    1   3.906  0.0005\n     CTR / C   5.022 3.810 Inf    1   2.129  0.1569\n     A / B     4.809 2.120 Inf    1   3.569  0.0020\n     A / C     3.563 2.590 Inf    1   1.749  0.2956\n     B / C     0.741 0.611 Inf    1  -0.364  0.9753\n\nThen, I am a bit lost. I am not truly sure if my model is correct and also to interpret it. From what I read, it seems: \n\n\\- A and B have an effect (compared to the CTR treat) on the probability of zeroes found\n\n\\- B and C have an effect on the variable (considering only the non-zeroes)\n\n**-** Based on the pairwise comparison, only B differs from CTR overall\n\nI would love to share my data, but I cannot, so based on this: is my model ok and is this interpretation correct?   \nAny help is appreciated, because I am desperate, thanks.",
    "author": "Prestigious-Road2030",
    "timestamp": "2025-07-03T10:34:59",
    "url": "https://reddit.com/r/rstats/comments/1lqvonb/glmm_with_zeroinflation_help_interpreting_results/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lqu0pw",
    "title": "Automatic Report Generation from Questionnaire Data",
    "content": "Hi all,\n\nI am trying to find a way for ai/software/code to create a safety culture report (and other kinds of reports) simply by submitting the raw data of questionnaire/survey answers. I want it to create a good and solid first draft that i can tweak if need be. I have lots of these to do, so it saves me typing them all out individually.\n\n¬†My report would include things such as an introduction, survey item tables, graphs and interpretative paragraphs of the results, plus a conclusion etc. I don't mind using different services/products.\n\n¬†I have a budget of a few hundred dollars per months - but the less the better. The reports are based on survey data using questions based on 1-5 Likert statements such as from strongly disagree to strongly agree.¬†¬†\n\nPlease, if you have any tips or suggestions, let me know!! Thanksssss\n\n",
    "author": "BodyFun5162",
    "timestamp": "2025-07-03T09:29:35",
    "url": "https://reddit.com/r/rstats/comments/1lqu0pw/automatic_report_generation_from_questionnaire/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lqnwxc",
    "title": "Lists [Syntax suggestion]",
    "content": "Hi everyone, I am actually building a staticlly typed version of the R programming language named [TypR](https://github.com/fabriceHategekimana/typr) and I need your opinion about the syntax of lists\n\nActually, in TypR, lists are called \"records\" (since they also gain the power of records in the type system) and take a syntax really similar to them, but I want to find a balance with R and bring some familiarity so a R user know their are dealing with a list.\n\nAll those variations are valid notation in TypR but I am curious to know wich one suit better in an official documentation (the first one was my initial idea). Thanks in advance !\n\n\n[View Poll](https://www.reddit.com/poll/1lqnwxc)",
    "author": "Artistic_Speech_1965",
    "timestamp": "2025-07-03T05:08:34",
    "url": "https://reddit.com/r/rstats/comments/1lqnwxc/lists_syntax_suggestion/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lplc03",
    "title": "The Test Set Podcast - first episode with Hadley Wickham and Michael Chow out now",
    "content": "",
    "author": "blankepitaph",
    "timestamp": "2025-07-01T20:11:55",
    "url": "https://reddit.com/r/rstats/comments/1lplc03/the_test_set_podcast_first_episode_with_hadley/",
    "score": 13,
    "num_comments": 2,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lpz9sk",
    "title": "Different models in Rstudio Github Copilot integration?",
    "content": "",
    "author": "chicagonyc",
    "timestamp": "2025-07-02T08:45:52",
    "url": "https://reddit.com/r/rstats/comments/1lpz9sk/different_models_in_rstudio_github_copilot/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lptlav",
    "title": "Multiple linear regression help!!",
    "content": "I really need some help from an expert as I've had differing opinions. I want to do a multiple linear regression with my dependant variable being continuous, and my independent variables are categorical but I've dummy coded them to 0 and 1. When I've searched this up it says it's okay to do so as a linear regression but I can't find any concrete answer if this is okay??\n\nI just want to confirm if it‚Äôs okay to use only categorical variables for my independent variables.\n\nI‚Äôve been told that it has to be continuous or a mix of continuous and categorical to do a linear regression.",
    "author": "ReflectionOk2310",
    "timestamp": "2025-07-02T04:39:52",
    "url": "https://reddit.com/r/rstats/comments/1lptlav/multiple_linear_regression_help/",
    "score": 2,
    "num_comments": 9,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lp4dag",
    "title": "Water quality monitoring using R, Posit and Esri - Virginia Case Study",
    "content": "Covering how developing an analytics community at the Virginia Department of Environmental Quality has led to technological integrations and process improvements:\n\n\"During the initial stages of this data collection modernization project, which was isolated to a single DEQ region, staff digitally collected over 91,427 data points across 225 sites across 657 sampling events. This data had enhanced QA applied to them both in the ArcGIS Survey123 interface and via the integration with R and Python-based QA and Posit Connect hosted shiny applications. This direct connection between the data and DEQ‚Äôs database undoubtedly removed manual transcription errors and saved at least 127 hours of staff time spent solely on data re-entry. Growing this effort to encompass more regions and more sampling programs has the potential to massively increase time savings and improve data quality.\"\n\nFind out more here: \n[https://r-consortium.org/posts/strength-in-numbers/](https://r-consortium.org/posts/strength-in-numbers/)",
    "author": "jcasman",
    "timestamp": "2025-07-01T08:18:28",
    "url": "https://reddit.com/r/rstats/comments/1lp4dag/water_quality_monitoring_using_r_posit_and_esri/",
    "score": 12,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lp4az4",
    "title": "tbl_summary",
    "content": "I absolutely love the tbl_summary() function from the gtsummary package for quickly &amp; easily creating presentable tables in R. However, I really need to know how to save longer tables. When I get to more than 8-10 rows the table cuts off and I have to scroll up and down to view different parts of it. When I save, it just saves the part I am currently looking at, rather than the whole table. Similarly if I have a wide table with many columns it will cut off at the side. I have tried converting to a gt and using gtsave but the same thing happens.\n\nTL:DR- Anyone got a solution so I can save large tables in tbl_summary?a",
    "author": "ferasius",
    "timestamp": "2025-07-01T08:15:59",
    "url": "https://reddit.com/r/rstats/comments/1lp4az4/tbl_summary/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lpp0ia",
    "title": "hi",
    "content": "if anyone sees this could u please take a couple minutes out of ur time to do my son questionnaire so I can gather data for my mini pip \n\n  \n[https://docs.google.com/forms/d/e/1FAIpQLSduvfBFExF0D0O5hW0p2QIWujbyoG5OvCjloyqQQVyvOJwnfA/viewform?usp=sharing&amp;ouid=112257011246207717235](https://docs.google.com/forms/d/e/1FAIpQLSduvfBFExF0D0O5hW0p2QIWujbyoG5OvCjloyqQQVyvOJwnfA/viewform?usp=sharing&amp;ouid=112257011246207717235)",
    "author": "Majestic-Holiday2200",
    "timestamp": "2025-07-01T23:47:37",
    "url": "https://reddit.com/r/rstats/comments/1lpp0ia/hi/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.13,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1looqi7",
    "title": "Generic methods only sometimes working in custom R package",
    "content": "I am sorely confused about how polymorphism works in R. I am making a custom R package for my company and I need generic methods to make my code 10x cleaner. But they sometimes work and sometimes don't with no discernible difference. For example:\n\n    foo &lt;- function(obj) {\n        UseMethod(\"foo\", obj)\n    }\n    \n    #' @method foo bar\n    #' @noRd\n    foo.bar &lt;- function(obj) {\n        print(\"foo.bar\")\n    }\n    \n    #' @method foo default\n    #' @noRd\n    foo.default &lt;- function(obj) {\n        print(\"foo.default\")\n    }\n\nWhen I run `devtools::document()` and `devtools::load_all()` and then try with a custom object I get this:\n\n    &gt; obj &lt;- 1\n    &gt; class(obj) &lt;- \"bar\"\n    &gt; foo(obj)\n    Error in UseMethod(\"foo\", obj) : \n      no applicable method for 'foo' applied to an object of class \"bar\"\n\nWhich obviously means it can't find it... but when I run `class(obj)` it says `[1] \"bar\"` and when I run `methods(\"foo\")` it tells me it knows what I'm talking about:\n\n    &gt; methods(\"foo\")\n    [1] foo.bar     foo.default\n    see '?methods' for accessing help and source code\n\nLastly, when I just run them in the global environment they work just fine, and to make matters worse, I have another generic further up in the exact same .R file structured the exact same way and that one works just fine the way it is. If someone better versed in R could explain what I'm missing, that would be great because LLMs have been woefully incorrect and unhelpful. Thanks in advance.",
    "author": "bee_tee_beats",
    "timestamp": "2025-06-30T18:07:49",
    "url": "https://reddit.com/r/rstats/comments/1looqi7/generic_methods_only_sometimes_working_in_custom/",
    "score": 2,
    "num_comments": 12,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lnz2wc",
    "title": "New R package: paddleR ‚Äî an interface to the Paddle API for subscription &amp; billing workflows",
    "content": "Hey folks,  \n  \nI just released a new R package called [`paddleR`](https://arnold-kakas.github.io/paddleR/index.html) on CRAN! üéâ\n\n`paddleR` provides a full-featured R interface to the [Paddle](https://paddle.com) API, a billing platform used for managing subscriptions, payments, customers, credit balances, and more.\n\nIt supports:\n\n* Creating, updating, and listing customers, subscriptions, addresses, and businesses\n* Managing payment methods and transactions\n* Sandbox and live environments with automatic API key selection\n* Tidy outputs (data frames or clean lists)\n* Convenient helpers for workflow automation\n\nIf you're working on a SaaS product with Paddle and want to automate billing or reporting pipelines in R, this might help!",
    "author": "Arnold891127",
    "timestamp": "2025-06-29T22:01:43",
    "url": "https://reddit.com/r/rstats/comments/1lnz2wc/new_r_package_paddler_an_interface_to_the_paddle/",
    "score": 12,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lndkyz",
    "title": "Project Template: Hardware-accelerated R Package (OpenCL, OpenGL, ...) with platform-independent linkage",
    "content": "I've created a CRAN-ready project template for linking against C or C++ libraries in a platform-independent way. The goal is to make it easier to develop hardware-accelerated R packages using **Rcpp** and **CMake**.\n\nüì¶ **GitHub Rep**o: [cmake-rcpp-template](https://github.com/jmaerte/cmake-rcpp-template)\n\n‚úçÔ∏è I‚Äôve also written a Medium article explaining the internals and rationale behind the design:  \n[Building Hardware-Accelerated R Packages with Rcpp and CMake](https://medium.com/@mail_17803/building-hardware-accelerated-r-packages-with-rcpp-and-cmake-a-practical-template-114d13b08a97)\n\nI‚Äôd love feedback from anyone working on similar problems or who‚Äôs interested in streamlining their native code integration with R. Any suggestions for improvements or pitfalls I may have missed are very welcome!",
    "author": "jmaerte",
    "timestamp": "2025-06-29T05:32:30",
    "url": "https://reddit.com/r/rstats/comments/1lndkyz/project_template_hardwareaccelerated_r_package/",
    "score": 14,
    "num_comments": 2,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ln4fkc",
    "title": "Minimizing correlation while visualizing data with Chernoff faces?",
    "content": "Working on an example to demonstrate correlation and randomness in data using visual models.\n\nI'm trying to find a dataset that would produce 8-12 Chernoff faces with the broadest range of \"features\" to the data. For example, [Flowing Data instructions](https://flowingdata.com/2010/08/31/how-to-visualize-data-with-cartoonish-faces/) use crime data by U.S. state. This data often demonstrates correlations that lead to similar \"features\" between samples. It makes sense that this data would show multiple correlations since similar kinds of crime rates would result from similar sociopolitical conditions across states. \n\nFor an example, see below. This data could be grouped as 4 and 10 having similar features based on shape and color, 6, 8, and 9 having similar features, and 5, 7, 11, and 12 serving in their own category. I'd like to find a data set that is least correlative, meaning that the features and colors will be seemingly random for the 8-12 faces.\n\nAny suggestions or could someone offer random data? It doesn't need to be a \"real\" data set to demonstrate the statistical phenomenon.\n\nhttps://preview.redd.it/xfib3bgp6s9f1.png?width=933&amp;format=png&amp;auto=webp&amp;s=bc2a8869b32e2e8e414825f340ac856dbe012a36",
    "author": "Creative-Repair5",
    "timestamp": "2025-06-28T20:00:09",
    "url": "https://reddit.com/r/rstats/comments/1ln4fkc/minimizing_correlation_while_visualizing_data/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lmtn32",
    "title": "Beginner question: Cant get a function() that uses rows from a dataframe to output to a dataframe/matrix",
    "content": "Hi! \n\nI hope someone have the time to help with a question I have, I have searched and tried anything I could think of (that is not much since I don't have many hours behind me in R), but I am stuck. I am taking a distance course in R and have no teacher to ask over the weekend, so I hope someone can point me in the right direction. I am not after a solution, just getting pointed in the right direction. so I can get my code working. \n\n\n\nThe task I have at hand.\n\n1. Write a function that the square root of the sum of squares of two number. DONE\n\n**Root\\_sum\\_squares &lt;- function(a,b)**{\n\n  \\# sqrt (a\\^2 + b\\^2)\n\n  a2 &lt;- a\\*\\*2\n\n  b2 &lt;- b\\*\\*2\n\n  sum\\_a2b2 &lt;- a2 + b2\n\n  sqrt\\_sum\\_a2b2 &lt;- sqrt(sum\\_a2b2)\n\n \\# sqrt\\_sum\\_a2b2&lt;- sqrt(a\\*\\*2 + b\\*\\*2)\n\n  return(sqrt\\_sum\\_a2b2)\n\n}\n\n2. Write a function that uses the function in 1 to calculate the distance between two points in a 2d plane. DONE. \n\np1 &lt;- c(2,2)\n\np2 &lt;- c(5,4)\n\np3 &lt;- c(2,2,3)\n\nan\n\n**Distance &lt;- function(p1 = c(3,0), p2 = c(0,4))**{\n\n  l\\_p1 &lt;- length(p1)\n\n  l\\_p2 &lt;- length(p2)\n\n \\# if(l\\_p1 != 2 | l\\_p2 != 2){\n\n\\#  stop('The length of either p1 or p2 is not two')\n\n\\#  }\n\n  p2\\_p1 &lt;- p2 - p1\n\n  p1\\_to\\_p2 &lt;- Root\\_sum\\_squares(p2\\_p1\\[1\\],p2\\_p1\\[2\\])\n\n  return((p1\\_to\\_p2))\n\n}\n\n3. Write a function that takes coordinates from 2 different dataframes (m1 and m2 3 points from each) and calculates the distance between every point in dataframe 1 and 2, so a total of 9 distances, and returns the result in a 3\\*3 matrix. \n\nEverything in 3 is done except getting it to a 3\\*3 matrix. When I try to output it it only goes into a list. \n\n\\#Defining dataframes with x &amp; y coordinates.   \nm1 &lt;- data.frame(x1 = c(5,6,7), y1=c(4,5,6))\n\nm2 &lt;- data.frame(x2 = c(1,2,3),  y2=c(2,4,6))\n\n**Distance\\_matrix = function(m,n)**{\n\n\\#Defining an output matrix\n\noutput &lt;- matrix(0, nrow = nrow(m), ncol = nrow(n))\n\n  \\# A counter just to see where I am in the loop  \n\n  k &lt;-1\n\n  for (i in 1:nrow(m)) {\n\nfor (j in 1:nrow(n)) {\n\noutput\\[i,j\\] &lt;-  Distance(m\\[i,\\], n\\[j,\\])\n\nprint(paste(\"Loop :\",k, \" i:\", i, \" j:\",j))\n\nprint(output)\n\nk &lt;- k+1\n\n}\n\n  }\n\n   return(output)\n\n}\n\n  \nIf I use just single points from the dataframes in the function Distance\\_matrix and take xy from m1 and m2, both from row 1 and it works. \n\n    &gt; x &lt;- Distance_matrix(m1[1,],m2[1,])\n    [1] \"Loop : 1  i: 1  j: 1\"\n            x2\n    1 4.472136&gt; x &lt;- Distance_matrix(m1[1,],m2[1,])\n    [1] \"Loop : 1  i: 1  j: 1\"\n            x2\n    1 4.472136\n\nIf I modify inside of the Distance\\_matrix function output\\[i,j\\] &lt;-  Distance(m\\[i,\\], n\\[j,\\]) to output &lt;-  Distance(m\\[i,\\], n\\[j,\\]) it goes thru all the points and I get a all 9 distances calculated but I only get the last calculated as an output.\n\nIf I try this output\\[i,j\\] &lt;-  Distance(m\\[i,\\], n\\[j,\\]) inside of the Distance\\_matrix function and the variable output is defined as a matrix \n\n    output &lt;- matrix(0, nrow = nrow(m), ncol = nrow(n))output &lt;- matrix(0, nrow = nrow(m), ncol = nrow(n))\n\nThe variable output is transformed to a list, and the function will not work. I want to fill in the matrix in this pattern. \n\n      x1 x2 x3\n    1  1  2  3\n    2  4  5  6\n    3  7  8  9  \n\nBut I get the error \"incorrect number of subscripts on matrix\" so that seems to be since my matrix \"output\" is remade into a vector. If someone can point me in the right direction, I would be thankful. \n\nI have searched for a solution, but it seems that I only find \"If you are dealing with a vector, then you fix it by simply removing the comma\" but since I am (at least trying) working with a matrix, that will not fix it. ",
    "author": "Relevant_Rope9769",
    "timestamp": "2025-06-28T11:21:32",
    "url": "https://reddit.com/r/rstats/comments/1lmtn32/beginner_question_cant_get_a_function_that_uses/",
    "score": 2,
    "num_comments": 8,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lm9o7e",
    "title": "ggplot2/patchwork combining commands",
    "content": "I often use Reduce('/',plot\\_list) to produce variable length set of plots for my data.  And, I like to include a \"doc\\_panel\" that shows the command line that produced the plots, for self documentation.  Since the command line is typically very short vertically, I use `plot_layout(heights=c(rep(10,n_plots), 0.1)` to give the plots lots of space and leave a little room for the doc\\_panel.\n\nIf I create a plot with the command:\n\n    big_plot &lt;- Reduce('/',plot_list) + plot_layout(heights=c(rep(10,n_plots), 0.1)\n\neverything works as expected.\n\nbut if I do:\n\n    big_plot &lt;- Reduce('/',plot_list)\n    big_plot_wdoc &lt;- big_plot + plot_layout(heights=c(rep(10,n_plots), 0.1)\n\nthen the doc\\_panel has the same height as the plots.  Why are these different?",
    "author": "fasta_guy88",
    "timestamp": "2025-06-27T17:30:58",
    "url": "https://reddit.com/r/rstats/comments/1lm9o7e/ggplot2patchwork_combining_commands/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1llt7iy",
    "title": "TODAY! Free R Consortium Webinar: Digitizing Water Quality Data Collection with R, Posit and Esri Integration",
    "content": "",
    "author": "jcasman",
    "timestamp": "2025-06-27T05:57:05",
    "url": "https://reddit.com/r/rstats/comments/1llt7iy/today_free_r_consortium_webinar_digitizing_water/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1llecjv",
    "title": "Dependency not installing",
    "content": "Hi, I'm trying to use the BDEsize package in R but when I install the package using \n\n`install.packages(\"BDEsize\", dependencies = TRUE)`\n\nthe following error appears:\n\n`Warning in install.packages :`  \n  `dependency ‚Äòfpow‚Äô is not available`\n\nIs there a way to solve this issue or is the package just broken?",
    "author": "r1c3bowl22",
    "timestamp": "2025-06-26T16:00:03",
    "url": "https://reddit.com/r/rstats/comments/1llecjv/dependency_not_installing/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lkug1z",
    "title": "Introducing my package bayesSSM: Bayesian Inference in State-Space Models",
    "content": "I made an R package for performing Bayesian inference in state-space models using Particle MCMC. It automatically tunes the number of particles to use in the particle filter and the proposal covariance. \n\nIf anyone is interested, you can check it out here: [https://github.com/BjarkeHautop/bayesSSM](https://github.com/BjarkeHautop/bayesSSM)\n\nAny feedback is also very welcome!",
    "author": "Latent-Person",
    "timestamp": "2025-06-26T01:03:20",
    "url": "https://reddit.com/r/rstats/comments/1lkug1z/introducing_my_package_bayesssm_bayesian/",
    "score": 19,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ljqlga",
    "title": "Best way to learn R for someone with no programming background, basic stats knowledge, and limited time?",
    "content": "Hello, I'm looking to learn R as much as I can ASAP. I have to take a stats class for my degree that uses R in a semester or two and based on what people already said about this course, students don't have a lot of time or room for learning about programming so I am trying to get a head start during the summer.\n\nI personally am not a huge CS or coding person at all and it's really hard for me to grasp CS concepts quickly so I want something that can explain all the programming aspects of it in a digestible and non-CS friendly way. I have very elementary CS knowledge from taking a AP CS class way back in high school and know the basic principles of CS but I have never really been able to learn a text based language.\n\nAdditionally, I have basic college stats knowledge and I am looking to use this for biological research in the future (not anything too fancy because I am pre-med and not aiming to go into research full time). Not trying to rush the fundamentals ofc but what are the best ways to go about learning R? Also, will I have to learn any other language along with this? I've heard people mention that they had to use Python and SQL along with R not specifically for this course but in general for biological research.\n\nedit: tysm to everyone for all the options! i really appreciate it :)",
    "author": "intellectual-veggie",
    "timestamp": "2025-06-24T16:36:24",
    "url": "https://reddit.com/r/rstats/comments/1ljqlga/best_way_to_learn_r_for_someone_with_no/",
    "score": 48,
    "num_comments": 43,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lk487u",
    "title": "DTW for classification?",
    "content": "I have previously used dynamic time warping for clustering, but after seeing some pages stating it can be used for classification, but without examples I'm wondering if anyone can help?\n\nI can't understand how it would work or where to look for a guide if anyone has any pointers?",
    "author": "Implement_Empty",
    "timestamp": "2025-06-25T05:23:26",
    "url": "https://reddit.com/r/rstats/comments/1lk487u/dtw_for_classification/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lig71z",
    "title": "Issue with home-made forest plot",
    "content": "I'm creating a forest plot for my logistic regression model in R. I am not happy with the forest plot created by some packages, especially because the names of the predictors and the levels of the factor in the model are very long. What I would like to do is to put the name of the variables, which are the bold black text on the left of the picture, just right above the coefficients associated with them. The idea is to save horizontal space.\n\nI tried to play with the options for faceting but couldn't make it myself. Thank you in advance!\n\nhttps://preview.redd.it/koxs735leo8f1.png?width=4157&amp;format=png&amp;auto=webp&amp;s=b0dedf7ea39aaa8d1de171f48906bc5c0e001f70\n\n  \nHere's relevant code.\n\n    #### DATA ####\n    tt &lt;- data.frame(\n      ind_vars = rep(1:14, c(3L, 7L, 6L, 4L, 4L, 1L, 5L, 5L, 5L, 5L, 5L, 5L, 4L, 4L)),\n      data_classes = rep(c(\"factor\", \"numeric\", \"factor\"), c(24L, 1L, 38L)),\n      reflevel = rep(\n        c(\n          \"female\", \"employed\", \"committed to a stable relationship\", \"no\", \"[35,50]\",\n          \"0\", \"never\", \"not at all willing\", \"never\", \"always\", \"not at all\",\n          \"no, I have never been vaccinated against either seasonal flu or covid\",\n          \"no, I was not vaccinated against either seasonal flu or covid last year\"\n        ),\n        c(3L, 7L, 6L, 4L, 4L, 1L, 10L, 5L, 5L, 5L, 5L, 4L, 4L)\n      ),\n      vars = factor(\n        rep(\n          c(\n            \"Gender\", \"Employment status\", \"Marital Status\", \"Living with cohabitants\",\n            \"Age\", \"Recently searched local news related to publich health\",\n            \"During the Covid-19 pandemic, did you increase your\\nuse of social media platforms to discuss health\\nissues or to stay informed about the evolution of the pandemic?\",\n            \"In the event of an outbreak of a respiratory infection similar\\nto the Covid-19 pandemic, would you prefer to shop online\\n(e.g., masks, medications, food, or other products) to avoid leaving your home?\",\n            \"How willing would you be to get vaccinated against an emerging\\npathogen if safe and effective vaccines were approved and\\nmade available on the market?\",\n            \"If infections were to spread, would you consider wearing masks useful?\",\n            \"If infections were to spread, do you think your family members and friends\\nwould adopt individual protective measures (e.g., wearing masks, social distancing, lockdowns)?\",\n            \"If infections were to spread, would adopting individual protective behaviors\\n (e.g., wearing masks, social distancing, lockdowns, etc.) require a high economic cost?\",\n            \"Have you ever been vaccinated against seasonal influenza and/or Covid?\",\n            \"In the past year (or last winter season), have you been vaccinated against seasonal influenza and/or Covid?\"\n          ),\n          c(3L, 7L, 6L, 4L, 4L, 1L, 5L, 5L, 5L, 5L, 5L, 5L, 4L, 4L)\n        ),\n        levels = c(\n          \"Gender\", \"Employment status\", \"Marital Status\", \"Living with cohabitants\",\n          \"Age\", \"Recently searched local news related to publich health\",\n          \"During the Covid-19 pandemic, did you increase your\\nuse of social media platforms to discuss health\\nissues or to stay informed about the evolution of the pandemic?\",\n          \"In the event of an outbreak of a respiratory infection similar\\nto the Covid-19 pandemic, would you prefer to shop online\\n(e.g., masks, medications, food, or other products) to avoid leaving your home?\",\n          \"How willing would you be to get vaccinated against an emerging\\npathogen if safe and effective vaccines were approved and\\nmade available on the market?\",\n          \"If infections were to spread, would you consider wearing masks useful?\",\n          \"If infections were to spread, do you think your family members and friends\\nwould adopt individual protective measures (e.g., wearing masks, social distancing, lockdowns)?\",\n          \"If infections were to spread, would adopting individual protective behaviors\\n (e.g., wearing masks, social distancing, lockdowns, etc.) require a high economic cost?\",\n          \"Have you ever been vaccinated against seasonal influenza and/or Covid?\",\n          \"In the past year (or last winter season), have you been vaccinated against seasonal influenza and/or Covid?\"\n        )\n      ),\n      coef = c(\n        \"female \", \"other \", \"male *\", \"employed \", \"self-employed \",\n        \"prefer not to answer \", \"student \", \"inactive **\",\n        \"employed with on-call, seasonal, casual work \", \"unemployed **\",\n        \"committed to a stable relationship \", \"widowed \",\n        \"never married or civilly united \", \"married or civilly united .\",\n        \"separated or divorced or dissolved civil union .\",\n        \"prefer not to answer ***\", \"no \", \"yes both types \", \"yes familiar \",\n        \"yes not familiar **\", \"[35,50] \", \"(50,65] *\", \"(65,75] ***\", \"(75,100] .\",\n        \"d3 ***\", \"never \", \"always \", \"sometimes \", \"rarely \", \"often *\", \"never \",\n        \"rarely \", \"sometimes **\", \"always ***\", \"often ***\", \"not at all willing \",\n        \"quite willing .\", \"little willing \", \"very willing ***\",\n        \"extremely willing ***\", \"never \", \"always ***\", \"often ***\", \"rarely ***\",\n        \"sometimes ***\", \"always \", \"often *\", \"sometimes **\", \"rarely **\",\n        \"never ***\", \"not at all \", \"quite *\", \"slightly *\", \"very ***\",\n        \"extremely **\",\n        \"no, I have never been vaccinated against either seasonal flu or covid \",\n        \"yes, I have been vaccinated against seasonal flu **\",\n        \"yes, I have been vaccinated against covid ***\",\n        \"yes, I have been vaccinated against both seasonal flu and covid ***\",\n        \"no, I was not vaccinated against either seasonal flu or covid last year \",\n        \"yes, I was vaccinated against seasonal flu last year ***\",\n        \"yes, I was vaccinated against covid last year ***\",\n        \"yes, I was vaccinated against both seasonal flu and covid last year ***\"\n      ),\n      estimate = c(\n        1, 1.1594381176560349, 1.1938990313409903, 1, 0.9345113103023006,\n        1.182961198511645, 1.1986525531956205, 1.3885987619435227, 1.4249393997680262,\n        1.6608221007597275, 1, 1.2306190558844832, 1.2511698137826779,\n        1.3025146544308737, 1.3921678095031182, 2.5765770390418052, 1,\n        1.0501974244025936, 0.9173415285717724, 1.6630854660369543, 1,\n        0.800201285826906, 0.619147977085642, 0.5916851874362801, 1.3446738044826476,\n        1, 0.9821138738140281, 1.115752845992493, 1.151676302402397,\n        1.3922179488382054, 1, 0.7963755128809387, 0.6371712438181103,\n        0.5359168828200498, 0.52285129136739, 1, 1.3006766155072604,\n        0.7505100003548196, 1.7776842754118605, 2.703051479564682, 1,\n        4.741038392845822, 5.934362782762892, 6.036773899188224, 8.825434764755212, 1,\n        1.2592273055270102, 1.5557681273924433, 1.8486058288997373,\n        3.8802172100549277, 1, 1.535155861618323, 1.561145156620264,\n        1.9720490757147962, 2.1060302234145145, 1, 1.822390024254432,\n        2.5834083197529223, 3.19131783617297, 1, 1.8573631891630529,\n        11.749226988364809, 22.39402505515249\n      ),\n      se = c(\n        0, 0.7957345407506708, 0.07569629175474867, 0, 0.12934240102667208,\n        0.3581432018092095, 0.7186617050966417, 0.11453425505512978,\n        0.24970014024395928, 0.17541003295888669, 0, 0.21787717379030114,\n        0.16561962733872138, 0.14055065342933543, 0.17758880314032413,\n        0.2673745275652827, 0, 0.21907120018625223, 0.10567040412382916,\n        0.19404722520361742, 0, 0.08931527483025398, 0.13566079829196406,\n        0.28889507837780726, 0.04027571944271817, 0, 0.20402191086067092,\n        0.1121123274188254, 0.11464110133052731, 0.12973172877640954, 0,\n        0.17244861947164766, 0.16244297378932024, 0.18264891069682213,\n        0.1683475894323182, 0, 0.15516969255754776, 0.1784961281145401,\n        0.16653435112184062, 0.16939006691926656, 0, 0.41716301464407385,\n        0.4195492072923107, 0.4219772930530366, 0.4172887856538571, 0,\n        0.1049755192658886, 0.13883787906399103, 0.19818533001974975,\n        0.33943935080446835, 0, 0.17562649853946533, 0.1770368138991044,\n        0.19409880094417853, 0.22703298633448182, 0, 0.22044384043316081,\n        0.17267511404056463, 0.18558845913735647, 0, 0.15106861356248374,\n        0.11820785166827097, 0.1351064300228206\n      ),\n      z = c(\n        0, 0.1859106257938456, 2.3412566708408757, 0, -0.5236608302452392,\n        0.46914414228773427, 0.2521326129922885, 2.8663490550709376,\n        1.4182182116188318, 2.8921533884970017, 0, 0.9524510375713973,\n        1.3529734869317107, 1.8804376865993249, 1.8630797752989627,\n        3.5398352925174055, 0, 0.2235719240785752, -0.8164578870445477,\n        2.6213958537286572, 0, -2.4955639010459687, -3.5338947036046258,\n        -1.8165091855083595, 7.353101650063636, 0, -0.08846116655031708,\n        0.9769610335418417, 1.2318316350105765, 2.5506337209733743, 0,\n        -1.3203031443446245, -2.7746157339042767, -3.4151651763027124,\n        -3.851900673274625, 0, 1.69417492683233, -1.6078909167715072,\n        3.454611883758754, 5.870363773637503, 0, 3.730570849534812, 4.244459589819272,\n        4.260584102982726, 5.2185391546570425, 0, 2.195733680377346,\n        3.1833488039876507, 3.1002887495513214, 3.9945019068287726, 0,\n        2.4405879406729816, 2.515971773931635, 3.498595245475999, 3.2806015404762188,\n        0, 2.722456833250876, 5.496504731156791, 6.252726875744174, 0,\n        4.098520712454235, 20.84284094017656, 23.009964693357368\n      ),\n      p_value = c(\n        1, 0.852514849292188, 0.019218949341118965, 1, 0.6005144639826616,\n        0.6389666085886305, 0.8009385625517982, 0.004152361260663706,\n        0.15612706651143315, 0.003826110982753214, 1, 0.34086828611885434,\n        0.1760641006276458, 0.06004845140810552, 0.062451043246119525,\n        0.0004003768235061839, 1, 0.8230904120221726, 0.41423830024367947,\n        0.00875705139523374, 1, 0.012575710232363623, 0.00040948417655822014,\n        0.06929230019089422, 1.936595465432012e-13, 1, 0.9295101479009097,\n        0.3285884438638566, 0.21801198338904584, 0.010752726571772354, 1,\n        0.18673382619559387, 0.005526696589432396, 0.0006374334411249112,\n        0.00011720456520099901, 1, 0.0902320478216673, 0.10785907154033761,\n        0.0005510855081592766, 4.348399555275052e-09, 1, 0.00019104640780832482,\n        2.19120848940901e-05, 2.03893337885495e-05, 1.8033985782047306e-07, 1,\n        0.028111010978579744, 0.0014558212298114914, 0.0019333206855010002,\n        6.483039974388384e-05, 1, 0.01466337531542233, 0.01187046890443521,\n        0.00046771600441410024, 0.0010358597091038562, 1, 0.006479849826805965,\n        3.8739270628393594e-08, 4.033471760062014e-10, 1, 4.157990352063954e-05,\n        1.7701583701819876e-96, 3.704764437784754e-117\n      ),\n      lwr = c(\n        1, 0.24367715600341078, 1.0292599381972212, 1, 0.7252228585004926,\n        0.586235908033007, 0.29300496659814207, 1.1093544153322326,\n        0.8734119959888871, 1.1775823198514948, 1, 0.8028570811372586,\n        0.9043140657189745, 0.9888436249589735, 0.9828899536894536,\n        1.5255243781518248, 1, 0.6835480436331928, 0.7457111902735307,\n        1.1368844512616407, 1, 0.6716800729903878, 0.4745722490287588,\n        0.33585021021936473, 1.2425933146287218, 1, 0.6583727615149036,\n        0.8956192214729547, 0.919883887061643, 1.0795995736797042, 1,\n        0.5679462015981974, 0.4634080525899224, 0.37463032186735795,\n        0.37588830767731246, 1, 0.9595524180683677, 0.5289289755252778,\n        1.2825636496959223, 1.9393109796811518, 1, 2.0928111774206113,\n        2.60734937349293, 2.639750883971089, 3.894804027068178, 1, 1.0250270217941126,\n        1.1850809204433688, 1.2534966671910905, 1.99471615545096, 1,\n        1.0880186823389362, 1.1033835873462692, 1.347955543470295, 1.3495363508098424,\n        1, 1.1829621666049654, 1.841575522237812, 2.2180586223282983, 1,\n        1.38129862008169, 9.319130500231545, 17.183514383836002\n      ),\n      upr = c(\n        1, 5.516712238117854, 1.384873581627568, 1, 1.2041972737713005,\n        2.3870888459894837, 4.903561738094752, 1.7381339047482132, 2.324735980655225,\n        2.342367071815249, 1, 1.8862924626146595, 1.7310644191698927,\n        1.7156852531436066, 1.971869996779993, 4.351781809059186, 1,\n        1.6135144273980102, 1.128473718804895, 2.4328358649588253, 1,\n        0.9533141202004918, 0.8077678758371987, 1.0424032809234791,\n        1.4551403256198105, 1, 1.4650479447518308, 1.389992960728278,\n        1.4418757890759486, 1.7953608581567542, 1, 1.1166796357325808,\n        0.8760900715464749, 0.7666408417235587, 0.7272731481694007, 1,\n        1.7630716428530586, 1.06491662717705, 2.463941172662909, 3.7675686765709426,\n        1, 10.740312019042985, 13.506690739439877, 13.805332666504496,\n        19.998002016440463, 1, 1.5469381521371337, 2.042404383073395,\n        2.7262485813383694, 7.54798398562256, 1, 2.16605059978827, 2.2088186084954544,\n        2.885093336992002, 3.2865830544496077, 1, 2.8074485340756543,\n        3.6240699694241325, 4.591632262985475, 1, 2.497503411864645,\n        14.813005872242064, 29.184504809012516\n      ),\n      sign_stars = c(\n        \"\", \"\", \"*\", \"\", \"\", \"\", \"\", \"**\", \"\", \"**\", \"\", \"\", \"\", \".\", \".\", \"***\", \"\",\n        \"\", \"\", \"**\", \"\", \"*\", \"***\", \".\", \"***\", \"\", \"\", \"\", \"\", \"*\", \"\", \"\", \"**\",\n        \"***\", \"***\", \"\", \".\", \"\", \"***\", \"***\", \"\", \"***\", \"***\", \"***\", \"***\", \"\",\n        \"*\", \"**\", \"**\", \"***\", \"\", \"*\", \"*\", \"***\", \"**\", \"\", \"**\", \"***\", \"***\", \"\",\n        \"***\", \"***\", \"***\"\n      ),\n      row.names = 2:64)\n    \n    #-------------------------------------------------------------------\n    \n    #### PLOT ####\n    \n    point_shape = 1\n    \n    point_size = 2\n    \n    outcome &lt;- \"Covid vaccination willingness or uptake:\\nYes ref. no\"\n    \n    p &lt;- ggplot(tt) + \n      geom_point(aes(x = estimate, y = coef),\n                 shape = point_shape,\n                 size = point_size) + \n      geom_vline(xintercept = 1, col = \"black\", linewidth = .2, linetype = 1) + \n      geom_errorbar(aes(x = estimate, y = coef, xmin = lwr, xmax = upr),\n                    linewidth = .5,\n                    width = 0) + \n      facet_grid(rows = vars(vars),\n                 scales = \"free_y\",\n                 space = \"free_y\",\n                 switch = \"y\") + \n      theme_minimal() +\n      labs(title = paste0(\"Outcome: \", outcome),\n           caption = \"p-value: &lt;0.001 ***; &lt;0.01 **; &lt;0.05 *; &lt; 0.1 .\") + \n      xlab(paste0(\"Estimate (\", level*100, \"% CI)\")) + ylab(\"\") +\n      theme(\n        # Pannelli delle strip\n        strip.background = element_rect(fill = \"white\", color = \"white\"),\n        strip.text = element_text(face = \"bold\", size = 9),\n        strip.text.y.left = element_text(angle = 0, hjust = 0.5, vjust = 0.5),\n        strip.placement = \"outside\",\n        # Sfondo\n        panel.background = element_rect(fill = \"white\", color = NA),\n        plot.background = element_rect(fill = \"white\", color = NA),\n        # Margini\n        plot.margin = margin(1, 1, 1, 1))\n\n  \n",
    "author": "Pool_Imaginary",
    "timestamp": "2025-06-23T06:10:42",
    "url": "https://reddit.com/r/rstats/comments/1lig71z/issue_with_homemade_forest_plot/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lg6bzr",
    "title": "What is missing from R according to you? What are your best recommendations?",
    "content": "R is an amazing programming language, and I really enjoy coding with it. It remains unmatched in statistics thanks to its large ecosystem for that purpose. However, we have entered an era where everyone only talks about AI (LLMs), and many packages are moving in this direction [there are at least 30 such packages](https://luisdva.github.io/llmsr-book/r-pkgs.html).\n\nWhile the enthusiasm is impressive, I wonder if we might be overlooking other ideas that could be more useful for the community? For example, I'm surprised there isn't an equivalent to Python's Transformers library. Are there other themes that deserve our attention?\n\nSo, I am interested in your opinion. What kind of package do you need? Is there a package that you appreciate but deserves more recognition? It would be great if you could answer these questions while specifying your profession and/or current use of R. For example:\n\n\"I am a Geography researcher, and I work extensively on 3D map visualization. It would be useful to have a package that... We don't talk enough about the package...\"\n\nThank you in advance!",
    "author": "cyuhat",
    "timestamp": "2025-06-20T08:07:20",
    "url": "https://reddit.com/r/rstats/comments/1lg6bzr/what_is_missing_from_r_according_to_you_what_are/",
    "score": 67,
    "num_comments": 98,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lfrgcm",
    "title": "R Markdown runs all code from the very beginning when I run a single line or a single chunk",
    "content": "I've just updated my RStudio version to see if that would fix it, but nope. I'm now on RStudio 2025.05.1+513 \"Mariposa Orchid\" Release (ab7c1bc795c7dcff8f26215b832a3649a19fc16c, 2025-06-01) for windows.\n\nVisually, I think my chunks are set up correctly. i.e., no loose backticks.\n\nAnyone know how to fix this or what causes it?\n\n  \nI didn't have this issue last week, and I don't think anything had changed.",
    "author": "StarfruitSoup",
    "timestamp": "2025-06-19T18:15:13",
    "url": "https://reddit.com/r/rstats/comments/1lfrgcm/r_markdown_runs_all_code_from_the_very_beginning/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lflw7a",
    "title": "Redistribute category values proportionally across two other categories by group",
    "content": "I have this table, and I want to reassign the case counts when the cause is C55. I want to redistribute it mathematically according to the proportion between C53 and C54 (that is, if both have 1, assign 50% of C55 to each). Always round down, and if there is any remaining whole number, assign it to C53. This should all be done separately for each age group.\n\n    # A tibble: 26 √ó 4\n        SEXO CAUSA GRUPEDAD CUENTA\n       \n    &lt;dbl&gt;\n     \n    &lt;chr&gt;\n     \n    &lt;chr&gt;\n         \n    &lt;dbl&gt;\n     1     2 C55   55 a 59       1\n     2     2 C54   70 a 74       1\n     3     2 C54   80 y mas      1\n     4     2 C53   45 a 49       5\n     5     2 C54   60 a 64       1\n     6     2 C53   50 a 54       1\n     7     2 C53   80 y mas      2\n     8     2 C54   55 a 59       1\n     9     2 C53   65 a 69       3\n    10     2 C55   75 a 79       3\n    # ‚Ñπ 16 more rows",
    "author": "International_Mud141",
    "timestamp": "2025-06-19T13:59:07",
    "url": "https://reddit.com/r/rstats/comments/1lflw7a/redistribute_category_values_proportionally/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lf5ly0",
    "title": "Bakepipe: turn script-based workflows into reproducible pipelines",
    "content": "",
    "author": "ichverstehe",
    "timestamp": "2025-06-19T01:28:11",
    "url": "https://reddit.com/r/rstats/comments/1lf5ly0/bakepipe_turn_scriptbased_workflows_into/",
    "score": 9,
    "num_comments": 4,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ldsx9h",
    "title": "New Free R Consortium Webinar: From Paper to Pixels: Digitizing Water Quality Data Collection with Posit and Esri Integration",
    "content": "New, free R Consortium webinar featuring speakers from the Virginia Department of Environmental Quality!\n\nFrom Paper to Pixels: Digitizing Water Quality Data Collection with Posit and Esri Integration\n\nJune 27, 10am PT / 1pm ET\n\nThe Virginia Department of Environmental Quality (DEQ) is responsible for administering laws and regulations associated with air quality, water quality and supply, renewable energy, and land protection in the Commonwealth of Virginia. These responsibilities generate tremendous quantities of data from monitoring environmental quality, managing permitting processes across environmental media, responding to pollution events, and more. The data collected by DEQ requires management and analysis to gain insight, inform decision making, and meet legal and public obligations.\n\nIn this webinar, we will focus on the integration of our Posit and Esri environments to modernize data collection methods for water quality monitoring. We'll begin with a review of historic water quality data collection processes. Then, we‚Äôll present the architecture of these environments and describe how they were leveraged to modernize mobile data collection at DEQ.\n\nSpeakers\n\n* Joe Famularo - Analytics System Administrator\n* Maddie Moore - GIS System Administrator\n* Emma Jones - Water Monitoring Supervisor\n* Scott Hasinger - Water Monitoring Supervisor\n\n### Register now! [https://r-consortium.org/webinars/from-paper-to-pixels-digitizing-water-quality-data-collection-with-posit-and-esri-integration.html](https://r-consortium.org/webinars/from-paper-to-pixels-digitizing-water-quality-data-collection-with-posit-and-esri-integration.html)",
    "author": "jcasman",
    "timestamp": "2025-06-17T10:14:45",
    "url": "https://reddit.com/r/rstats/comments/1ldsx9h/new_free_r_consortium_webinar_from_paper_to/",
    "score": 6,
    "num_comments": 2,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ldw2y1",
    "title": "Categorical data plot",
    "content": "Years ago, there was a plot of the Titanic disaster data. I think Wickam did it, but I can't find it anywhere. It\n\nwasn't the usual type of plot, but kind of a cumulative plot connecting each variable. Anybody remember this?",
    "author": "paystreak",
    "timestamp": "2025-06-17T12:14:46",
    "url": "https://reddit.com/r/rstats/comments/1ldw2y1/categorical_data_plot/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ld814i",
    "title": "What R tool/package do you wish existed but doesn't?",
    "content": "I'm a research psychologist who works in R daily, but am still often faced with tasks that could be significantly streamlined with the right tools. I'm curious to hear what features or functionalities you all wish were readily available in the R ecosystem?\n\nI'm particularly interested in hearing from other social scientists about their pain points and unmet needs. What tools do¬†*you*¬†wish existed to make your research more efficient and effective? Let's discuss!",
    "author": "Neat-Instance-6537",
    "timestamp": "2025-06-16T16:24:53",
    "url": "https://reddit.com/r/rstats/comments/1ld814i/what_r_toolpackage_do_you_wish_existed_but_doesnt/",
    "score": 32,
    "num_comments": 42,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ldkhoo",
    "title": "Help with the R package ReddiExtractoR: is it limited to \"10 pages\" of results?",
    "content": "I'm using the R package RedditExtractoR to extract thread URLs from a specific subreddit. Here's the code I'm using:\n\n    subreddit_threads &lt;- find_thread_urls(subreddit = \"SubredditName\", sort_by = \"new\", period = \"all\")\n\nHowever, in the console, I see that it only parses up to 10 pages:\n\n    parsing URLs on page 1...\n    ...\n    parsing URLs on page 10...\n\nIt looks like find\\_thread\\_urls() stops automatically after \"10 pages\" of results. My question is: is there a way to go beyond this limit and get all the thread URLs from a subreddit?\n\nAny alternative is more than welcome.\n\nThanks in advance",
    "author": "_aliskiren",
    "timestamp": "2025-06-17T04:19:05",
    "url": "https://reddit.com/r/rstats/comments/1ldkhoo/help_with_the_r_package_reddiextractor_is_it/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lcz22t",
    "title": "R Consortium Working Group Webinar! R for Health Technology Assessment (HTA): Identifying Needs, Streamlining Processes, and Building Bridges",
    "content": "R for Health Technology Assessment (HTA): Identifying Needs, Streamlining Processes, and Building Bridges\n\nJune 30, 2025 - 7am PT / 10am ET / 4pm CEST\n\n[https://r-consortium.org/webinars/r-for-health-technology-assessment-identifying-needs-streamlining-processes-and-building-bridges.html](https://r-consortium.org/webinars/r-for-health-technology-assessment-identifying-needs-streamlining-processes-and-building-bridges.html)\n\nThe Health Technology Assessment (HTA) working group within the R Consortium was established last year with the goal of supporting the use of R for HTA across academia, industry and authorities.\n\nOne work stream of the working group is mapping out stakeholders, their process and their unmet needs that R can support. In this webinar we will present our initial findings regarding the key unmet needs. We have mapped internal processes on the industry side in creating an (EU) HTA submission and identified automatable streams and critical interfaces between work packages. We will highlight the challenges in the processes, our planned approach for dissecting complexities to create clarity, and suggest next steps, ensuring R support throughout the submission.\n\nAt the end we have allocated time for discussion, so please bring your own perspective on using (or not) R in HTA submissions.\n\nThis webinar is aimed at everyone in the field of statistics in the world of (EU) HTA. Whether you are an expert in the industry, HTA bodies or academia, this event is for you!\n\n\nSpeakers\n\nKarolin Struck - SmartStep\n\nAfter studying Mathematical Biometry at Ulm University, Karolin joined SmartStep 8 years ago and since then she analyzes, leads analyses, and gives strategic advice in the German HTA context in various therapeutic areas.\n\nChristian Haargaard Olsen - Novo Nordisk\n\nAfter finishing a PhD in Biomathematics at North Carolina State University, Christian joined Novo Nordisk doing statistical analysis within Hemophilia. Three years ago, Christian shifted focus to HTA, where he is now looking for ways to streamline the process of doing statistical analyses.\n\nRose Hart - Dark Peak Analytics\n\nRose Hart is a Director and Health Economist at Dark Peak Analytics, specializing in researching, consulting and teaching health economics in R. She is experienced in developing bespoke health economic models and value tools in both Excel and R.",
    "author": "jcasman",
    "timestamp": "2025-06-16T10:30:33",
    "url": "https://reddit.com/r/rstats/comments/1lcz22t/r_consortium_working_group_webinar_r_for_health/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lch575",
    "title": "Anyone using LLM's locally with R?",
    "content": "I'm interested in people's experiences with using LLM's locally to help with coding tasks in R. I'm still fairly new to all this stuff but it seems the main advantages of doing this compared to API-based integration is that it doesn't cost anything, and it offers some element of data security? Ollama seems to be the main tool in this space.\n\nSo, is anyone using these models locally in R? How specced out are your computers (RAM etc) vs model parameter count? (I have a 64Gb Mac M2 which I have to actually try but seems might run a 32b parameter model reasonably) What models do you use? How do they compare to API-based cloud models? How secure is your data in a local LLM environment (i.e. does it get uploaded at all)?\n\nThanks.",
    "author": "paulgs",
    "timestamp": "2025-06-15T19:01:01",
    "url": "https://reddit.com/r/rstats/comments/1lch575/anyone_using_llms_locally_with_r/",
    "score": 24,
    "num_comments": 13,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lcxe5s",
    "title": "Finlay-Wilkinson Stability Analysis",
    "content": "Does anyone here have any experience working with Finlay-Wilkinson analysis? I'm struggling to figure this one out, but I've been asked to include it in a manuscript. \n\n  \nI have been looking into gxeFw in the statgenGxE package but I don't understand the 'TD' object. \n\n  \nAny guidance would be appreciated!",
    "author": "In-the-dirt-01",
    "timestamp": "2025-06-16T09:28:09",
    "url": "https://reddit.com/r/rstats/comments/1lcxe5s/finlaywilkinson_stability_analysis/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lc3att",
    "title": "Two-Wave vs. Three-Wave Mediation in lavaan",
    "content": "Hi everyone, my main question is: Under what conditions is it fine to conduct a two-wave mediation analysis of a treatment effect? I want to understand what mediated the effect of an intervention on the outcome in an RCT (intervention N = 78 vs waitlist-control N = 80). I have my data at three time points: pre, post (8 weeks) and follow-up (3 months) and several potential mediators. In my spaghetti plots I see that the change in my variables happened between T1 and T2 both in the mediators and outcome and than remained more or less stable until follow-up. Does this suggest that I should stick to a two-wave mediation (pre-post) or is there value in conducting a three-wave mediation? In case of three-wave mediation I am thinking of Cross-lagged Mediation or Parallel Latent Growth Curve Model, but I would use Sum Scores of my questionnaires, as modeling latent factors with my items would be too complex for my sample size. Would such an approach be fine? Do you have any suggestions for conducting a mediation analysis given my study design? I am grateful for any insights!",
    "author": "Historical-Arm5316",
    "timestamp": "2025-06-15T08:41:04",
    "url": "https://reddit.com/r/rstats/comments/1lc3att/twowave_vs_threewave_mediation_in_lavaan/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lb7kri",
    "title": "I converted most of tune library from tidymodels. It is now mostly using tidytable instead of using dplyr and tidyr (and hopefully purrr and tibble in the future). It still needs a bit of work to convert completely, but unfamiliar with library development. Can I ask for some feedback?",
    "content": "",
    "author": "BIOffense",
    "timestamp": "2025-06-14T05:29:59",
    "url": "https://reddit.com/r/rstats/comments/1lb7kri/i_converted_most_of_tune_library_from_tidymodels/",
    "score": 29,
    "num_comments": 27,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lbcy05",
    "title": "Staggered DiD with unbalanced panel data and \"cumulative\" dependent variable",
    "content": "Hi everyone,  \n‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã  \nI‚Äôm conducting an analysis on the impact that receiving a Michelin star has on the opening of new business ventures associated with a restaurant. The database covers the full history of each restaurant, from its opening year (which varies by restaurant) to the present. It includes the number of associated businesses in each year, the year (if any) in which the restaurant received a star, its location, and the type of ownership. There‚Äôs also a 1:1 control group consisting of restaurants that were never included in the guide but are otherwise similar to the treated ones.  \nAt the moment, I‚Äôm considering a Difference-in-Differences (DiD) approach. I started with a classical 2x2 DiD, using a window of two years before (to account for potential anticipation effects) and five years after the treatment for each restaurant. However, this approach is overly simplistic since the year of treatment (i.e., when the star is awarded) varies across restaurants, which introduces well-known identification issues. I'm therefore considering the Callaway and Sant‚ÄôAnna ATT estimator, which allows for an event-study-style analysis and better handles the staggered nature of the treatment.  \nMy main concerns revolve around  \n‚Ä¢¬†¬† ¬†the staggered timing  \n‚Ä¢¬†¬† ¬†the unbalanced nature of the panel some restaurants have data covering the full observation period (e.g., one opened in 1960 with treatment in 2009 has data from 2000 to 2024), while others like one opened in 2007 lack earlier years. I can't simply fill in missing pre-opening years with zeros for diversification, as that would bias the analysis.)¬†  \n‚Ä¢¬†¬† ¬†the dependent variable: the number of business ventures is cumulative, meaning it either increases or remains constant. One possible solution is to use the year-over-year difference, but the numbers are very small, and I‚Äôm worried about losing meaningful signals.\n\nI'm using this package for Callaway [https://github.com/bcallaway11/did](https://github.com/bcallaway11/did)\n\nAny suggestions or references to similar work would be very welcome.",
    "author": "Marcoss-11",
    "timestamp": "2025-06-14T09:37:29",
    "url": "https://reddit.com/r/rstats/comments/1lbcy05/staggered_did_with_unbalanced_panel_data_and/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1lah3ht",
    "title": "Struggling with replacing NAs for date data in R",
    "content": "Hi!\n\nI've rarely worked with date data in R, so I could use some help. I wrote the below code after using as.Date().\n\nI get appropriate 1s for dates from last fall and appropriate 2s for dates from this spring, however I keep getting NAs for all the other cells when I want to change those NAs to zeros. I've tried a couple different solutions like replace\\_na() to no avail. Those cells are still NAs.\n\nAny help/guidance would be appreciated! There must be something specific about dates that I don't know enough about to troubleshoot on my own.\n\n`mydata$newvar &lt;- ifelse(mydata$date &gt;= '2024-08-01' &amp; mydata$date &lt; '2025-01-01', 1, #fall`\n\n`ifelse(mydata$date &gt;= '2025-01-01', 2, #spring`\n\n`ifelse(is.na(mydata$date), 0, 0)))`",
    "author": "IndividualPiece2359",
    "timestamp": "2025-06-13T07:17:47",
    "url": "https://reddit.com/r/rstats/comments/1lah3ht/struggling_with_replacing_nas_for_date_data_in_r/",
    "score": 7,
    "num_comments": 14,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l8c9j9",
    "title": "New free R Consortium webinar: Scaling the r-spatial ecosystem for the modern composable data pipeline",
    "content": "Scaling the r-spatial ecosystem for the modern composable data pipeline\n\nJune 24, 1pm ET\n\nSign up now! [https://r-consortium.org/webinars/scaling-the-r-spatial-ecosystem-for-the-modern-composable-data-pipeline.html](https://r-consortium.org/webinars/scaling-the-r-spatial-ecosystem-for-the-modern-composable-data-pipeline.html)\n\nR has long been a top choice for spatial statistics, building on the pioneering sp and spdep packages and the wide ecosystem surrounding them. With the introduction of the sf package, R became home to a first-class spatial data frame API. \n\nA growing number of R users, however, need to scale beyond the capabilities of sf. \n\nThis webinar will cover three broad categories of techniques to scale spatial workflows in R, including (1) ensuring that sf code is appropriately using features targeted at larger analyses, (2) using libraries that provide lower-level access to the primitives on which sf builds, including s2, wk, and geos, and (3) using database connectors and in-memory databases to write spatial SQL and perform computations in engines like PostGIS, DuckDB, and Apache Sedona. \n\nFinally, this webinar will provide an overview of the technologies that underlie these techniques, including GeoArrow, GeoParquet, and Apache Iceberg.\n\nSpeaker: \n\nDewey Dunnington, Senior Software Engineer at Wherobots\n\nDewey Dunnington (Ph.D., P.Geo.) is a software engineer and geoscientist based in Winnipeg, Manitoba. As a software engineer he works on scaling spatial data science using Apache Sedona and Apache Arrow at Wherobots. Dewey is a co-creator of GeoArrow, nanoarrow, and a contributor to the Arrow Database Connectivity (ADBC) project. As a geoscientist, he has worked in contaminated site remediation, taught Applied Geomorphology at Acadia University, and has authored more than a dozen articles on lake water and sediment geochemistry. Dewey is an Apache Arrow Project Management Committee member, an RStudio-certified tidyverse instructor, an NSERC Postgraduate Scholarship (Doctoral) recipient, and maintainer of dozens of R, Python, C, and C++ libraries at the intersection of geoscience, geospatial data, and enterprise data connectivity.",
    "author": "jcasman",
    "timestamp": "2025-06-10T15:22:26",
    "url": "https://reddit.com/r/rstats/comments/1l8c9j9/new_free_r_consortium_webinar_scaling_the/",
    "score": 31,
    "num_comments": 3,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l80j5j",
    "title": "Selecting a suitable dataset",
    "content": "Im tasked with writing a short(&lt;20pages) report, applying statistical tools I've learned about in a course to some dataset of my choice. \nBut I'm having a really hard time in selecting one, could someone help me in that?\n(The course was mainly on Bayesian techniques, ie Laplace Approximation, Acceptance-rejection method, Gibbs Sampling and Metropolis-Hastings)",
    "author": "Tasty-Temperature569",
    "timestamp": "2025-06-10T07:43:33",
    "url": "https://reddit.com/r/rstats/comments/1l80j5j/selecting_a_suitable_dataset/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l78mfi",
    "title": "F-Statistic and R squared",
    "content": "Hello,\n\nI am trying to get my head around my single linear regression output in R. In basic terms, my understanding is that the R-squared figure tells me how well the model is fitting the data (the closer to 1, the better it fits the data) and my understand of the F-statistic is that it tells me whether the model as a whole explains the variation in the response variable/s. These both sound like variations of the same thing to me, can someone provide an explanation that might help me understand? Thank you for your help!",
    "author": "SoamesGhost",
    "timestamp": "2025-06-09T09:02:19",
    "url": "https://reddit.com/r/rstats/comments/1l78mfi/fstatistic_and_r_squared/",
    "score": 9,
    "num_comments": 5,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l7bngu",
    "title": "Brewing Success with R.U.M.: How Inclusivity Fuels Manchester's Thriving R Community",
    "content": "The R Consortium recently had the opportunity to interview Anthony Evans, Martin Herrerias Azcue, Rowan Green, Sian Bladon, and Stavrina Dimosthenous, organizers of the R User Group at the University of Manchester (R.U.M.). \n\nThese leaders bring together a diverse community of students, researchers, and staff dedicated to advancing their skills in R programming. Their shared goal is to create an inclusive environment for learning and collaboration, which has driven the growth and success of the R.U.M. community.\n\nFind out more!\n[https://r-consortium.org/posts/brewing-success-with-rum-how-inclusivity-fuels-manchesters-thriving-r-community/](https://r-consortium.org/posts/brewing-success-with-rum-how-inclusivity-fuels-manchesters-thriving-r-community/)",
    "author": "jcasman",
    "timestamp": "2025-06-09T10:58:35",
    "url": "https://reddit.com/r/rstats/comments/1l7bngu/brewing_success_with_rum_how_inclusivity_fuels/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l5k8vr",
    "title": "Rdatasets Archive: 3400 free and documented datasets for fun and exploration",
    "content": "üö® Big milestone for Rdatasets üö® \n\nThe web archive now hosts 3400+ free and documented datasets in CSV format. It's a fantastic resource for exploration, teaching, and testing! \n\nThe archive itself is language-agnostic. Anyone can download and search datasets from the website.\n\n{Rdatasets} is a new [\\#RStats](https://bsky.app/hashtag/RStats) üì¶ for easy download and search \n\nWeb archive: [vincentarelbundock.github.io/Rdatasets](https://vincentarelbundock.github.io/Rdatasets) \n\nR üì¶: [vincentarelbundock.github.io/Rdatasetspkg](https://vincentarelbundock.github.io/Rdatasetspkg)\n\nhttps://preview.redd.it/9ur3sqs26i5f1.jpg?width=2000&amp;format=pjpg&amp;auto=webp&amp;s=a921fb97800b1e216ea565e7ed80414eb990cdc6\n\n",
    "author": "dudeski_robinson",
    "timestamp": "2025-06-07T05:58:57",
    "url": "https://reddit.com/r/rstats/comments/1l5k8vr/rdatasets_archive_3400_free_and_documented/",
    "score": 62,
    "num_comments": 3,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l53284",
    "title": "Using rLakeAnalyzer to determine thermocline depth in Arctic Ocean water column",
    "content": "Please forgive me if my question is very basic, I'm only just starting to learn how to use R.\n\nTo give you some background, a CTD was used to obtain a high resolution vertical profile of the Arctic water column (temp, chlorophyll, depth, salinity) over 15 days. Using the plots below I can guesstimate what the thermocline and halocline was each day but for my research I want to properly calculate the exact depth at which those gradients occur. \n\nAfter searching for a while I found the rLakeAnalyzer package (https://search.r-project.org/CRAN/refmans/rLakeAnalyzer/html/thermo.depth.html). While this seems to be more geared towards calculating the thermocline in lakes, I want to at least see how it'd fare with my dataset.\n\nI'm however having a hard time figuring how to format my data for analysis by this package. Below is a screenshot of what my excel sheet looks like and as you can see depth was recorded every 0.167s from the surface to the seabed (\\~125m), resulting in a large number of values. Given how large my dataset is, it needs to be loaded using [load.ts](https://search.r-project.org/CRAN/refmans/rLakeAnalyzer/help/load.ts.html), but I'm having a hard time figuring out how to organize my data into a file that can be read by this package.\n\n  \nDoes anyone have any ideas as to how I could format my data or any suggestions for other packages that may allow me to identify the depth of the thermocline, halocline and mixed layer.\n\n[ This is the vertical profile of temperature as the CTD moved down the water column. The number at the top indicates the day when the CTD was deployed. ](https://preview.redd.it/bwiud9s3xc5f1.png?width=2864&amp;format=png&amp;auto=webp&amp;s=06afdae753d10823b87fed3f4998c5a5f6c13e7b)\n\nhttps://preview.redd.it/nkcmov9lxc5f1.png?width=2862&amp;format=png&amp;auto=webp&amp;s=fbea30b1480a74be84255197c8bbbdc98404b9a5\n\nhttps://preview.redd.it/tobrxexkyc5f1.png?width=1066&amp;format=png&amp;auto=webp&amp;s=34692163856cf3f3cdccff935fc5555394df0984\n\n  \n",
    "author": "Late-Medium589",
    "timestamp": "2025-06-06T13:47:56",
    "url": "https://reddit.com/r/rstats/comments/1l53284/using_rlakeanalyzer_to_determine_thermocline/",
    "score": 7,
    "num_comments": 5,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l40qx4",
    "title": "Tidy topological machine learning with TDAvec and tdarec by Jason Cory Brunson, Alexsei Luchinsky, Umar Islambekov",
    "content": "Topological data analysis (TDA) is a rapidly growing field that uses techniques from algebraic topology to analyze the shape and structure of data. \n\nTDA is increasingly integrated into machine learning. This project introduces two R packages‚ÄîTDAvec and tdarec‚Äîto bridge TDA with the Tidymodels ecosystem, offering efficient persistent homology vectorization and tidy ML pipelines.\n\nThe team welcomes bug reports, feature requests, and code contributions from the community!\n\nFind out more: [https://r-consortium.org/posts/tidy-topological-machine-learning-with-tdavec-and-tdarec/](https://r-consortium.org/posts/tidy-topological-machine-learning-with-tdavec-and-tdarec/)",
    "author": "jcasman",
    "timestamp": "2025-06-05T07:34:30",
    "url": "https://reddit.com/r/rstats/comments/1l40qx4/tidy_topological_machine_learning_with_tdavec_and/",
    "score": 16,
    "num_comments": 3,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l3h7du",
    "title": "Hardware question! Anyone with 128GB?",
    "content": "Building a new computer, my main computational demands will be R, complex statistical models.\nI have 64GB and some models still take a few days. Has anyone tried 128GB and noticed it makes a difference? Considering the costs ($$) and benefits",
    "author": "Brighteye",
    "timestamp": "2025-06-04T14:10:25",
    "url": "https://reddit.com/r/rstats/comments/1l3h7du/hardware_question_anyone_with_128gb/",
    "score": 21,
    "num_comments": 25,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l3741b",
    "title": "Cor.test Kendall's tau - what is the difference between T and z scores?",
    "content": "I ran a few Kendall's tau tests on different variables using a loop. Some of the summary tables provide a T-value (always an integer?) and some provide a z-score. There are no NAs in the data, and I have 40 observations and 12 variables in two \"groups\". I tested the correlation between variables A &amp; B and 1-10 (so 20 tests, total). For variable A only 3 observations are the same while all other observations are unique, although I did get a warning that the test could not compute exact p-values with ties (but only when running tests with variable A). I get z-scores for most of the correlations except for \\~8 when correlating variable B with variables 1-10 (so these should all be unique) where I get T scores and no warning.\n\nI searched online to understand what the difference is, as the help file does not explain what the T score is. None of the pages I found online explain the difference between the test providing a T or z score; they only discuss one of the two. Generally, they only focus on the p-value and the tau.\n\nI don't understand why I get different kinds of results for these 8 correlations (i.e., a T score, instead of a z score), so I don't know how to reproduce it or make dummy data (I don't want to share my actual data online).\n\nETA: more details.",
    "author": "OscarThePoscar",
    "timestamp": "2025-06-04T07:33:54",
    "url": "https://reddit.com/r/rstats/comments/1l3741b/cortest_kendalls_tau_what_is_the_difference/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l30bcq",
    "title": "GNU GPL / R package / how to aknowledge reuse of 3rd party code?",
    "content": "Hi,  \nI am reusing code from a third party (GNU GPL) in my R package (also GNU GPL).   \nI am planning to publish my package (github, CRAN).   \nWhere should I aknowledge the reuse?   \nDo I mention it in the DESCRIPTION file? how?   \nOr do I just insert in my package the unmodified source files?   \nThanks in advance for your advice.   \nKr ",
    "author": "oqube",
    "timestamp": "2025-06-04T01:29:51",
    "url": "https://reddit.com/r/rstats/comments/1l30bcq/gnu_gpl_r_package_how_to_aknowledge_reuse_of_3rd/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l1lh1a",
    "title": "Introducing diffuseR - a native R implementation of the diffusers library!",
    "content": "[diffuseR](https://github.com/cornball-ai/diffuseR) is the R implementation of the Python diffusers library for creating generative images. It is built on top of the torch package for R, which relies only on C++. No Python required! This post will introduce you to diffuseR and how it can be used to create stunning images from text prompts.\n\nhttps://preview.redd.it/hphc91ondj4f1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=ea90f3fbfc12eb1139df936d68ba60b28803a8d6\n\n# Pretty Pictures\n\nPeople like pretty pictures. They like making pretty pictures. They like sharing pretty pictures. If you've ever presented academic or business research, you know that a good picture can make or break your presentation. Somewhere along the way, the R community ceded that ground to Python. It turns out people want to make more than just pretty statistical graphs. They want to make all kinds of pretty pictures!\n\nThe Python community has embraced the power of generative models to create AI images, and they have created a number of libraries to make it easy to use these models. The Python library [diffusers](https://huggingface.co/docs/diffusers/index) is one of the most popular in the AI community. Diffusers are a type of generative model that can create high-quality images, video, and audio from text prompts. If you're not aware of AI generated images, you've got some catching up to do and I won't go into that here, but if you're interested in learning more about diffusers, I recommend checking out the [Hugging Face documentation](https://huggingface.co/docs/diffusers/index) or the [Denoising Diffusion Probabilistic Models paper](https://arxiv.org/abs/2006.11239).\n\n# torch\n\nUnder the hood, the diffusers library relies predominantly on the [PyTorch](https://pytorch.org/) deep learning framework. PyTorch is a powerful and flexible framework that has become the de facto standard for deep learning in Python. It is widely used in the AI community and has a large and active community of developers and users. As neither Python nor R are fast languages in and of themselves, it should come as no surprise that under the hood of PyTorch [\"lies a robust C++ backend\"](https://medium.com/@pouyahallaj/libtorch-the-c-powerhouse-driving-pytorch-ee0d4f7b8743). This backend provides a readily available foundation for a complete C++ interface to PyTorch, [libtorch](https://pytorch.org/cppdocs/). You know what else can interface C++? R via [Rcpp](https://rcpp.org/)! Rcpp is a widely used package in the R community that provides a seamless interface between R and C++. It allows R users to call C++ code from R, making it easy to use C++ libraries in R.\n\nIn 2020, Daniel Falbel released the [torch](https://github.com/mlverse/torch/) package for R relying on libtorch integration via Rcpp. This allows R users to take advantage of the power of PyTorch without having to use any Python. This is a fundamentally different approach from [TensorFlow](https://tensorflow.rstudio.com/) for R, which relies on interfacing with Python via the `reticulate` package and requires users to install Python and its libraries.\n\nAs R users, we are blessed with the existence of CRAN and have been largely insulated from the dependency hell of frequently long and version-specific list of libraries that is the `requirements.txt` file found in most Python projects. Additionally, if you're also a Linux user like myself, you've likely fat-fingered a `venv` command and inadvertently borked your entire OS. With the torch package, you can avoid all of that and use libtorch directly from R.\n\nThe torch package provides an R interface to PyTorch via the C++ libtorch, allowing R users to take advantage of the power of PyTorch without having to touch any Python. The package is actively maintained and has a growing number of features and capabilities. It is, IMHO, the best way to get started with deep learning in R today.\n\n\n\n# diffuseR\n\nSeeing the lack of generative AI packages in R, my goal with this package is to provide diffusion models for R users. The package is built on top of the torch package and provides a simple and intuitive interface (for R users) for creating generative images from text prompts. It is designed to be easy to use and requires no prior knowledge of deep learning or PyTorch, but does require some knowledge of R. Additionally, the resource requirements are somewhat significant, so you'll want experience or at least awareness of managing your machine's RAM and VRAM when using R.\n\nThe package is still in its early stages, but it already provides a number of features and capabilities. It supports Stable Diffusion 2.1 and SDXL, and provides a simple interface for creating images from text prompts.\n\nTo get up and running quickly, I wrote the basic machinery of diffusers primarily in base R, while the heavy lifting of the pre-trained deep learning models (i.e. unet, vae, text\\_encoders) is provided by [TorchScript](https://docs.pytorch.org/docs/stable/jit.html) files exported from Python. Those large TorchScript objects are hosted on our [HuggingFace page](https://huggingface.co/cornball-ai) and can be downloaded using the package. The TorchScript files are a great way to get PyTorch models into R without having to migrate the entire model and weights to R. Soon, hopefully, those TorchScript files will be replaced by standard torch objects.\n\n# Getting Started\n\nTo get started, go to the [diffuseR github page](https://github.com/cornball-ai/diffuseR) and follow the instructions there. Contributions are welcome! Please feel free to submit a Pull Request.\n\nThis project is licensed under the Apache 2.\n\nThanks to Hugging Face for the original diffusers library, Stability AI for their Stable Diffusion models, to the R and torch communities for their excellent tooling and support, and also to [Claude](https://claude.ai/) and [ChatGPT](https://chatgpt.com/) for their suggestions that weren't hallucinations ;)",
    "author": "TroyHernandez",
    "timestamp": "2025-06-02T08:47:52",
    "url": "https://reddit.com/r/rstats/comments/1l1lh1a/introducing_diffuser_a_native_r_implementation_of/",
    "score": 46,
    "num_comments": 2,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l1k8yj",
    "title": "R Consortium Webinar: Super‚Äëcharging R with Oracle Database: Getting Started with the ROracle Driver",
    "content": "R Consortium webinar THIS WEEK! June 5, 2025 - 9am PT / 12pm ET \n\nSign up now! \n[https://r-consortium.org/webinars/super-charging-r-with-oracle-database.html](https://r-consortium.org/webinars/super-charging-r-with-oracle-database.html)\n\nUnlock the power of Oracle Database in R with the ROracle driver. Find out about ROracle installation and configuration steps, key features, performance best practices, and the future roadmap of the driver. \n\nThe webinar includes with a practical demo showcasing real-world data exploration and AI vector similarity search.",
    "author": "jcasman",
    "timestamp": "2025-06-02T07:59:54",
    "url": "https://reddit.com/r/rstats/comments/1l1k8yj/r_consortium_webinar_supercharging_r_with_oracle/",
    "score": 6,
    "num_comments": 1,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kzz9g2",
    "title": "Meetups in NYC",
    "content": "Are there any R programming meetups in the New York metropolitan area? I know of nyhackr, but they seemed to have transformed into an AI/ML meetup. ",
    "author": "player_tracking_data",
    "timestamp": "2025-05-31T08:06:34",
    "url": "https://reddit.com/r/rstats/comments/1kzz9g2/meetups_in_nyc/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1l01wyu",
    "title": "Need advice on finding datasets",
    "content": "I have an assessment that requires me to find a dataset from a reputable, open-access source (e.g., Pavlovia, Kaggle, OpenNeuro, GitHub, or similar public archive), that should be suitable for a t-test and an ANOVA analysis. I've attempted to explore the aforementioned websites to find datasets, however, I'm having trouble finding appropriate ones (perhaps it's because I don't know how to use them properly), with many of the datasets that I've found providing only minimal information with no links to the actual paper (particularly the ones on kaggle). Does anybody have any advice/tips for finding suitable datasets?",
    "author": "xmishieee",
    "timestamp": "2025-05-31T09:59:48",
    "url": "https://reddit.com/r/rstats/comments/1l01wyu/need_advice_on_finding_datasets/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kzdo6d",
    "title": "[Question] How to Apply Non-Negative Least Squares (NNLS) to Longitudinal Data with Fixed/Random Effects?",
    "content": "I have a dataset with repeated measurements (longitudinal) where observations are influenced by covariates like `age`, `time point`, `sex`, etc. I need to perform regression with **non-negative coefficients** (i.e., no negative parameter estimates), but standard mixed-effects models (e.g., `lme4` in R) are too slow for my use case.\n\n  \nI‚Äôm using a fast NNLS implementation (`nnls` in R) due to its speed and constraint on coefficients. However, I have not accounted for the metadata above.\n\nMy questions are:\n\n1. Can I split the dataset into groups (e.g., by `sex` or `time point`) and run NNLS separately for each subset? Would this be statistically sound, or is there a better way?\n\n2. Is there a way to incorporate **fixed and random effects** into NNLS (similar to `lmer` but with non-negativity constraints)? Are there existing implementations (R/Python) for this?\n\n3. Are there adaptations of NNLS for longitudinal/hierarchical data? Any published work on NNLS with mixed models?\n\n\n\n",
    "author": "Odd-Establishment604",
    "timestamp": "2025-05-30T12:51:24",
    "url": "https://reddit.com/r/rstats/comments/1kzdo6d/question_how_to_apply_nonnegative_least_squares/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kz4n55",
    "title": "K-INDSCAL package for R?",
    "content": "Originally posted on r/AskStatistics but was recommended to post here...  \n\nI want to use a type of multidimensional scaling (MDS) called K-INDSCAL (basically K means clustering and individual differences scaling combined) but I can't find a pre-existing R package and I can't figure out how people did it in the papers written about it.¬†[The original paper](https://pubmed.ncbi.nlm.nih.gov/27519687/)¬†has lots of formulas and examples, but no source code or anything.\n\nHas anyone worked with this before and/or can point me in the right direction for how to run this in R? Thanks so much!",
    "author": "woolorca10",
    "timestamp": "2025-05-30T06:46:43",
    "url": "https://reddit.com/r/rstats/comments/1kz4n55/kindscal_package_for_r/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kzcit9",
    "title": "How Do I Test a Moderated Mediation Model with Multiple Moderators in R?",
    "content": "Hello!  \nI‚Äôve been trying to learn R over the past two days and would appreciate some guidance on how to test this model. I‚Äôm familiar with SPSS and PROCESS Macro, but PROCESS doesn‚Äôt include the model I want to test. I also looked for tutorials, but most videos I found use an R extension of PROCESS, which wasn‚Äôt helpful.\n\nBelow you can find the model I want to test along with the code I wrote for it.\n\nI would be grateful for any feedback. If you think this approach isn‚Äôt ideal and have any suggestions for helpful resources or study materials, please share them with me. Thank you!\n\nhttps://preview.redd.it/2oydoie6wy3f1.jpg?width=2360&amp;format=pjpg&amp;auto=webp&amp;s=25ee9b45eac8803955baf211c397bf6dd141c644\n\n",
    "author": "hiraethwl",
    "timestamp": "2025-05-30T12:03:46",
    "url": "https://reddit.com/r/rstats/comments/1kzcit9/how_do_i_test_a_moderated_mediation_model_with/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kyzikn",
    "title": "model selection : dredge() doesn't return models' weights",
    "content": "Hey,\n\nI'm having a hard time understanding why no weights are calculated for my models (the column is created but is full of NAs). Here is the full model :  \nglmmTMB(LULARB\\~etat\\_parcelle\\*typeMC2+vent+temp+pol+neb+occ\\_sol+Axe1+date+heure+mat(pos\\_env+0|id\\_env)+(1|obs),family = binomial(link=\"logit\"),data=compil\\_env.bi,ziformula=\\~1, na.action=\"na.pass\")\n\nand a glimpse of my results :\n\nhttps://preview.redd.it/35sb2nm5xv3f1.png?width=1494&amp;format=png&amp;auto=webp&amp;s=67e61e30d54ef9f0211fc5163f38bdfb802de92a\n\n  \nDoes anyone could shed a light on this ..?  \nMay the dredge() function not handling glmmTMB() or some of its arguments (ziformula for zero-inflated model for example) be the reason of my problem?\n\nHave a good day !",
    "author": "haliaetus92",
    "timestamp": "2025-05-30T02:04:09",
    "url": "https://reddit.com/r/rstats/comments/1kyzikn/model_selection_dredge_doesnt_return_models/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kybpsg",
    "title": "R Consortium‚Äôs Infrastructure Steering Committee (ISC) announcing first round 2025 grant recipients",
    "content": "The R Consortium‚Äôs Infrastructure Steering Committee (ISC) is proud to announce the first round of 2025 grant recipients. \n\nFind out about the seven new projects receiving support to enhance and expand the capabilities of the R ecosystem. The projects range from economic policy tools and ecological data pipelines to foundational software engineering improvements.\n\nThe post also covers funding news about our Top-Level Projects, R-Ladies+ and R-Universe!\n\n[https://r-consortium.org/posts/r-consortium-awards-first-round-of-2025-isc-grants/](https://r-consortium.org/posts/r-consortium-awards-first-round-of-2025-isc-grants/)",
    "author": "jcasman",
    "timestamp": "2025-05-29T07:07:59",
    "url": "https://reddit.com/r/rstats/comments/1kybpsg/r_consortiums_infrastructure_steering_committee/",
    "score": 23,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kxu9c3",
    "title": "Which programing langage for market access/clinical trials?",
    "content": "Hi everyone, \n\nI'm going back to (a French) business school to get a Msc in biopharmaceutical management and biotechnology. I am a lawyer, and I really really don't want to end up in regulatory affairs.\n\nI want to be at the interface between market access and data. I'll do my internship in a think tank which specialises in AI in health care. I know I am no engeener but I think I can still make myself usefully. If I doesn't go well, I'll be going into venture capital or private equity. \n\nR is still a standard in the industry, but is python becoming more and more important? I know a little bit of R. \n\nThank you :) ",
    "author": "Real-Pianist-8864",
    "timestamp": "2025-05-28T15:15:39",
    "url": "https://reddit.com/r/rstats/comments/1kxu9c3/which_programing_langage_for_market/",
    "score": 4,
    "num_comments": 11,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ky46tw",
    "title": "If my client wanted to increase the CSAT target from 80 to 85. What statistical method can I use to determine if the new goal is achievable?",
    "content": "",
    "author": "ram0120",
    "timestamp": "2025-05-28T23:56:03",
    "url": "https://reddit.com/r/rstats/comments/1ky46tw/if_my_client_wanted_to_increase_the_csat_target/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kxrds5",
    "title": "Using survey weights in lmer (or an equivalent)",
    "content": "I have been using R exclusively for about a year after losing access to SAS. In SAS, I would do something like the following\n\nnewweight=(weight1)\\*(weight2); (per the documentation guidelines)\n\n  \nproc mixed method = ml covtest ic;\n\nclass region;\n\nmodel dv= iv1 iv2 region\\_iv\n\n/solution ddfm=bw notest; weight newweight;\n\nrandom int /subject = region G TYPE = VC;\n\nrun;\n\nIn R I have\n\n  \nevs$combined\\_weight &lt;- evs$dweight \\* evs$pweight\n\n\n\nm1 &lt;- lmer(andemo \\~ iv1 + iv2 + cntry\\_iv1 +\n\n(1 | cntry\\_factor), data = evs, weights = combined\\_weight)\n\n  \nIn this case, I get an error message because the combined weight has negative values. In other cases, the model converges and produces results, but I have read conflicting accounts about how well lmer handles weights, whether I weight the entire dataset or apply the weights to the lmer function. \n\nWould anyone happen to have recommendations for how to move forward? Is there another package for multilevel models that can handle this better?\n\n",
    "author": "milkthrasher",
    "timestamp": "2025-05-28T13:19:39",
    "url": "https://reddit.com/r/rstats/comments/1kxrds5/using_survey_weights_in_lmer_or_an_equivalent/",
    "score": 2,
    "num_comments": 12,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kxma5r",
    "title": "Help! Correcting violated regression assumptions",
    "content": "Hi everyone, I could really use your help with my master‚Äôs thesis.\n\nI‚Äôm running a moderated mediation analysis using PROCESS Model 7 in R. After checking the regression assumptions, I found:\n\t‚Ä¢\tHeteroskedasticity in the outcome models, and\n\t‚Ä¢\tNon-normal distribution of residuals.\n\nFrom what I understand, bootstrapping in PROCESS takes care of this for indirect effects. However, I‚Äôve also read that for interpreting direct effects (X ‚Üí Y), I should use HC4 robust standard errors to account for these violations.\n\nSo my questions are:\n\t1.\tIs it correct that I should run separate regression models with HC4 for interpreting direct effects?\n\t2.\tShould I use only the PROCESS output for the indirect and moderated mediation effects, since those are bootstrapped and robust?\n\nFor context: I have one IV, one mediator, one moderator, and three DVs (regret, confidence, excitement) ‚Äî tested in separate models.\n\nI would really appreciate your help as my deadline is approaching and this is stressing me out ü•≤",
    "author": "Creative-Dare2578",
    "timestamp": "2025-05-28T09:57:43",
    "url": "https://reddit.com/r/rstats/comments/1kxma5r/help_correcting_violated_regression_assumptions/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kw6h4h",
    "title": "ggplot2 tabbed labels in figure legends",
    "content": "I would like to put a label and a number in my figure legend for color, and I would like the numbers to be left-justified above each other, rather than simply spaced behind the label.  Both the labels and the numbers are the same length, so I could simply use a mono-spaced font.  But ggplot only offers courier as a mono-spaced font, and it looks quite ugly compared with the Helvetica used for the other labels.\n\nIs there a way for me to make a text object that effectively has a tabbed spacing between two fields that I can put in a legend?",
    "author": "fasta_guy88",
    "timestamp": "2025-05-26T14:44:58",
    "url": "https://reddit.com/r/rstats/comments/1kw6h4h/ggplot2_tabbed_labels_in_figure_legends/",
    "score": 3,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kw379o",
    "title": "Advice/ suggestions",
    "content": "I'm am from clinical field, wanting to do a career shift to biomed Sci, since I love the research part.\n\nMy biomed program offers electives like R, epidemiology, fundamentals of data Sci, BMDA (high throughtput bio med data analysis)\n\nAs of the trends these days, I understand data analysis is more important. And I really wanna do BMDA (to sustain and stay relevant in the field)\n\nAny advice regarding how to work towards this journey is much appreciated. \n\nPs: I am a newbie, like can't even type faster in PC ",
    "author": "olesomecookie_",
    "timestamp": "2025-05-26T12:28:38",
    "url": "https://reddit.com/r/rstats/comments/1kw379o/advice_suggestions/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kw1uek",
    "title": "Question about the learning material",
    "content": "Hello,  \nI have been wandering for months between all the different types of materials without actually doing anything because I am not satisfied with anything, so I want to ask everyone for an opinion.  \nI followed a course in data analysis (although I don't recall much), and my professor advised me to focus more on practicing and reading articles, even though he did saw how much I suck (he said I should review the slides but I don't find them very complete).  \nI am currently preparing for a 6-month internship for my thesis, which will cover R applied to machine learning and data analysis for metabolomics data types.  \nI was thinking of following my professor's advice, using a dataset I create or find online to practice, and reading a lot of articles about my thesis topic. To understand more about the statistical part, I was thinking of using the book \"Practical Statistics for Data Scientists\" , but I am reading a lot of different reviews about it being good for beginners or not.  \nWhat do you think I should do? Sorry if it's messy",
    "author": "Bumblebee0000000",
    "timestamp": "2025-05-26T11:33:33",
    "url": "https://reddit.com/r/rstats/comments/1kw1uek/question_about_the_learning_material/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kvds1f",
    "title": "Qualitative data analysis",
    "content": "I'm trying to analyze data which has both continuous and categorical variables. I've looked into probit analysis using the glm function of the 'aod' package. The problem is not all my variables are binary as required for probit analysis.\n\nFor example, I'm trying to find a relationship between age (categorical variable) and climate change concern (categorical variable with 3 responses). Probit seems somewhat inappropriate, but I'm struggling to find another analysis method that works with categorical data that still provides a p-value.\n\n  \nR output:\n\n\\*there is an additional age range not included in the output- not sure how to interpret this.\n\n    Call:\n    glm(formula = CFCC ~ AGE, family = binomial(link = \"probit\"), \n        data = sdata)\n    \n    Coefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)\n    (Intercept)             -5.019    235.034  -0.021    0.983\n    AGE26 - 35 years         5.019    235.034   0.021    0.983\n    AGE36 - 45 years         4.619    235.034   0.020    0.984\n    AGE46 - 55 years         4.765    235.034   0.020    0.984\n    AGE56 years and older    4.825    235.034   0.021    0.984\n    \n    (Dispersion parameter for binomial family taken to be 1)\n    \n        Null deviance: 118.29  on 87  degrees of freedom\n    Residual deviance: 116.34  on 83  degrees of freedom\n    AIC: 126.34\n    \n    Number of Fisher Scoring iterations: 13\n\n  \n",
    "author": "In-the-dirt-01",
    "timestamp": "2025-05-25T14:37:07",
    "url": "https://reddit.com/r/rstats/comments/1kvds1f/qualitative_data_analysis/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ktkol6",
    "title": "Use rix to restore old environment or \"what to do I do if a package from github requires other packages that no longer exist\"",
    "content": "There was this post where OP asked what to do if a package hosted on GitHub requires packages that no longer exist: [https://www.reddit.com/r/rstats/comments/1kstd55/what\\_do\\_i\\_do\\_if\\_a\\_package\\_from\\_github\\_requires/](https://www.reddit.com/r/rstats/comments/1kstd55/what_do_i_do_if_a_package_from_github_requires/)\n\nOP found a solution (there‚Äôs an updated version of the package that works with current packages), but in case you ever find yourselves in such a conundrum, you might want to try my package rix, which makes it easy to set up reproducible development environments using the Nix package manager (which you need to install first).\n\nSimply write this script:\n\n    library(\"rix\")\n    \n    path_default_nix &lt;- \".\"\n    \n    rix(\n    \n      date = \"2023-08-15\",\n    \n       r_pkgs = NULL, # add R packages from CRAN here\n    \n       git_pkgs = list(\n    \n        package_name = \"ellipsenm\",\n    \n        repo_url = \"https://github.com/marlonecobos/ellipsenm\",\n    \n        commit = \"0a2b3453f7e1465b197750b486a5e5ed6596a1da\"\n    \n      ),\n    \n      ide = \"none\", # Change to rstudio for rstudio\n    \n      project_path = path_default_nix,\n    \n      overwrite = TRUE,\n    \n      print = TRUE\n    )  \n\nwhich will generate the appropriate Nix file defining the environment. You can then build the environment using \\`nix-build\\` and then activate the environment using \\`nix-shell\\`. It turns out that \\`ellipsenm\\` doesn‚Äôt list \\`formatR\\` as one of its dependencies, even though it requires it, so in this particular case you‚Äôd need to add \\`formatR\\` to the list of dependencies in the \\`default.nix\\` for the expression to build successfully. This is why CRAN is so important!\n\n  \nrix makes it also easy to add Python and Julia packages.\n\nFor a 5-minute video intro to rix, take a look at [https://www.youtube.com/watch?v=t4MfjKgqDOc](https://www.youtube.com/watch?v=t4MfjKgqDOc)",
    "author": "brodrigues_co",
    "timestamp": "2025-05-23T07:16:13",
    "url": "https://reddit.com/r/rstats/comments/1ktkol6/use_rix_to_restore_old_environment_or_what_to_do/",
    "score": 31,
    "num_comments": 5,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ktpv4a",
    "title": "Are there any screencasts of people making libraries? Bonus points if it's converting libraries (taking an existing library, transforming it to create a new library with new name)",
    "content": "Similar to Hadley's video 'Whole Game' or Julia Silge's screencasts, I was just wondering if there are screencasts for making + transforming libraries.",
    "author": "jinnyjuice",
    "timestamp": "2025-05-23T10:47:49",
    "url": "https://reddit.com/r/rstats/comments/1ktpv4a/are_there_any_screencasts_of_people_making/",
    "score": 11,
    "num_comments": 5,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ktuw3m",
    "title": "Is there a package for detecting bot responses in surveys",
    "content": "To make a long story short, I thought I had the bot detection turned on in Qualtrics, and I was wrong! Anyway, now I have a boatload of data to sift through that might be 90% bots. Is there a package that can help automate this process? \n\nI had found that there was a package called rIP that would do this with IP addresses, but unfortunately, that package has been removed from CRAN as a dependency package has been removed as well. Is there anything similar?",
    "author": "Interesting-Ad6827",
    "timestamp": "2025-05-23T14:19:12",
    "url": "https://reddit.com/r/rstats/comments/1ktuw3m/is_there_a_package_for_detecting_bot_responses_in/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kts44p",
    "title": "Struggling with Zero-Inflated, Overdispersed Count Data: Seeking Modeling Advice",
    "content": "I‚Äôm working on predicting what factors influence where biochar facilities are located. I have data from 113 counties across four northern U.S. states. My dataset includes over 30 variables, so I‚Äôve been checking correlations and grouping similar variables to reduce multicollinearity before running regression models.\n\nThe outcome I‚Äôm studying is the number of biochar facilities in each county (a count variable). One issue I‚Äôm facing is that many counties have zero facilities, and I‚Äôve tested and confirmed that the data is zero-inflated. Also, the data is overdispersed ‚Äî the variance is much higher than the mean ‚Äî which suggests that a zero-inflated negative binomial (ZINB) regression model would be appropriate.\n\nHowever, when I run the ZINB model, it doesn‚Äôt converge, and the standard errors are extremely large (for example, a coefficient estimate of 20 might have a standard error of 200).\n\nMy main goal is to understand which factors significantly influence the establishment of these facilities ‚Äî not necessarily to create a perfect predictive model.\n\nGiven this situation, I‚Äôd like to know:\n\n1. Is there any way to improve or preprocess the data to make ZINB work?\n2. Or, is there a different method that would be more suitable for this kind of problem?",
    "author": "LocoSunflower_07",
    "timestamp": "2025-05-23T12:20:02",
    "url": "https://reddit.com/r/rstats/comments/1kts44p/struggling_with_zeroinflated_overdispersed_count/",
    "score": 4,
    "num_comments": 15,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ksxz8d",
    "title": "The 80/20 Guide to R You Wish You Read Years Ago",
    "content": "Hey r/rstats! After years of R programming, I've noticed most intermediate users get stuck writing code that works but isn't optimal. We learn the basics, get comfortable, but miss the workflow improvements that make the biggest difference.\n\nI just wrote up the handful of changes that transformed my R experience - things like:\n\n* Why DuckDB (and data.table) can handle datasets larger than your RAM\n* How renv solves reproducibility issues\n* When vectorization actually matters (and when it doesn't)\n* The native pipe |&gt; vs %&gt;% debate\n\nThese aren't advanced techniques - they're small workflow improvements that compound over time. The kind of stuff I wish someone had told me sooner.\n\nRead the [full article here.](https://open.substack.com/pub/borkar/p/the-8020-guide-to-r-you-wish-you?r=2qg9ny&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true)\n\nWhat workflow changes made the biggest difference for you?",
    "author": "Capable-Mall-2067",
    "timestamp": "2025-05-22T11:23:38",
    "url": "https://reddit.com/r/rstats/comments/1ksxz8d/the_8020_guide_to_r_you_wish_you_read_years_ago/",
    "score": 242,
    "num_comments": 20,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ktsrwt",
    "title": "Newbie to EBI Image analyser and trying to get the values from a ranged bar chart in .tif file Format",
    "content": "I've been at this for hours, and maybe I'm an idiot and can't see how this works, but this is wrecking me. I have a greyscale bar chart with the temperature ranges of nine countries and I'm trying to get the min and max values for one country in particular? Would anyone please know how? I've tried different types of code but it keeps getting stuck on the image having the wrong number of dimensions, as it seems to have three not two.",
    "author": "SilverLadybird",
    "timestamp": "2025-05-23T12:48:33",
    "url": "https://reddit.com/r/rstats/comments/1ktsrwt/newbie_to_ebi_image_analyser_and_trying_to_get/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kstfhp",
    "title": "Making Computer Vision for R Easily Accessible",
    "content": "{kuzco} is an R package that reimagines how image classification and computer vision can be approached using large language models (LLMs). \n\nIn this interview, we talk with Frank Hull, director of data science &amp; analytics leading a data science team in the energy sector, an open source contributor, and a developer of {kuzco}. We explore the ideas behind {kuzco}, its use of LLMs, and how it differs from conventional deep learning frameworks like {keras} and {torch} in R. \n\n{kuzco} is open source and the project is actively looking for contributions, both technical and non-technical. \n\nTry it out now! \n\n[https://r-consortium.org/posts/exploring-kuzco-making-computer-vision-for-r-easily-accessible/](https://r-consortium.org/posts/exploring-kuzco-making-computer-vision-for-r-easily-accessible/)",
    "author": "jcasman",
    "timestamp": "2025-05-22T08:21:47",
    "url": "https://reddit.com/r/rstats/comments/1kstfhp/making_computer_vision_for_r_easily_accessible/",
    "score": 38,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kstd55",
    "title": "What do I do if a package from github requires other packages that no longer exist?",
    "content": "Basically what the title says. I'm trying to install ellipsenm (a package up on github for ENM ellipsoid analysis) but the installation fails because it seems to require rgdal and rgeos. However both packages were archived in 2023 and don't exist for my version of R (4.5), their pages on CRAN suggest using sf or terra instead, which I have, but I don't know how make the installation work with those- if it even is something I can fix myself?\n\nThank you",
    "author": "Unreasonableberry",
    "timestamp": "2025-05-22T08:19:08",
    "url": "https://reddit.com/r/rstats/comments/1kstd55/what_do_i_do_if_a_package_from_github_requires/",
    "score": 8,
    "num_comments": 16,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kt9s29",
    "title": "Help ‚Äî getting error message that ‚Äúcontrasts can be applied only to factors with 2 or more levels‚Äù (crossposted because my assignment is due soon and I really need to figure this out‚Ä¶)",
    "content": "",
    "author": "Sir-Crumplenose",
    "timestamp": "2025-05-22T20:27:48",
    "url": "https://reddit.com/r/rstats/comments/1kt9s29/help_getting_error_message_that_contrasts_can_be/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kt549u",
    "title": "Installing Python in RStudio",
    "content": "I am having trouble installing Python in my RStudio. I am willing to bet it is not Rocket Science. Does anyone know an easy resource I can refer to so I can write and work with both codes simultaneously? Thank you. ",
    "author": "BlackHoles_NCC1701D",
    "timestamp": "2025-05-22T16:25:48",
    "url": "https://reddit.com/r/rstats/comments/1kt549u/installing_python_in_rstudio/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kscg6o",
    "title": "Newbie here. Don't know much, but need help.",
    "content": "I am a doctor who has starting out to do biomedical research involving complex databases of patients, and I have recently learnt that it requires me to learn data languages such as R. Can anyone please share a list of resources I need to procure to start this? Thank you so much for sparing a moment to help me.",
    "author": "HelicopterHour930",
    "timestamp": "2025-05-21T16:32:11",
    "url": "https://reddit.com/r/rstats/comments/1kscg6o/newbie_here_dont_know_much_but_need_help/",
    "score": 5,
    "num_comments": 10,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ks71z8",
    "title": "For loop to perform paired t-test for each row in a tibble?",
    "content": "Hello! I'm a beginner to R and stats, and I'm trying to perform a paired t-test (and also understand what I'm doing...). I've arranged my data looks like this, which I was told would be more compatible with performing t-tests:\n\nhttps://preview.redd.it/rdm12gs4u62f1.png?width=1288&amp;format=png&amp;auto=webp&amp;s=8d9a0328712a2846ed1cccb3bc096e9caa05762e\n\nIn English, I would say, \"for each gene, perform a t-test comparing the means of strain1\\_half\\_lives and strain2\\_half\\_lives, and pair the values in each vector.\"\n\nFor example, in the first row, 0.8444763 would be paired with 0.7871189.\n\nI will then do an FDR correction on the p-values.\n\nThank you so much!",
    "author": "adventuriser",
    "timestamp": "2025-05-21T12:41:23",
    "url": "https://reddit.com/r/rstats/comments/1ks71z8/for_loop_to_perform_paired_ttest_for_each_row_in/",
    "score": 5,
    "num_comments": 19,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1krygy1",
    "title": "test significance of environmental variables in dbRDA",
    "content": "I want to perform dbrda to identify the interaction of environmental variables with ecological abundance data. How do I test for significance of each environmental variable in a DB RDA\n\nalso how do i find fhe percent contribution of each variable?? ",
    "author": "marinebiot",
    "timestamp": "2025-05-21T06:56:53",
    "url": "https://reddit.com/r/rstats/comments/1krygy1/test_significance_of_environmental_variables_in/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kr5e5c",
    "title": "classification algorithms based on longitudinal data",
    "content": "Can someone suggest a R package that is useful for taking longitudinal data and using it for a classification algorithm?",
    "author": "tensor314",
    "timestamp": "2025-05-20T06:51:39",
    "url": "https://reddit.com/r/rstats/comments/1kr5e5c/classification_algorithms_based_on_longitudinal/",
    "score": 5,
    "num_comments": 7,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kql6cp",
    "title": "Where to learn R",
    "content": "Hello everyone,\n\nSo I am starting out my MSc course in agriculture soon but I've realised that my technical knowledge is lacking in statistics specially when it comes to using softwares like R. Can I get some good recommendations where I can start from basics. I am looking for something that can help me understand better how to visualise hypothetical models, predictive models such and such.\n\nI'd really appreciate any information. You can name youtube channels, any free materials, paid courses work as well as long as they r not lengthy and expensive.",
    "author": "WhiteTigerLeo",
    "timestamp": "2025-05-19T12:45:36",
    "url": "https://reddit.com/r/rstats/comments/1kql6cp/where_to_learn_r/",
    "score": 36,
    "num_comments": 49,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kqqj5e",
    "title": "Easy beginner projects to do in R",
    "content": "Tomorrow I have an interview and it said to be familiar with R. I‚Äôm not really sure how familiar they want us to be but I want to do a mini project just in case ! I studied R a little bit while I was in my statistics class and we had to do a project using t.test, 2-p test etc. we also learned the basics of R like mean, median, standard deviation etc. I‚Äôm wondering if anyone can recommend a mini project to showcase knowledge! Thank you! ",
    "author": "OutrageousMention836",
    "timestamp": "2025-05-19T16:30:42",
    "url": "https://reddit.com/r/rstats/comments/1kqqj5e/easy_beginner_projects_to_do_in_r/",
    "score": 3,
    "num_comments": 9,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kqrrow",
    "title": "R online AI environment project -- ADVICE REQUESTED",
    "content": "Heya all! I am a recent college grad and have been studying R code for several years now. I also recently learned a lot about coding with AI in python, with integrations for chat and coding environments. I am looking to create a project involving a free online R studio-type coding environment with an AI assistant. I would love some advice on what y'all would want out of this! For now my main points of interest to distinguish using this over RStudio is:  \n\\- AI context reading: the AI will know your code, data files, and console outputs without you having to copy paste line after line in, making it easier to ask simple questions and get simple responses  \n\\- Short and sweet answers: the AI will also answer your questions based on YOUR skill level and knowledge. If you only need to know how to load mtcars data, it will only tell you that! No fluff!\n\nI would love any advice on issues you all have in your daily R coding that could be solved through an AI integration in this manner. I'm really looking to distinguish from ChatGPT and other co-pilot style coding AIs out there through a more seamless integration, rather than a constant back and forth of not-so-great answers and/or problem-solving. Let me know! I'm also open to criticism!",
    "author": "OkaySituations",
    "timestamp": "2025-05-19T17:31:40",
    "url": "https://reddit.com/r/rstats/comments/1kqrrow/r_online_ai_environment_project_advice_requested/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kpe9zy",
    "title": "15 New Books added to Big Book of R - Oscar Baruffa",
    "content": "6 English and 9 Portuguese books have been added to the collection of over 400 free, open source books",
    "author": "oscarb1233",
    "timestamp": "2025-05-18T00:22:41",
    "url": "https://reddit.com/r/rstats/comments/1kpe9zy/15_new_books_added_to_big_book_of_r_oscar_baruffa/",
    "score": 48,
    "num_comments": 1,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kpjpv7",
    "title": "Basic examples of deploying tidyverse models to GCP",
    "content": "Hi,\n\nStruggling to get tidymodels to work with vetiver, docker and GCP, does anyone have an end to end example of deploying iris or mtcars etc to an end point on GCP to serve up predictions?\n\nThanks",
    "author": "EmptyVector",
    "timestamp": "2025-05-18T06:15:17",
    "url": "https://reddit.com/r/rstats/comments/1kpjpv7/basic_examples_of_deploying_tidyverse_models_to/",
    "score": 6,
    "num_comments": 4,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kpqkzp",
    "title": "How to get RServe to enforce user and password from remote Java code?",
    "content": "I've created the /etc/Rserve.conf file with both:\n\nremote enable\n\nauth required\n\nAlso, created in /home/ubuntu, the .Rservauth file with user and password (tab separated).\n\nMade sure to:\n\nsudo chmod 600 /home/ubuntu/.Rservauth\n\nsudo chown ubuntu:ubuntu /home/ubuntu/.Rservauth \n\nI reloaded everything and even rebooted the AWS Ubuntu Linux instance twice, but the Java code can still run R fine with a bogus user and password.\n\nThe .Rservauth file has:\n\nmyuser&lt;TAB&gt;mypassword\n\n\\----  \nDoes this functionality work where you can tell Rserve to only allow Java connections with user and password?\n\n  \nThanks in advance for what I could be missing.",
    "author": "German-411",
    "timestamp": "2025-05-18T11:18:48",
    "url": "https://reddit.com/r/rstats/comments/1kpqkzp/how_to_get_rserve_to_enforce_user_and_password/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kpkwvc",
    "title": "I'm having trouble installing basic libraries in R on AWS Ubuntu Linux.",
    "content": "Below is a detailed interaction on trying to install libraries in R. I had several others fail also, but the problems were similar to the results below. I had successfully installed these libraries back in 2018 so I realize something has changed. I just don't know what.\n\nWould appreciate any ideas.  \n  \n**Here's what I did to demonstrate this issue:**\n\nCreate new unbuntu t3.large, 8 GB RAM, 25 GB Disk\n\nConnect with SSH Client\n\nDid a \"sudo apt update &amp;&amp; sudo apt upgrade -y\"\n\nInstall R\n\nsudo apt install -y dirmngr gnupg apt-transport-https ca-certificates software-properties-common\n\nAdd the CRAN GPG Key\n\nsudo apt-key adv --keyserver [keyserver.ubuntu.com](http://keyserver.ubuntu.com) \\--recv-keys '51716619E084DAB9'\n\nAdd the CRAN Repo\n\nsudo apt install -y software-properties-common dirmngr\n\nReading package lists... Done\n\nBuilding dependency tree... Done\n\nReading state information... Done\n\nsoftware-properties-common is already the newest version (0.99.49.2).\n\nsoftware-properties-common set to manually installed.\n\ndirmngr is already the newest version (2.4.4-2ubuntu17.2).\n\ndirmngr set to manually installed.\n\n0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n\nInstall R\n\nsudo apt update\n\nsudo apt install -y r-base\n\n(long display but no errors)\n\nGet R version:\n\n$ R --version\n\nR version 4.3.3 (2024-02-29) -- \"Angel Food Cake\"\n\nCopyright (C) 2024 The R Foundation for Statistical Computing\n\nPlatform: x86\\_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\n\nYou are welcome to redistribute it under the terms of the\n\nGNU General Public License versions 2 or 3.\n\nFor more information about these matters see\n\nhttps://www.gnu.org/licenses/.\n\nInstall System Libraries\n\nsudo apt install -y libcurl4-openssl-dev libssl-dev libxml2-dev libxt-dev libjpeg-dev\n\n(no errors)\n\nTry to install \"erer\" R library:\n\n$ sudo R\n\n\\&gt; install.packages(\"erer\", dependencies=TRUE)\n\nErrors or warnings (examples):\n\n./inst/include/Eigen/src/Core/arch/SSE/Complex.h:298:1: note: in expansion of macro 'EIGEN\\_MAKE\\_CONJ\\_HELPER\\_CPLX\\_REAL'\n\n298 | EIGEN\\_MAKE\\_CONJ\\_HELPER\\_CPLX\\_REAL(Packet1cd,Packet2d)\n\n| \\^\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\n\nIn file included from ../inst/include/Eigen/Core:165:\n\n../inst/include/Eigen/src/Core/util/XprHelper.h: In instantiation of 'struct Eigen::internal::find\\_best\\_packet&lt;float, 4&gt;':\n\n../inst/include/Eigen/src/Core/Matrix.h:22:57:   required from 'struct Eigen::internal::traits&lt;Eigen::Matrix&lt;float, 4, 1&gt; &gt;'\n\n../inst/include/Eigen/src/Geometry/Quaternion.h:266:49:   required from 'struct Eigen::internal::traits&lt;Eigen::Quaternion&lt;float&gt; &gt;'\n\n../inst/include/Eigen/src/Geometry/arch/Geometry\\_SIMD.h:24:46:   required from here\n\n../inst/include/Eigen/src/Core/util/XprHelper.h:190:44: warning: ignoring attributes on template argument 'Eigen::internal::packet\\_traits&lt;float&gt;::typ' {aka '\\_\\_m128'} \\[-Wignored-attributes\\]\n\n190 |          bool Stop = Size==Dynamic || (Size%unpacket\\_traits&lt;PacketType&gt;::size)==0 || is\\_same&lt;PacketType,typename unpacket\\_traits&lt;PacketType&gt;::half&gt;::value&gt;\n\n|                                       \\~\\~\\~\\~\\~\\^\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\n\n../inst/include/Eigen/src/Core/util/XprHelper.h:190:83: warning: ignoring attributes on template argument 'Eigen::internal::packet\\_traits&lt;float&gt;::typ' {aka '\\_\\_m128'} \\[-Wignored-attributes\\]\n\n190 | Dynamic || (Size%unpacket\\_traits&lt;PacketType&gt;::size)==0 || is\\_same&lt;PacketType,typename unpacket\\_traits&lt;PacketType&gt;::half&gt;::value&gt;\n\n| \\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\^\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\n\n../inst/include/Eigen/src/Core/util/XprHelper.h:190:83: warning: ignoring attributes on template argument 'Eigen::internal::packet\\_traits&lt;float&gt;::typ' {aka '\\_\\_m128'} \\[-Wignored-attributes\\]\n\n../inst/include/Eigen/src/Core/util/XprHelper.h:190:83: warning: ignoring attributes on template argument 'Eigen::internal::unpacket\\_traits&lt;\\_\\_vector(4) float&gt;::half' {aka '\\_\\_m128'} \\[-Wignored-attributes\\]\n\n../inst/include/Eigen/src/Core/util/XprHelper.h:208:88: warning: ignoring attributes on template argument 'Eigen::internal::packet\\_traits&lt;float&gt;::typ' {aka '\\_\\_m128'} \\[-Wignored-attributes\\]\n\n208 | st\\_packet\\_helper&lt;Size,typename packet\\_traits&lt;T&gt;::type&gt;::type type;\n\n|                                                              \\^\\~\\~\\~\n\nR library \"erer\" installation continued...\n\nAt end, had these messages:\n\nWarning messages:\n\n1: In install.packages(\"erer\", dependencies = TRUE) :\n\ninstallation of package 'nloptr' had non-zero exit status\n\n2: In install.packages(\"erer\", dependencies = TRUE) :\n\ninstallation of package 'lme4' had non-zero exit status\n\n3: In install.packages(\"erer\", dependencies = TRUE) :\n\ninstallation of package 'pbkrtest' had non-zero exit status\n\n4: In install.packages(\"erer\", dependencies = TRUE) :\n\ninstallation of package 'car' had non-zero exit status\n\n5: In install.packages(\"erer\", dependencies = TRUE) :\n\ninstallation of package 'systemfit' had non-zero exit status\n\n6: In install.packages(\"erer\", dependencies = TRUE) :\n\ninstallation of package 'erer' had non-zero exit status\n\nTest to see if library erer is running/installed:\n\nlibrary(erer)\n\nResult:\n\n\\&gt; library(erer)\n\nError in library(erer) : there is no package called 'erer'\n\nTry to install one of the above (nloptr) separately.\n\nlots of warnings like:\n\nsrc/operation.hpp:141:7: warning: 'T Sass::Operation\\_CRTP&lt;T, D&gt;::operator((Sass::MediaRule\\*) \\[with T = Sass::Expression\\*; D = Sass::Eval\\]' was hidden \\[-Woverloaded-virtual=\\]\n\n141 |     T operator()(MediaRule\\* x) { return static\\_cast&lt;D\\*&gt;(this)-&gt;fallback(x); }\n\n|       \\^\\~\\~\\~\\~\\~\\~\\~\n\nsrc/eval.hpp:96:17: note:   by 'Sass::Eval::operator()'\n\n96 |     Expression\\* operator()(Parent\\_Reference\\*);\n\n|                 \\^\\~\\~\\~\\~\\~\\~\\~\n\nsrc/operation.hpp:140:7: warning: 'T Sass::Operation\\_CRTP&lt;T, D&gt;::operator((Sass::SupportsRule\\*) \\[with T = Sass::Expression\\*; D = Sass::Eval\\]' was hidden \\[-Woverloaded-virtual=\\]\n\n140 |     T operator()(SupportsRule\\* x)         { return static\\_cast&lt;D\\*&gt;(this)-&gt;fallback(x); }\n\n|       \\^\\~\\~\\~\\~\\~\\~\\~\n\nsrc/eval.hpp:96:17: note:   by 'Sass::Eval::operator()'\n\n96 |     Expression\\* operator()(Parent\\_Reference\\*);\n\n|                 \\^\\~\\~\\~\\~\\~\\~\\~\n\nsrc/operation.hpp:139:7: warning: 'T Sass::Operation\\_CRTP&lt;T, D&gt;::operator((Sass::Trace\\*) \\[with T = Sass::Expression\\*; D = Sass::Eval\\]' was hidden \\[-Woverloaded-virtual=\\]\n\n139 |     T operator()(Trace\\* x)                  { return static\\_cast&lt;D\\*&gt;(this)-&gt;fallback(x); }\n\n|       \\^\\~\\~\\~\\~\\~\\~\\~\n\nsrc/eval.hpp:96:17: note:   by 'Sass::Eval::operator()'\n\n96 |     Expression\\* operator()(Parent\\_Reference\\*);\n\n|                 \\^\\~\\~\\~\\~\\~\\~\\~\n\nsrc/operation.hpp:138:7: warning: 'T Sass::Operation\\_CRTP&lt;T, D&gt;::operator((Sass::Bubble\\*) \\[with T = Sass::Expression\\*; D = Sass::Eval\\]' was hidden \\[-Woverloaded-virtual=\\]\n\n138 |     T operator()(Bubble\\* x)                 { return static\\_cast&lt;D\\*&gt;(this)-&gt;fallback(x); }\n\n|       \\^\\~\\~\\~\\~\\~\\~\\~\n\nsrc/eval.hpp:96:17: note:   by 'Sass::Eval::operator()'\n\n96 |     Expression\\* operator()(Parent\\_Reference\\*);\n\n|                 \\^\\~\\~\\~\\~\\~\\~\\~\n\nsrc/operation.hpp:137:7: warning: 'T Sass::Operation\\_CRTP&lt;T, D&gt;::operator((Sass::StyleRule\\*) \\[with T = Sass::Expression\\*; D = Sass::Eval\\]' was hidden \\[-Woverloaded-virtual=\\]\n\n137 |     T operator()(StyleRule\\* x)                { return static\\_cast&lt;D\\*&gt;(this)-&gt;fallback(x); }\n\n|       \\^\\~\\~\\~\\~\\~\\~\\~\n\nsrc/eval.hpp:96:17: note:   by 'Sass::Eval::operator()'\n\n96 |     Expression\\* operator()(Parent\\_Reference\\*);\n\n|                 \\^\\~\\~\\~\\~\\~\\~\\~\n\nsrc/operation.hpp:134:7: warning: 'T Sass::Operation\\_CRTP&lt;T, D&gt;::operator((Sass::AST\\_Node\\*) \\[with T = Sass::Expression\\*; D = Sass::Eval\\]' was hidden \\[-Woverloaded-virtual=\\]\n\n134 |     T operator()(AST\\_Node\\* x)               { return static\\_cast&lt;D\\*&gt;(this)-&gt;fallback(x); }\n\n... installation continues..\n\nEnd result:\n\nThe downloaded source packages are in\n\n    '/tmp/Rtmppn2Nu6/downloaded\\_packages'\n\nWarning message:\n\nIn install.packages(\"nloptr\", dependencies = TRUE) :\n\ninstallation of package 'nloptr' had non-zero exit status\n\nTest install:\n\n\\&gt; library(nloptr)\n\nError in library(nloptr) : there is no package called 'nloptr'",
    "author": "German-411",
    "timestamp": "2025-05-18T07:12:46",
    "url": "https://reddit.com/r/rstats/comments/1kpkwvc/im_having_trouble_installing_basic_libraries_in_r/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kpgsvl",
    "title": "Pie charts in package scatterpie appear as lines on ggplot",
    "content": "Please find a fully reproducible example of my code using fake data :\n\n    library(dplyr)\n    library(ggplot2)\n    library(scatterpie)  \n    library(colorspace) \n    \n    set.seed(123)  # SEED\n    years &lt;- c(1998, 2004, 2010, 2014, 2017, 2020)\n    origins &lt;- c(\"Native\", \"Europe\", \"North Africa\", \"Sub-Saharan Africa\", \"Other\")\n    \n    composition_by_origin &lt;- expand.grid(\n      year = years,\n      origin_group = origins\n    )\n    \n    composition_by_origin &lt;- composition_by_origin %&gt;%\n      mutate(\n        # Patrimoine moyen total par groupe et ann√©e\n        mean_wealth = case_when(\n          origin_group == \"Native\" ~ 200000 + (year - 1998) * 8000 + rnorm(n(), 0, 10000),\n          origin_group == \"Europe\" ~ 150000 + (year - 1998) * 7000 + rnorm(n(), 0, 9000),\n          origin_group == \"North Africa\" ~ 80000 + (year - 1998) * 4000 + rnorm(n(), 0, 5000),\n          origin_group == \"Sub-Saharan Africa\" ~ 60000 + (year - 1998) * 3000 + rnorm(n(), 0, 4000),\n          origin_group == \"Other\" ~ 100000 + (year - 1998) * 5000 + rnorm(n(), 0, 7000)\n        ),\n        \n        mean_real_estate = case_when(\n          origin_group == \"Native\" ~ mean_wealth * (0.55 + rnorm(n(), 0, 0.05)),\n          origin_group == \"Europe\" ~ mean_wealth * (0.50 + rnorm(n(), 0, 0.05)),\n          origin_group == \"North Africa\" ~ mean_wealth * (0.65 + rnorm(n(), 0, 0.05)),\n          origin_group == \"Sub-Saharan Africa\" ~ mean_wealth * (0.70 + rnorm(n(), 0, 0.05)),\n          origin_group == \"Other\" ~ mean_wealth * (0.60 + rnorm(n(), 0, 0.05))\n        ),\n        \n        mean_financial = case_when(\n          origin_group == \"Native\" ~ mean_wealth * (0.25 + rnorm(n(), 0, 0.03)),\n          origin_group == \"Europe\" ~ mean_wealth * (0.30 + rnorm(n(), 0, 0.03)),\n          origin_group == \"North Africa\" ~ mean_wealth * (0.15 + rnorm(n(), 0, 0.03)),\n          origin_group == \"Sub-Saharan Africa\" ~ mean_wealth * (0.10 + rnorm(n(), 0, 0.03)),\n          origin_group == \"Other\" ~ mean_wealth * (0.20 + rnorm(n(), 0, 0.03))\n        ),\n        \n        mean_professional = case_when(\n          origin_group == \"Native\" ~ mean_wealth * (0.15 + rnorm(n(), 0, 0.02)),\n          origin_group == \"Europe\" ~ mean_wealth * (0.15 + rnorm(n(), 0, 0.02)),\n          origin_group == \"North Africa\" ~ mean_wealth * (0.10 + rnorm(n(), 0, 0.02)),\n          origin_group == \"Sub-Saharan Africa\" ~ mean_wealth * (0.10 + rnorm(n(), 0, 0.02)),\n          origin_group == \"Other\" ~ mean_wealth * (0.12 + rnorm(n(), 0, 0.02))\n        )\n      )\n    \n    composition_by_origin &lt;- composition_by_origin %&gt;%\n      mutate(\n        mean_other = mean_wealth - (mean_real_estate + mean_financial + mean_professional),\n        # Corriger les valeurs n√©gatives potentielles\n        mean_other = ifelse(mean_other &lt; 0, 0, mean_other)\n      )\n    \n    prepare_scatterpie_data &lt;- function(composition_data) {\n      # S√©lectionner et renommer les colonnes pertinentes\n      plot_data &lt;- composition_data %&gt;%\n        select(\n          year, \n          origin_group, \n          mean_wealth,\n          mean_real_estate,\n          mean_financial,\n          mean_professional,\n          mean_other\n        ) %&gt;%\n        # Filtrer pour exclure les valeurs NA ou 0 pour mean_wealth\n        filter(!is.na(mean_wealth) &amp; mean_wealth &gt; 0)\n      \n      return(plot_data)\n    }\n    \n    create_color_palette &lt;- function() {\n      base_colors &lt;- c(\n        \"Native\" = \"#1f77b4\",\n        \"Europe\" = \"#4E79A7\",\n        \"North Africa\" = \"#F28E2B\", \n        \"Sub-Saharan Africa\" = \"#E15759\",\n        \"Other\" = \"#76B7B2\"\n      )\n      \n      all_colors &lt;- list()\n      \n      for (group in names(base_colors)) {\n        base_color &lt;- base_colors[group]\n        \n        all_colors[[paste0(group, \"_real_estate\")]] &lt;- colorspace::darken(base_color, 0.3)  # Version fonc√©e\n        all_colors[[paste0(group, \"_professional\")]] &lt;- base_color  # Version standard\n        all_colors[[paste0(group, \"_financial\")]] &lt;- colorspace::lighten(base_color, 0.3)  # Version claire\n        all_colors[[paste0(group, \"_other\")]] &lt;- colorspace::lighten(base_color, 0.6)  # Version tr√®s claire\n      }\n      \n      return(all_colors)\n    }\n    \n    plot_wealth_composition_scatterpie &lt;- function(composition_data) {\n      # Pr√©parer les donn√©es\n      plot_data &lt;- prepare_scatterpie_data(composition_data)\n      \n      all_colors &lt;- create_color_palette()\n      \n      max_wealth &lt;- max(plot_data$mean_wealth, na.rm = TRUE)\n      plot_data$pie_size &lt;- sqrt(plot_data$mean_wealth / max_wealth) * 10\n      \n      plot_data &lt;- plot_data %&gt;%\n        rowwise() %&gt;%\n        mutate(\n          r_real_estate = mean_real_estate / mean_wealth,\n          r_financial = mean_financial / mean_wealth,\n          r_professional = mean_professional / mean_wealth,\n          r_other = mean_other / mean_wealth\n        ) %&gt;%\n        ungroup()\n      \n      plot_data &lt;- plot_data %&gt;%\n        rowwise() %&gt;%\n        mutate(\n          total_ratio = sum(r_real_estate, r_financial, r_professional, r_other),\n          r_real_estate = r_real_estate / total_ratio,\n          r_financial = r_financial / total_ratio,\n          r_professional = r_professional / total_ratio,\n          r_other = r_other / total_ratio\n        ) %&gt;%\n        ungroup()\n      \n      group_colors &lt;- list()\n      for (group in unique(plot_data$origin_group)) {\n        group_colors[[group]] &lt;- c(\n          all_colors[[paste0(group, \"_real_estate\")]],\n          all_colors[[paste0(group, \"_financial\")]],\n          all_colors[[paste0(group, \"_professional\")]],\n          all_colors[[paste0(group, \"_other\")]]\n        )\n      }\n      \n      ggplot() +\n        geom_line(\n          data = plot_data,\n          aes(x = year, y = mean_wealth, group = origin_group, color = origin_group),\n          size = 1.2\n        ) +\n        geom_scatterpie(\n          data = plot_data,\n          aes(x = year, y = mean_wealth, group = origin_group, r = pie_size),\n          cols = c(\"r_real_estate\", \"r_financial\", \"r_professional\", \"r_other\"),\n          alpha = 0.8\n        ) +\n        scale_color_manual(values = c(\n          \"Native\" = \"#1f77b4\",\n          \"Europe\" = \"#4E79A7\",\n          \"North Africa\" = \"#F28E2B\", \n          \"Sub-Saharan Africa\" = \"#E15759\",\n          \"Other\" = \"#76B7B2\"\n        )) +\n        scale_y_continuous(\n          labels = scales::label_number(scale_cut = scales::cut_short_scale()),\n          limits = c(0, max(plot_data$mean_wealth) * 1.2),\n          expand = expansion(mult = c(0, 0.2))\n        ) +\n        scale_x_continuous(breaks = unique(plot_data$year)) +\n        labs(\n          x = \"Year\",\n          y = \"Average Gross Wealth\",\n          color = \"Origin\"\n        ) +\n        theme_minimal() +\n        theme(\n          legend.position = \"bottom\",\n          panel.grid.minor = element_blank(),\n          axis.title = element_text(face = \"bold\"),\n          plot.title = element_text(size = 14, face = \"bold\"),\n          plot.subtitle = element_text(size = 11)\n        ) +\n        guides(\n          color = guide_legend(\n            title = \"Origine\",\n            override.aes = list(size = 3)\n          )\n        )\n    }\n    \n    scatterpie_wealth_plot &lt;- plot_wealth_composition_scatterpie(composition_by_origin)\n    print(scatterpie_wealth_plot)\n    \n\nIf you run this R code from scratch, you'll notice that there will be lines instead of pie charts. My goal is to have at each point the average wealth composition (between financial, professional and real estate wealth) for each immigrant group. However for a reason I don't know the pie charts appear as lines. I know it either has to do with the radius or with the scale of my Y axis but every time I try to make changes the pie charts either become gigantic or stretched horizontally or vertically.\n\nMy point is just to have small pie charts at each point. Is this possible to do?",
    "author": "prwav",
    "timestamp": "2025-05-18T03:24:10",
    "url": "https://reddit.com/r/rstats/comments/1kpgsvl/pie_charts_in_package_scatterpie_appear_as_lines/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ko57c8",
    "title": "A unifying toolbox for handling persistence data - by Aymeric Stamm, Jason Cory Brunson",
    "content": "Topological data analysis (TDA) is a rapidly growing field that uses techniques from algebraic topology to analyze the shape and structure of data. \n\nThe {phutil} package provides a unified toolbox for handling persistence data. It offers consistent data structures and methods that work seamlessly with outputs from various TDA packages. \n\nFind out more! \n\n[https://r-consortium.org/posts/unifying-toolbox-for-handling-persistence-data/](https://r-consortium.org/posts/unifying-toolbox-for-handling-persistence-data/)",
    "author": "jcasman",
    "timestamp": "2025-05-16T09:29:16",
    "url": "https://reddit.com/r/rstats/comments/1ko57c8/a_unifying_toolbox_for_handling_persistence_data/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1knvmxk",
    "title": "Self-teaching statistics - possible or not ? If yes, how to do it ?",
    "content": "Hello everyone,\n\nThe title is a bit self-explanatory but let me add some details and context.\n\nI learned the basic of epidemiology on R during my master degree (two really intensive weeks to be precise) and when I landed my current job, I decided to learn statistics mostly because I like statistics and no one at my current lab is trained. They use basic tests like Students and Mann-Whitney but they clearly don't know the first thing about the why and when (they got kind of mad when I told them that they've apparently been using the wrong test for several years)  \n  \nI found and completed a Coursera Specialization course by the Duke University called \"Data Analysis in R\" which definitely upped my game and allowed me to get a better understanding of the subject as well as helping me find and understand new informations...\n\nBut it's painfully obvious that I still only skimmed the surface and it bothers me a lot. When I ask questions here, people are often nice enough to explain but there's so much nuance and complexity that completely elude me\n\nIf it was possible, I would have tried to do a master degree in statistics or applied math or something to do parallel to my job but it's currently not in the realm of possibility (already doing a thesis and have toddler...)\n\nWhat would you guys suggest I could do to get better at statistics ? Is there book, online courses or thing like that I could do on my free time that would actually go deep into explaining things while remaining understandable for a novice ?\n\nThank you very much",
    "author": "Intelligent-Gold-563",
    "timestamp": "2025-05-16T01:17:17",
    "url": "https://reddit.com/r/rstats/comments/1knvmxk/selfteaching_statistics_possible_or_not_if_yes/",
    "score": 13,
    "num_comments": 16,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kn8kqc",
    "title": "What are some biggest advancement in R in the last few years?",
    "content": "I started using R 15+ years ago and reached a level where I would consider myself an expert but haven't done much coding in R besides some personal toy projects in the last 5 years due to moving more into a leadership role. \n\nI still very much love R and want to get back into it. I saw the introduction and development of Rstudio, Shiny, RMarkdown and Tidyverse. What has been some new development in the past 5 years that I should be aware of as I get back into utilizing R to its full potential?\n\nEDIT: I am so glad I made this post. So many exciting new things. Learning new things and tinkering always brings me a lot of joy and seems like there are really cool things to explore in R now. Thanks everyone. This is awesome. ",
    "author": "statguy",
    "timestamp": "2025-05-15T06:44:26",
    "url": "https://reddit.com/r/rstats/comments/1kn8kqc/what_are_some_biggest_advancement_in_r_in_the/",
    "score": 243,
    "num_comments": 119,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kngxc7",
    "title": "Learning R - complete newbie",
    "content": "Hi, I'm an undergrad student (biological engineering major) and I've just started/planned to learn R in my summer break. I need help as to like what roadmap I can follow and any learning sources and things like that (Textbooks/Online Courses/Any resource ever). \n\nAnd, How do I practice after learning the concepts?\n\nI have also seen some yt playlists by MarinStatsLectures for R. Is MarinStatsLectures YouTube channel good for learning especiallt since I'm a complete beginner?\n\nThanks in advance!!",
    "author": "No-Tomatillo-1456",
    "timestamp": "2025-05-15T12:24:07",
    "url": "https://reddit.com/r/rstats/comments/1kngxc7/learning_r_complete_newbie/",
    "score": 16,
    "num_comments": 13,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kno8xu",
    "title": "different p-value in ggbetweenstats and lm results",
    "content": "why is the p value in my ggbetweenstats differnt from the p value  i computed from the lm model? i wanted to perform one way anova, so i made sure the type of the ggbetweenstats output is parametric, and from the lm, i performed an anova on it. tho they have the same variables, it still ddint yield the same results. i tried the non-parametric, both are similar. anyone knows why?\n\n",
    "author": "marinebiot",
    "timestamp": "2025-05-15T17:53:36",
    "url": "https://reddit.com/r/rstats/comments/1kno8xu/different_pvalue_in_ggbetweenstats_and_lm_results/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kmgau2",
    "title": "Just nostalgically posting that it‚Äôd be nice to run an OLS model again one day‚Ä¶",
    "content": "Been doing data work for about 12 years now.\n\nProbably haven‚Äôt run a single numeric algorithm in like 2 years. Just NLP, regex, engineering UIs, and AI prompting.\n\nI‚Äôd love to make a quantitative graph again one day.",
    "author": "genobobeno_va",
    "timestamp": "2025-05-14T07:16:50",
    "url": "https://reddit.com/r/rstats/comments/1kmgau2/just_nostalgically_posting_that_itd_be_nice_to/",
    "score": 61,
    "num_comments": 13,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kn6qn5",
    "title": "consolidating ggplot guide_legend specification",
    "content": "I have a plot with color, shape, alpha, and size determined by a factor.  Right now, in guides(), I have a guide\\_legend(position='inside') for each of the features (color, size, etc).  Is there a way to say I want the same guide\\_legend() for a list of features?",
    "author": "fasta_guy88",
    "timestamp": "2025-05-15T05:18:12",
    "url": "https://reddit.com/r/rstats/comments/1kn6qn5/consolidating_ggplot_guide_legend_specification/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kmhh5n",
    "title": "rv, a project based package manager",
    "content": "Hello there,\n\nWe have been building a package manager for R inspired by Cargo in Rust.\nThe main idea behind rv is to be explicit about the R version in use as well as declaring which dependencies are used in a `rproject.toml` file. There's no renv::snapshot equivalent, everything needs to be declared up front, the config file (and resulting lockfile) is the source of truth.\n\nIf you have used Cargo/npm/any Python package manager/etc, it will be very familiar. We've been replacing most (all?) of our renv usage internally with rv so it's pretty usable already.\n\nThe repo  is https://github.com/A2-ai/rv if you want to check it out!",
    "author": "Elession",
    "timestamp": "2025-05-14T08:04:18",
    "url": "https://reddit.com/r/rstats/comments/1kmhh5n/rv_a_project_based_package_manager/",
    "score": 45,
    "num_comments": 11,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1klperl",
    "title": "Dataset suggestion for Bayesian Weibull Survival regression",
    "content": "I'm working on a university project implementing Bayesian Weibull Survival Regression and I'm looking for an interesting, non-medical dataset to demonstrate the model's applications.\n\nWhile survival analysis is commonly applied to medical data, I'd like to explore more creative or unconventional applications to showcase the versatility of this statistical approach.\n\nAny suggestions for publicly available datasets would be greatly appreciated!",
    "author": "Pool_Imaginary",
    "timestamp": "2025-05-13T08:50:33",
    "url": "https://reddit.com/r/rstats/comments/1klperl/dataset_suggestion_for_bayesian_weibull_survival/",
    "score": 18,
    "num_comments": 5,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1klufno",
    "title": "Can not run R markdown",
    "content": "Hi!\n\nI'm facing this frusting error when i knit an r markdown document\n\nError: could not find function \"install.packages\"  \nExecution halted\n\n  \nI have tried to reinstall R and Rstudio like 4 times still didn't work.\n\nAny help will be appreciated",
    "author": "66alpha",
    "timestamp": "2025-05-13T12:06:09",
    "url": "https://reddit.com/r/rstats/comments/1klufno/can_not_run_r_markdown/",
    "score": 3,
    "num_comments": 36,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1klndil",
    "title": "Method for analysis (MMM, bayesian MMM)",
    "content": "Hello, i need some help to understand what method to use for my analysis. I have digital ads data (campaign level) from meta, tiktok and google ads. The marketing team wants to see similar results to foshpa (campaign optimization). main metric needed is roas and comparison between modeled one to real one for each campaign. I have each campaigns revenue, which summed up probably is inflated as different platforms might attribute the same orders ( I believe that might be a problem). My data is aggregated weekly i have such metrics as revenue, clicks, impressions and spend. What method would you suggest, similar to MMM but have in mind that i have over 100 campaigns. ",
    "author": "No-Banana-370",
    "timestamp": "2025-05-13T07:29:17",
    "url": "https://reddit.com/r/rstats/comments/1klndil/method_for_analysis_mmm_bayesian_mmm/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1klmac5",
    "title": "preserve legend position with multiple legends",
    "content": "I have a plot that has two legends, one for shape and one for color.  When my color factor has 3 values in the data, the color legend is above the shape legend.  But when both factors have 2 values in the data, the shape is on top and the color below.\n\nHow can I keep the color on top?",
    "author": "fasta_guy88",
    "timestamp": "2025-05-13T06:43:44",
    "url": "https://reddit.com/r/rstats/comments/1klmac5/preserve_legend_position_with_multiple_legends/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kljx2j",
    "title": "Statistical Analysis Guidance in RStudio for Amphibian Ecology Study",
    "content": "Hello,\n\nI am currently undertaking an internship as part of my Master's program in Ecology and am encountering challenges in selecting appropriate statistical analyses to perform in RStudio. My research focuses on the relationships between various ecological factors and the presence of amphibians in forest ponds.\n\nI would appreciate guidance on the appropriate analytical approaches for the following cases, specifying the types of variables involved:\n\n1. Relationship between the presence of indicator amphibians and the ecological status of ponds\n   * *Indicator amphibian presence/absence*: binary\n   * *Ecological status of ponds*: categorical (e.g., degraded, moderate, good)\n2. Relationship between amphibian species richness and the functional connectivity of aquatic habitats\n   * *Amphibian species richness*: continuous\n   * *Functional connectivity*: continuous\n3. Relationship between the presence of indicator amphibians and the functional connectivity of aquatic habitats\n   * *Indicator amphibian presence/absence*: binary\n   * *Functional connectivity*: continuous\n4. Relationship between the combined effect of functional connectivity and pond ecological status on the presence of indicator amphibians\n   * *Ecological status of ponds*: categorical\n   * *Functional connectivity*: continuous\n   * *Indicator amphibian presence/absence*: binary\n\nFor each scenario, I seek advice on:\n\n* Selecting suitable statistical tests or models\n* Verifying model assumptions (e.g., normality, homoscedasticity, independence)\n* Addressing violations of these assumptions (e.g., data transformations, alternative models)\n* Analyzing final models and interpreting residuals\n\nThank you in advance for your assistance.",
    "author": "Nearby_Guest4405",
    "timestamp": "2025-05-13T04:51:05",
    "url": "https://reddit.com/r/rstats/comments/1kljx2j/statistical_analysis_guidance_in_rstudio_for/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kl3gwi",
    "title": "Was there ever a \"Kable\" stand-alone package? (Not Knitr or KableExtra)",
    "content": "I was opening a copy of one of my team's old RMDs in an isolated renv environment for a new task.\n\nI looked at the packages I was loading. I saw that I loaded a package called `kable`, which was separate from knitr and KableExtra.\n\nI can not find any evidence of a package by this name ever existing on CRAN or via a web search. These searches return only references to the function `knitr::kable()` and the `KableExtra` package.\n\nThe fact that we were loading it suggests that we did so for a reason, but I can not for the life of me find it on my computer or anywhere else. I even asked my boss (the only other person who uses R on my team) if she knew anything about it, and she did not. We both vaguely remember it existing, but neither of us can tell you where.\n\nWas there ever a package that went by that name?\n\nWas this a strange team-size hallucination?\n\n  \n\\*Edit: Fixed a typo",
    "author": "bookwrm119",
    "timestamp": "2025-05-12T13:44:18",
    "url": "https://reddit.com/r/rstats/comments/1kl3gwi/was_there_ever_a_kable_standalone_package_not/",
    "score": 17,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kl05q5",
    "title": "Disease Outbreak Mapping, Open Source, and Outreach - Unijos R Users Group in Nigeria Leads the Way",
    "content": "Iko Musa, founder of the Unijos R Users Group at the University of Jos (UNIJOS), Nigeria, spoke with the R Consortium about how the group built an inclusive and cross-disciplinary R community in northern Nigeria.\n\nIko explained how the group supported students and professionals in transitioning from proprietary tools like SPSS to R. \n\nHe highlighted their efforts to improve accessibility through online sessions, providing internet support for undergraduates, and hosting practical events like a recent Meetup on outbreak mapping in R.\n\n[https://r-consortium.org/posts/disease-outbreak-mapping-open-source-and-outreach-unijos-r-users-group-in-nigeria-leads-the-way/](https://r-consortium.org/posts/disease-outbreak-mapping-open-source-and-outreach-unijos-r-users-group-in-nigeria-leads-the-way/)",
    "author": "jcasman",
    "timestamp": "2025-05-12T11:34:25",
    "url": "https://reddit.com/r/rstats/comments/1kl05q5/disease_outbreak_mapping_open_source_and_outreach/",
    "score": 12,
    "num_comments": 0,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kkn5i7",
    "title": "Question about normality testing and non-parametric tests",
    "content": "Hello everyone !\n\n  \nSo that's something that I feel comes up a lot in statistics forum, subreddit and stackexchange discussion, but given that I don't have a formal training in statistics (I learned stats through an R specialisation for biostatistics and lot of self-teaching) I don't really understand this whole debate.\n\n\n\nIt seems like some kind of consensus is forming/has been formed that testing for normality with a Pearson/Spearman/Bartlett/Levene before choosing the appropriate test is a bad thing (for reason I still have a hard time understanding too).\n\n\n\nWould that mean that unless your data follow the Central Limit Theorem, in which case you would just go with a Student's or an ANOVA directly, it's better to automatically chose a non-parametric test such as a Mann-Whitney or a Kruskal-Wallis ?\n\nThanks for the answer (and please, explain like I'm five !)",
    "author": "Intelligent-Gold-563",
    "timestamp": "2025-05-12T01:09:36",
    "url": "https://reddit.com/r/rstats/comments/1kkn5i7/question_about_normality_testing_and/",
    "score": 10,
    "num_comments": 10,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kkksc8",
    "title": "pkgdown.offline: Build pkgdown websites without an internet connection",
    "content": "",
    "author": "nanxstats",
    "timestamp": "2025-05-11T22:26:31",
    "url": "https://reddit.com/r/rstats/comments/1kkksc8/pkgdownoffline_build_pkgdown_websites_without_an/",
    "score": 11,
    "num_comments": 2,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kjvjis",
    "title": "rixpress: an R package to set up multi-language reproducible analytics pipelines (2 Minute intro video)",
    "content": "",
    "author": "brodrigues_co",
    "timestamp": "2025-05-11T00:40:27",
    "url": "https://reddit.com/r/rstats/comments/1kjvjis/rixpress_an_r_package_to_set_up_multilanguage/",
    "score": 24,
    "num_comments": 9,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kj9fk3",
    "title": "BS in Mathematics or BS in Applied Mathematics?",
    "content": "Hi everyone, thank you for reading. I'm wondering whether I should enter into a BS in Mathematics or Applied Mathematics? I am interested in statistics and data science but I do not want to pigeonhole myself. Is going for Applied Mathematics somehow lesser than going for a BS in Maths? Is Applied Mathematics less rigorous? Considering I am interested in a field that is inherently applied, am I going to get lost in the formalism and proofs of a BS in Maths and loose sight of the specific know-how I want to have towards the end of my schooling? Or am I underestimating the ability a rigorous mathematical education gives one? I am afraid of getting lost in a field so abstract that I will be a very clever, book-smart person with zero employability towards the end, heh heh.",
    "author": "HenryHyacinth",
    "timestamp": "2025-05-10T05:44:45",
    "url": "https://reddit.com/r/rstats/comments/1kj9fk3/bs_in_mathematics_or_bs_in_applied_mathematics/",
    "score": 7,
    "num_comments": 8,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kiycke",
    "title": "i strongly enjoy rbind.fill",
    "content": "i love using rbind.fill\n\ndo.call(rbind.fill, list(x, y))\n\nits really comfy",
    "author": "amonglilies",
    "timestamp": "2025-05-09T18:07:38",
    "url": "https://reddit.com/r/rstats/comments/1kiycke/i_strongly_enjoy_rbindfill/",
    "score": 17,
    "num_comments": 9,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kifnld",
    "title": "TypR: a statically typed version of the R programming language",
    "content": "Written in Rust, this language aim to bring safety, modernity and ease of use for R, leading to better packages both maintainable and scalable !\n\nThis project is still new and need some work to be ready to use\n\nThe link to the repositity is [here](https://github.com/fabriceHategekimana/typr)",
    "author": "Artistic_Speech_1965",
    "timestamp": "2025-05-09T03:59:09",
    "url": "https://reddit.com/r/rstats/comments/1kifnld/typr_a_statically_typed_version_of_the_r/",
    "score": 96,
    "num_comments": 35,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kiihlw",
    "title": "MMM using R",
    "content": "I want to do MMM model for paid ads campaigns. Maybe someone knows a good example using r? Robyn package works for channels but not for 100 and more campaigns.\n",
    "author": "No-Banana-370",
    "timestamp": "2025-05-09T06:28:08",
    "url": "https://reddit.com/r/rstats/comments/1kiihlw/mmm_using_r/",
    "score": 8,
    "num_comments": 8,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kht75a",
    "title": "Is there a more efficient way to process this raster?",
    "content": "I need to do some math to a single-band raster that's beyond what ArcGIS seems capable of handling. So I brought it into R with the \"raster\" package.\n\nThe way I've set up what I need to process is this:\n\n    df &lt;- as.data.frame(raster_name)\n    for (i in 1:nrow(df){\n      rasterVal &lt;- df[i,1]\n      rasterProb &lt;- round(pnorm(rasterVal, mean = 0, sd = 5, lower.tail=FALSE), 2)\n      df[i,2] &lt;- rasterProb\n    }\n\nThen I'll need to turn the dataframe back into a raster. The for loop seems to take a very, very long time. Even though it seems like an \"easy\" calculation, the raster does have a few million cells. Is there an approach I could use here that would be faster?",
    "author": "yellow-bold",
    "timestamp": "2025-05-08T08:39:12",
    "url": "https://reddit.com/r/rstats/comments/1kht75a/is_there_a_more_efficient_way_to_process_this/",
    "score": 7,
    "num_comments": 13,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1khlu4s",
    "title": "Anyone here ever tried to use a Intel Optane drive for paging when they run out of RAM?",
    "content": "Back of a napkin math tells me i need around 500GB of RAM for what I plan to do in R. Im not buying that much RAM. Once you get passed 128 you often need enterprise level MoBos anyway (or at least thats how it was a couple of years ago). I randomly remembered that Intel Optane was a thing a couple of years ago. \n\nFor the uninitiated: These were special SSD drives that had random access latency pretty mach right between what RAM and a regular SSD can do. They also had very good sequencial speeds. And they could survive way more read/write cycles than a regular SSD. \n\nSo I thought id find a used one and use it as a dedicated paging drive. Im probably gonna try it out anyway, just out of curiosity, bit have any of you tried this before to deal with massive RAM requirements in R?",
    "author": "404phil_not_found",
    "timestamp": "2025-05-08T02:27:26",
    "url": "https://reddit.com/r/rstats/comments/1khlu4s/anyone_here_ever_tried_to_use_a_intel_optane/",
    "score": 10,
    "num_comments": 14,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kgwrzh",
    "title": "üõ†Ô∏è Need Help Adding Visual Diff View for Text Changes in Shiny App",
    "content": "Hi everyone,\n\nI'm currently working on a Shiny app that compares posts collected over time and highlights changes using Levenshtein distance. The code I've implemented calculates edit distances and uses diffChr() (from diffobj) to highlight additions and deletions in a side-by-side HTML format. The goal is to visualize text changes (like deletions, additions, or modifications) between versions of posts.\n\nHere‚Äôs a brief overview of what it does:\n\n* Detects matching posts based on IDs.\n* Calculates Levenshtein and normalized distances.\n* Displays the 20 most edited posts.\n* Shows deletions with strikethrough/red background and additions in green.\n\nhttps://preview.redd.it/hb8nqjkjycze1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=acc7d5542dbbb4f03dd4ce10cbfd95248999fa68\n\nThe core logic is functional, but the visualization is not quite working as expected. Issues I‚Äôm facing:\n\n* Some of the HTML formatting doesn't render consistently inside the DataTable.\n* Additions and deletions are sometimes not aligned clearly for the reader.\n* The user experience of comparing long texts is still clunky.\n\nüìå I'm looking for help to:\n\n* Improve the visual clarity of differences (ideally more like GitHub diffs or side-by-side code comparisons).\n* Enhance alignment of differences between original and modified texts.\n* Possibly replace or supplement diffChr if better options exist in the R ecosystem. If anyone has experience with better text diffing/visualization approaches in Shiny (or even JS integration), I‚Äôd really appreciate the help or suggestions.\n\nThanks in advance üôè  \nHappy to share more if needed!",
    "author": "Grand_Internet7254",
    "timestamp": "2025-05-07T06:01:14",
    "url": "https://reddit.com/r/rstats/comments/1kgwrzh/need_help_adding_visual_diff_view_for_text/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kfsmt2",
    "title": "In what way do you install and use fonts in R? What are your few steps?",
    "content": "Pardon my language but it's such a stratospheric amount of pain in the 4$$ everytime.\n\nCan you just simply tell me what do you do when you have a new font to install that you want to use in R? I think it would simpler this way.\n\nBUT if you want to know what I've tried, here it is :\n\nI install the fonts in Windows, I see that LibreOffice Writer doesn't argue and let me use it, but RStudio won't.\n\nI load the following :\n\n`library(tidyverse)`\n\n`library(ragg)`\n\n`library(extrafont)`\n\n`library(showtext)`\n\nI run all the following multiple times, before and after installing fonts, to be sure R gets it :\n\n`showtext::showtext_auto()`\n\n`showtext::loadfonts()`\n\n`extrafont:font_import() # takes forever to check every police only to add the few that I just installed and not find it later`\n\n`extrafont::fonts() #to see them`\n\nR lists them all (the fonts) and says for everyone single one that's it's already registered and all.\n\nBut when it comes to use it in a ggplot within theme() and element\\_text(), whatever fonts I try apparently don't exist, it turns out. Even some fonts that were already in the system and that I didn't install myself (like \"Impact\"!)\n\nI've also used `font_add_google(\"Some Font\")` and then do `showtext_auto()` but I have to do it at every session, it seems.\n\nI've changed my RStudio advanced graphics options to AGG because once it did work, but not today it seems.\n\nI get the following warnings 50 times everytime when running ggplot() (even though said font was supposedly \"already registered\") :\n\n    50: In grid.Call(C_stringMetric, as.graphicsAnnot(x$label)) :\n      font family 'Roboto' not found, will use 'sans' instead\n\nAnyway, what do you do when you just casually add some font and use it successfully in a plot?",
    "author": "thrashourumov",
    "timestamp": "2025-05-05T18:26:47",
    "url": "https://reddit.com/r/rstats/comments/1kfsmt2/in_what_way_do_you_install_and_use_fonts_in_r/",
    "score": 20,
    "num_comments": 9,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kfwmzt",
    "title": "Utilizing GLMs where the coefficient matrix is ln(coefficient)",
    "content": "A bit of a weird request - a model specification I'm working with utilizes a log link where the coefficient matrix looks like \\[ln(B1), ln(B2), ln(B3), etc.\\] where all predictors are categorical predictors. This in order to get the model to become the applicable coefficients multiplied by each other.\n\n Is it possible to do this specification in R without using matrix algebra? ",
    "author": "Canadian_Arcade",
    "timestamp": "2025-05-05T22:09:42",
    "url": "https://reddit.com/r/rstats/comments/1kfwmzt/utilizing_glms_where_the_coefficient_matrix_is/",
    "score": 2,
    "num_comments": 9,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kfcq91",
    "title": "Can I still use a parametic test  if my data fails normality tests?",
    "content": "Hi everyone,\nI'm working on an assignment, My dataset has 250 + participants , and I ran normality tests\n\nThe issue is: all variables failed both the Kolmogorov-Smirnov and Shapiro-Wilk tests (p &lt; .001 in all cases).\n\n\n\nSkewness: 0.92 (males), 1.36 (females)\n\nKurtosis: ~ -0.5 (male), 0.75 (female)\n\nMedian is lower than the mean\n\nData is on a 1‚Äì7 Likert scale\n\nFor most other variables, skewness is low to moderate (e.g., -0.3 to 0.6), but 2 are clearly skewed.\n\nI know that with larger n , the Central Limit Theorem suggests I can still use a t-test, pearsons r corelation, but I want to make sure I'm not violating assumptions too severely.\n\nSo my questions are:\n\nIs it statistically acceptable to run independent-samples t-",
    "author": "Sluae1",
    "timestamp": "2025-05-05T07:17:06",
    "url": "https://reddit.com/r/rstats/comments/1kfcq91/can_i_still_use_a_parametic_test_if_my_data_fails/",
    "score": 8,
    "num_comments": 19,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kfc7iw",
    "title": "Request - Help with GGPLOT2 Scatterplot",
    "content": "Hi, I want to plot a scatterplot for a dataframe with 3 columns and 1200 rows. I am using the following command to generate a scatterplot -\n\nggplot(data, aes(x, y)) + geom\\_point() + geom\\_text( label=rownames(data), nudge\\_x = 0.25, nudge\\_y = 0.25)\n\nSince there are about 1200 data points, it gets cluttered. I am interested in plotting a graph in such a way that only Top 20 and Bottom 20 points are labelled, and the other 1160 points not labelled.\n\nAny help will be appreciated. Thanks.",
    "author": "Downtown_Macaroon_30",
    "timestamp": "2025-05-05T06:54:50",
    "url": "https://reddit.com/r/rstats/comments/1kfc7iw/request_help_with_ggplot2_scatterplot/",
    "score": 4,
    "num_comments": 8,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ke62lz",
    "title": "I love R",
    "content": "A little bit of context i currently work as a Head of Analytics at a \"reputable\" company and i am so bored with my current leadership role in analytics, i am so dependent on it because it pays well but i would love to become an individual contributor again and get to work with R everyday. Do you happen to have any tips for me? And can i actually quit and make a living by being an R developer.\n",
    "author": "megzar",
    "timestamp": "2025-05-03T16:25:00",
    "url": "https://reddit.com/r/rstats/comments/1ke62lz/i_love_r/",
    "score": 223,
    "num_comments": 25,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kek41e",
    "title": "Need help installing R",
    "content": "Edit Nr. 2: at least it worked ! I installed an older version of R (4.4.2. AND changed TMP, TEMP, TMPDIR to C:/Temp, as i had a space in my username and I think, that is what led to the issue.\n\nEdit: i couldn't add a second picture, so here's the text of the error message: \"An error occured while attempting to load the selected version of R. Please select a different R installation\"\n\nHello everyone, I've got some serious problems installing R.  \nI've downloaded the most actual version of R and RStudio - and unfortunately each time I receive an error message.  \nI've installed and de-installed R and R Studio already 5 times - and each time there was that error message.\n\nAnyone any ideas, what the problem could be?\n\nThanks in advance for your help !\n\nhttps://preview.redd.it/9ic85gy6xrye1.png?width=1199&amp;format=png&amp;auto=webp&amp;s=ca500fa1604df50a3fe04a1b8c890b8ebc1a4c96\n\n  \n",
    "author": "Legal_Put3362",
    "timestamp": "2025-05-04T06:43:01",
    "url": "https://reddit.com/r/rstats/comments/1kek41e/need_help_installing_r/",
    "score": 2,
    "num_comments": 9,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kdvevj",
    "title": "Lasso Regression with metric and categorical data",
    "content": "Hey, I'm conducting a Lasso regression where my predictors consist of approximately 15 metric and 60 dichotomous variables (dummy coding of 20 categorical variables) with approximately 270 observations. I have the following questions:\n\n1. Does Group Lasso make more sense in my case, and what would be the advantages? Would it be easier to interpret and/or would it make the model more accurate?\n\n2. Does it matter for Lasso whether the dummy coding is created with a reference category or not? Or is it just a matter of whether or not you want to interpret the results in relation to the reference category?\n\n3. In general, is my ratio of metric and categorical or dichotomous variables a problem for the model?\n\nThank you so much for your help!",
    "author": "Odd-Two",
    "timestamp": "2025-05-03T08:21:12",
    "url": "https://reddit.com/r/rstats/comments/1kdvevj/lasso_regression_with_metric_and_categorical_data/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kdqkqg",
    "title": "Species distribution models with different observation sources",
    "content": "I‚Äôm creating species distribution models for a couple of species. I have two main data sources; camera traps and citizen science. I do not know how much survey effort was used for the citizen science observations. I do know how long the different camera traps were deployed for. Some traps were deployed for a couple of weeks whereas others were deployed for several years. Therefore, the survey effort is highly variable between different camera locations.\n\nI have produced some models with MaxEnt using the dismo package. The results are reasonable but I don‚Äôt think that MaxEnt‚Äôs presence/pseudo-absence structure is making full use of my dataset.\n\nCan anyone suggest a better solution? \n\nThanks for any responses.",
    "author": "Bitter_Eggplant_9970",
    "timestamp": "2025-05-03T04:15:09",
    "url": "https://reddit.com/r/rstats/comments/1kdqkqg/species_distribution_models_with_different/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kczbn4",
    "title": "Shinyscholar - a template for creating reproducible shiny apps",
    "content": "I'm the developer of this package and am giving a workshop about it next month in case anyone is interested in learning more: [https://sites.google.com/view/dariia-mykhailyshyna/main/r-workshops-for-ukraine#h.svl2ujruwf92](https://sites.google.com/view/dariia-mykhailyshyna/main/r-workshops-for-ukraine#h.svl2ujruwf92) It enables producing shiny apps to conduct complex analyses which are also fully reproducible outside of the app. Other features include being able to load/save at any point, a flexible logging system and guidance for users. ",
    "author": "simonsmart88",
    "timestamp": "2025-05-02T05:07:39",
    "url": "https://reddit.com/r/rstats/comments/1kczbn4/shinyscholar_a_template_for_creating_reproducible/",
    "score": 29,
    "num_comments": 1,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kcmoa3",
    "title": "Supercharge your R workflows with DuckDB",
    "content": "",
    "author": "Capable-Mall-2067",
    "timestamp": "2025-05-01T16:21:01",
    "url": "https://reddit.com/r/rstats/comments/1kcmoa3/supercharge_your_r_workflows_with_duckdb/",
    "score": 24,
    "num_comments": 2,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kctfj0",
    "title": "normality of residuals not on raw data",
    "content": "so i have a question. why are most examples on the internet about the use of shapiro test used on raw data itself rather than the residuals from, say, a linear regression?\n\nkinda confusing esp for those not familiar with stats. would appreciate ur response\n\nheres an example that uses shapiro on raw data and not on residuals  \n[https://rpubs.com/MajstorMaestro/240657](https://rpubs.com/MajstorMaestro/240657)",
    "author": "marinebiot",
    "timestamp": "2025-05-01T22:31:24",
    "url": "https://reddit.com/r/rstats/comments/1kctfj0/normality_of_residuals_not_on_raw_data/",
    "score": 5,
    "num_comments": 14,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kcdr64",
    "title": "Interview with R Users and R-Ladies Warsaw",
    "content": "Kamil Sijko, organizer of both the R Users and R-Ladies Warsaw groups, recently spoke with the R Consortium about the evolving R community in Poland and the group's efforts to connect users across academia, industry, and open-source development. \n\nKamil shared his journey from discovering R as a student to taking over the leadership of the Warsaw R community in 2024. \n\nHe discussed the group‚Äôs hybrid meetups, industry collaborations with companies like AstraZeneca and Appsilon, and the importance of making R accessible through recorded sessions and international outreach. \n\nHe also highlighted a recent open-source project on patient randomization, demonstrating how R can be effectively integrated into modern software ecosystems, particularly in medical applications.\n\n[https://r-consortium.org/posts/microservices-randomization-apis-and-r-in-the-medical-sector-warsaws-data-community-in-focus/](https://r-consortium.org/posts/microservices-randomization-apis-and-r-in-the-medical-sector-warsaws-data-community-in-focus/)",
    "author": "jcasman",
    "timestamp": "2025-05-01T09:59:35",
    "url": "https://reddit.com/r/rstats/comments/1kcdr64/interview_with_r_users_and_rladies_warsaw/",
    "score": 10,
    "num_comments": 0,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kcer9g",
    "title": "Definitive Screening Designs in R",
    "content": "Is there a way to fit a DSD in R and find the estimates of the coefficients of the factors?",
    "author": "Skoupojulo",
    "timestamp": "2025-05-01T10:40:38",
    "url": "https://reddit.com/r/rstats/comments/1kcer9g/definitive_screening_designs_in_r/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kbrbfk",
    "title": "Virtual R/Medicine data challenge - Analyze MMR vaccination rates over time",
    "content": "Deadline May 20, 2025\n\n$200 prize each for Students or Professionals. Submit as an individual or a team!\n\nChanging attitudes towards vaccination in the US have significantly lowered childhood measles vaccination rates, as uptake of the recommended two doses of MMR vaccine before entering school has frequently fallen below the 95% recommended for community immunity.\n\nAnalyze MMR vaccination rates over time and by geographical area, as well as measles case rates and complications.\n\nExamples, guidelines, and more available at:\n\n[https://rconsortium.github.io/RMedicine_website/Competition.html](https://rconsortium.github.io/RMedicine_website/Competition.html)",
    "author": "jcasman",
    "timestamp": "2025-04-30T14:02:36",
    "url": "https://reddit.com/r/rstats/comments/1kbrbfk/virtual_rmedicine_data_challenge_analyze_mmr/",
    "score": 16,
    "num_comments": 4,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kbru5k",
    "title": "Post-hoc Procedures for Ordinal GEE",
    "content": "The `emmeans` package supports `geeglm(`) objects from the package `geepack`. However, `emmeans` throws errors for `ordgee(`) objects. Should I use a different post-hoc package? Or, maybe I need an entirely different toolchain other than `geepack` and `emmeans`?",
    "author": "carabidus",
    "timestamp": "2025-04-30T14:24:56",
    "url": "https://reddit.com/r/rstats/comments/1kbru5k/posthoc_procedures_for_ordinal_gee/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kbp9xm",
    "title": "Display Live R Console Message in Shiny Dashboard",
    "content": "I have a R Shiny app which i am running from Posit. It is running perfectly by running app.R file and the dashboard is launching and the corresponding logs / outputs are getting displayed in R studio in Posit.\nIs there a way i can show live real time outputs/logs from R studio consol directly to R Shiny Dashboard frontend? Also adding a progress bar to check status how much percentage of the overall code has run in the UI ?\n\nI have this attached function  LogMessageWithTimestamp which logs all the messages in the Posit R Studio Console. Can i get exactly the same messages in R Shiny dashboard real time. For example if i see something in console like\nTimestamp Run Started!\n\nAt the same time same moment i should see the same message in the Shiny Dashboard\n\nTimestamp Run Started!\n\nEverything will happen in real time live logs.\n\nI was able to mirror the entire log in the Shiny dashboard once the entire application/program runs in the backend, that once the entire program finishes running in the backend smoothly. \n\nBut i want to see the updates real time in the frontend which is not happening.\n\nI tried with future and promise. I tried console.output\nI tried using withCallinghandlers and observe as below. But nothing is working.",
    "author": "Srijit1994",
    "timestamp": "2025-04-30T12:35:48",
    "url": "https://reddit.com/r/rstats/comments/1kbp9xm/display_live_r_console_message_in_shiny_dashboard/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kbiujo",
    "title": "Dickey-Fuller Testing in R",
    "content": "Could anybody help me with some code on how to do the Dickey Fuller test/test for stationary in R without using the adf.test() command. Specifically on how to do what my professor said:\n\nIf you want to know the exact model that makes the series stationary, you need to know how to do the test yourself (more detailed code. The differenced series as a function of other variables). You should also know when you run the test yourself, which parameter is used to conclude.\n\nThank you!! ",
    "author": "Ms-Frizzle53",
    "timestamp": "2025-04-30T08:09:02",
    "url": "https://reddit.com/r/rstats/comments/1kbiujo/dickeyfuller_testing_in_r/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kbdxio",
    "title": "Measuring effect size of 2x3 (or larger) contingency table with fisher.test",
    "content": "Hey, \n\nI have a dataset with categorical (dichotomous and more) and continuous data. I wanna measure association between categorical/categorical and categorical/continous variables using chisq.test and fisher.test. Since most of my expected chisq.test-values are below 5, I used fisher.test. Now I wanna calculate the effect size of chisq.test and fisher.test. For chisq.test I used Cramers V, but for fisher.test it doesn't work. Odds ratio isn't shown in a test for 2x3 contingency tables.\n\nWhat do I do?\n\n  \nThanks for your help :)",
    "author": "Historical_Local237",
    "timestamp": "2025-04-30T04:18:42",
    "url": "https://reddit.com/r/rstats/comments/1kbdxio/measuring_effect_size_of_2x3_or_larger/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kb9qdi",
    "title": "Test-retest reliability and validity of a questionnaire",
    "content": "Hey guys!!! Good morning :) \n\nI conduct a questionnaire-based study and I want to assess the reliability and its validity. As far as am concerned for the reliability I will need to calculate Cohen's kappa. Is there any strategy on how to apply that? Let's say I have two respondents taking the questionnaire at two different time-points, a week apart. My questionnaire consists of 2 sections of only categorical questions. What I have done so far is calculating a Cohen's Kappa for each section per student. Is that meaningful and scientifically approved ? Do I just report the Kappa of each section of my questionnaire as calculated per student, or is there any way to draw an aggregate value ? \n\nRegarding the validation process ? What is an easy way to perform ? \n\nThank you in advance for your time, may you all have a blessed day!!!!  ",
    "author": "Intrepid-Star7944",
    "timestamp": "2025-04-29T23:21:08",
    "url": "https://reddit.com/r/rstats/comments/1kb9qdi/testretest_reliability_and_validity_of_a/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kb06aq",
    "title": "Issue with Confidence Interval when Making Kaplan-Meier Curve",
    "content": "Hello. I am having difficulty with my confidence interval go to the end of my follow-up time frame when I use ggsurvplot. When I use plot survfit, it works, but when I use ggsurvplot it does not and idk why. If anyone has any insight into how to remedy this I would greatly appreciate it. I attached photos to illustrate what I mean. It should go all the way because the sample size is large enough for a 95% CI and when I run the summary function I get values for the upper and lower bounds. Thank you in advance.\n\nhttps://preview.redd.it/c27fta57juxe1.jpg?width=2387&amp;format=pjpg&amp;auto=webp&amp;s=d92382dfbe32a30018591572be8a3a8341855428\n\n",
    "author": "ANIIS5",
    "timestamp": "2025-04-29T14:56:53",
    "url": "https://reddit.com/r/rstats/comments/1kb06aq/issue_with_confidence_interval_when_making/",
    "score": 10,
    "num_comments": 2,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kau28c",
    "title": "How to Fuzzy Match Two Data Tables with Business Names in R or Excel?",
    "content": "I have two data tables:\n\n* **Table 1:**¬†Contains 130,000 unique business names.\n* **Table 2:**¬†Contains 1,048,000 business names along with approximately 4 additional data fields.\n\nI need to find the best match for each business name in Table 1 from the records in Table 2. Once the best match is identified, I want to append the corresponding data fields from Table 2 to the business names in Table 1.\n\nI would like to know the best way to achieve this using either¬†**R**¬†or¬†**Excel**. Specifically, I am looking for guidance on:\n\n1. **Fuzzy Matching Techniques:**¬†What methods or functions can be used to perform fuzzy matching in R or Excel?\n2. **Implementation Steps:**¬†Detailed steps on how to set up and execute the fuzzy matching process.\n3. **Handling Large Data Sets:**¬†Tips on managing and optimizing performance given the large size of the data tables.\n\nAny advice or examples would be greatly appreciated!",
    "author": "grizzlyriff",
    "timestamp": "2025-04-29T10:42:58",
    "url": "https://reddit.com/r/rstats/comments/1kau28c/how_to_fuzzy_match_two_data_tables_with_business/",
    "score": 18,
    "num_comments": 13,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kbginp",
    "title": "Oggi ho chiamato ‚Äúmamma‚Äù la mia capa davanti a tutti",
    "content": "Oggi, durante una riunione con tutta la mia squadra e il direttore generale, la mia capa mi stava spiegando una procedura piuttosto complicata. Io, stressato e con tre caff√® addosso, ho cercato di rispondere con sicurezza ma invece le ho detto: *‚ÄúS√¨ mamma‚Ä¶ ehm, volevo dire s√¨, dottoressa.‚Äù*\n\nSilenzio. Poi risate. Tante risate. La mia capa ha detto sorridendo: *‚ÄúBeh, almeno so di essere autoritaria.‚Äù*  \nIo invece sto ancora pensando di cambiare citt√†.",
    "author": "Appropriate_Fan_3671",
    "timestamp": "2025-04-30T06:29:00",
    "url": "https://reddit.com/r/rstats/comments/1kbginp/oggi_ho_chiamato_mamma_la_mia_capa_davanti_a_tutti/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.11,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kasd4k",
    "title": "Plain-language reporting of comparisons from ordinal logistic regression?",
    "content": "I need to report results from a set of ordinal logistic regression analyses to a non-technical audience. Each analysis predicts differences in a Likert-type outcome (*Poor* \\-&gt; *Excellent*) between four groups (i.e., categorical predictor). I ran the analyses with `ordinal::clm()` and made comparisons between each group and the mean of the other groups via `emmeans::emmeans(model, \"del.eff\" ~ Group)`.\n\n**Is there a concise way to describe the results of the comparisons from emmeans() in \"real-world\" terms to a non-technical audience?** By comparison, for binary logistic regression results, I typically report the relative risk, since this is easily interpretable in real-world terms by my audience (e.g., \"Group A is 1.8 times as likely to respond \"Yes\" compared to the average across other groups\").\n\nThe [documentation for emmeans](https://cran.r-project.org/web/packages/emmeans/vignettes/sophisticated.html#ordinal) says that the comparisons are \"on the 'latent' scale\", but I'm not sure how the latent scale is scaled; i.e., in the example in the documentation (reproduced below), is the estimate for pairwise differences of temp (-1.07) expressed in terms of standard deviations, levels of the outcome variable, or something else entirely? **Is there a way to express the effect size of the comparison in real-world terms, beyond just \"more/less positive response\"?**\n\n    # From the emmeans docs\n    library(\"ordinal\")\n    \n    wine.clm &lt;- clm(rating ~ temp + contact, scale = ~ judge,\n                    data = wine, link = \"probit\")\n    \n    emmeans(wine.clm, list(pairwise ~ temp, pairwise ~ contact))\n    \n    ## $`emmeans of temp`\n    ##  temp emmean    SE  df asymp.LCL asymp.UCL\n    ##  cold -0.884 0.290 Inf    -1.452    -0.316\n    ##  warm  0.601 0.225 Inf     0.161     1.041\n    ## \n    ## Results are averaged over the levels of: contact, judge \n    ## Confidence level used: 0.95 \n    ## \n    ## $`pairwise differences of temp`\n    ##  1           estimate    SE  df z.ratio p.value\n    ##  cold - warm    -1.07 0.422 Inf  -2.547  0.0109\n    ## \n    ## Results are averaged over the levels of: contact, judge \n    ## \n    ## $`emmeans of contact`\n    ##  contact emmean    SE  df asymp.LCL asymp.UCL\n    ##  no      -0.614 0.298 Inf   -1.1990   -0.0297\n    ##  yes      0.332 0.201 Inf   -0.0632    0.7264\n    ## \n    ## Results are averaged over the levels of: temp, judge \n    ## Confidence level used: 0.95 \n    ## \n    ## $`pairwise differences of contact`\n    ##  1        estimate    SE  df z.ratio p.value\n    ##  no - yes   -0.684 0.304 Inf  -2.251  0.0244\n    ## \n    ## Results are averaged over the levels of: temp, judge\n\n  \n",
    "author": "four_hawks",
    "timestamp": "2025-04-29T09:34:46",
    "url": "https://reddit.com/r/rstats/comments/1kasd4k/plainlanguage_reporting_of_comparisons_from/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1kaay8u",
    "title": "why can't I add geom_line()?",
    "content": "Im trying to do an very simple plot, but I can't add geom\\_line().\n\nThis is the code I used:\n\n`estudios %&gt;%`\n\n  `arrange(fecha) %&gt;%`\n\n  `ggplot(aes(x = fecha,` \n\n`y = col)) +`\n\n  `geom_line(size = 1) +`\n\n  `geom_point(size = 2) +`\n\n  `labs(x = \"Fecha\",`\n\n`y = \"Valor\") +`\n\n  `theme_minimal() +`\n\n  `theme(legend.title = element_blank())`\n\n  \nThis is my plot\n\nhttps://preview.redd.it/qymst3444oxe1.png?width=1040&amp;format=png&amp;auto=webp&amp;s=edba9bb81587ded756cbee06635be9b96aaa4e51\n\nAnd this is what R tells me\n\n    geom_line()`: Each group consists of only one observation.\n    ‚Ñπ Do you need to adjust the group aesthetic?\n\n&gt;",
    "author": "International_Mud141",
    "timestamp": "2025-04-28T17:24:16",
    "url": "https://reddit.com/r/rstats/comments/1kaay8u/why_cant_i_add_geom_line/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k9whyd",
    "title": "[Q] Approaches for structured data modeling with interaction and interpretability?",
    "content": "",
    "author": "kelby99",
    "timestamp": "2025-04-28T07:10:50",
    "url": "https://reddit.com/r/rstats/comments/1k9whyd/q_approaches_for_structured_data_modeling_with/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k9npgr",
    "title": "Need help understanding which tests to use for data set",
    "content": "Hi guys, \n\nI am really lost at understanding which tests to use when looking at my data sample for a university practice report. I know roughly how to perform tests in R but knowing what ones to use in this instance really confuses me.\n\nThey have given use 2 sets of before and after for a test something like this:  \nTest values are given on a scale of 1-7\n\nTest 1   \nID 1-30 | Before | After |\n\nTest 2  \nID 31-60 | Before | After |\n\n(not going to input all the values) \n\nMy thinking is that I should run 2 different paired tests as the factors are dependent but then I am lost at comparing Test 1 and 2 to each other. \n\nShould I perhaps calculate the differences between before and after for each ID and then run nonpaired t-test to compare Test 1 to Test 2? My end goal is to see which test has the higher result (closer to 7). \n\nBecause there are only 2 groups my understanding is that I shouldnt use ANOVA?\n\nThank you, ",
    "author": "PinkEevee21",
    "timestamp": "2025-04-27T22:03:58",
    "url": "https://reddit.com/r/rstats/comments/1k9npgr/need_help_understanding_which_tests_to_use_for/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k94mq0",
    "title": "Can anyone recommend code and tutorial for fitting a Nested ANOVA?",
    "content": "I want to fit a nested ANOVA in R, using the data shown in the screenshot. For context, the data shows spore quantities measured at 4 separate locations (A,B, C and D) and these locations are nested into 2 categories (A and B are Near Water and C and D are Far From Water). The response variable is Quantity, which was measured simultaneously in each site on 9 separate occasions. I wish to know if there is a significant different in spore quantities between each site, and also if being near or far from water affects spore quantities. However, after looking online there seems to be a lot of potential options for fitting a Nested ANOVA and some of these tutorials are quite old so I don't know if they all hold up in current versions of R. I have tried to follow some of these tutorials so far, but keep getting error codes I cannot fix. Can anyone recommend a tutorial or code? After reviewing my methodology,  I don't need to consider factors such as spatial or temporal autocorrelation. I am grateful or any advice at all.\n\nhttps://preview.redd.it/dgjxmlluvdxe1.png?width=243&amp;format=png&amp;auto=webp&amp;s=d8acd4a69e4bdb833df6b2036644612a21da00bd\n\n  \n",
    "author": "ScarlyLamorna",
    "timestamp": "2025-04-27T07:00:50",
    "url": "https://reddit.com/r/rstats/comments/1k94mq0/can_anyone_recommend_code_and_tutorial_for/",
    "score": 8,
    "num_comments": 2,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k8so0o",
    "title": "Is there an R job board anywhere?",
    "content": "Posit/Rstudio used to have an R Jobs board, but it is thoroughly defunct.  Is there an active one anywhere?",
    "author": "LetltSn0w",
    "timestamp": "2025-04-26T18:37:13",
    "url": "https://reddit.com/r/rstats/comments/1k8so0o/is_there_an_r_job_board_anywhere/",
    "score": 31,
    "num_comments": 7,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k8ldkf",
    "title": "Help with two-way repeated measures ANOVA",
    "content": "Hi, I hope this is allowed and if so I appreciate any help. I am trying to run a Two-Way repeated measures ANOVA. However, when I get to the code: res.aov &lt;- anova\\_test( data = data, dv = VALUE, wid = ID, within = c(TREATMENT, TIME) ) get\\_anova\\_table(res.aov)\n\nI get an error saying 0 non-NA cases. I checked if I have all cases and I do. When I do colSums(is.na(data)), I get 0 for all my columns.\n\nI suspect it may be related to the way my ID is set up but unsure of how to do it. I have esentially 5 treatments with 5 time points for each treatment and 5 replicates for each time point for each treatment for a total of 125 values and therefore an ID for each value. For example\n\nID : A1 Treatment : Apple Time: 0 Value: 100\n\nID: A2 Treatment: Apple Time: 0 Value: 120\n\nID: A3 Treatment: Apple Time: 10 Value: 150\n\nID: A4 Treatment: Pear Time: 0 Value: 90\n\nID: A5 Treatment: Pear Time: 0 Value: 100\n\nID: A6 Treatment: Pear Time: 10 Value: 160\n\nIf related to the way ID is set up, how could I fix it or if not I appreciate any help!",
    "author": "[deleted]",
    "timestamp": "2025-04-26T12:42:29",
    "url": "https://reddit.com/r/rstats/comments/1k8ldkf/help_with_twoway_repeated_measures_anova/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k7m1dr",
    "title": "How R's data analysis ecosystem shines against Python",
    "content": "",
    "author": "Capable-Mall-2067",
    "timestamp": "2025-04-25T07:16:33",
    "url": "https://reddit.com/r/rstats/comments/1k7m1dr/how_rs_data_analysis_ecosystem_shines_against/",
    "score": 119,
    "num_comments": 43,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k7hitc",
    "title": "R Newsletters/Communities in 2025?",
    "content": "I'm a daily R user, still thoroughly enjoy using it and am reluctant to move to Python. However, mostly due to my own fault, I feel like I'm stalling a bit as an intemediate user; I'm not really staying on top of new packages and releases, or improving my programming. I'm wondering where the most active R communities/newsletters are in 2025, beyond this subreddit. I'd like to somehow stay on top of the big new developments in the R ecosystem.\n\nStackoverflow acitivity is, as we know, hitting lows not seen since the early teens‚Äîunsurprising given the advent of LLMs, though the downward trend predates their widespread usage. Is there an R-bloggers or R-weekly newsletter that is good?\n\nWould be grateful if you could point me to some valuable streams, it'd be great if R users get news and use state of the art packages!",
    "author": "nodespots",
    "timestamp": "2025-04-25T03:22:44",
    "url": "https://reddit.com/r/rstats/comments/1k7hitc/r_newsletterscommunities_in_2025/",
    "score": 29,
    "num_comments": 12,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k89gyr",
    "title": "Understanding barriers to AI adoption in SMEs. Advice on analyzing survey data in RStudio",
    "content": "Hi everyone,\n\nI'm currently working on analyzing data from a survey conducted via Google Forms, which investigates the adoption of Artificial Intelligence (AI) in small and medium-sized enterprises (SMEs). The main goal is to understand the barriers that influence the decision to adopt AI, and to identify which categorical variables have the strongest impact on these barriers.\n\nThe survey includes:\n\n* 6 categorical variables:\n   * Industry sector\n   * Company size\n   * Revenue\n   * Location\n   * AI technologies already adopted\n   * AI technologies planned for adoption in the next 12 months\n* 11 Likert-scale questions related to barriers:\n   * Economic barriers\n   * Technological barriers\n   * Organizational and cultural barriers\n   * Legal and security barriers\n\nWhat I've Done So Far:\n\nI have already conducted some **descriptive analysis**, including:\n\n1. **Descriptive Analysis of Categorical Variables:**\n   * I‚Äôve calculated the frequency distributions (absolute and relative) for the categorical variables (e.g., Industry, Company Size, Family Ownership) using `table()` and `prop.table()`.\n   * Visualized the distributions with bar plots using `ggplot2`, which includes frequency counts and percentage labels.\n2. **Descriptive Analysis of Likert Scale Variables:**\n   * For each of the Likert-scale questions (e.g., Economic Barriers, Technological Barriers), I‚Äôve calculated basic descriptive statistics like the mode, mean, median, and standard deviation using `table(), mean(), median(), and sd().`\n   * I‚Äôve also visualized the distribution of responses for each Likert-scale variable using bar plots with `ggplot2`.\n3. **Boxplot Analysis:**\n   * I‚Äôve created **boxplots** to compare Likert-scale variables across different categories (e.g., Industry, Company Size, Revenue) to visualize how responses vary by category. This helps to assess if there are noticeable differences in barrier perceptions between different groups.\n   * Added mean labels on the boxplots using `stat_summary()` to indicate the average score for each group.\n4. **Exploring Percentages in Bar Charts:**\n   * For each Likert-scale variable, I‚Äôve visualized the distribution of responses, including relative frequencies as percentages, to provide better insight into the distribution of responses.\n5. **Correlation Analysis (Optional):**\n   * I‚Äôve also computed a **correlation matrix** between the Likert-scale variables using the `cor()` function, though I‚Äôm not sure if it's relevant for the next steps. This analysis shows how strongly related the different barrier variables are to each other.\n\n**Regarding the inferential analysis:**  \nI‚Äôm trying to further explore the relationships between the categorical variables and Likert scale responses to understand **which factors significantly influence the barriers to AI adoption in SMEs**. Here‚Äôs what I plan to do for the inferential part of the analysis:\n\n1. **Chi-Square Tests**: I will perform Chi-Square tests to check for associations between **categorical variables** (e.g., **industry**, **company size**, **AI adoption status**) and **Likert scale responses** (e.g., **economic barriers**, **technological barriers**).\n2. **ANOVA (Analysis of Variance)**: To compare the means of **Likert scale variables** across different categories, I‚Äôll use ANOVA. For instance, I will test if the **importance of AI adoption** varies significantly by **industry** or **company size**. \n3. Would you suggest any other methods like: **Multinomial Logistic Regression**, **Correlation Analysis**, **Linear Regression**, **Principal Component Analysis (PCA)**.\n\n  \nI'd appreciate any suggestions or recommendations for the analysis! Let me know if further information are required.\n\nThanks in advance for your help!",
    "author": "Possible-Mirror-1367",
    "timestamp": "2025-04-26T03:00:00",
    "url": "https://reddit.com/r/rstats/comments/1k89gyr/understanding_barriers_to_ai_adoption_in_smes/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k7oo0o",
    "title": "Trouble using KNN in RStudio",
    "content": "Hello All,\n\nI am attempting to perform a KNN function on a dataset I got from Kaggle (link below) and keep receiving this error. I did some research and found that some of the causes might stem from Factor Variables and/or Colinear Variables. All of my predictors are qualitative with several levels, and my response variable is quantitative. I was having issues with QDA using the same data and I solved the issue by deleting a variable \"Extent\\_Of\\_Fire\" and it seemed to help. When I tried the same for KNN it did not solve my issue. I am very new to RStudio and R so I apologize in advance if this is a very trivial problem, but any help is greatly appreciated!\n\n[https://www.kaggle.com/datasets/reihanenamdari/fire-incidents](https://www.kaggle.com/datasets/reihanenamdari/fire-incidents)",
    "author": "Cello_my_dude",
    "timestamp": "2025-04-25T09:05:48",
    "url": "https://reddit.com/r/rstats/comments/1k7oo0o/trouble_using_knn_in_rstudio/",
    "score": 9,
    "num_comments": 15,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k721ax",
    "title": "Online R Program?",
    "content": "I hope this hasn‚Äôt been asked here a ton of times, but I‚Äôm looking for advice on a good online course to take to learn R for total beginners. I‚Äôm a psych major and only know SPSS but want to learn R too. Recommendations?",
    "author": "zestypastacraver",
    "timestamp": "2025-04-24T13:08:12",
    "url": "https://reddit.com/r/rstats/comments/1k721ax/online_r_program/",
    "score": 33,
    "num_comments": 24,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k781ct",
    "title": "Data Engineering, Scientific Applications and AI - Inside R User Group Philippines‚Äô Growth",
    "content": "Joe Brillantes (https://lnkd.in/dhwWVcJJ), organizer of the R User Group Philippines (RUG-PH), shares how the group has evolved with new interests emerging among its members. \n\nFrom a growing presence of data engineers exploring R to an increasing focus on scientific applications, the group continues to expand its reach. He discussed their upcoming plans for AI-focused meetups, the importance of ethical considerations in predictive modeling, and their efforts to support members in software engineering and analytics.\n\nFind out more!\n\n[https://r-consortium.org/posts/data-engineering-scientific-applications-and-ai-inside-r-user-group-philippines-growth/](https://r-consortium.org/posts/data-engineering-scientific-applications-and-ai-inside-r-user-group-philippines-growth/)",
    "author": "jcasman",
    "timestamp": "2025-04-24T17:39:28",
    "url": "https://reddit.com/r/rstats/comments/1k781ct/data_engineering_scientific_applications_and_ai/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k6dvas",
    "title": "Cascadia R Conf 2025 ‚Äì Come Hang Out with R Folks in Portland",
    "content": "Hey r/rstats folks,\n\nJust wanted to let you know that registration is now open for¬†**Cascadia R Conf 2025**, happening June 20‚Äì21 in Portland, Oregon at PSU and OHSU.\n\nA few reasons you might want to come:\n\n* **David Keyes is giving the keynote**, talking about \"25 Things You Didn‚Äôt Know You Could Do with R.\" It‚Äôs going to be fun and actually useful.\n* We‚Äôve got¬†**workshops on everything from Shiny to GIS to Rust for R users**¬†(yep, that‚Äôs a thing now).\n* It's a good chance to¬†**meet other R users**, share ideas, and gripe about package dependencies in person.\n\nRegister (and check out the agenda) here:¬†[https://cascadiarconf.com](https://cascadiarconf.com/)\n\nIf you‚Äôre anywhere near the Pacific Northwest, this is a great regional conf with a strong community vibe. Come say hi!\n\nHappy to answer questions in the comments. Hope to see some of you there!",
    "author": "mulderc",
    "timestamp": "2025-04-23T16:37:18",
    "url": "https://reddit.com/r/rstats/comments/1k6dvas/cascadia_r_conf_2025_come_hang_out_with_r_folks/",
    "score": 30,
    "num_comments": 2,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k74a2x",
    "title": "How to assess the quality of written feedback/ comments given my managers.",
    "content": "\nI have the feedback/comments given by managers from the past two years (all levels).\n\nMy organization already has an LLM model. They want me to analyze these feedbacks/comments and come up with a framework containing dimensions such as clarity, specificity, and areas for improvement. The problem is how to create the logic from these subjective things to train the LLM model (the idea is to create a dataset of feedback). How should I approach this?\n\nI have tried LIWC (Linguistic Inquiry and Word Count), which has various word libraries for each dimension and simply checks those words in the comments to give a rating. But this is not working.\n\nCurrently, only word count seems to be the only quantitative parameter linked with feedback quality (longer comments = better quality).\n\nAny reading material on this would also be beneficial.",
    "author": "Sandwichboy2002",
    "timestamp": "2025-04-24T14:42:35",
    "url": "https://reddit.com/r/rstats/comments/1k74a2x/how_to_assess_the_quality_of_written_feedback/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.13,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k6dm7t",
    "title": "Quarterly Round Up from the R Consortium",
    "content": "Executive Director Terry Christiani highlights upcoming events like R/Medicine 2025 and useR! 2025, opportunities for non-members to join Working Groups, and tons more! \n\n[https://r-consortium.org/posts/quarterly-round-up-from-the-r-consortium/](https://r-consortium.org/posts/quarterly-round-up-from-the-r-consortium/)",
    "author": "jcasman",
    "timestamp": "2025-04-23T16:25:38",
    "url": "https://reddit.com/r/rstats/comments/1k6dm7t/quarterly_round_up_from_the_r_consortium/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k6v08o",
    "title": "Project with RMarkdown",
    "content": "I have to do a PW whose goal is to be able to implement through R the notions of exploratory analysis, unsupervised and supervised learning\n\n\n\nThe output of the analysis must preferably be an RMarkDown.\n\n\n\nIf someone is willing to help me, I can pay",
    "author": "Bonnicapo",
    "timestamp": "2025-04-24T08:25:49",
    "url": "https://reddit.com/r/rstats/comments/1k6v08o/project_with_rmarkdown/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k6uzq2",
    "title": "Help with Rmarkdown",
    "content": "I have to do a PW whose goal is to be able to implement through R the notions of exploratory analysis, unsupervised and supervised learning\n\n\n\nThe output of the analysis must preferably be an RMarkDown.\n\n\n\nIf someone is willing to help me, I can pay",
    "author": "Bonnicapo",
    "timestamp": "2025-04-24T08:25:13",
    "url": "https://reddit.com/r/rstats/comments/1k6uzq2/help_with_rmarkdown/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k5fbwj",
    "title": "I set up a Github Actions workflow to update this graph each day. Link to repo with code and documentation in the description.",
    "content": "I shared a version of this years ago. At some point in the interim, the code broke, so I've gone back and rewritten the workflow. It's much simpler now and takes advantage of some improvement in R's Github Actions ecosystem.\n\nHere's the link: [https://github.com/jdjohn215/milwaukee-weather](https://github.com/jdjohn215/milwaukee-weather)\n\nI've benefited a lot from tutorials on the internet written by random people like me, so I figured this might be useful to someone too.",
    "author": "[deleted]",
    "timestamp": "2025-04-22T12:34:53",
    "url": "https://reddit.com/r/rstats/comments/1k5fbwj/i_set_up_a_github_actions_workflow_to_update_this/",
    "score": 164,
    "num_comments": 20,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k5qpdc",
    "title": "Request for R scripts handling monthly data",
    "content": "I absolutely love how the R community publishes the script to allow the user to exactly replicate the examples (see R-Graph-Gallery website). This allows me to systematically work from code that works(!) and modify the script with my own data and allows me to change attributes as needed. \n\nThe main challenge I have is that all of my datasets are monthly. I am required to publish my data in a MMM-YYYY format. I can easily do this in excel. I have found no ggplot2 R scripts that I can work from that allow me to import my data in a MM/DD/YYYY format and publish in MMM-YYYY format. If anyone has seen scripts that involve creating graphics (ggplot2 or gganimate) with a monthly interval (and multi-year) interval, I would love to see and study it! I've seen the examples that go from Jan, Feb...Dec, but they only cover the span of 1 year. I'm interesting in creating graphics with data displayed on monthly interval from Jan-1985 through Dec-1988. If you have any tips or tricks to deal with monthly data, I'd love to hear them because I'm about to throw my computer out the window. Thanks in advance!  ",
    "author": "Adorable_Kale_840",
    "timestamp": "2025-04-22T21:35:51",
    "url": "https://reddit.com/r/rstats/comments/1k5qpdc/request_for_r_scripts_handling_monthly_data/",
    "score": 13,
    "num_comments": 12,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k4oa45",
    "title": "How can I get daily average climate data for a specific location in R?",
    "content": "I want to obtain daily average climate data (rainfall, snowfall, temps) for specific locations (preferably using lat/long coordinates). Is there a package that can do this simply? I don't need to map the data as raster, I just want to be able to generate a dataframe and make simple plots. X would be days of the year, 1-365, Y would be the climate variable. Thanks.",
    "author": "landschaften",
    "timestamp": "2025-04-21T13:50:22",
    "url": "https://reddit.com/r/rstats/comments/1k4oa45/how_can_i_get_daily_average_climate_data_for_a/",
    "score": 15,
    "num_comments": 16,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k3ruma",
    "title": "Why I'm still betting on R",
    "content": "",
    "author": "damageinc355",
    "timestamp": "2025-04-20T10:32:18",
    "url": "https://reddit.com/r/rstats/comments/1k3ruma/why_im_still_betting_on_r/",
    "score": 75,
    "num_comments": 43,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k3hdyo",
    "title": "Popular python packages among R users",
    "content": "I'm currently writing an R package called [rixpress](https://github.com/b-rodrigues/rixpress) which aims to set up reproducible pipelines with simple R code by using Nix as the underlying build tool. Because it uses Nix as the build tool, it is also possible to write targets that are built using Python. [Here is an example of a pipeline that mixes R and Python.\n](https://github.com/b-rodrigues/rixpress_demos/tree/master/python_r)\n\nTo make sure I test most use cases, I'm looking for examples of popular Python packages among R users.\n\nSo R users, which Python packages do you use, if any?",
    "author": "brodrigues_co",
    "timestamp": "2025-04-20T00:40:51",
    "url": "https://reddit.com/r/rstats/comments/1k3hdyo/popular_python_packages_among_r_users/",
    "score": 39,
    "num_comments": 19,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k2vl3a",
    "title": "Has anyone tried working with Cursor?",
    "content": "The title says it all.\n\nLately I've been looking into AI tools to speed up work and I see that Rstudio is lagging far behind as an IDE. Don't get me wrong, I love RStudio, it's still my IDE of choice for R. \n\nI've also been trying out positron, I like the idea of opening and coding, avoiding all the Vscode setup to use R, but you can't access copilot like you can in Vscode, and I don't really like the idea of using LLM's Api Keys.\n\nThis is where Cursor comes in. I came across it this week and have been looking for information about how to use R. Apparently, it's the same setup steps as Vscode (terrible), but Cursor might be worth all the hassle. Yes, it's paid and there are local alternatives, but I like the idea of a single monthly payment and one-click access to the latest models.\n\nHas anyone had experience with Cursor for R programming? I'm very interested in being able to execute code line by line.\n\nThanks a lot community!",
    "author": "SwimmingProgrammer36",
    "timestamp": "2025-04-19T05:52:35",
    "url": "https://reddit.com/r/rstats/comments/1k2vl3a/has_anyone_tried_working_with_cursor/",
    "score": 6,
    "num_comments": 9,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k30gv7",
    "title": "HELP ME ESTIMATING HIERARCHICAL COPULAS",
    "content": "I am writing a master thesis on hierarchical copulas (mainly Hierarchical Archimedean Copulas) and i have decided to model hiararchly the dependence of the S&amp;P500, aggregated by GICS Sectors and Industry Group. I have downloaded data from 2007 for 400 companies ( I have excluded some  for missing data).\n\nActually i am using R as a software and I have installed two different packages: copula and HAC.\n\nTo start, i would like to estimate a copula as it follow:\n\nI consider the 11 GICS Sector and construct a copula for each sector. the leaves are represented by the companies belonging to that sector. \n\nThen i would aggregate the copulas on the sector by a unique copula. So in the simplest case i would have 2 levels. The HAC package gives me problem with the computational effort.\n\nMeanwhile i have tried with copula package. Just to trying fit something i have lowered the number of sector to 2, Energy and Industrials and i have used the functions 'onacopula' and 'enacopula'. As i described the structure, the root copula has no leaves. However the following code, where U\\_all is the matrix of pseudo observations :\n\nd1=c(1:17)\n\nd2=c(18:78)\n\nU\\_all &lt;- cbind(Uenergy, Uindustry)  \n\nhier=onacopula('Clayton',C(NA\\_real\\_,NULL , list(C(NA\\_real\\_, d1), C(NA\\_real\\_, d2))))\n\nfit\\_hier &lt;- enacopula(U\\_all, hier\\_clay, method=\"ml\")\n\nsummary(fit\\_hier)\n\n  \nreturns me the following error message:\n\n    Error in enacopula(U_all, hier_clay, method = \"ml\") : \n      max(cop@comp) == d is not TRUE",
    "author": "Particular_Chart8156",
    "timestamp": "2025-04-19T09:40:36",
    "url": "https://reddit.com/r/rstats/comments/1k30gv7/help_me_estimating_hierarchical_copulas/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k2979z",
    "title": "Posit is being rude (R)",
    "content": "So, I'm having issues rendering a quarto document through Posit. The code I have within the document runs to make a histogram, and that part runs perfectly. However, when I try to render the document to make it a website link, it says that the file used to make that histogram cannot be found, and it stops rendering that document. Anyone have any ideas on what this can be? I've left my screen above with the code it backtraced to.",
    "author": "L_Medea_432",
    "timestamp": "2025-04-18T09:32:14",
    "url": "https://reddit.com/r/rstats/comments/1k2979z/posit_is_being_rude_r/",
    "score": 8,
    "num_comments": 10,
    "upvote_ratio": 0.59,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k19h5r",
    "title": "Decent crosstable functions in R",
    "content": "I've just been banging my head against a wall trying to look for decent crosstable functions in R that do all of the following things:\n\n1. Provide counts, totals, row percentages, column percentages, and cell percentages.\n2. Provide clean output in the console.\n3. Show percentages of missing values as well.\n4. Provide outputs in formats that can be readily exported to Excel.\n\nIf you know of functions that do all of these things, then please let me know.\n\nUpdate: I thought I'd settle for something that was easy, lazy, and would give me some readable output. I was finding output from CrossTable() and sjPlot's tab\\_xtab difficult to export. So here's what I did.\n\n1) I used tabyl to generate four cross tables: one for totals, one for row percentages, one for column percentages, and one for total percentages.\n\n2) I renamed columns in each percentage table with the suffix \"\\_r\\_pct\", \"\\_c\\_pct\", and \"\\_t\\_pct\".\n\n3) I did a cbind for all the tables and excluded the first column for each of the percentage tables. ",
    "author": "themadbee",
    "timestamp": "2025-04-17T03:28:51",
    "url": "https://reddit.com/r/rstats/comments/1k19h5r/decent_crosstable_functions_in_r/",
    "score": 22,
    "num_comments": 36,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k190ls",
    "title": "Differences in R and Stata for logistic regression?",
    "content": "Hi all, \n\nBeginner in econometrics and in R here, I'm much more familiar with Stata but unfortunately I need to switch to R. So I'm replicating a paper. I'm using the same data than author, and I know I'm doing alright so far because the paper involves a lot of variables creation and descriptive statistics and so far I end up with exactly the same numbers, every digit is the same. \n\nBut the problem comes when I try to replicate the regression part. I'm heavily suspecting the author worked on Stata. The author mentionned the type of model she did (logit regression), the variables she used, and explained everything in the table. What I don't know tho is what command with what options exactly she ran. \n\nI'm getting completely different marginal effects and SEs than hers. I suspect this is because of the model. Could there be this much difference between Stata and R? \n\nI'm using \n\n`design &lt;- svydesign(ids = ~1, weights = ~pond, data = model_data)`\n\n`model &lt;- y ~ x`\n\n`svyglm(model, design, family = quasibinomial())`\n\n\n\nis this a perfect equivalent on the Stata command \n\n`logit y x [pweight = pond]`\n\n? If no, could you explain what options do I have to try to estimate as closely as possible the equivalent of a logistic regression in Stata please. ",
    "author": "prwav",
    "timestamp": "2025-04-17T02:57:39",
    "url": "https://reddit.com/r/rstats/comments/1k190ls/differences_in_r_and_stata_for_logistic_regression/",
    "score": 5,
    "num_comments": 8,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k0z0i5",
    "title": "Logging package that captures non-interactive script outputs?",
    "content": "",
    "author": "Vegetable_Cicada_778",
    "timestamp": "2025-04-16T16:45:54",
    "url": "https://reddit.com/r/rstats/comments/1k0z0i5/logging_package_that_captures_noninteractive/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k0rm6f",
    "title": "Edinburgh R User group is expanding collaborations with neighboring user groups",
    "content": "Ozan Evkaya, University Teacher at the University of Edinburgh and one of the local organizers of the Edinburgh R User group, spoke with the R Consortium about his journey in the R community and his efforts to strengthen R adoption in Edinburgh. \n\nOzan discussed his experiences hosting R events in Turkey during the pandemic, the importance of online engagement, and his vision for expanding collaborations with neighboring user groups. \n\nHe covers his research in dependence modeling and contributions to open-source R packages, highlighting how R continues to shape his work in academia and community building.\n\n[https://r-consortium.org/posts/strengthening-r-communities-across-borders-ozan-evkaya-on-organizing-the-edinburgh-r-user-group/](https://r-consortium.org/posts/strengthening-r-communities-across-borders-ozan-evkaya-on-organizing-the-edinburgh-r-user-group/)",
    "author": "jcasman",
    "timestamp": "2025-04-16T11:26:45",
    "url": "https://reddit.com/r/rstats/comments/1k0rm6f/edinburgh_r_user_group_is_expanding/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1k0qvx0",
    "title": "Quick question regarding nested resampling and model selection workflow",
    "content": "Just wanted some feedback as to if my though process is correct.  \n  \nThe premise:\n\nNeed to train dev a model and I will need to perform nested resmapling to prevent against spatial and temporal leakage.  \nOuter samples will handle spatial leakage.  \nInner samples will handle temporal leakage.  \nI will also be tuning a model.\n\nVia the diagram below, my model tuning and selection will be as follows:  \n\\-Make inital 70/30 data budget  \n\\-Perfrom some number of spatial resamples (4 shown here)  \n\\-For each spatial resample (1-4), I will make N (4 shown) spatial splits  \n\\-For each inner time sample i will train and test N (4 shown) models and mark their perfromance  \n\\-For each outer samples' inner samples - one winner model will be selected based on some criteria  \n\\--e.g Model A out performs all models trained innner samples 1-4 for outer sample #1  \n\\----Outer/spatial #1 -- winner model A  \n\\----Outer/spatial #2 -- winner model D  \n\\----Outer/spatial #3 -- winner model C  \n\\----Outer/spatial #4 -- winner model A  \n\\-I take each winner from the previous step and train them on their entire train sets and validate on their test sets  \n\\--e.g train model A on outer #1 train and test on outer #1 test  \n\\----- train model D on outer #2 train and test on outer #2 test  \n\\----- and so on   \n\\-From this step the model the perfroms the best is then selected from these 4 and then trained on the entire inital 70% train and evalauated on the inital 30% holdout.\n\nhttps://preview.redd.it/7ugl5oqyj8ve1.png?width=1260&amp;format=png&amp;auto=webp&amp;s=0ddc21e57126a94ba6d4c6f3870f050e0cb72217\n\n",
    "author": "showme_watchu_gaunt",
    "timestamp": "2025-04-16T10:57:28",
    "url": "https://reddit.com/r/rstats/comments/1k0qvx0/quick_question_regarding_nested_resampling_and/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jz675s",
    "title": "Use use() in R",
    "content": "[https://erikgahner.dk/2025/use-use-in-r/](https://erikgahner.dk/2025/use-use-in-r/)",
    "author": "erikglarsen",
    "timestamp": "2025-04-14T11:32:11",
    "url": "https://reddit.com/r/rstats/comments/1jz675s/use_use_in_r/",
    "score": 66,
    "num_comments": 37,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jzfh8z",
    "title": "Should you use polars in R? [Erik Gahner Larsen]",
    "content": "",
    "author": "thefringthing",
    "timestamp": "2025-04-14T18:16:02",
    "url": "https://reddit.com/r/rstats/comments/1jzfh8z/should_you_use_polars_in_r_erik_gahner_larsen/",
    "score": 9,
    "num_comments": 3,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jyh0z3",
    "title": "Two Complaints about R",
    "content": "I have been using R almost every day for more than 10 years. It is perfect for my work but has two issues bothering me. \n\nFirst, the naming convention is bad. Since the dot (.) has many functional meanings, it should not be allowed in variable names. I am glad that Tidyverse encourages the snake case naming convention. Also, I don't understand why package names cannot be snake case. \n\nSecond, the OOP design is messy. Not only do we have S3 and S4, R6 is also used by some packages. S7 is currently being worked on. Not sure how this mess will end.     ",
    "author": "BOBOLIU",
    "timestamp": "2025-04-13T13:20:43",
    "url": "https://reddit.com/r/rstats/comments/1jyh0z3/two_complaints_about_r/",
    "score": 78,
    "num_comments": 27,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jz2kxk",
    "title": "I can't open my proyect in R",
    "content": "Hi, I have a problem\n\nI was working in R when suddenly my computer turned off.\n\nWhen I turned it on again I opened my project in R and I got the following message \n\n`Project ‚ÄòC:/Users/.....‚Äô could not be opened: file line number 2 is invalid.`\n\nAnd the project closes. I can't access it, what can I do?",
    "author": "International_Mud141",
    "timestamp": "2025-04-14T09:05:11",
    "url": "https://reddit.com/r/rstats/comments/1jz2kxk/i_cant_open_my_proyect_in_r/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jynyku",
    "title": "checking normality only after running a test",
    "content": "i just learned that we test the normaity on the residuals, not on the raw data. unfortunately, i have ran nonparametric tests due to the data not meeting the assumptions after days of checking normality of the raw data instead.  waht should i do?\n\n\n\n1. should i rerun all tests with 2way anova? then swtich to non parametric (ART ANOVA) if the residuals fail the assumptions?\n\n2. does this also go with eequality of variances?\n\n3. is there a more efficient way of checking the assumptions before deciding which test to  perform? ",
    "author": "marinebiot",
    "timestamp": "2025-04-13T19:03:03",
    "url": "https://reddit.com/r/rstats/comments/1jynyku/checking_normality_only_after_running_a_test/",
    "score": 3,
    "num_comments": 18,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jyg3u3",
    "title": "Data Profiling in R",
    "content": "Hey! I got a uni assignment to do Data Profiling on a set of data representing reviews about different products. I got a bunch of CSV files. \n\nThe initial idea of the task was to use sql server integration services: load the data into the database and explore it using different profiles, e.g. detect foreign keys, anomalies, check data completeness, etc.\n\nSince I already chose the path of completing this course in R, I was wondering what is the set of libraries designed specifically for profiling? Which tools I should better use to match the functionality of SSIS?\n\nI already did some profiling here and there just using skimr and tidyverse libraries, I'm just wondering whether there are more libraries available\n\nAny suggestions about the best practices will be welcomed too",
    "author": "jyve-belarus",
    "timestamp": "2025-04-13T12:40:42",
    "url": "https://reddit.com/r/rstats/comments/1jyg3u3/data_profiling_in_r/",
    "score": 10,
    "num_comments": 2,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jynx5z",
    "title": "checking normality assumptio ony after running anova",
    "content": "i just learned that we test the normaity on the residuals, not on the raw data. unfortunately, i have ran nonparametric tests due to the data not meeting the assumptions after days of checking normality of the raw data instead.  waht should i do?\n\n\n\n1. should i rerun all tests with 2way anova? then swtich to non parametric (ART ANOVA) if the residuals fail the assumptions?\n\n2. does this also goes with eequality of variances?\n\n3. is there a more efficient way iof checking the assumptions before deciding which test to  perform? ",
    "author": "marinebiot",
    "timestamp": "2025-04-13T19:01:02",
    "url": "https://reddit.com/r/rstats/comments/1jynx5z/checking_normality_assumptio_ony_after_running/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jyg7xg",
    "title": "Paired t-test. \"cannot use 'paired' in formula method\"",
    "content": "Dear smart people,\n\nI just don‚Äôt understand what happened to my R (or my brain), but all my scripts that used a paired t-test have suddenly stopped working. Now I get the error: *\"cannot use 'paired' in formula method.\"*\n\nEverything worked perfectly until I updated R and RStudio.\n\nHere‚Äôs a small table with some data: I just want to run a t-test for InvvStan by type. To make it work now I have to rearrange the table for some reason... Do you have any idea why this is happening or how to fix it?\n\n    &gt; t.Abund &lt;- t.test(InvStan ~ Type, data = Inv, paired = TRUE)\n    Error in t.test.formula(InvStan ~ Type, data = Inv, paired = TRUE) : \n      cannot use 'paired' in formula method\n\nhttps://preview.redd.it/ek8055ajnnue1.png?width=490&amp;format=png&amp;auto=webp&amp;s=878af29facf4e81eccd2b279e659a196da6d42f9\n\n  \n",
    "author": "nmotss",
    "timestamp": "2025-04-13T12:45:41",
    "url": "https://reddit.com/r/rstats/comments/1jyg7xg/paired_ttest_cannot_use_paired_in_formula_method/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jy6vu2",
    "title": "more debugging information (missing points with go-lot)",
    "content": "With ggplot, I sometimes get the message:\n\n    4: Removed 291 rows containing missing values or values outside the scale range (geom_point()`).`\n\nbut this often happens on a page with multiple plots, so it is unclear where the error is.\n\nIs there an option to make 'R' tell me what line produced the error message?  Better still, to tell me which rows had the bad points?",
    "author": "fasta_guy88",
    "timestamp": "2025-04-13T05:48:58",
    "url": "https://reddit.com/r/rstats/comments/1jy6vu2/more_debugging_information_missing_points_with/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jy25ur",
    "title": "Ordered factors in Binary Logistic Regression",
    "content": "Hi! I'm working on a binary logistic regression for my special project, and I have ordinal predictors. I'm using the glm function, just like we were taught. However, the summary of my model includes .L, .Q, and .C for my ordinal variables. I just want to ask how I can remove these while still treating the variables as ordinal.",
    "author": "reixanne",
    "timestamp": "2025-04-13T00:25:05",
    "url": "https://reddit.com/r/rstats/comments/1jy25ur/ordered_factors_in_binary_logistic_regression/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jy5nq4",
    "title": "Is R really dying slowly?",
    "content": "I apologize with my controversial post here in advance. I am just curious if R really won't make it into the future, and significantly worrying about learning R. My programming toolkit mainly includes R, Python, C++, and secondarily SQL and a little JavaScript. I am improving my skills for my 3 main programming languages for the past years, such as data manipulation and visualization in R, performing XGBoost for both R and Python, and writing my own fast exponential smoothing in C++. Yet, I worried if my learnings in R is going to be wasted. ",
    "author": "Embarrassed-Bed3478",
    "timestamp": "2025-04-13T04:36:02",
    "url": "https://reddit.com/r/rstats/comments/1jy5nq4/is_r_really_dying_slowly/",
    "score": 0,
    "num_comments": 15,
    "upvote_ratio": 0.39,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jxsnq5",
    "title": "March YoY CPI prediction model",
    "content": "I used time series forecasting to predict CPI for March and this is what I got. I also place a $30 bet on kalshi for \"Yes above 2.7%\". Was I wrong to place that bet?\n\nhttps://preview.redd.it/bhlu7zjq8hue1.png?width=472&amp;format=png&amp;auto=webp&amp;s=0ef22400d8cac171b1b96ece9c1d66050f35d3ff",
    "author": "Lou_Morningstar",
    "timestamp": "2025-04-12T15:04:54",
    "url": "https://reddit.com/r/rstats/comments/1jxsnq5/march_yoy_cpi_prediction_model/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jxg7r3",
    "title": "Why isn‚Äôt my Stargazer table displaying in the format I want it to?",
    "content": "I am trying to have my table formatted in a more presentable way, but despite including all the needing changes, it still outputs in default text form. Why is this?\n\n",
    "author": "Formal_Outside_5149",
    "timestamp": "2025-04-12T05:31:02",
    "url": "https://reddit.com/r/rstats/comments/1jxg7r3/why_isnt_my_stargazer_table_displaying_in_the/",
    "score": 3,
    "num_comments": 15,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jxfbh3",
    "title": "Interpretation of elastic net Regressioncoefficients",
    "content": "Can I classify my regression coefficients from elastic net regression using a scale like RC = 0-0.1 for weak effect, 0.1-0.2 for moderate effect, and 0.2-0.3 for strong effect? I'm looking for a way to identify the best predictors among highly correlated variables, but I haven‚Äôt found any literature on this so far. Any thoughts or insights on this approach? I understood that a higher RC means that the effect of the variable on the model is higher than the effect of a variable with a lower RC. I really appreciate your help, thanks in advance.\n\n\n\nhttps://preview.redd.it/k5jrdhkm4eue1.jpg?width=605&amp;format=pjpg&amp;auto=webp&amp;s=022593e963a4bb97b73d5fd645c0ea8e03a75893\n\n",
    "author": "Exotic_Month_5357",
    "timestamp": "2025-04-12T04:38:06",
    "url": "https://reddit.com/r/rstats/comments/1jxfbh3/interpretation_of_elastic_net/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jwwkxs",
    "title": "How bad is it that I don't seem to \"get\" a lot of dplyr and tidyverse?",
    "content": "It's not that I can't read or use it, in fact I use the pipe and other tidyverse functions fairly regularly. But I don't understand why I'd exclusively use dplyr. It doesn't seem to give me a lot of solutions that base R can't already do. \n\nAm I crazy? Again, I'm not *against* it, but stuff like boolean indexing, lists, %in% and so on are very flexible and are very explicit about what they do. \n\nCurious to know what you guys think, and also what other languages you like. I think it might be a preference thing; While i'm primarily an R user I really learned to code using Java and C, so syntax that looks more C-like and using lists as pseudo-pointers has always felt very intuitive for me. ",
    "author": "givemesendies",
    "timestamp": "2025-04-11T11:13:47",
    "url": "https://reddit.com/r/rstats/comments/1jwwkxs/how_bad_is_it_that_i_dont_seem_to_get_a_lot_of/",
    "score": 49,
    "num_comments": 53,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jx8c5s",
    "title": "Looking for a guide to read code",
    "content": "\n\nI want to be able to *read* code and understand it, not necessarily *write* it.\n\nDoes that make sense? Is there an app or other reference that teaches how ro read R code?\n\nThanks.",
    "author": "Minimum_Professor113",
    "timestamp": "2025-04-11T20:39:27",
    "url": "https://reddit.com/r/rstats/comments/1jx8c5s/looking_for_a_guide_to_read_code/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jwa45k",
    "title": "POTUS economic scorecard shinylive app",
    "content": "Built this¬†[shinylive app ](https://jhelvy.github.io/potus-econ-scorecard/)¬†to track economic indicators over different administrations going back to Eisenhower (1957). It was fun to build and remarkably simple now with shinylive and Quarto. I wanted to share it with R users in case you're interested in building something similar for other applications.\n\nIt was inspired by my¬†[post](https://www.reddit.com/r/dataisbeautiful/comments/1jqy998/comparison_of_sp_500_performance_dyeing_first_100/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button)¬†from last week in r/dataisbeautiful (which was taken down for no stated reason) and allows users to view different indicators, including market indicators, unemployment, and inflation. You can also view performance referenced to either inauguration day or the day before the election. \n\nThe app is built using:\n\n* [R Shiny](https://shiny.posit.co/)¬†for the interactive web application.\n* [shinylive](https://posit-dev.github.io/r-shinylive/)¬†for browser-based execution without a server.\n* [Quarto](https://quarto.org/)¬†for website publishing.\n* [plotly](https://plotly.com/r/)¬†for interactive visualizations.\n\nLive app is available at¬†[https://jhelvy.github.io/potus-econ-scorecard/](https://jhelvy.github.io/potus-econ-scorecard/)\n\nSource code is available at¬†[https://github.com/jhelvy/potus-econ-scorecard](https://github.com/jhelvy/potus-econ-scorecard)",
    "author": "jhelvy",
    "timestamp": "2025-04-10T15:03:15",
    "url": "https://reddit.com/r/rstats/comments/1jwa45k/potus_economic_scorecard_shinylive_app/",
    "score": 46,
    "num_comments": 10,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jwi2h8",
    "title": "Transforming a spreadsheet so R can properly read it",
    "content": "Hi everyone, I am hoping someone can help me with this. I don't know how to succinctly phrase it so I haven't been able to find an answer through searching online. I am preparing a spreadsheet to run an ANOVA (possibly MANOVA). I am looking at how a bunch of different factors affect coral bleaching, and looking at factors such as \"Region\" (Princess Charlotte Bay, Cairns, etc), Bleached % (0%, 50%, etc), \"Species\" (Acropora, Porites, etc),¬†Size (10cm, 20cm, 30cm, etc) and a few others factors. This is a very large dataset and as it is laid out at the moment, it is 3000 rows long. \n\nIt is currently laid out as:\n\nColumns: Region --- Bleached % --- Species --- 10cm ---20cm --- 30cm\n\nso for instance a row of data would look like:\n\nCairns --- 50% --- Acropora --- 2 --- 1 --- 4\n\nwith the 2, 1, and 4 corresponding to how many of each size class there are, so for instance there are 2 10cm Acroporas that are 50% bleached at Cairns, 1 that is 20cm and 50% bleached, and 4 that are 30cm and 50% bleached. Ideally I would have the spreadsheet laid out so each row represented one coral, so this above example would transform into 7 rows that would read:\n\nCairns --- 50%¬†--- Acropora --- 10cm\n\nCairns --- 50% --- Acropora --- 10cm\n\nCairns --- 50% --- Acropora --- 20cm\n\nCairns --- 50% --- Acropora --- 30cm\n\nCairns --- 50% --- Acropora --- 30cm\n\nCairns --- 50% --- Acropora --- 30cm\n\nCairns --- 50% --- Acropora --- 30cm\n\nbut with my dataset being so large, it would take ages to do this manually. Does anyone know if there is a trick to getting excel to transform the spreadsheet in this way? Or if R would accept and properly read a dataset that it set up as I currently have it? Thanks very much for your help!  \n",
    "author": "Ocean_Optimist",
    "timestamp": "2025-04-10T22:01:17",
    "url": "https://reddit.com/r/rstats/comments/1jwi2h8/transforming_a_spreadsheet_so_r_can_properly_read/",
    "score": 5,
    "num_comments": 18,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jwgdtp",
    "title": "Does it make sense to use cross-validation on a small dataset (n = 314) w/ a high # of variables (29) to find the best parameters for a MLR model?",
    "content": "I have a small dataset, and was wondering if it would make sense to do CV to fit a MLR with a high number of variables? There's an R data science book I'm looking through that recommends CV for regularization techniques, but it didn't use CV for MLR, and I'm a bit confused why. ",
    "author": "GhostGlacier",
    "timestamp": "2025-04-10T20:21:45",
    "url": "https://reddit.com/r/rstats/comments/1jwgdtp/does_it_make_sense_to_use_crossvalidation_on_a/",
    "score": 4,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jwb3p0",
    "title": "Regression model violates assumptions even after transformation ‚Äî what should I do?",
    "content": "hi everyone, i'm working on a project using the \"balanced skin hydration\" dataset from kaggle. i'm trying to predict electrical capacitance (a proxy for skin hydration) using TEWL, ambient humidity, and a binary variable called target.\n\ni fit a linear regression model and did box-cox transformation. TEWL was transformed using log based on the recommended lambda. after that, i refit the model but still ran into issues.\n\nhere‚Äôs the problem:\n\n* shapiro-wilk test fails (residuals not normal, p &lt; 0.01)\n* breusch-pagan test fails (heteroskedasticity, p &lt; 2e-16)\n* residual plots and qq plots confirm the violations\n\n[Before and After Transformation](https://preview.redd.it/uxte99ve63ue1.png?width=558&amp;format=png&amp;auto=webp&amp;s=113de10d394e9521a290b6231841606360a624fb)",
    "author": "Longjumping_Pick3470",
    "timestamp": "2025-04-10T15:47:52",
    "url": "https://reddit.com/r/rstats/comments/1jwb3p0/regression_model_violates_assumptions_even_after/",
    "score": 6,
    "num_comments": 9,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jvvkg7",
    "title": "Post hoc dunns test not printing all rows- only showing 1000",
    "content": "",
    "author": "pickletheshark",
    "timestamp": "2025-04-10T04:26:58",
    "url": "https://reddit.com/r/rstats/comments/1jvvkg7/post_hoc_dunns_test_not_printing_all_rows_only/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jvvgdy",
    "title": "[Q] Statistical advice for entomology research; NMDS?",
    "content": "",
    "author": "puekid",
    "timestamp": "2025-04-10T04:20:27",
    "url": "https://reddit.com/r/rstats/comments/1jvvgdy/q_statistical_advice_for_entomology_research_nmds/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jvv70w",
    "title": "[Q] Career advice, pharmacist",
    "content": "Hi everyone, I am a pharmacist in Europe, age early thirties, , working in regulatory affairs.\n\n Currently I am doing a post grad statistics and data science course.\n\nI am hoping this will present new opportunities.  Am I being too optimistic / naive in thinking so? \n\nDo you have any suggestions / advice moving forward? \n\nIs it worth  pursuing such a course? Anyone in a similar career path?\n\n",
    "author": "Big-Ad-3679",
    "timestamp": "2025-04-10T04:04:47",
    "url": "https://reddit.com/r/rstats/comments/1jvv70w/q_career_advice_pharmacist/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jv9gfo",
    "title": "Two way mixed effects anova controlling for a variable",
    "content": "Hello!! I need to analyse data for a long term experiment looking at the impact of three treatment types on plant growth overtime. I thought I had the correct analysis (a two way mixed effects ANOVA), which (with a post hoc test) gave me two nice table outputs showing me the significance between treatments at each timepoint and within treatment type across timepoints. However, I've just realised that a two way mixed effects ANOVA might not work because my data is count data and more importantly I need to account for the fact that some of the plants are in the same pond and some are not (eg accounting for pseudoreplication). I then thought that a glmer may be the most suitable but I can't seem to get a good post hoc test to give me the same output as previously. Any suggestions on which test or even where I should be looking for extra info would be greatly appreciated! TIA",
    "author": "BrokenFridge507",
    "timestamp": "2025-04-09T09:04:51",
    "url": "https://reddit.com/r/rstats/comments/1jv9gfo/two_way_mixed_effects_anova_controlling_for_a/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jv0hcz",
    "title": "Extremely Wide confidence intervals",
    "content": "Hey guys! Hope you all have a blessed week.\nI‚Äôve been running some logistic and multinomial regressions in R, trying to analyse a survey I conducted a few months back. Unfortunately I ran into a problem. In multiple regressions (mainly multinomials), ORs as well as CIs are extremely wide, and some range from 0 to inf. How should I proceed? I feel kinda stucked. Is there any way to check for multicollinearity or perfect separation in multinomial regressions? Results from the questionnaire seemed fine, with adequate respondents in each category. Any insight would be of great assistance!!! Thank you in advance. Have a great end of the week.",
    "author": "Intrepid-Star7944",
    "timestamp": "2025-04-09T01:01:26",
    "url": "https://reddit.com/r/rstats/comments/1jv0hcz/extremely_wide_confidence_intervals/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1juvbzy",
    "title": "Beginner Predictive Model Feedback/Analysis",
    "content": "My predictive modeling folks, beginner here could use some feedback guidance. Go easy on me, this is my first machine learning/predictive model project and I had very basic python experience before this.\n\n\nI‚Äôve been working on a personal project building a model that predicts NFL player performance using full career, game-by-game data for any offensive player who logged a snap between 2017‚Äì2024. \n\nI trained the model using data through 2023 with XGBoost Regressor, and then used actual 2024 matchups ‚Äî including player demographics (age, team, position, depth chart) and opponent defensive stats (Pass YPG, Rush YPG, Points Allowed, etc.) ‚Äî as inputs to predict game-level performance in 2024.\n\nThe model performs really well for some stats (e.g., R¬≤ &gt; 0.875 for Completions, Pass Attempts, CMP%, Pass Yards, and Passer Rating), but others ‚Äî like Touchdowns, Fumbles, or Yards per Target ‚Äî aren‚Äôt as strong.\n\nHere‚Äôs where I need input:\n\n-What‚Äôs a solid baseline R¬≤, RMSE, and MAE to aim for ‚Äî and does that benchmark shift depending on the industry?\n\n-Could trying other models/a combination of models improve the weaker stats? Should I use different models for different stat categories (e.g., XGBoost for high-R¬≤ ones, something else for low-R¬≤)?\n\n-How do you typically decide which model is the best fit? Trial and error? Is there a structured way to choose based on the stat being predicted?\n\n-I used XGBRegressor based on common recommendations ‚Äî are there variants of XGBoost or alternatives you'd suggest trying? Any others you like better?\n\n-Are these considered ‚Äúgood‚Äù model results for sports data?\n\n-Are sports models generally harder to predict than industries like retail, finance, or real estate?\n\n-What should my next step be if I want to make this model more complete and reliable (more accurate) across all stat types?\n\n-How do people generally feel about manually adding in more intangible stats to tweak data and model performance? Example: Adding an injury index/strength multiplier for a Defense that has a lot of injuries, or more player‚Äôs coming back from injury, etc.? Is this a generally accepted method or not really utilized?\n\nAny advice, criticism, resources, or just general direction is welcomed.",
    "author": "ynwFreddyKrueger",
    "timestamp": "2025-04-08T19:30:03",
    "url": "https://reddit.com/r/rstats/comments/1juvbzy/beginner_predictive_model_feedbackanalysis/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jufv6a",
    "title": "How to sum across rows with misspelled data while keeping non-misspelled data",
    "content": "Let's say I have the following dataset:\n\n`temp &lt;- data.frame(ID = c(1,1,2,2,2,3,3,4,4,4),`\n\n`year = c(2023, 2024, 2023, 2023, 2024, 2023, 2024, 2023, 2024, 2024),`\n\n`tool = c(\"Mindplay\", \"Mindplay\", \"MindPlay\", \"Mindplay\", \"Mindplay\", \"Amira\", \"Amira\", \"Freckle\", \"Freckle\", \"Frekcle\"),`\n\n`avg_weekly_usage = c(14, 15, 11, 10, 20, 12, 15, 25, 13, 10))`\n\nMindplay, Amira, and Freckle are reading remediation tools schools use to help K-3 students improve reading. Data registered for Mindplay is sometimes spelled \"Mindplay\" and \"MindPlay\" even though it's data from the same tool; same with \"Freckle\" and \"Frekcle.\" I need to add avg\\_weekly\\_usage for the rows with the same ID and year but with the two different spellings of Mindplay and Freckle while keeping the avg\\_weekly\\_usage for all other rows with correctly spelled tool names. So for participant #2, year 2023, tool Mindplay average weekly usage should be 21 minutes and for #4, 2024, Freckle, average weekly usage should be 23 minutes like the image below.\n\nPlease help!\n\nhttps://preview.redd.it/wnqg8qpbpmte1.png?width=926&amp;format=png&amp;auto=webp&amp;s=85b03d757aa0e35014649a07c225eed6e6676e4d\n\n",
    "author": "IndividualPiece2359",
    "timestamp": "2025-04-08T08:08:25",
    "url": "https://reddit.com/r/rstats/comments/1jufv6a/how_to_sum_across_rows_with_misspelled_data_while/",
    "score": 3,
    "num_comments": 6,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1judpsx",
    "title": "Modeling Highly Variable Fisheries Discard Data ‚Äî Seeking Advice on GAMs, Interpretability, and Strategy Changes Over Time",
    "content": "Hi all , I‚Äôm working with highly variable and spatially dispersed discard data from a fisheries dataset (some hauls have zero discards, others a lot). I‚Äôm currently modeling it using **GAMs** with a **Tweedie or ZINB family**, incorporating spatial smoothers and factor interactions (e.g., `s(Lat, Lon, by = Period)`, `s(Depth)`, `s(DayOfYear, bs = \"cc\")`) and many other variables that are register by people on the boats.\n\nMy goal is to understand how fishing strategies have changed over three time periods, and to identify the most important variables that explain discards.  \nMy question is: what would be the right approach to model this data in depth while still keeping it understandable?\n\n  \nThanks!!!!\n\n",
    "author": "Santy7701",
    "timestamp": "2025-04-08T06:35:38",
    "url": "https://reddit.com/r/rstats/comments/1judpsx/modeling_highly_variable_fisheries_discard_data/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1juh4vi",
    "title": "How do I check a against a vector of thresholds?",
    "content": "I have two data sets: one with my actual data and one with thresholds for the variables I measured. I want to check if the value I measured is above the threshold stored in the second data set for all data columns, but I can't figure out how. I have tried to search online, but haven't found the answer to my problem yet. I would like to create new columns that show whether a value is equal to or less than the threshold or not.\n\nEdit: I figured it out, see comments.\n\n    df_1 &lt;- data.frame(ID = LETTERS[1:10], var1 = rnorm(10, 5, 1), var2 = rnorm(10, 1, 0.25), var3 = rnorm(10, 0.01, 0.02))\n    df_2 &lt;- data.frame(var1 = 3.0, var2 = 0.75, var3 = 0.001)",
    "author": "OscarThePoscar",
    "timestamp": "2025-04-08T09:01:06",
    "url": "https://reddit.com/r/rstats/comments/1juh4vi/how_do_i_check_a_against_a_vector_of_thresholds/",
    "score": 3,
    "num_comments": 6,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1juozgu",
    "title": "R Notebook issue when plotting multiple times from within a function",
    "content": "",
    "author": "wunderforce",
    "timestamp": "2025-04-08T14:22:44",
    "url": "https://reddit.com/r/rstats/comments/1juozgu/r_notebook_issue_when_plotting_multiple_times/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1junvps",
    "title": "Need code for PCA on R",
    "content": "So I have a dataset with a bunch of explanatory variables and about 1300 observations. The observations are grouped per site (6 sites) (and in 2 transects within the sites). One of the variables is  frequency, which is a factor variable with 2 levels (long and short)\n\nI want to create a PCA with all the explanatory variables and grouped per site. I also want a legend whereby the dots are coloured by long or short frequencies.\n\nTHANK YOU FOR YOUr heeeeelp",
    "author": "Aggravating_Young940",
    "timestamp": "2025-04-08T13:36:12",
    "url": "https://reddit.com/r/rstats/comments/1junvps/need_code_for_pca_on_r/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jtrivv",
    "title": "Non-convereged estimation windows when rolling estiamtion in rugarch",
    "content": "Please guys, I need help. First off, I'm not the best statistitian and definately don't have any coding skills, little to none code understatning. Anyway, I'm trying to do a rolling estimation for an eGARCH model using a rugarch library. I keep getting the error:\n\nObject contains non-converged estimation windows. Use resume method to re-estimate.  \n  \nI tried plenty of different solver options with no effect whatsoever. \n\nPlease guys, I need your help in solving this problem. I paste my code below:\n\n    install.packages(\"rugarch\")\n    install.packages(\"openxlsx\")\n    \n    library(rugarch)\n    library(parallel)\n    library(openxlsx)\n    library(dplyr)\n    \n    #Importing data\n    df &lt;- read.xlsx(\"dane_pelne.xlsx\", sheet = 1, colNames = TRUE, detectDates = TRUE)\n    \n    df$Data &lt;- as.Date(df$Data, format = \"%d.%m.%Y\")  # Date conversion\n    df$Cena &lt;- as.numeric(df$Cena)  # Conversion to numeric\n    \n    # 1. First subset: filtering date from 01.01.2015\n    df_podzbior1 &lt;- df %&gt;%\n      filter(Data &lt;= as.Date(\"2015-01-01\"))\n    df_podzbior1 &lt;- df_podzbior1 %&gt;%\n      slice(-1)\n    \n    #Adding dichotomic exogenous variables to model the outliers\n    df_podzbior1_ze_zmiennymi &lt;- df_podzbior1 %&gt;%\n      mutate(\n        xt1 = ifelse(Data == as.Date(\"2010-07-22\"), 1, 0),  # xt1 = 1 dla 22.07.2010\n        xt2 = ifelse(Data == as.Date(\"2011-10-17\"), 1, 0),  # xt2 = 1 dla 17.10.2011\n        xt3 = ifelse(Data == as.Date(\"2013-11-18\"), 1, 0)   # xt3 = 1 dla 18.11.2013\n      )\n    \n    stopy_1 &lt;- as.matrix(df_podzbior1_ze_zmiennymi$rt)\n    \n    ##################################################################\n    #   Finding the best ARMA(m,n) specification - yet withOUT GARCH #\n    ##################################################################\n    \n    arma.models1 &lt;- autoarfima(stopy_1, \n                               ar.max = 2, #maksymalny rzƒÖd op√≥≈∫nienia\n                               ma.max = 2, #maksymalny\n                               criterion = c(\"BIC\", \"AIC\"),\n                               method = \"full\",\n                               arfima = FALSE,\n                               include.mean = TRUE, \n                               distribution.model = \"norm\",\n                               cluster = NULL,\n                               external.regressors = cbind(df_podzbior1_ze_zmiennymi$xt1, df_podzbior1_ze_zmiennymi$xt2, df_podzbior1_ze_zmiennymi$xt3), \n                               solver = \"hybrid\",\n                               solver.control=list(),\n                               fit.control=list(),\n                               return.all = FALSE)\n    show(arma.models1)\n    head(arma.models1$rank.matrix)\n    arma.models1$fit\n    \n    ######Estimating eGARCH \n    specification1_egarch &lt;- ugarchspec(\n      variance.model = list(\n        model = \"eGARCH\", \n        garchOrder = c(1, 1), \n        submodel = NULL, \n        external.regressors = NULL, \n        variance.targeting = FALSE\n      ),\n      \n      mean.model = list(\n        armaOrder = c(1, 0), \n        include.mean = TRUE, \n        archm = FALSE, \n        archpow = 1, \n        arfima = FALSE, \n        external.regressors = cbind(df_podzbior1_ze_zmiennymi$xt1, df_podzbior1_ze_zmiennymi$xt2, df_podzbior1_ze_zmiennymi$xt3)\n      ), \n      \n      distribution.model = \"std\"\n    )\n\narma1.egarch11.std &lt;- ugarchfit(spec = specification1\\_egarch, data = stopy\\_1, solver = \"hybrid\")\n\n\n\n\\##### ROLLING ESTIMATION #####\n\ncl = makePSOCKcluster(10) #r√≥wnoleg≈Çy cluster z rozproszonymi obliczeniami\n\nroll = ugarchroll(specification1\\_egarch, stopy\\_1, n.start = 1000, refit.every = 100,\n\nrefit.window = \"moving\", solver = \"hybrid\", calculate.VaR = TRUE,\n\nVaR.alpha = c(0.01,0.05), cluster = cl, keep.coef = TRUE)\n\n\n\nshow(roll)\n\nroll = resume(roll, solver=\"lbfgs\")\n\nshow(roll)\n\nstopCluster(cl)",
    "author": "AlarmedQuail",
    "timestamp": "2025-04-07T10:51:06",
    "url": "https://reddit.com/r/rstats/comments/1jtrivv/nonconvereged_estimation_windows_when_rolling/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jtcxw4",
    "title": "Help understanding \"tuneLength\" in the caret library for elastic net parameter tuning?",
    "content": "I'm trying to find the optimal alpha &amp; lambda parameters in my elastic net model, and came across this github page¬†[https://daviddalpiaz.github.io/r4sl/elastic-net.html](https://daviddalpiaz.github.io/r4sl/elastic-net.html)\n\nIn the example from the page (code shown below) it sets tuneLength = 10, &amp; describes it as such:\n\n\"by setting¬†`tuneLength = 10`, we will search 10¬†Œ±¬†values and 10¬†Œª¬†values for each.¬†\". What exactly is mean by \"for each\", for each what? And how many different combinations and values of alpha and lambda will it search?\n\n    set.seed(42)\n    cv_5 = trainControl(method = \"cv\", number = 5)\n\n`hit_elnet_int = train(Salary ~ . ^ 2, data = Hitters, method = \"glmnet\", trControl = cv_5, tuneLength = 10)`",
    "author": "GhostGlacier",
    "timestamp": "2025-04-06T21:25:40",
    "url": "https://reddit.com/r/rstats/comments/1jtcxw4/help_understanding_tunelength_in_the_caret/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jsvc7y",
    "title": "Am unfamiliar with R and statistics in general - need help with ANOVAs!",
    "content": "So I'm currently using R to perform statistical analysis for an undergrad project. I'm essentially applying 3 different treatments to the subjects (24 total for each treatment, n=72) and recording different measures over a period of a few days. \n\nTwo of my measures are heart rate and body length, so the ANOVAs was relatively simple to do (since heart rate and body length represent the quantitative variable and the treatment represents the categorical variable). However, my other 2 measures are yes/no (abnormality, survival), so aren't really quantitative. \n\nWith this in mind, what is the best way to go about seeing if there is a statistically signficant relationship between my treatments and the yes/no measures? Can I adapt the data to fit an ANOVA (quantifying the numbers of Yes's for abnormality, number of No's for survival)? How do I make sure I'm relating my analysis to the day of measurement or subject number?\n\n  \nThanks in advance!",
    "author": "JuanFran21",
    "timestamp": "2025-04-06T07:27:23",
    "url": "https://reddit.com/r/rstats/comments/1jsvc7y/am_unfamiliar_with_r_and_statistics_in_general/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jt4wac",
    "title": "hybrid method of random forest survival and SVM model",
    "content": "hi. I want to do a hybrid method of random forest survival and SVM model in R software . does anyone have the R codes for running the hybrid one to help me? thanks in advanced",
    "author": "Conscious_Many_8701",
    "timestamp": "2025-04-06T14:23:32",
    "url": "https://reddit.com/r/rstats/comments/1jt4wac/hybrid_method_of_random_forest_survival_and_svm/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1js269n",
    "title": "Mixed models: results from summary() and anova() in separate tables?",
    "content": "Is it common to present model results from summary() and anova() Type III table from the same model in two tables for scientific papers? Alternatively incorporate results for both in one table (seems like it would make for a lot of columns‚Ä¶). Or just one of them? What do people in here do? ",
    "author": "MountainImportance69",
    "timestamp": "2025-04-05T05:18:59",
    "url": "https://reddit.com/r/rstats/comments/1js269n/mixed_models_results_from_summary_and_anova_in/",
    "score": 4,
    "num_comments": 11,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jrkwr2",
    "title": "R in Maine: Connecting Ecologists, Medical Researchers, and Data Scientists",
    "content": "Donald Szlosek, the MaineR Users Group organizer, recently spoke with the R Consortium about the group‚Äôs transition from a city-based meetup to a statewide community and its efforts to engage a diverse audience. Donald shared insights into organizing events, the challenges of hybrid formats, and the shift toward virtual workshops based on community feedback. \n\nHe also highlighted his work in real-world evidence studies, where R is critical in causal inference and machine learning validation.\n\n[https://r-consortium.org/posts/r-in-maine-connecting-ecologists-medical-researchers-and-data-scientists/](https://r-consortium.org/posts/r-in-maine-connecting-ecologists-medical-researchers-and-data-scientists/)",
    "author": "jcasman",
    "timestamp": "2025-04-04T12:50:17",
    "url": "https://reddit.com/r/rstats/comments/1jrkwr2/r_in_maine_connecting_ecologists_medical/",
    "score": 11,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jrp59r",
    "title": "[Question] [Rstudio] linear regression model standardised residuals",
    "content": "",
    "author": "Big-Ad-3679",
    "timestamp": "2025-04-04T15:55:58",
    "url": "https://reddit.com/r/rstats/comments/1jrp59r/question_rstudio_linear_regression_model/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jpotba",
    "title": "Logit model for panel data (N = 100,000, T = 5) with pglm package - unable to finish in &gt;24h",
    "content": "Hi!\n\nI'm estimating a random-effects logit model for panel data using the pglm package. My data setup is as follows:\n\n* N = 100,000 individuals\n* T = 5 periods (monthly panel)\n* \\~10 explanatory variables\n\nThe estimation doesn't finish even after 24+ hours on my local machine (Dell XPS 13). I‚Äôve also tried running the code on Google Colab and Kaggle Notebooks, but still no success.\n\nHas anyone run into similar issues with pglm?\n\nAny help is much appreciated.\n\n  \nEDIT: forgot to add that \\~99% of the observations in the dependent variable are 0. That might explain why subsampling wasn't giving many clues about the model. Anyway, I reduced the number of quadrature points for the integral approximation from 5 (default) to 3, and it worked, both for logit and probit. ",
    "author": "lopreatozun",
    "timestamp": "2025-04-02T06:40:43",
    "url": "https://reddit.com/r/rstats/comments/1jpotba/logit_model_for_panel_data_n_100000_t_5_with_pglm/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jpk0rc",
    "title": "Wilcoxon ranked-sum variance assumption",
    "content": "Hi, \n\nPlease consider that I am a novice in the statistics field, so I apologize if this is very basic :) \n\nI am assessing intake of a dietary variable in two different groups (n = 700 in each). Because the variable is somewhat skewed, I opted for Wilcoxon ranked-sum. The test returned significant p-value, although the median is identical in the two groups. Box plotting the data shows that the 25p for one of the groups is quite a bit lower. \n\nI have two questions: \n\n1) Does this boxplot indicate that the assumption of equal variance is not fulfilled? And therefore that this test is inappropriate to perform? I performed both Levene and Fligner-Killeen test for homogeneity of variances, both returned very high p-values\n\n2) Would you agree with my interpretation, which is that while the median in men and women are identical, more women than men have a lower intake of the dietary variable in question?\n\n  \nThank you in advance for any input!\n\nhttps://preview.redd.it/xz6gvb0hxdse1.png?width=712&amp;format=png&amp;auto=webp&amp;s=51ff8a34ccea6f1f09f3f6b4e3a6dd2082e253a5\n\n",
    "author": "Practical-Ladder7304",
    "timestamp": "2025-04-02T01:53:06",
    "url": "https://reddit.com/r/rstats/comments/1jpk0rc/wilcoxon_rankedsum_variance_assumption/",
    "score": 4,
    "num_comments": 30,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jpyx5c",
    "title": "Cards Question on my data test",
    "content": "Hi guys i had a question on a data mangment test recently and it was asking to find the probability of a poker hand with not all cards being the same suits and it being in numerical order with the ace being high or low.  I wasnt fully sure how to do it does anyone know how?",
    "author": "Successful_Map6282",
    "timestamp": "2025-04-02T13:27:52",
    "url": "https://reddit.com/r/rstats/comments/1jpyx5c/cards_question_on_my_data_test/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jp0xgp",
    "title": "oRm: An Object-Relational Mapping (ORM) Framework for R",
    "content": "For those familiar with sqlalchemy, this is my R interpretation thereof. I had a simple shiny app that was going to take some user input here and there and store in a backend db. But I wanted a more stable, repeatable way to work with the data models. So I wrote oRm to define tables, manage connections, and perform CRUD on records. The link will take you to the pkgdown site, but if you're curious for  quick preview of what it all looks like, see below:\n\nhttps://kent-orr.github.io/oRm/index.html\n\n\n    library(oRm)\n    \n    engine &lt;- Engine$new(\n      drv = RSQLite::SQLite(),\n      dbname = \":memory:\",\n      persist = TRUE\n    )\n    \n    User &lt;- engine$model(\n      \"users\",\n      id = Column(\"INTEGER\", primary_key = TRUE, nullable = FALSE),\n      organization_id = ForeignKey(\"INTEGER\", references = \"organizations.id\"),\n      name = Column(\"TEXT\", nullable = FALSE),\n      age = Column(\"INTEGER\")\n    )\n    \n    Organization &lt;- engine$model(\n      \"organizations\",\n      id = Column(\"INTEGER\", primary_key = TRUE, nullable = FALSE),\n      name = Column(\"TEXT\", nullable = FALSE)\n    )\n    \n    Organization$create_table()\n    User$create_table()\n    \n    User |&gt; define_relationship(\n      local_key = \"organization_id\",\n      type = \"belongs_to\",\n      related_model = Organization,\n      related_key = \"id\",\n      ref = \"organization\",\n      backref = \"users\"\n    )\n    \n    Organization$record(id = 1L, name = \"Widgets, Inc\")$create()\n    User$record(id = 1L, organization_id = 1L, name = \"Kent\", age = 34)$create()\n    User$record(id = 2L, organization_id = 1L, name = \"Dylan\", age = 25)$create()\n    \n    kent &lt;- User$read(id == 1, mode = \"get\")\n    kent$data$name\n    \n    org &lt;- kent$relationship(\"organization\")\n    org$data$name\n    \n    org$relationship(\"users\")  # list of user records",
    "author": "binarypinkerton",
    "timestamp": "2025-04-01T10:09:22",
    "url": "https://reddit.com/r/rstats/comments/1jp0xgp/orm_an_objectrelational_mapping_orm_framework_for/",
    "score": 16,
    "num_comments": 9,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jojip5",
    "title": "My workplace is transitioning our shared programs from closed- to open-source. Some want R (\"better for statistics\"), some want Python (\"better for big data\"). Should I push for R?",
    "content": "Management wants to transition from closed-source programming to either R or Python. Management doesn't care which one, so the decision is largely falling to us. Slightly more people on the team know R, but either way nearly everyone on the team will have to re-skill, as the grand majority know only the closed-source langauge we're leaving behind. \n\nThe main program we need to rewrite will be used by dozens of employees and involves connecting to our our data lake/data warehouse, pulling data, wrangling it, de-duplicating it, and adding hyperlinks to ID variables that take the user to our online system. The data lake/warehouse has millions of rows by dozens of columns. \n\nI prefer R because it's what I know. However, I don't want to lobby for something that turns out to be a bad choice years down the road. The big arguments I've heard so far for R are that it'll have fewer dependencies whereas the argument for Python is that it'll be \"much faster\" for big data. \n\nAm I safe to lobby for R over Python in this case?",
    "author": "coip",
    "timestamp": "2025-03-31T18:09:22",
    "url": "https://reddit.com/r/rstats/comments/1jojip5/my_workplace_is_transitioning_our_shared_programs/",
    "score": 113,
    "num_comments": 85,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jp3gg3",
    "title": "Help with PCA Analysis: Environmental and Performance Data",
    "content": "dummy\\_data &lt;- data.frame(\n\n  Hatchery = sample(LETTERS\\[1:6\\], 250, replace = TRUE),  # A-F\n\n  Fish\\_Strain = sample(c(\"aa\", \"bb\", \"cc\", \"dd\", \"ee\", \"ff\", \"gg\"), 250, replace = TRUE),  # aa-gg\n\n  Temperature = runif(250, 40, 65),  # Random values between 40 and 65\n\n  pH = runif(250, 6, 8),  # Random values between 6 and 8\n\n  Monthly\\_Length\\_Gain = runif(250, 0.5, 3.5),  # Example range for length gain\n\n  Monthly\\_Weight\\_Gain = runif(250, 10, 200),  # Example range for weight gain\n\n  Percent\\_Survival = runif(250, 50, 100),  # Survival rate between 50% and 100%\n\n  Conversion\\_Factor = runif(250, 0.8, 2.5),  # Example range for feed conversion\n\n  Density\\_Index = runif(250, 0.1, 1.5),  # Example range for density index\n\n  Flow\\_Index = runif(250, 0.5, 3.0),  # Example range for flow index\n\n  Avg\\_Temperature = runif(250, 40, 65)  # Random values for average temperature\n\n)\n\n\n\n\\# View first few rows\n\nhead(dummy\\_data)\n\n\n\nI am having some trouble with PCAs and wanted some advice. I have included some dummy data, that includes 6 fish hatcheries and 7 different strains of fish. The PCA is mostly being used for data reduction. The primary research question is ‚Äúdo different hatcheries or fish strains perform better than others?‚Äù I have a number of ‚Äúperformance‚Äù level variables (monthly length gain, monthly weight gain, percent survival, conversion factor) and ‚Äúenvironmental‚Äù level variables (Temperature, pH, density index, flow index). When I have run PCA in the past, the columns have been species abundance and the rows have represented different sampling sites. This one is a bit different and I am not sure how to approach it. Is it correct to run one (technically 2, one for hatchery and one for strain) with environmental and performance variables together in the dataset? Or is it better if I split out environmental and performance variables and run a PCA for each? How would you go about analyzing a multivariate dataset like this?\n\n  \nWith just the environmental data with \"hatcheries\" I get something that looks like this:\n\nhttps://preview.redd.it/h4rm5nbsr9se1.png?width=1269&amp;format=png&amp;auto=webp&amp;s=48cdcb5754f7ca828251857908575b70f68bccc7\n\n",
    "author": "fuzzytrout",
    "timestamp": "2025-04-01T11:49:29",
    "url": "https://reddit.com/r/rstats/comments/1jp3gg3/help_with_pca_analysis_environmental_and/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jo9513",
    "title": "Melbourne Users of R Network (MELBURN)",
    "content": "Lito P. Cruz, organizer of the Melbourne Users of R Network (MELBURN), speaks about the evolving R community in Melbourne, Australia, and the group‚Äôs efforts to engage data professionals across government, academia, and industry. \n\nFind out more!\n\n[https://r-consortium.org/posts/revitalizing-the-melbourne-users-of-r-network-hybrid-events-collaboration-and-the-future-of-r/](https://r-consortium.org/posts/revitalizing-the-melbourne-users-of-r-network-hybrid-events-collaboration-and-the-future-of-r/)",
    "author": "jcasman",
    "timestamp": "2025-03-31T10:41:35",
    "url": "https://reddit.com/r/rstats/comments/1jo9513/melbourne_users_of_r_network_melburn/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jn4tps",
    "title": "Free fake data resources needed for R and Python",
    "content": "This may have been asked and answered before, but does anyone know where I can find free fake data resources that mimic patient information, small and large data sets, to run statistical tools and models in R and Python? I am using it to practice. I am not in school right now. ",
    "author": "BlackHoles_NCC1701D",
    "timestamp": "2025-03-29T21:33:08",
    "url": "https://reddit.com/r/rstats/comments/1jn4tps/free_fake_data_resources_needed_for_r_and_python/",
    "score": 6,
    "num_comments": 26,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jmviwd",
    "title": "For those who have done thematic analysis on free text data, what is a good quantitative statistical analysis method for my thesis project?",
    "content": "I am a neuropsychology student working on my master thesis project on early symptoms in frontotemporal dementia (FTD). For this, I have collected free text data from patient dossiers of FTD patients, Alzheimer's patients and a control group. I have coded this free text data into (1) broader symptom categories (e.g. behavioural symptoms) and (2) more narrow subcategories (e.g. loss of empathy, loss of inhibition, apathy etc.) using ATLAS.ti. \n\nI am looking for tips/ideas for a good quantitative statistical analysis pipeline with the following goals in mind (A) identifying which symptom categories are present in a single patient and (B) identifying the severity of a symptom categorie based on the number of subcategories that are present in a patient and (C) finally comparing the three groups (FTD, AD and control).\n\nThanks in advance for your help! :)",
    "author": "wessel-rm",
    "timestamp": "2025-03-29T13:32:29",
    "url": "https://reddit.com/r/rstats/comments/1jmviwd/for_those_who_have_done_thematic_analysis_on_free/",
    "score": 15,
    "num_comments": 11,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jlfqx9",
    "title": "How do I subtract first and last values for each individual in a group of 4000 individuals?",
    "content": "Hi, very new to R and just getting to grips with it. I have a table of data of a measurement of individuals which has changed over time. The data is all in one table like so...\n\n|Measurement|Date|Individual|\n|:-|:-|:-|\n|3|2025|A|\n|2|2024|A|\n|1|2023|A|\n|4|2025|B|\n|3|2024|B|\n|2|2023|B|\n|1|2022|B|\n|2|2023|C|\n|1|2022|C|\n\nI want to calculate the change in measurement over time, so individual A would be 3-1=2.\n\nThe difficulty is there are varying numbers of datapoints for each individual and the data is all in this three column table. I'm struggling with how to do this on R. \n\nWould be grateful for your help!\n\n",
    "author": "throwawayfish72",
    "timestamp": "2025-03-27T15:29:46",
    "url": "https://reddit.com/r/rstats/comments/1jlfqx9/how_do_i_subtract_first_and_last_values_for_each/",
    "score": 5,
    "num_comments": 9,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jkuu5j",
    "title": "How to add a column to a dataframe conditionally?",
    "content": "Hi all,\n\nI have a dataset of Australian weather data with a variable for location that only has the township and not the state. I need to filter the data down to only one state.\n\nI have found another dataset with Australian towns and their corresponding state. How can I use this dataset to add the correct state to my first dataset?\n\n  \nThank you all!",
    "author": "kamonamarthh",
    "timestamp": "2025-03-26T20:29:04",
    "url": "https://reddit.com/r/rstats/comments/1jkuu5j/how_to_add_a_column_to_a_dataframe_conditionally/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jknra1",
    "title": "Leftjoin ecological data with synonyms as plant names.",
    "content": "Hello!\n\nSo i have a big traittable for my species data. I use left join to add data from another table to the table, but some of the species name have a separate column for the synonyms so there will be some missing data.\n\nIs there a way to add data to the original table, based on the synonym table ONLY if there is no data in the corresponding column?\n\nThis is the code I used:\n\n`traittable_3 &lt;- left_join(traittable_2,`\n\n`tolm_unique %&gt;% select(Accepted_synonym_The_plant_list, Tolm_kombineeritud),`\n\n`by = c(\"Accepted_SPNAME\" = \"Accepted_synonym_The_plant_list\"))`\n\nNow in traittable 3 and 2 there is another column from synonyms called \"Synonyms\". I want to add data to traittable\\_3 from tolm\\_unique  by = c(\"Synonyms\" = \"Accepted\\_synonym\\_The\\_plant\\_list\"), BUT ONLY if the data is missing in the traittable\\_3 column \"Tolm\\_kombineeritud\"\n\nHopefully you understand.",
    "author": "kooopaorav",
    "timestamp": "2025-03-26T14:50:41",
    "url": "https://reddit.com/r/rstats/comments/1jknra1/leftjoin_ecological_data_with_synonyms_as_plant/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jke2r6",
    "title": "üì¢ Call for Submissions! R/Medicine 2025 is looking for your insights!",
    "content": "Submit your talks, demos, and workshops on using R tools for health &amp; medicine. Share your work with the community!\n\n‚è≥ Deadline: April 11, 2025\n\n üîó Submit now: \n\n[https://rconsortium.github.io/RMedicine_website/Abstracts.html](https://rconsortium.github.io/RMedicine_website/Abstracts.html)\n\nSeeking abstracts for:\n\n* Lightning talks (10 min, Thursday June 12 or Friday June 13) Must pre-record and be live on chat to answer questions\n* Regular talks (20 min, Thursday June 12 or Friday June 13) Must pre-record and be live on chat to answer questions\n* Demos (1 hour demo of an approach or a package, Tuesday June 10 or Wednesday June 11) Done live, preferably interactive\n* Workshops (2-3 hours on a topic, Tuesday June 10 or Wednesday June 11) Detailed instruction on a topic, usually with a website and a repo, participants can choose to code along, include 5-10 min breaks each hour.",
    "author": "jcasman",
    "timestamp": "2025-03-26T08:11:19",
    "url": "https://reddit.com/r/rstats/comments/1jke2r6/call_for_submissions_rmedicine_2025_is_looking/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jkqqj8",
    "title": "ggplot2: Creating 3d barplots?",
    "content": "Does anyone know how to create a barplot with 3d bars? The plot would still have two variables; I just want the bars to be rectangular prisms.",
    "author": "tiramisufairy",
    "timestamp": "2025-03-26T17:01:41",
    "url": "https://reddit.com/r/rstats/comments/1jkqqj8/ggplot2_creating_3d_barplots/",
    "score": 2,
    "num_comments": 21,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jkdxfz",
    "title": "Where to put package state?",
    "content": "\nI'm writing a package for use in my company.\n\nUnder certain conditions, it should check a remote git repo for updates, and clone them if found (the check_repo() function).  I want it to do this in a lazy way, only when I call the do_the_thing() function, and at most once a day.\n\nHow should I trigger the check_repo() action? Using .onLoad was my first thought, but this immediately triggers the check and download, and I would prefer not to trigger it until needed.\n\nAnother option would be to set a counter of some kind, and check elapsed time at each run of do_the_thing(). So the first run would call check_repo(), and subsequent runs would not, until some time had passed.  If that is the right approach, where would you put the elapsed_time variable?\n\nI may be overthinking this! Thanks!\n",
    "author": "royksoft",
    "timestamp": "2025-03-26T08:04:57",
    "url": "https://reddit.com/r/rstats/comments/1jkdxfz/where_to_put_package_state/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jjj2z8",
    "title": "Stacked bar plot help",
    "content": "Hi, I'm making a stacked bar plot and just wanted to include the taxa that had the highest percentages. I have 2 sites (and 2 bars) so I need the top 10 from each site. I used head( 10) though it's only taking the overall top 10 and not the top 10 from each site. How do I fix this?  \n\n\nAny help is appreciated, here is my code:\n\n  \nggplot(head(mydata, 10), aes(x= Site, y= Totals, fill= ST))+\n\n  geom\\_bar(stat = \"identity\", position = \"fill\")  ",
    "author": "pickletheshark",
    "timestamp": "2025-03-25T06:15:05",
    "url": "https://reddit.com/r/rstats/comments/1jjj2z8/stacked_bar_plot_help/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jinc66",
    "title": "Announcing rixpress - build polyglott data science pipelines using R and Nix",
    "content": "",
    "author": "brodrigues_co",
    "timestamp": "2025-03-24T03:17:17",
    "url": "https://reddit.com/r/rstats/comments/1jinc66/announcing_rixpress_build_polyglott_data_science/",
    "score": 17,
    "num_comments": 5,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jj7kt9",
    "title": "Q, Rstudio, Logistic regression, burn1000 dataset from {aplore3} package",
    "content": "",
    "author": "Big-Ad-3679",
    "timestamp": "2025-03-24T18:10:40",
    "url": "https://reddit.com/r/rstats/comments/1jj7kt9/q_rstudio_logistic_regression_burn1000_dataset/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ji8gsc",
    "title": "Advice for what test to use in R for my analysis",
    "content": "I'm trying to analyze some data from a study I did over the past two years that sampled moths on five separate sub-sites in my study area. I basically have the five sub-sites and the total number of individuals I got for the whole study. I want to see if sub-site has a significant affect on the number of moths I got. Same for number of moth species.\n\nWhat would be the best statistical test in R to check this?",
    "author": "gdofseattle",
    "timestamp": "2025-03-23T13:10:43",
    "url": "https://reddit.com/r/rstats/comments/1ji8gsc/advice_for_what_test_to_use_in_r_for_my_analysis/",
    "score": 13,
    "num_comments": 9,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1ji7hz0",
    "title": "Multiple statistical tests give exact same results on different data",
    "content": "**UPDATE:** I have figured out the issue! Everything was correct...  As this is a non-parametric test (as my data did not meet assumptions), the test is done on the ranks rather than the data itself.  Friedman's is similar to a repeated measures anova.  My groups had no overlap, meaning all samples in group \"youngVF\" were smaller than their counterparts in group \"youngF\", etc.  So, the rankings were exactly the same for every sample.  Therefore, the test statistic was also the same for each pairwise comparison, and hence the p-values.  To test this, I manually changed three data points to make the rankings be altered for three samples, and my results reflected those changes.  \n\n\n\nI am running a Friedman's test (similar to repeated measures ANOVA) followed by post-hoc pair-wise analysis using Wilcox.  The code works fine, but I am concerned about the results.  (In case you are interested, I am comparing C-scores (co-occurrence patterns) across scales for many communities.)\n\nHere is the code:\n\n`friedman.test(y=scaleY$Cscore, groups=scaleY$Matrix, blocks=scaleY$Genome)`\n\nHere are the results:\n\n`data: scaleM$Cscore, scaleM$Matrix and scaleM$Genome`\n\n`Friedman chi-squared = 189, df = 3, p-value &lt; 2.2e-16`\n\nFollowed by the Wilcox test:\n\n`wilcox_test(Cscore~Matrix, data=scaleY, paired=T, p.adjust.method=\"bonferroni\")`\n\nHere are the results:\n\n`# A tibble: 6 √ó 9`\n\n`.y. group1 group2 n1 n2 statistic p p.adj p.adj.signif`\n\n`* &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;`\n\n`1 Cscore young_VF young_F 63 63 2016 5.29e-12 3.17e-11 ****`\n\n`2 Cscore young_VF young_M 63 63 2016 5.29e-12 3.17e-11 ****`\n\n`3 Cscore young_VF young_C 63 63 2016 5.29e-12 3.17e-11 ****`\n\n`4 Cscore young_F young_M 63 63 2016 5.29e-12 3.17e-11 ****`\n\n`5 Cscore young_F young_C 63 63 2016 5.29e-12 3.17e-11 ****`\n\n`6 Cscore young_M young_C 63 63 2016 5.29e-12 3.17e-11 ****`\n\nI am aware of the fact that R does not report p-values smaller than 2.2e-16.  My concern is that the Wilcox results are all exactly the same.  Is this a similar issue that R does not report p-values smaller than 2.2e-16?  Can I get more specific results?",
    "author": "SilverFire08",
    "timestamp": "2025-03-23T12:29:50",
    "url": "https://reddit.com/r/rstats/comments/1ji7hz0/multiple_statistical_tests_give_exact_same/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jhuqvl",
    "title": "Anlysis after propensity score matching",
    "content": "When using propensity score-related methods (such as PSM and PSW), especially after propensity score matching (PSM), for subsequent analyses like survival analysis with Cox regression, should I use standard Cox regression or a mixed-effects Cox model? How about KM curve or logrank test?",
    "author": "Amazing_Dig9478",
    "timestamp": "2025-03-23T01:34:42",
    "url": "https://reddit.com/r/rstats/comments/1jhuqvl/anlysis_after_propensity_score_matching/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jhm3my",
    "title": "Need help with making a bar graph!!!",
    "content": "",
    "author": "Numerous_Watch_3271",
    "timestamp": "2025-03-22T16:46:51",
    "url": "https://reddit.com/r/rstats/comments/1jhm3my/need_help_with_making_a_bar_graph/",
    "score": 1,
    "num_comments": 12,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jgtc1l",
    "title": "R/Medicine 2025 - Early Bird Pricing",
    "content": "üöÄ Early Bird Pricing for RMedicine 2025 is still available! üöÄ\n\nRegister now to save on your ticket and join the premier R conference health and medicine. Don't miss out‚Äîprices go up soon!\n\nüîó Register today: [https://rconsortium.github.io/RMedicine_website/Register.html](https://rconsortium.github.io/RMedicine_website/Register.html)\n\nSome info on R/Medicine\n\nThe R/Medicine conference provides a forum for sharing R based tools and approaches used to analyze and gain insights from health data. Conference workshops and demos provide a way to learn and develop your R skills, and to try out new R packages and tools. Conference talks share new packages, and successes in analyzing health, laboratory, and clinical data with R and Shiny, and an opportunity to interact with speakers in the chat during their pre-recorded talks.",
    "author": "jcasman",
    "timestamp": "2025-03-21T15:24:50",
    "url": "https://reddit.com/r/rstats/comments/1jgtc1l/rmedicine_2025_early_bird_pricing/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jg3mqg",
    "title": "Exploring geometa: An R Package for Managing Geographic Metadata",
    "content": "**geometa** provides an essential object-oriented data model in R, enabling users to efficiently manage geographic metadata. The package facilitates handling of ISO and OGC standard geographic metadata and their dissemination on the web, ensuring that spatial data and maps are available in an open, internationally recognized format. As a widely adopted tool within the geospatial community, geometa plays a crucial role in standardizing metadata workflows.\n\nSince 2018, the **R Consortium** has supported the development of geometa, recognizing its value in bridging metadata standards with R‚Äôs data science ecosystem. \n\nYou can try geometa yourself here: [CRAN ‚Äì geometa](https://cran.r-project.org/package=geometa).\n\nIn this interview, we speak with **Emmanuel Blondel**, the author of geometa, ows4R, geosapi, geonapi and geoflow‚Äîkey R packages for geospatial data management. \n\n[https://r-consortium.org/posts/exploring-geometa-an-r-package-for-managing-geographic-metadata/](https://r-consortium.org/posts/exploring-geometa-an-r-package-for-managing-geographic-metadata/)",
    "author": "jcasman",
    "timestamp": "2025-03-20T17:00:08",
    "url": "https://reddit.com/r/rstats/comments/1jg3mqg/exploring_geometa_an_r_package_for_managing/",
    "score": 28,
    "num_comments": 2,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jgmn8s",
    "title": "SEM: A single factor in Measurement Model does not significant",
    "content": "https://preview.redd.it/b8bvy3b9w2qe1.png?width=718&amp;format=png&amp;auto=webp&amp;s=63535fd68dc56ce1ae4114fc266e931d9b252852\n\n  \nIt is from a psychometric, built in reflective model, the CFA and other SEM fit are excellent except one factor  violates the significant level.\n\nAre there any solution for this issue? I try to make covariance among the factor but it got worse. ",
    "author": "Easy_Beginning_1595",
    "timestamp": "2025-03-21T10:39:03",
    "url": "https://reddit.com/r/rstats/comments/1jgmn8s/sem_a_single_factor_in_measurement_model_does_not/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jfl7sy",
    "title": "[Q] Adequate measurement for longitudinal data?",
    "content": "I am writing a research paper on the quality of debate in the German parliament and how this has changed with the entry of the AfD into parliament. I have conducted a computational analysis to determine the cognitive complexity (CC) of each speech from the last 4 election periods. In 2 of the 4 periods the AfD was represented in parliament, in the other two not. The CC is my outcome variable and is metrically scaled. My idea now is to test the effect of the AfD on the CC using an interaction term between a dummy variable indicating whether the AfD is represented in parliament and a variable indicating the time course.\nI am not sure whether a regression analysis is an adequate method, as the data is longitudinal. In addition, the same speakers are represented several times, so there may be problems with multicollinearity. What do you think? Do you know an adequate method that I can use in this case?",
    "author": "KokainKevin",
    "timestamp": "2025-03-20T02:48:35",
    "url": "https://reddit.com/r/rstats/comments/1jfl7sy/q_adequate_measurement_for_longitudinal_data/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jfjc0a",
    "title": "[Q] Need Assistance with Forest Plot",
    "content": "Hello I am conducting a meta-analysis exercise in R. I want to conduct only R-E model meta-analysis. However, my code also displays F-E model. Can anyone tell me how to fix it?\n\n  \n\\# Install and load the necessary package\n\ninstall.packages(\"meta\")  # Install only if not already installed\n\nlibrary(meta)\n\n\n\n\\# Manually input study data with association measures and confidence intervals\n\nstudy\\_names &lt;- c(\"CANVAS 2017\", \"DECLARE TIMI-58 2019\", \"DAPA-HF 2019\", \n\n\"EMPA-REG OUTCOME 2016\", \"EMPEROR-Reduced 2020\", \n\n\"VERTIS CV 2020 HF EF &lt;45%\", \"VERTIS CV 2020 HF EF &gt;45%\", \n\n\"VERTIS CV 2020 HF EF Unknown\")  # Add study names\n\n\n\nmeasure &lt;- c(0.70, 0.87, 0.83, 0.79, 0.92, 0.96, 1.01, 0.90)  # OR, RR, or HR from studies\n\nlower\\_CI &lt;- c(0.51, 0.68, 0.71, 0.52, 0.77, 0.61, 0.66, 0.53)  # Lower bound of 95% CI\n\nupper\\_CI &lt;- c(0.96, 1.12, 0.97, 1.20, 1.10, 1.53, 1.56, 1.52)  # Upper bound of 95% CI\n\n\n\n\\# Convert to log scale\n\nlog\\_measure &lt;- log(measure)\n\nlog\\_lower\\_CI &lt;- log(lower\\_CI)\n\nlog\\_upper\\_CI &lt;- log(upper\\_CI)\n\n\n\n\\# Calculate Standard Error (SE) from 95% CI\n\nSE &lt;- (log\\_upper\\_CI - log\\_lower\\_CI) / (2 \\* 1.96)\n\n\n\n\\# Perform meta-analysis using a Random-Effects Model (R-E)\n\nmeta\\_analysis &lt;- metagen(TE = log\\_measure, \n\nseTE = SE, \n\nstudlab = study\\_names, \n\nsm = \"HR\",  # Change to \"OR\" or \"RR\" as needed\n\nmethod.tau = \"REML\")  # Random-effects model\n\n\n\n\\# Generate a Forest Plot for Random-Effects Model only\n\nforest(meta\\_analysis, \n\nxlab = \"Hazard Ratio (log scale)\",\n\ncol.diamond = \"#2a9d8f\", \n\ncol.square = \"#005f73\",\n\nlabel.left = \"Favors Control\",\n\nlabel.right = \"Favors Intervention\",\n\nprediction = TRUE)\n\nIt displays common effect model, even though I already specified only R-E model:\n\nhttps://preview.redd.it/4lleoykqpspe1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=4c7f4b2262515ea703fe0f0444421a3dc98078ee\n\n  \n",
    "author": "Signal_Owl_6986",
    "timestamp": "2025-03-20T00:20:00",
    "url": "https://reddit.com/r/rstats/comments/1jfjc0a/q_need_assistance_with_forest_plot/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jfei9u",
    "title": "Need some assistance with a radial plot",
    "content": "",
    "author": "Professional_East281",
    "timestamp": "2025-03-19T19:15:09",
    "url": "https://reddit.com/r/rstats/comments/1jfei9u/need_some_assistance_with_a_radial_plot/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jesu8c",
    "title": "Finding correlation between Count Data and categorical variables",
    "content": "Greetings, I've been doing some statistics for my thesis, so I'm not a Pro and the solution shouldn't be  too complicated.\n\nI've got a dataset with several Count Data (Counts of individuals of several groups) as target variables. There's different predictors (continuous, binary, categorical (ordinal and nominal)). I wanna find out which predictors have an effect on my Count Data.  I don't wanna do a multivariate analysis. For some of the count data I fitted mixed models with a Random effect and the distribution seems normal. But some models I can't get to be normally distributed (I tried log and sqrt-transformation). I also have a lot of correlation going on between some of my predictor variables (but I'm not sure if I tested it correctly). \n\nSo my first question is: How do you deal with correlation between predictors in a linear mixed model?Do you just don't fit them together in one model or is there another way?\n\nMy second question is: What do I do with the models that don't follow a normal distribution? Am I just going to test for correlation (e.g. spearman, Kendall) for each predictor and the target variables without fitting models?\n\nThe third question is (and Ive seen a lot of posts about this topic): Which test is suitable for testing the correlation between a nominal variable with 3 or more levels and a continuous variable, if the target data isn't normally distributed?\n\nI've found answers that say I can use spearmans rho, if I just turn my predictor to as.numeric. Some say that's only possible with dichotomous variables. I also used X¬≤ and Fishers-Test between predictor variables that were both nominal, and between variables where one was continuous and one was nominal. \n\nAs you can see I'm quite confused because of the different answers I found... Maybe someone can help  to get my thoughts organized :) Thanks in advance!",
    "author": "Historical_Local237",
    "timestamp": "2025-03-19T02:12:37",
    "url": "https://reddit.com/r/rstats/comments/1jesu8c/finding_correlation_between_count_data_and/",
    "score": 10,
    "num_comments": 16,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jfbvqr",
    "title": "HELP does my R code actually answer my research questions for my psych project *crying*",
    "content": "Hii I'm doing a project about an intervention predicting behaviours over time and I need human assistance (chatGPT works, but keep changing its mind rip). Basically want to know if my code below *actually* answers my research questions...\n\n**MY RESEARCH QUESTIONS:**\n\n1. testing whether an intervention improves mindfulness when compared to a control group\n2. testing whether baseline mindfulness predicts overall behaviour improvement\n\n**HOW I'M TESTING**\n\n1st Research Q: Linear Mixed Modelling (LMM)\n\n2nd Research Q: Multi-level modelling (MLM)\n\n**MY DATASET COLUMNS:**\n\n(see image)\n\nhttps://preview.redd.it/jrnqr5nzlqpe1.png?width=575&amp;format=png&amp;auto=webp&amp;s=1b470e2a9cd58b846821289260f2cb38c8b74e68\n\n**MY CODE** (with my #comments to help me understand wth I'm doing)\n\n\\## STEP 1: GETTING EVERYTHING READY IN R\n\nlibrary(tidyverse)\n\nlibrary(lme4)\n\nlibrary(mice)\n\nlibrary(mitml)\n\nlibrary(car)\n\nlibrary(readxl)\n\n\\# Setting the working directory\n\nsetwd(\"location\\_on\\_my\\_laptop\")\n\n\\# Loading dataset\n\ndf &lt;- read\\_excel(\"Mindfulness.xlsx\")\n\n\\## STEP 2: PREPROCESSING THE DATASET\n\n\\# Convert missing values (coded as 999) to NA\n\ndf\\[df == 999\\] &lt;- NA\n\n\\# Convert categorical variables to factors\n\ndf$Condition &lt;- as.factor(df$Condition)\n\ndf$Dropout\\_T1 &lt;- as.factor(df$Dropout\\_T1)\n\ndf$Dropout\\_T2 &lt;- as.factor(df$Dropout\\_T2)\n\n\\# Reshaping to long format\n\ndf\\_long &lt;- pivot\\_longer(df, cols = c(T0, T1, T2), names\\_to = \"Time\", values\\_to = \"Mind\\_Score\")\n\n\\# Add a unique ID column\n\ndf\\_long$ID &lt;- rep(1:(nrow(df\\_long) / 3), each = 3)\n\n\\# Move ID to the first column\n\ndf\\_long &lt;- df\\_long %&gt;% select(ID, everything())\n\n\\# Remove \"T\" and convert Time to numeric\n\ndf\\_long$Time &lt;- as.numeric(gsub(\"T\", \"\", df\\_long$Time))\n\n\\# Create Change Score for Aim 2\n\ndf\\_wide &lt;- pivot\\_wider(df\\_long, names\\_from = Time, values\\_from = Mind\\_Score)\n\ndf\\_wide$Change\\_T1\\_T0 &lt;- df\\_wide$\\`1\\` - df\\_wide$\\`0\\`\n\ndf\\_long &lt;- left\\_join(df\\_long, df\\_wide %&gt;% select(ID, Change\\_T1\\_T0), by = \"ID\")\n\n\\## STEP 3: APPLYING MULTIPLE IMPUTATION WITH M = 50\n\n\\# Creating a correct predictor matrix\n\npred\\_matrix &lt;- quickpred(df\\_long)\n\n\\# Dropout\\_T1 and Dropout\\_T2 should NOT be used as predictors for imputation\n\npred\\_matrix\\[, c(\"Dropout\\_T1\", \"Dropout\\_T2\")\\] &lt;- 0\n\n\\# Run multiple imputation\n\nimp &lt;- mice(df\\_long, m = 50, method = \"pmm\", predictorMatrix = pred\\_matrix, seed = 123)\n\n\\# Checking for logged events (should return NULL if correct)\n\nprint(imp$loggedEvents)\n\n\\## STEP 4: RUNNING THE LMM MODEL ON IMPUTED DATA\n\n\\# Convert to mitml-compatible format\n\nimp\\_mitml &lt;- as.mitml.list(lapply(1:50, function(i) complete(imp, i)))\n\n\\# Fit Model for Both Aims:\n\nfit\\_mitml &lt;- with(imp\\_mitml, lmer(Mind\\_Score \\~ Time \\* Condition + Change\\_T1\\_T0 + (1 | ID)))\n\n\\## STEP 5: POOLING RESULTS USING mitml\n\nsummary(testEstimates(fit\\_mitml, [extra.pars](http://extra.pars) = TRUE))\n\nThat's everything (I think??). Changed a couple of names here and there for confidentiality, so if something doesn't seem right, PLZ lmk and happy to clarify. Basically, **just want to know if the code i have right now actually answers my research questions**. I think it does, but I'm also not a stats person, so want people who are smarter than me to please confirm.\n\nAppreciate the help in advance! Your girl is actually losing it xxxx",
    "author": "LiviaQuaintrelle",
    "timestamp": "2025-03-19T17:05:45",
    "url": "https://reddit.com/r/rstats/comments/1jfbvqr/help_does_my_r_code_actually_answer_my_research/",
    "score": 0,
    "num_comments": 23,
    "upvote_ratio": 0.22,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1je5r3k",
    "title": "interactive R session on big('ish) data on aws cloud?",
    "content": "Currently at work I have a powerful linux box (40 cores, 1T ram), my typical workflow involve ingesting big'ish data sets (csv, binary files) into R through fread/custom binary file reader into data.table in an R interactive session (mostly command line, occasionally I use Rstudio free version). The session will remains open for days/weeks while I work on the data set, running data transformation, data exploration code, generating reports, summary stats, linear fitting, making ggplot on condensed version of the data, running some custom RCpp code on the data etc etc‚Ä¶, just basically pretty general data science exploration/research work‚Ä¶ The memory footprint of the R process will be hundreds of Gb (data.tables sized at a few hundreds millions rows), grow and shrink as I spawn multi-threaded processing on the dataset.  \n  \nI have been thinking about possibility of moving this kind of workflow onto aws cloud (company already using Aws) - what would some possible setups looks like? What would you use for data storage (currently csv, columnized binary data, on local disk of the box, but open to switch to other storage format if it makes sense...), how would you run an interactive R session for ingesting the data and running ad-hoc / interactive analysis on cloud? The cost of renting/leasing a high spec box 24x7x365 will actually be more expensive than owning a high-end physical box? Or there are smart ways to breakdown the dataset / compute so that I don‚Äôt need such a high spec box yet I can still run ad-hoc analysis on that size of data interactively pretty easily?",
    "author": "JuanManuelFangio32",
    "timestamp": "2025-03-18T07:05:54",
    "url": "https://reddit.com/r/rstats/comments/1je5r3k/interactive_r_session_on_bigish_data_on_aws_cloud/",
    "score": 7,
    "num_comments": 18,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jefu94",
    "title": "Guttmans Scalogram in R",
    "content": "Hello everyone,\n\nMy supervisor has asked me to make a scalogram of the theory of mind tasks within our dataset. I have 5 tasks on about 300 participants. For each row that belongs to a participant, the binary digits \"0\" and \"1\" implicate if the task is passed or failed by that participant. Now I need to make a scalogram.. It should resemble the image in this post. Can somebody pls help me! I tried a lot.\n\nKind regards,\n\nMe",
    "author": "Little_Yard9262",
    "timestamp": "2025-03-18T14:01:48",
    "url": "https://reddit.com/r/rstats/comments/1jefu94/guttmans_scalogram_in_r/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jc15zi",
    "title": "Tuning a Down-sampled Random Forest Model",
    "content": "I am trying to find the best way to tune a down-sampled random forest model in R. I generally don't use random forest because it is prone to overfitting, but I don't have a choice due to some other constraints in the data.\n\nI am using the package `randomForest`. It is for a species distribution model (presence/pseudoabsence response) and I am using regression rather than classification.\n\nI use the function `expand.grid()` to create a dataframe with all the combinations of settings for the function's parameters, including `sampsize`, `nodesize`, `maxnodes`, `ntree`, and `mtry`.\n\nWithin each run, I am doing a four-fold crossvalidation and recording the mean and standard deviation of the AUC for training and test data, the mean r-squared, and the mean of squared residuals.\n\n**Any idea on how can I use these statistics to select the parameters for a model that is both generalizable and fairly good at prediction?** My first thought was looking at parameters that had a difference between mean train AUC and mean test AUC, but I'm not sure if that is the best place to start or what.\n\n  \nThanks!",
    "author": "amoonand3balls",
    "timestamp": "2025-03-15T11:05:57",
    "url": "https://reddit.com/r/rstats/comments/1jc15zi/tuning_a_downsampled_random_forest_model/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jbrh79",
    "title": "Where to get coral cover datasets?",
    "content": "Hello! I'm currently working on a paper and needs detailed coral cover datasets of different coral reefs all over the word. (Specifically, weekly or monthly observations of these coral reefs). Does anyone know where to get them? I have emailed a few researchers and only a few provided the datasets. Some websites have datasets but usually it's just the Great Barrier Reef. It would be a great help if anyone could help. Thank you! :)",
    "author": "takoyaki_elle",
    "timestamp": "2025-03-15T02:35:13",
    "url": "https://reddit.com/r/rstats/comments/1jbrh79/where_to_get_coral_cover_datasets/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jam3w2",
    "title": "MAGA trigger word screener shinylive app",
    "content": "Made an app so you can see if your document contains any of the MAGA trigger words (\"diversity\", etc.) that you can't use in grant proposals, etc. Hopefully it makes proposal writing a little easier.\n\nIt's an entirely static site powered by web assembly to run everything in the browser. Built with **#Quarto**, **#rshiny**, **#shinylive**, **#Rstats**, and rage.\n\n[https://jhelvy.github.io/magaScreener/](https://jhelvy.github.io/magaScreener/)\n\nGIF of demo:\n\n[https://raw.githubusercontent.com/jhelvy/magaScreener/refs/heads/main/demo.gif](https://raw.githubusercontent.com/jhelvy/magaScreener/refs/heads/main/demo.gif)",
    "author": "jhelvy",
    "timestamp": "2025-03-13T13:38:35",
    "url": "https://reddit.com/r/rstats/comments/1jam3w2/maga_trigger_word_screener_shinylive_app/",
    "score": 209,
    "num_comments": 15,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "rstats",
    "post_id": "1jb1tit",
    "title": "Data Cleaning",
    "content": "I have a fairly large data set (12,000 rows). Problem I'm having is there are certain variables outside of the valid range. For example negative values for duration/tempo. I am already planning to perform imputation after, but am I better off removing the rows completely which would leave me with about 11,000 rows or replacing the invalid values as NA and include them in the imputation later on. Thanks",
    "author": "Upstairs_Mammoth9866",
    "timestamp": "2025-03-14T04:36:07",
    "url": "https://reddit.com/r/rstats/comments/1jb1tit/data_cleaning/",
    "score": 4,
    "num_comments": 14,
    "upvote_ratio": 0.67,
    "is_original_content": false
  }
]