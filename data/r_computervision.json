[
  {
    "subreddit": "computervision",
    "post_id": "1occ6og",
    "title": "We built LightlyStudio, an open-source tool for curating and labeling ML datasets",
    "content": "Over the past few years we built **LightlyOne**, which helped ML teams curate and understand large vision datasets. But we noticed that most teams still had to switch between different tools to label and QA their data.\n\nSo we decided to fix that.\n\n**LightlyStudio** lets you **curate, label, and explore multimodal data** (images, text, 3D) all in one place. It is open source, fast, and runs locally. You can even handle ImageNet-scale datasets on a laptop with 16 GB of RAM.\n\nBuilt with **Rust**, **DuckDB**, and **Svelte**. Under Apache 2.0 license.\n\nGitHub: [https://github.com/lightly-ai/lightly-studio](https://github.com/lightly-ai/lightly-studio)",
    "author": "igorsusmelj",
    "timestamp": "2025-10-21T05:55:25",
    "url": "https://reddit.com/r/computervision/comments/1occ6og/we_built_lightlystudio_an_opensource_tool_for/",
    "score": 56,
    "num_comments": 17,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ocd16r",
    "title": "Quantum-Enhanced Computer Vision: What Every ML Engineer Should Know",
    "content": "Read the full blog here: [https://farukalamai.substack.com/p/a-deep-dive-into-quantum-enhanced](https://farukalamai.substack.com/p/a-deep-dive-into-quantum-enhanced)",
    "author": "yourfaruk",
    "timestamp": "2025-10-21T06:31:00",
    "url": "https://reddit.com/r/computervision/comments/1ocd16r/quantumenhanced_computer_vision_what_every_ml/",
    "score": 42,
    "num_comments": 9,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ocgfos",
    "title": "Serverless Inference Providers Compared [2025]",
    "content": "",
    "author": "dat1-co",
    "timestamp": "2025-10-21T08:44:18",
    "url": "https://reddit.com/r/computervision/comments/1ocgfos/serverless_inference_providers_compared_2025/",
    "score": 27,
    "num_comments": 2,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oc3qje",
    "title": "Intrigued that I could get my phone to identify objects.. fully local",
    "content": "So I cobbled together quickly just this html page that used my Pixel 9‚Äôs camera feed, runs TensorFlow.js with the COCO-SSD model directly in-browser, and draws real-time bounding boxes and labels over detected objects. no cloud, no install, fully on-device!\n\nmaybe I'm a newbie, but I can't imagine the possibilities this opens to... all the possible personal use cases. any suggestions??",
    "author": "IntroductionSouth513",
    "timestamp": "2025-10-20T21:39:41",
    "url": "https://reddit.com/r/computervision/comments/1oc3qje/intrigued_that_i_could_get_my_phone_to_identify/",
    "score": 94,
    "num_comments": 30,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ocoutt",
    "title": "Symbol recognition",
    "content": "https://preview.redd.it/4xort1v82jwf1.png?width=3644&amp;format=png&amp;auto=webp&amp;s=ac79402a37b08c048566b064d4eac9fb49f18fe2\n\nHey everyone! Back in 2019, I tackled symbol recognition using OpenCV. It worked reasonably well but struggled when symbols were partially obscured. Now, seven years later, I'm revisiting this challenge.\n\nI've done research but haven't found a popular library specifically for symbol recognition or template matching. With OpenCV template matching you can just hand a PNG symbol and it‚Äôll try to match instances in the drawing to it. Is there any model that can do similar? These symbols are super basic in shape but the issue is overlapping elements.\n\nI've looked into vision-language models like QWEN 2.5, but I'm not clear on how to apply them to this use case. I've also seen references to YOLOv9, SAM2, CLIP, and DINOv2 for segmentation tasks, but it seems like these would require creating a training dataset and significant compute resources for each symbol.\n\nIs that really the case? Do I actually need to create a custom dataset and fine-tune a model just to find symbols in SVG documents, or are there more straightforward approaches available? Worst case I can do this, it‚Äôs just not very scalable given our symbols change frequently.\n\nAny guidance would be greatly appreciated!",
    "author": "Starxel",
    "timestamp": "2025-10-21T13:58:22",
    "url": "https://reddit.com/r/computervision/comments/1ocoutt/symbol_recognition/",
    "score": 3,
    "num_comments": 10,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oclojb",
    "title": "I converted the xView2 (xBD) satellite dataset into YOLO format ‚Äì 3 new public versions now on Roboflow",
    "content": "Hey everyone, I‚Äôve reworked the popular xView-2 (xBD) satellite damage-assessment dataset and made it YOLO-ready for anyone to use on Roboflow. All images are high‚Äêresolution (1024√ó1024) and I released 3 versions: v1 has a rebalanced train/valid/test split and combines ‚Äúno-subtype‚Äù + ‚Äúun-classified‚Äù into one class; v2 is the same dataset but grayscaled for simpler experiments; v3 includes data-augmentation to improve model generalization. The dataset is available here: [https://app.roboflow.com/emins-workspace/xview2\\_dataset\\_images-k8qdd/4](https://app.roboflow.com/emins-workspace/xview2_dataset_images-k8qdd/4?utm_source=chatgpt.com)",
    "author": "eminaruk",
    "timestamp": "2025-10-21T11:59:43",
    "url": "https://reddit.com/r/computervision/comments/1oclojb/i_converted_the_xview2_xbd_satellite_dataset_into/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ocjum7",
    "title": "FineVision: Opensource multi-modal dataset from Huggingface",
    "content": "[From: https:\\/\\/arxiv.org\\/pdf\\/2510.17269](https://preview.redd.it/eiuh6u5e5iwf1.png?width=632&amp;format=png&amp;auto=webp&amp;s=9cd3b454dd1b7d567bbc93830210b771c456f7fd)\n\n[Huggingface just released FineVision;](https://arxiv.org/pdf/2510.17269)\n\n&gt;\"Today, we release¬†**FineVision**, a new multimodal dataset with¬†**24 million samples**. We created FineVision by collecting over¬†**200 datasets**¬†containing¬†**17M images**,¬†**89M question-answer turns**, and¬†**10B answer tokens**, totaling¬†**5TB of high-quality data**. Additionally, we extensively processed all datasets to unify their format, clean them of duplicates and poor data, and rated all turns using 32B VLMs across 4 qualitative metrics with a score from 1-5 to enable the construction and study of individual training mixtures.\"\n\n  \nIn the paper they also discuss how they process the data and how they deal with near-duplicates and test-set decontamination.\n\nSince I never had the data or the compute to work with VLMs I was just wondering how or whether you could use this dataset in any normal computer vision projects.",
    "author": "koen1995",
    "timestamp": "2025-10-21T10:52:35",
    "url": "https://reddit.com/r/computervision/comments/1ocjum7/finevision_opensource_multimodal_dataset_from/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ocrsnb",
    "title": "Side walk question",
    "content": "Hey guys,\nJust wondering if anyone has any thoughts on how to make or knows of any available models good at detecting a sidewalk and the edges of it. Assuming something like this exists for delivery robots?\n\nThanks so much!",
    "author": "Techguy1423",
    "timestamp": "2025-10-21T15:56:26",
    "url": "https://reddit.com/r/computervision/comments/1ocrsnb/side_walk_question/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ocrsk7",
    "title": "Side walk question",
    "content": "Hey guys,\nJust wondering if anyone has any thoughts on how to make or knows of any available models good at detecting a sidewalk and the edges of it. Assuming something like this exists for delivery robots?\n\nThanks so much!",
    "author": "Techguy1423",
    "timestamp": "2025-10-21T15:56:20",
    "url": "https://reddit.com/r/computervision/comments/1ocrsk7/side_walk_question/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ocos5b",
    "title": "How to build a real-time anime filter like Snapchat‚Äôs?",
    "content": "Snapchat has a filter that turns your face into an anime-style character in real time (and also the background), not just a static frame. It tracks expressions, lip movement, and head motion incredibly smoothly, all while stylizing the video output live on mobile hardware.\n\nI‚Äôm curious about how something like that is built and what‚Äôs publicly feasible today.\n\nI‚Äôm not talking about post-processing (e.g., Stable Diffusion, EbSynth, etc.), but true live video inference where a user‚Äôs camera feed is stylized like Snapchat‚Äôs anime lens.\n\nDoes anyone here know:\n\n1. Whether any open-source or commercial SDKs can do this (e.g., DeepAR, Banuba, BytePlus Effects)?\n2. How they achieve that level of latency and coherence on mobile ‚Äî low flicker, consistent face identity, etc.?\n\ntldr; how could an indie team or SaaS replicate Snapchat‚Äôs anime filter using available frameworks or APIs?\n\nFor reference, here's how it appears: https://www.snapchat.com/lens/b8c89687c5194c3fb5db63d33eb04617\n\nAny insights, research papers, or SDK pointers would be hugely appreciated.",
    "author": "danlion02",
    "timestamp": "2025-10-21T13:55:32",
    "url": "https://reddit.com/r/computervision/comments/1ocos5b/how_to_build_a_realtime_anime_filter_like/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ocaun8",
    "title": "Experts, how did you come to satellite images?",
    "content": "Hello\n\nI've recently become interested in one of the computer vision fields ‚Äî satellite imagery.\nSo I‚Äôd like to ask you, experts:\nHow did you get into this field? What do you like the most about it, and what don‚Äôt you like? What are the main challenges? What kind of work do you usually do?\n\nI‚Äôd be really grateful if you could satisfy my curiosity.\n\nThanks for attention!\n",
    "author": "JustSovi",
    "timestamp": "2025-10-21T04:52:50",
    "url": "https://reddit.com/r/computervision/comments/1ocaun8/experts_how_did_you_come_to_satellite_images/",
    "score": 5,
    "num_comments": 16,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oclwci",
    "title": "Not so fast model recommendations",
    "content": "I am working on a project where I may only need to process 5-10 fps on GPU, but want best precision possible - any recommendations of different models I should try out? \n\nEdit: Object detection of small objects - rare class but have a 20k image dataset\n\nI suppose I‚Äôm wondering if there are object detection models slower than YOLO and rf-detr but fast enough to do 10fps and can get me better precision ",
    "author": "AIPoweredToaster",
    "timestamp": "2025-10-21T12:07:31",
    "url": "https://reddit.com/r/computervision/comments/1oclwci/not_so_fast_model_recommendations/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oca83y",
    "title": "I want to train a sr diffusion (super resolution)",
    "content": "If want to train a sr diffusion for my campus from scratch \nI don't know  how much gpu run time it take \nIf anyone know please tell which data set how many number of epochs and code I can use ?\n\nI'm trying to reduce the cost as much ad possible \n(I read all the research papers related diffusion , efficient way to train  diffusion and sr related papers )",
    "author": "ZookeepergameFlat744",
    "timestamp": "2025-10-21T04:19:34",
    "url": "https://reddit.com/r/computervision/comments/1oca83y/i_want_to_train_a_sr_diffusion_super_resolution/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oc2fjd",
    "title": "PR request is dead on Open3D. What can I do?",
    "content": "I have made a PR request a couple of weeks ago on Open3d. It was just an easy bug fix. But now my PR request is dead with no response, no commens, nothing. What can I do?\n\nContext: I came across the issue couple of times and I saw that someone has already opened an issue on github so I thought someone will take care of it. After waiting a while nobody fixed it so I spent a couple of weekends to dig deeper and came up with a working solution. I don't know if i did the right thing but having no response at all is confusing. Is there something I can do or is it normal for open source projects?\n\nLink to PR: https://github.com/isl-org/Open3D/pull/7343",
    "author": "lbluestone",
    "timestamp": "2025-10-20T20:29:46",
    "url": "https://reddit.com/r/computervision/comments/1oc2fjd/pr_request_is_dead_on_open3d_what_can_i_do/",
    "score": 9,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oc77qa",
    "title": "Need Advice: Choosing Camera Setup for Cable Anomaly Detection System",
    "content": "I‚Äôm developing a **visual anomaly detection system** for **cables** roughly the size of a **pen** in circumference. The goal is to detect **defects at the cable head** ‚Äî things like scratches, deformities, or small misalignments. During data collection and inference, multiple cameras (probably 2-3 from different angles) will capture high-quality images of cable heads. The images will be used to train an **unsupervised anomaly detection model** (e.g., autoencoder-based). I need **very clear, consistent lighting and image sharpness** because tiny surface defects matter.\n\n  \nDuring Deployment, the camera will continuously capture new cable head images. These images will be sent to a **GPU server** running the trained model. The server will output a defect score or anomaly mask. That signal will be sent to **two robot arms** that perform the sorting/filtering operation ( I am not concerned about this step as it is not my part).  \nI‚Äôve **never worked directly with industrial cameras** or imaging hardware before.  \nSo right now, I‚Äôm trying to figure out what **camera hardware and setup details** I need to get right early on to avoid bottlenecks later.\n\n  \nWhat I think I need:  \n**Resolution:** it should be enough to capture fine surface details on small cable heads ( roughly 1-2 cm diameter).  \n**Lens Type:** Should I go with **macro lenses** or just high-resolution lenses with adjustable focus? I‚Äôll probably mount the cameras very close to the object (a few centimeters away).  \n**Camera Interface:** USB3, GigE, or something else? I‚Äôll send images to a **GPU server** ‚Äî is bandwidth going to be a problem if I scale to multiple cameras?\n\nIf you‚Äôve worked on **visual inspection** systems ‚Äî especially small-object or manufacturing defect detection ‚Äî I‚Äôd love to hear what to watch out for, what mistakes to avoid, and what specific camera brands/setups worked best for you.\n\nThanks in advance!\n\n",
    "author": "Ordinary_Pineapple27",
    "timestamp": "2025-10-21T01:14:24",
    "url": "https://reddit.com/r/computervision/comments/1oc77qa/need_advice_choosing_camera_setup_for_cable/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ocdv3u",
    "title": "Pen tablet for image annotation: yay or nay?",
    "content": "Hey there, guys! I have a CV novice question: for my project I have to annotate several hundreds of images containing organic shapes. The quality of the images is not great. I use Label Studio with Segment Anything Model, yet each image needs some (actually quite a lot) manual tweaking with the brush tool. This \"colouring book\" activity is very laborious and my eyes start to hurt quickly after annotating several images in one batch. So I was wondering, if getting a pen tablet (like Wacom or similar) could speed things up and reduce fatigue. Why or why not? ",
    "author": "rows_and_columns_me",
    "timestamp": "2025-10-21T07:04:18",
    "url": "https://reddit.com/r/computervision/comments/1ocdv3u/pen_tablet_for_image_annotation_yay_or_nay/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1occ8pr",
    "title": "Where can I learn YOLOv8 and how to apply it in a mobile app?",
    "content": "Hi everyone! üëã  \nI‚Äôm a college student currently working on our thesis.\n\nOur project involves using YOLOv8 for real-time object detection, and we plan to deploy it in a mobile application that provides audio feedback to help visually impaired users identify objects around them.\n\nI‚Äôve already read a bit about YOLOv8, but I‚Äôm still unsure where to start learning how to:\n\n* Train a custom YOLOv8 model (with my own dataset), and\n* Integrate or deploy it on a mobile platform (like Android or iOS).\n\nCould anyone recommend tutorials, courses, GitHub projects, or documentation that explain the full process from training to mobile deployment?  \nAny advice or guidance from those who‚Äôve done something similar would be super helpful. üôè\n\nThanks in advance!",
    "author": "justiinbriiza",
    "timestamp": "2025-10-21T05:57:53",
    "url": "https://reddit.com/r/computervision/comments/1occ8pr/where_can_i_learn_yolov8_and_how_to_apply_it_in_a/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1obuo38",
    "title": "Physical AI Data Pipelines with NVIDIA Omniverse NuRec, Cosmos and FiftyOne",
    "content": "Register for the Nov 5 Zoom: [https://link.voxel51.com/physical-ai-launch-reddit](https://link.voxel51.com/physical-ai-launch-reddit)",
    "author": "sickeythecat",
    "timestamp": "2025-10-20T14:38:52",
    "url": "https://reddit.com/r/computervision/comments/1obuo38/physical_ai_data_pipelines_with_nvidia_omniverse/",
    "score": 16,
    "num_comments": 1,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oc95su",
    "title": "Create dashboards for industrial applications. What GUI library to use?",
    "content": "Hi all, We are creating custom machine vision solutions for various industries. (Packaging, bottling etc) and I need to create dashboards for the same.   \nIt will be displaying various analytics, current count, production rate etc.   \nWhat GUI library can I use with python/C++ for using with it devices like a regular desktop/ embedded systems and single board computers (Like raspberry and Nvidia Jetson)? (Windows/ Linux).  \nWe'll also be using industrial cameras like basler, HIKvision etc for getting the input feed. ",
    "author": "atmadeep_2104",
    "timestamp": "2025-10-21T03:19:06",
    "url": "https://reddit.com/r/computervision/comments/1oc95su/create_dashboards_for_industrial_applications/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oc8i70",
    "title": "[Question] Difficulty Segmenting White LEGO Bricks on White Background with OpenCV",
    "content": "",
    "author": "Gummmo-www",
    "timestamp": "2025-10-21T02:39:20",
    "url": "https://reddit.com/r/computervision/comments/1oc8i70/question_difficulty_segmenting_white_lego_bricks/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1obqt12",
    "title": "Are Image-Text-to-Text models becoming the next big AI?",
    "content": "I‚Äôve been checking the trending models lately and it‚Äôs crazy how many of them are Image-Text-to-Text. Out of the top 7 right now, 5 fall in that category (PaddleOCR-VL, DeepSeek-OCR, Nanonets-OCR2-3B, Qwen3-VL, etc). DeepSeek even dropped their own model today.\n\nPersonally, I have been playing around with a few of them (OCR used to be such a pain earlier, imo) and the jump in quality is wild. They‚Äôre getting better at understanding layout, handwriting, tables data.  \n(ps: My earlier fav was Mistral OCR)\n\nIt feels like companies are getting quite focused on multimodal systems that can understand and reason over images directly.\n\nthoughts?",
    "author": "Full_Piano_3448",
    "timestamp": "2025-10-20T12:14:44",
    "url": "https://reddit.com/r/computervision/comments/1obqt12/are_imagetexttotext_models_becoming_the_next_big/",
    "score": 13,
    "num_comments": 2,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oc8102",
    "title": "Indoor fire detection dataset",
    "content": "Hello everyone i need good indoor fire detection dataset to train yolov11lL on it",
    "author": "Ok-Meat9548",
    "timestamp": "2025-10-21T02:08:22",
    "url": "https://reddit.com/r/computervision/comments/1oc8102/indoor_fire_detection_dataset/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oc81r3",
    "title": "Fire detection dataset",
    "content": "Hello everyone i need fired3tection dataset to train yolov11 with it",
    "author": "Ok-Meat9548",
    "timestamp": "2025-10-21T02:09:41",
    "url": "https://reddit.com/r/computervision/comments/1oc81r3/fire_detection_dataset/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oc4eld",
    "title": "YOLOv11 question",
    "content": "I am new to computer vision and have messed around with call of duty detections. I am trying to figure out a way that I could label the models as teammate or enemy and have it use the name tag color to either identify the operator as an enemy or the teammate. That or use the name tag color as teammate and choose to ignore that in the detections. Any help on how to do this would be greatly appreciated. Thank you!",
    "author": "Basic_Palpitation142",
    "timestamp": "2025-10-20T22:17:32",
    "url": "https://reddit.com/r/computervision/comments/1oc4eld/yolov11_question/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1obloe0",
    "title": "Last week in Multimodal AI - Vision Edition",
    "content": "I curate a weekly newsletter on multimodal AI. Here are the vision-related highlights from last week:\n\nCtrl-VI - Controllable Video Synthesis via Variational Inference  \n‚Ä¢Handles text prompts, 4D object trajectories, and camera paths in one system.  \n‚Ä¢Produces diverse, 3D-consistent videos using variational inference.  \n‚Ä¢[Paper](https://arxiv.org/abs/2510.07670)¬†\n\nhttps://reddit.com/link/1obloe0/video/6pnmadewtiwf1/player\n\nFlashWorld - High-Quality 3D Scene Generation in Seconds  \n‚Ä¢Generates 3D scenes from text or images in 5-10 seconds with direct 3D Gaussian output.  \n‚Ä¢Combines 2D diffusion quality with geometric consistency for fast vision tasks.  \n‚Ä¢[Project Page](https://imlixinyang.github.io/FlashWorld-Project-Page/)¬†|¬†[Paper](https://arxiv.org/abs/2510.13678)¬†|¬†[GitHub](https://github.com/imlixinyang/FlashWorld)¬†|¬†[Announcement](https://x.com/TencentHunyuan/status/1979087059198324805)\n\nTrace Anything - Representing Videos in 4D via Trajectory Fields  \n‚Ä¢Maps video pixels to continuous 3D trajectories in a single pass.  \n‚Ä¢State-of-the-art for trajectory estimation and motion-based video search.  \n‚Ä¢[Project Page](https://trace-anything.github.io/)¬†|¬†[Paper](https://huggingface.co/papers/2510.13802)¬†|¬†[Code](https://github.com/ByteDance-Seed/TraceAnything)¬†|¬†[Model](https://huggingface.co/depth-anything/trace-anything)¬†\n\nhttps://reddit.com/link/1obloe0/video/vc7h5b4ytiwf1/player\n\nVIST3A - Text-to-3D by Stitching Multi-View Reconstruction  \n‚Ä¢Unifies video generators with 3D reconstruction via lightweight linear mapping.  \n‚Ä¢Generates 3D representations from text without 3D training labels.  \n‚Ä¢[Project Page](https://gohyojun15.github.io/VIST3A/)¬†|¬†[Paper](https://arxiv.org/abs/2510.13454)\n\nhttps://reddit.com/link/1obloe0/video/q0ny57f1uiwf1/player\n\nVirtually Being - Camera-Controllable Video Diffusion  \n‚Ä¢Ensures multi-view character consistency and 3D camera control using 4D Gaussian Splatting.  \n‚Ä¢Ideal for virtual production workflows with vision focus.  \n‚Ä¢[Project Page](https://eyeline-labs.github.io/Virtually-Being/)¬†|¬†[Paper](https://arxiv.org/pdf/2510.14081)\n\nhttps://reddit.com/link/1obloe0/video/pysr9pr3uiwf1/player\n\nPaddleOCR VL 0.9B - Multilingual VLM for OCR  \n‚Ä¢Efficient 0.9B parameter model for vision-based OCR across languages.  \n‚Ä¢[Hugging Face](https://huggingface.co/PaddlePaddle/PaddleOCR-VL)¬†|¬†[Paper](https://arxiv.org/pdf/2510.14528)\n\nSee the full newsletter for more demos, papers, more): [https://thelivingedge.substack.com/p/multimodal-monday-29-sampling-smarts](https://thelivingedge.substack.com/p/multimodal-monday-29-sampling-smarts)",
    "author": "Vast_Yak_4147",
    "timestamp": "2025-10-20T08:26:41",
    "url": "https://reddit.com/r/computervision/comments/1obloe0/last_week_in_multimodal_ai_vision_edition/",
    "score": 8,
    "num_comments": 1,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oboaw3",
    "title": "[LLM model-Tool Auto Labeling]",
    "content": "Currently I am using CVAT to host a web for labeling data about traffic vehicles. However, this is quite manual and time-consuming because the number of object boxes that need to be labeled is very large, so I am looking for a tool or application that integrates LLM models + uses prompts to save time on labeling. Please share if you have any suggestions",
    "author": "Vol1801",
    "timestamp": "2025-10-20T10:30:23",
    "url": "https://reddit.com/r/computervision/comments/1oboaw3/llm_modeltool_auto_labeling/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oawnq3",
    "title": "Local image features in real-time, 1080p, on a laptop iGPU (Vulkan)",
    "content": "",
    "author": "TinySpidy",
    "timestamp": "2025-10-19T11:11:15",
    "url": "https://reddit.com/r/computervision/comments/1oawnq3/local_image_features_in_realtime_1080p_on_a/",
    "score": 85,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ob5fkl",
    "title": "RF-DETR vs YOLOV11",
    "content": "https://preview.redd.it/wyof47gsr5wf1.png?width=701&amp;format=png&amp;auto=webp&amp;s=2b281de02b9377213c29b4f30f8efbe9d40f6578\n\nHi everyone,\n\nReading this article inspired me to make a practical comparison between yolov11 and rf-detr, I didn‚Äôt wanted to compare them quantitively, just how to use them in code. [Link](https://farukalamai.substack.com/p/rf-detr-vs-yolov12-a-comprehensive) \n\nIn this tutorial I showed how you do inference with these models. I showed how you can fine-tune one on a synthetic dataset. And how you can visualize some of these results.\n\nI am thinking about just adding some more things to this notebook, maybe batch inference or just comparing how much vram/compute both of these models use. What do you guys think?\n\n[Tutorial](https://www.kaggle.com/code/koenbotermans/computer-vision-rf-detr-vs-yolov11)\n\nEdit: added the correct link",
    "author": "koen1995",
    "timestamp": "2025-10-19T17:10:36",
    "url": "https://reddit.com/r/computervision/comments/1ob5fkl/rfdetr_vs_yolov11/",
    "score": 19,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oba5si",
    "title": "What happened to Kili Technology's datasets on HuggingFace?",
    "content": "[https://huggingface.co/Kili/datasets](https://huggingface.co/Kili/datasets)\n\n[https://huggingface.co/kili-technology](https://huggingface.co/kili-technology)\n\nTheir public open datasets are just gone?\n\n[https://kili-technology.com/datasets](https://kili-technology.com/datasets) \n\nI also checked their websites but there are none?",
    "author": "KingsmanVince",
    "timestamp": "2025-10-19T21:05:36",
    "url": "https://reddit.com/r/computervision/comments/1oba5si/what_happened_to_kili_technologys_datasets_on/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1obk9w1",
    "title": "Hi, In which sub can I talk about my computer graphics YouTube channel in Spanish?",
    "content": "Please, can you help me?",
    "author": "Joel0630",
    "timestamp": "2025-10-20T07:23:50",
    "url": "https://reddit.com/r/computervision/comments/1obk9w1/hi_in_which_sub_can_i_talk_about_my_computer/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ob1ooc",
    "title": "VLA-R1: A Smarter Way for AI Models to See, Think, and Act",
    "content": "VLA-R1 is a new model that helps AI systems reason better when connecting vision, language, and actions. Most existing Vision-Language-Action (VLA) models just look at an image, read a command, and act without really explaining how they make decisions. They often ignore physical limits, like what actions are possible with an object, and rely too much on simple fine-tuning after training. VLA-R1 changes that by teaching the model to think step by step using a process called Chain-of-Thought supervision. It‚Äôs trained on a new dataset with 13,000 examples that show detailed reasoning connected to how objects can be used and how movements should look. After that, it goes through a reinforcement learning phase that rewards it for accurate actions, realistic movement paths, and well-structured answers. A new optimization method called Group Relative Policy Optimization also helps it learn more efficiently. As a result, VLA-R1 performs better both in familiar environments and in completely new ones, showing strong results in simulations and on real robots. The team plans to release the model, dataset, and code to help others build smarter and more reliable AI systems.\n\nPaper link: [https://arxiv.org/pdf/2510.01623](https://arxiv.org/pdf/2510.01623)  \nCode sample: [https://github.com/GigaAI-research/VLA-R1?utm\\_source=catalyzex.com](https://github.com/GigaAI-research/VLA-R1?utm_source=catalyzex.com)",
    "author": "eminaruk",
    "timestamp": "2025-10-19T14:27:36",
    "url": "https://reddit.com/r/computervision/comments/1ob1ooc/vlar1_a_smarter_way_for_ai_models_to_see_think/",
    "score": 18,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ob9nen",
    "title": "Distance Estimation Between Objects",
    "content": "**Context:** I'm working on a project to estimate distances between workers and vehicles, or between workers and lifted loads, to identify when workers enter dangerous zones. The distances need to be in real-world units (cm or m).\n\nThe camera is positioned at a fairly high angle relative to the ground plane, but not high enough to achieve a true bird's-eye view.\n\n**Current Approach:** I'm currently using the average height of a person as a known reference object to convert pixels to meters. I calculate distances using 2D Euclidean distance (x, y) in the image plane, ignoring the Z-axis. I understand this approach is only robust when the camera has a top-down view of the area.\n\n**Challenges:**\n\n1. **Homography limitations:** I cannot manually select a reference plane because the ground is highly variable with uneven surfaces, especially in areas where workers are unloading materials. \n2. **Depth estimation integration(Depth anything v2):** I've considered incorporating depth estimation to obtain Z-axis information and calculate 3D Euclidean distances. However, I'm unsure how to convert these measurements to real-world units, since x and y are in pixels while z is normalized (0-1 range).\n\n  \n**Limitation: For now, I only have access to a single camera**\n\n**Question:** Are there alternative methods or approaches that would work better for this scenario, given the current challenges and limitations?\n\n",
    "author": "AbilityFlashy6977",
    "timestamp": "2025-10-19T20:39:01",
    "url": "https://reddit.com/r/computervision/comments/1ob9nen/distance_estimation_between_objects/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1obeg6b",
    "title": "Image Classification Advice",
    "content": "In my project, accuracy is important and I want to have few false detections as much as possible. \n\nSince I want to have good accuracy, will it be better to use Vision-Language Models instead and train them on large amounts of data? Will this have better accuracy compared to fine-tuning an image classification model (CNN or Vision Transformers)?",
    "author": "Immediate-Bug-1971",
    "timestamp": "2025-10-20T01:17:53",
    "url": "https://reddit.com/r/computervision/comments/1obeg6b/image_classification_advice/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1obdxo2",
    "title": "Real 3D vision use cases what are you working on?",
    "content": "Curious to hear what people are actually using 3D vision for.\nDo you work with LiDAR, ToF, or depth cameras?\n\nIs it for SLAM, object tracking, inspection, or reconstruction?\n\nAny tips on calibration or sensor fusion are welcome.",
    "author": "Big-Mulberry4600",
    "timestamp": "2025-10-20T00:48:11",
    "url": "https://reddit.com/r/computervision/comments/1obdxo2/real_3d_vision_use_cases_what_are_you_working_on/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oas9ki",
    "title": "Production OCR in 2025 - What are you actually deploying?",
    "content": "Hello,\n\n\n\nI'm spinning up a new production OCR project for a non-English language with lots of tricky letters.\n\n\n\nI'm seeing a ton of different \"SOTA\" approaches, and I'm trying to figure out what people are really using in prod today.\n\n\n\nAre you guys still building the classic 2-stage (CRAFT + TrOCR) pipelines? Or are you just fine-tuning VLMs like Donut? Or just piping everything to some API?\n\n\n\nI'm trying to get a gut check on a few things:\n\n\n\n\\- What's your stack? Is it custom-trained models, fine-tuned VLMs, or just API calls?\n\n\n\n\\- What's the most stubborn part that still breaks? Is it bad text detection (weird angles/lighting) or bad recognition (weird fonts/characters)?\n\n\n\n\\- How do LLMs fit in? Are you just using them to clean up the messy OCR output?\n\n\n\n\\- Data: Is 10M synthetic images still the way, or are you getting better results fine-tuning a VLM with just 10k clean, human labeled data?\n\n\n\nTrying to figure out where to focus my effort. Appreciate any \"in the trenches\" advice.",
    "author": "No_Nefariousness971",
    "timestamp": "2025-10-19T08:19:11",
    "url": "https://reddit.com/r/computervision/comments/1oas9ki/production_ocr_in_2025_what_are_you_actually/",
    "score": 20,
    "num_comments": 7,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oa9o7d",
    "title": "Computer Vision =/= only YOLO models",
    "content": "I get it, training a yolo model is easy and fun. However it is very repetitive that I only see \n\n1. How to start Computer vision? \n2. I trained a model that does X! (Trained a yolo model for a particular use case)\n\nposts being posted here.\n\nThere is tons of interesting things happening in this field and it is very sad that this community is headed towards sharing about these topics only",
    "author": "yagellaaether",
    "timestamp": "2025-10-18T15:42:12",
    "url": "https://reddit.com/r/computervision/comments/1oa9o7d/computer_vision_only_yolo_models/",
    "score": 144,
    "num_comments": 36,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oaebqt",
    "title": "Card segmentation",
    "content": "Hello, I would like to be able to surround my cards with a trapezoid, diamond, or rectangle like in these videos. I‚Äôve spent the past four days without success. I can do it using the function VNDetectRectanglesRequest, but it only works on a white background (on iPhone).\n\nI also tried it on PC‚Ä¶ I managed to create some detection models that frame my card (like surveillance cameras). I trained my own models (and discovered this whole world), but I‚Äôm not sure if I‚Äôm going in the right direction. I feel like I‚Äôm reinventing the wheel and there must already be a functional solution that would be quick to implement.\n\nFor now, I‚Äôm experimenting in Python and JavaScript because Swift is a bit complicated‚Ä¶ I‚Äôm doing everything no-code with Claude Opus 4.1, ChatGPT-5, and Gemini 2.5 Pro‚Ä¶ but I still need to figure out the best way to implement a solution. Could you help me? Thank you.\n",
    "author": "passio-777",
    "timestamp": "2025-10-18T19:29:00",
    "url": "https://reddit.com/r/computervision/comments/1oaebqt/card_segmentation/",
    "score": 67,
    "num_comments": 6,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oab29c",
    "title": "A New Deepfake Detection Method Combining Facial Landmarks and Adaptive Neural Networks",
    "content": "The LAKAN model (Landmark-Assisted Adaptive Kolmogorov-Arnold Network) introduces a new way to detect face forgeries, such as deepfakes, by combining facial landmark information with a more flexible neural network structure. Unlike traditional deepfake detection models that often rely on fixed activation functions and struggle with subtle manipulation details, LAKAN uses Kolmogorov-Arnold Networks (KANs), which allow the activation functions to be learned and adapted during training. This makes the model better at recognizing complex and non-linear patterns that occur in fake images or videos. By integrating facial landmarks, LAKAN can focus more precisely on important regions of the face and adapt its parameters to different expressions or poses. Tests on multiple public datasets show that LAKAN outperforms many existing models, especially when detecting forgeries it hasn‚Äôt seen before. Overall, LAKAN offers a promising step toward more accurate and adaptable deepfake detection systems that can generalize better across different manipulation types and data sources. \n\nPaper link: [https://arxiv.org/pdf/2510.00634](https://arxiv.org/pdf/2510.00634)",
    "author": "eminaruk",
    "timestamp": "2025-10-18T16:45:28",
    "url": "https://reddit.com/r/computervision/comments/1oab29c/a_new_deepfake_detection_method_combining_facial/",
    "score": 81,
    "num_comments": 5,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ob4jyv",
    "title": "Low cost reconnaissance UAVs",
    "content": "",
    "author": "-0x539",
    "timestamp": "2025-10-19T16:31:38",
    "url": "https://reddit.com/r/computervision/comments/1ob4jyv/low_cost_reconnaissance_uavs/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oaukp9",
    "title": "How can I determine OCR confidence level when using a VLM",
    "content": "I‚Äôm building an OCR pipeline that uses a VLM to extract structured fields from receipts/invoices (e.g., supplier name, date, total amount).\n\nI‚Äôd like to automatically detect when the model‚Äôs output is *uncertain*, so I can ask the user to re-upload a clearer image. But unlike traditional OCR engines (which give word-level confidence scores), VLMs don‚Äôt expose confidence directly.\n\nI‚Äôve thought about using the **image resolution** as a proxy, but that‚Äôs not always reliable ‚Äî higher resolution doesn‚Äôt always mean clearer text (tiny text could still be unreadable, while a lower-resolution image with large text might be fine).\n\nHow do people usually approach this?\n\n* Can I infer confidence from the model‚Äôs logits or token probabilities (if exposed)?\n* Would a text-region quality metric (e.g., average text height or contrast) work better?\n* Any heuristics or post-processing methods that worked for you to flag ‚Äúlow-confidence‚Äù OCR results from VLMs?\n\nWould love to hear how others handle this kind of uncertainty detection.",
    "author": "Ok_Television_9000",
    "timestamp": "2025-10-19T09:50:03",
    "url": "https://reddit.com/r/computervision/comments/1oaukp9/how_can_i_determine_ocr_confidence_level_when/",
    "score": 3,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oapd8v",
    "title": "Need an advice from pwople who are in the R&amp;D side of Computer Vision and Robot Vision.",
    "content": "I am sorry but this is an unusual query as I am a newbie.\n\nI am a S Asian. And currently planning to do my Master's from Europe as I am interested in the core depth side of Computer Vision and also have a goal of publishing a research paper in Tier 1 conference during Master's.\n\nBut when I see research roles or even Computer Vision roles in Computer Vision, 90% of them require PhD. I did have this thought of doing PhD in Computer Vision, like I am totally ready to go all in. But on the flip side, my parents are of the opinion that I should get married soon and the pressure is building up day by day. But the thing is if I go for PhD as an international student I will have minimal capacity to earn money in that journey as not only the working hours are limited but the amount of energy and attention the PhD level research requires. Being a CS undergrad graduate, part time open source contributor and full time employee, relationship is a thing far away from me.:3 And as I have read that the stipend in PhD is hardly enough to suppprt one ownself. So I had a thought that why should I even make things difficult for a partner for my own dreams.\n\nSo I wanted to know that is it hard to get into Computer Vision Engineer or AI research roles without a PhD or are there any alternative route? Or is it possible for a couple to survive on PhD stipend and internships as international student?",
    "author": "Distinct-Ebb-9763",
    "timestamp": "2025-10-19T06:15:14",
    "url": "https://reddit.com/r/computervision/comments/1oapd8v/need_an_advice_from_pwople_who_are_in_the_rd_side/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oamub9",
    "title": "Looking for math behind motion capture systems",
    "content": "Hey! I‚Äôm looking for mathematical explanations or models of how motion capture systems work - how 3D positions are calculated, tracked, and reconstructed (marker-based or markerless). Any good papers or resources would be awesome. Thanks!  \nEDIT:  \nCurrently, I‚Äôve divided motion capture into three methods: optical, markerless, and sensor-based. Out of curiosity, I wanted to understand the mathematical foundation of each of them - a basic, simple mathematical model that underlies how they work.",
    "author": "Full_Bother_319",
    "timestamp": "2025-10-19T04:03:53",
    "url": "https://reddit.com/r/computervision/comments/1oamub9/looking_for_math_behind_motion_capture_systems/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oam8rr",
    "title": "Student - How are you guys still able to use older repos?",
    "content": "Hi guys,  I‚Äôm trying to make my own detection model for iOS and so far I tried to learn Centernet and then YoloX.  My problem is that the information i‚Äôm finding is too old to work now,  or the tutorials I follow have issues mid way through with no solution.  I see so many people here who actively still use yolox because of the apache 2.0 license so is there something I‚Äôm missing?  Are you guys running it on your own environments or just PCs?  Google Colab?  any help is really appreciated :)",
    "author": "kaiser_exe",
    "timestamp": "2025-10-19T03:27:32",
    "url": "https://reddit.com/r/computervision/comments/1oam8rr/student_how_are_you_guys_still_able_to_use_older/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oaqcbu",
    "title": "A novel new task",
    "content": "What does the community think about this paper? This seems like a simple yet genius idea.\n\nhttps://arxiv.org/pdf/2410.05869",
    "author": "No_Difference9752",
    "timestamp": "2025-10-19T06:59:16",
    "url": "https://reddit.com/r/computervision/comments/1oaqcbu/a_novel_new_task/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oamris",
    "title": "Budget freindly setup",
    "content": "Im planing on building a system to deploy in a big room that automatically detects empty seats. Im new to machine learning and computer vision so i dont k ow too much. The room can fit around 300 people sitting down. Any suggestions on hardware that will work well for this deployment?\nThe room is longer than wider its about 40m √ó 10m. The buget i put is around 300$ but understood that might be a bit hard. I looked around a bit and saw that people use nvidia jetsons for stuff like this and on the other hand a raspbery pi, but i dont understand enough to know what would be better and more cost effective for me. What camera and computer to run the module would you guys recomend?\n\nThanks in advace.",
    "author": "eddy_213",
    "timestamp": "2025-10-19T03:59:35",
    "url": "https://reddit.com/r/computervision/comments/1oamris/budget_freindly_setup/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oae9lk",
    "title": "Engineers who started a B2B venture: How did you find your first problem?",
    "content": "Hello everyone,\n\n\n\nI've spent the last few years as a Computer Vision engineer, focusing mostly on the deep technical side of things, optimizing complex C++/Python SDKs and maximizing performance on edge devices.\n\n\n\nRecently, I‚Äôve decided to start my own B2B venture, but I'm facing a bit of a classic challenge. I feel like I have a strong set of technical skills ready to deploy, but I'm finding it difficult to pinpoint a specific, real-world problem that a business would genuinely pay to have solved. I'm very confident in the \"how,\" but I'm realizing the \"what\" is a completely different skill set.\n\n\n\nFor the engineers here who have successfully made that jump into entrepreneurship, how did you discover your first business idea? What was your process for finding that initial problem to solve? Did you start by reaching out directly to potential clients?\n\n\n\nI'm feeling a bit stuck on how to begin searching for a problem from the outside. Any stories or advice you could share would be greatly appreciated.",
    "author": "No_Nefariousness971",
    "timestamp": "2025-10-18T19:26:03",
    "url": "https://reddit.com/r/computervision/comments/1oae9lk/engineers_who_started_a_b2b_venture_how_did_you/",
    "score": 8,
    "num_comments": 8,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oaj063",
    "title": "AI or ML powered camera to detect if all units in a batch are sampled",
    "content": "I am new to AI and ML and was wondering if it is possible to implement a camera device that detects if the person sampling the units has sampled every bag. \n\nLets say there are 500 bags in a storage unit. A person manually samples each bag using a sampling gun that pulls out a little bit of sample from each bag as it is being moved from the storage unit. Can we build a camera that can accurately detect and alert if the person sampling missed any bags or accidentally sampled one twice?\n\nWhat kind of learning would I need to do to implement something of this sort?",
    "author": "gloomysnot",
    "timestamp": "2025-10-18T23:58:19",
    "url": "https://reddit.com/r/computervision/comments/1oaj063/ai_or_ml_powered_camera_to_detect_if_all_units_in/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oa9gw1",
    "title": "Those working on SfM and SLAM",
    "content": "I‚Äôm wondering if anyone who works on SfM or SLAM has notable recipes or tricks which ended up improving their pipeline. Obviously what‚Äôs out there in the literature and open packages is a great starting point, but I‚Äôm sure in the real world many practitioners end up having to use additional tricks on top of this.\n\nOne obvious one would be using newer learnt keypoint descriptors or matchers, though personally I‚Äôve found this can perform counterintuitively (spurious matches).",
    "author": "Zealousideal_Low1287",
    "timestamp": "2025-10-18T15:33:12",
    "url": "https://reddit.com/r/computervision/comments/1oa9gw1/those_working_on_sfm_and_slam/",
    "score": 10,
    "num_comments": 23,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oa2mwi",
    "title": "Is it worth working as a freelancer in computer vision?",
    "content": "Hi everyone,\n\nis it hard to find CV projects as a freelancer? Is it possible to work from home full time ? How and where to start? \n\nEdit: \nI have a PhD in robotics (vision) with 15,+ years experience as a research scientist. Now I am a teacher since 3 years and I want to go back to computer vision research. \n\nThanks.",
    "author": "Vast_Umpire_3713",
    "timestamp": "2025-10-18T11:02:06",
    "url": "https://reddit.com/r/computervision/comments/1oa2mwi/is_it_worth_working_as_a_freelancer_in_computer/",
    "score": 15,
    "num_comments": 32,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o9xok3",
    "title": "From shaky phone footage to 3D worlds (discussion of a research paper)",
    "content": "A team from Google DeepMind used videos taken with their phones for 3D reconstruction ‚Äî a breakthrough that won the Best Paper Honorable Mention at CVPR 2025.\n\nFull reference : Li, Zhengqi, et al. ‚Äú[MegaSaM: Accurate, fast and robust structure and motion from casual dynamic videos.](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_CVPR_2025_paper.pdf)‚Äù *Proceedings of the Computer Vision and Pattern Recognition Conference*. 2025.\n\n# Context\n\nWhen we take a video with our phone, we capture not only moving objects but also subtle shifts in how the camera itself moves. Figuring out the path of the camera and the shape of the scene from such everyday videos is a long-standing challenge in computer vision. Traditional methods work well when the camera moves a lot and the scene stays still. But they often break down with hand-held videos where the camera barely moves, rotates in place, or where people and objects are moving around.\n\n# Key results\n\nThe new system is called *MegaSaM* and it allows computers to accurately and quickly recover both the camera‚Äôs path and the 3D structure of a scene, even when the video is messy and full of movement. In essence, *MegaSaM* builds on the idea of *Simultaneous Localisation and Mapping* (SLAM). The idea of the process if to figure out ‚ÄúWhere am I?‚Äù (camera position) and ‚ÄúWhat does the world look like?‚Äù (scene shape) from video. Earlier SLAM methods had two problems: they either struggled with shaky or limited motion, or suffered from moving people and objects. *MegaSaM* improves upon them with three key innovations:\n\n1. **Filtering out moving objects:** The system learns to identify which parts of the video belong to moving things and diminishes their effect. This prevents confusion between object motion and camera motion.\n2. **Smarter depth starting point:** Instead of starting from scratch, *MegaSaM* uses existing single-image depth estimators as a guide, giving it a head start in understanding the scene‚Äôs shape.\n3. **Uncertainty awareness:** Sometimes, a video simply doesn‚Äôt give enough information to confidently figure out depth or camera settings (for example, when the camera barely moves). *MegaSaM* knows when it‚Äôs uncertain and uses depth hints more heavily in those cases. This makes it more robust to difficult footage.\n\nIn experiments, *MegaSaM* was tested on a wide range of datasets: animated movies, controlled lab videos, and handheld footage. The approach outperformed other state-of-the-art methods, producing more accurate camera paths and more consistent depth maps while running at competitive speeds. Unlike many recent systems, *MegaSaM* does not require slow fine-tuning for each video. It works directly, making it faster and more practical.\n\nThe Authors also examined how different parts of their design mattered. Removing the moving-object filter, for example, caused errors when people walked in front of the camera. Without the uncertainty-aware strategy, performance dropped in tricky scenarios with little camera movement. These tests confirmed that each piece of *MegaSaM*‚Äôs design was crucial.\n\nThe system isn‚Äôt perfect: it can still fail when the entire frame is filled with motion, or when the camera‚Äôs lens changes zoom during the video. Nevertheless, it represents a major step forward. By combining insights from older SLAM methods with modern deep learning, *MegaSaM* brings us closer to a future where casual videos can be reliably turned into 3D maps. This could help with virtual reality, robotics, filmmaking, and even personal memories. Imagine *re-living* the first steps of your kids in 3D ‚Äî how cool would *that* be!\n\n# My take\n\nI think *MegaSaM* is an important and practical step for making 3D understanding work better on normal videos people record every day. The system builds on modern SLAM methods, like [DROID-SLAM](https://proceedings.neurips.cc/paper/2021/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf), but it improves them in a smart and realistic way. It adds a way to find moving objects, to use good single-image depth models, and to check how sure it is about the results. These ideas help the system avoid common mistakes when the scene moves or the camera does not move much. The results are clearly stronger than older methods such as [CasualSAM](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930020.pdf) or [MonST3R](https://arxiv.org/pdf/2410.03825). The fact that the Authors share their code and data is also very good for research. In my opinion, *MegaSaM* can be useful for many applications, like creating 3D scenes from phone videos, making AR and VR content, or supporting visual effects.\n\nWhat do you think?",
    "author": "PiotrAntonik",
    "timestamp": "2025-10-18T07:44:23",
    "url": "https://reddit.com/r/computervision/comments/1o9xok3/from_shaky_phone_footage_to_3d_worlds_discussion/",
    "score": 14,
    "num_comments": 3,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o9nfyf",
    "title": "I know how to use Opencv functions, but I have no idea what rk actually do with them",
    "content": "I've learned how to use various OpenCV functions,  but I'm struggling to understand how to actually apply them to solve real problems. How do I learn what algorithms to use for different tasks, and how to connect the pieces to build something useful ",
    "author": "Due-Frosting-5113",
    "timestamp": "2025-10-17T22:24:57",
    "url": "https://reddit.com/r/computervision/comments/1o9nfyf/i_know_how_to_use_opencv_functions_but_i_have_no/",
    "score": 60,
    "num_comments": 18,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o9bt0d",
    "title": "Real-time head pose estimation for perspective correction - feedback?",
    "content": "Working on a computer vision project for real-time head tracking and 3D perspective adjustment.\n\n**Current approach:**\n\n* Head pose estimation from facial geometry\n* Per-frame camera frustum correction\n\nAnyone worked on similar real-time tracking projects? Happy to hear your thoughts!",
    "author": "Portality3D",
    "timestamp": "2025-10-17T13:10:22",
    "url": "https://reddit.com/r/computervision/comments/1o9bt0d/realtime_head_pose_estimation_for_perspective/",
    "score": 306,
    "num_comments": 53,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oakebe",
    "title": "M.Tech Embedded System",
    "content": "One whose is interested in Computer Vision and Learning Embedded System What To Do next , How He/She are move forward üñ•Ô∏è‚å®Ô∏èüñ±Ô∏è.",
    "author": "The_UnderDog_666",
    "timestamp": "2025-10-19T01:28:14",
    "url": "https://reddit.com/r/computervision/comments/1oakebe/mtech_embedded_system/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oafwsz",
    "title": "Machcreator",
    "content": "help me find a charger replacement for my Machcreator A\n",
    "author": "Sisteretchay-9549",
    "timestamp": "2025-10-18T20:54:37",
    "url": "https://reddit.com/r/computervision/comments/1oafwsz/machcreator/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oa5iod",
    "title": "Latency discrepancy on Sony FCB-EV9500L with LVDS-to-SDI Interface Board (TV80 0019)",
    "content": "Hi everyone,\n\nI‚Äôm using a Sony FCB-EV9500L camera with an LVDS-to-SDI interface board (model: TV80 0019). According to the datasheet, the interface board latency is around 13 microsecond. However, when I connect the camera to a monitor and measure the end-to-end latency, I observe approximately 70 ms.\n\nI‚Äôve set the camera latency settings to **Low Latency**, and the **video buffer is set to 1**, with a **frame rate of 30 fps**, but there is still this discrepancy.\n\nI‚Äôm wondering:\n\nWhy is the actual latency lower than the datasheet value of the interface board?\n\nAre there other factors in the camera, interface board, or monitor that could reduce or alter the perceived latency?\n\nCould the datasheet latency include internal buffering or worst-case scenarios that are not present in my setup?\n\nI would appreciate any insights from people who have experience with Sony block cameras, LVDS/SDI interface boards, or latency optimization.\n\n**I would be happy to hear your suggestions for additional camera interface boards.**",
    "author": "thegkhn",
    "timestamp": "2025-10-18T12:52:57",
    "url": "https://reddit.com/r/computervision/comments/1oa5iod/latency_discrepancy_on_sony_fcbev9500l_with/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o9vruk",
    "title": "Training on bigger datasets",
    "content": "Hi! I just started an attempt to train my YOLO model on coco minitrain. Previously I have only used smaller datasets in the range from 300-2000 images. This one hold 30k images. What should I expect from the mAP curve? \n\nThis far:  \nepoch 1 mAP 0.0045  \nepoch 2 mAP 0.0048  \nepoch 3 mAP 0.0053  \nepoch 4 mAP 0.0070\n\nTraining and val losses are dropping slow, is it normal for mAP to be this low in the early stages? I have checked labels and images and they are correct. The model does make some correct detections already and boxes do look ok on the things that gets detected. I just want some insight in to what I should expect on a bigger training session, since I have no previous experience with this. ",
    "author": "ConferenceSavings238",
    "timestamp": "2025-10-18T06:26:13",
    "url": "https://reddit.com/r/computervision/comments/1o9vruk/training_on_bigger_datasets/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1oa3x4u",
    "title": "Retail shelf/fixture dataset (blurred faces, eval-only) Kanops Open Access (‚âà10k)",
    "content": "Sharing **Kanops Open Access ¬∑ Imagery (Retail Scenes v0)**, a real-world retail dataset for:\n\n* Shelf/fixture detection &amp; segmentation\n* Category/zone classification (e.g., ‚ÄúPumpkins‚Äù, ‚ÄúShippers‚Äù, ‚ÄúBranding Signage‚Äù)\n* Planogram/visual merchandising reasoning\n* OCR on in-store signage (no PII)\n* Several other use cases \n\n**What‚Äôs inside**\n\n* \\~10.8k JPEGs across multiple retailers/years; seasonal ‚ÄúHalloween 2024‚Äù\n* Directory structure by retailer/category; plus MANIFEST.csv, metadata.csv, checksums.sha256\n* Faces blurred; EXIF/IPTC ownership &amp; terms embedded\n* **License:** evaluation-only (no redistribution of data or model weights trained exclusively on it)\n* **Access:** gated on HF (short request)\n\n**Link:** [https://huggingface.co/datasets/dresserman/kanops-open-access-imagery](https://huggingface.co/datasets/dresserman/kanops-open-access-imagery)\n\nOnce you have access: \n\nfrom datasets import load\\_dataset\n\nds = load\\_dataset(\"imagefolder\",\n\ndata\\_dir=\"hf://datasets/dresserman/kanops-open-access-imagery/train\")\n\n[Sample 1](https://preview.redd.it/y2o7ikte2xvf1.jpg?width=3264&amp;format=pjpg&amp;auto=webp&amp;s=ba9e35aa8c2d7f22d13b0588791770e31e3df72c)\n\n[Sample 2 ](https://preview.redd.it/ch04jfte2xvf1.jpg?width=3264&amp;format=pjpg&amp;auto=webp&amp;s=cb24c7527fd83b0504d91cf697f5c376b5becda1)\n\n[Sample 3](https://preview.redd.it/1rsq6eue2xvf1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=678299069ae2f414b080d55433390071150208f3)\n\n**Notes:** We‚Äôre iterating toward v1 with weak labels &amp; CVAT exports. Feedback on task design and splits welcome.",
    "author": "malctucker",
    "timestamp": "2025-10-18T11:51:26",
    "url": "https://reddit.com/r/computervision/comments/1oa3x4u/retail_shelffixture_dataset_blurred_faces/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o9u4q6",
    "title": "Need Advice Regarding Alzheimer's Classification Using CNNs",
    "content": "I am trying to train a ResNet50 model with pretrained ImageNet weights for Alzheimer's classification. My dataset is ADNI1 Baseline. I am currently going for AD vs CN classification.\n\nEach MRI was in nifti format and was preprocessed by ADNI (MPR, GradWarp, B1 Correction and N3 Normalization)\n\nHere are my data preprocessing steps:\n1. Skull stripping using SynthStrip\n2. WhiteStripe \n3. Registration to MNI-152 using AntsPy\n\nThen the patients' MRIs were first split into train-val-test sets. This ensured patient level splitting, preventing data leakage. Finally each MRI was sliced along the coronal plane. 30 slices were extracted from the hippocampus region.\n\nThis gave:\n8372 images for training \n1820 images for validation\n1876 images for testing\n\nFor the training, a learning rate of 1e-4 was used. Each consecutive 3 images were treated as 3 channels. Data augmentation was applied like horizontal flips, random rotation, random affine, gaussian blur etc.\n\nThe problem is that the training accuracy gradually rises (over 90%) but the validation accuracy does not. Rather the validation loss INCREASES over time. I cannot solve this problem in any way. Any advice would be very appreciated.",
    "author": "trailblazer41",
    "timestamp": "2025-10-18T05:12:19",
    "url": "https://reddit.com/r/computervision/comments/1o9u4q6/need_advice_regarding_alzheimers_classification/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o955ve",
    "title": "3D Human Pose Estimation Using Temporal Graph Networks",
    "content": "I wanted to share an interesting paper on estimating human poses in 3D from videos using something called Temporal Graph Networks. Imagine mapping the body as a network of connected joints, like points linked with lines. This paper uses a smart neural network that not only looks at each moment (each frame of a video) but also how these connections evolve over time to predict very accurate 3D poses of a person moving.\n\nThis is important because it helps computers understand human movements better, which can be useful for animation, sports analysis, or even healthcare applications. The method achieves more realistic and reliable results by capturing how movement changes frame by frame, instead of just looking at single pictures.\n\nYou can find the paper and resources here:  \n[https://arxiv.org/pdf/2505.01003](https://arxiv.org/pdf/2505.01003)\n\n",
    "author": "eminaruk",
    "timestamp": "2025-10-17T08:56:29",
    "url": "https://reddit.com/r/computervision/comments/1o955ve/3d_human_pose_estimation_using_temporal_graph/",
    "score": 98,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o9sfm4",
    "title": "Looking for some experienced advice, How do you match features of a same person from multiple cameras?",
    "content": "Hey everyone, I am working on a project/product, where I need to track the same person from multiple cameras.  \nAll the cameras are same and in a fixed positions (could be known or unknown) of a given space, I want to match one person whom I see on one camera with a different perspective of the other camera.\n\nI don't come from ML/AI background, but I am aware how the ViT work on a surface level, is there any model which can do feature matching across cameras and not just in the given image?   \nIf no, how can I attain this?\n\nPosting with the hope to not find a direct solution (if there is something, great), because I am well aware this is an active field of research even now. But I do want to take a stab at it, so if you're experienced and have a perspective on which direction should i head to solve this problem, do help me out.\n\n",
    "author": "sourav_bz",
    "timestamp": "2025-10-18T03:37:40",
    "url": "https://reddit.com/r/computervision/comments/1o9sfm4/looking_for_some_experienced_advice_how_do_you/",
    "score": 3,
    "num_comments": 8,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o9nuum",
    "title": "Help with product matching from known catalogue",
    "content": "I want to detect the appearance of products from a cataloge of product images. I am currently using a finetuned YOLO model to isolate relevant products + CLIP to match them against the catalogue.\n\nEach product only has 2-4 images available and I am considering that perhaps I should create synthetic images to improve the performance of the CLIP embedding + retrieval.\n\nCurrent issues are that if the a person appears in several different product images, CLIP seems to misidentify the product, e.g if a person appears in the photo for products A, B and C, the current pipeline results in product A being mislabeled as product A B or C. \n\nAlso I'm not sure the fine tuned YOLO is even needed as I've tried doing a grid based based matching system where CLIP splits each input frame into a grid of squares and then scans for any matches from the products.\n\nI am hoping someone could suggest alternative approaches / workflows for improved results.",
    "author": "Whizz5",
    "timestamp": "2025-10-17T22:49:58",
    "url": "https://reddit.com/r/computervision/comments/1o9nuum/help_with_product_matching_from_known_catalogue/",
    "score": 6,
    "num_comments": 2,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o9k9hr",
    "title": "Where‚Äôs the best place to find someone who can train a YOLO model for aerial object detection?",
    "content": "I‚Äôm working at an early state startup on an autonomy project and we need to train a YOLO model for aerial object detection ‚Äî real data, custom classes, edge deployment.\n\nI‚Äôm not looking for a crowdsourced annotation service or generic freelancer. I‚Äôm trying to find someone who actually knows how to tune detection models and work with domain-specific datasets.\n\nIs there like a job board you‚Äôd recommend?",
    "author": "daftmonkey",
    "timestamp": "2025-10-17T19:31:03",
    "url": "https://reddit.com/r/computervision/comments/1o9k9hr/wheres_the_best_place_to_find_someone_who_can/",
    "score": 10,
    "num_comments": 35,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o9tkhn",
    "title": "Using pretrained DenseNet/ResNet101 as U-Net encoder for small datasets",
    "content": "I‚Äôm working on an medical image segmentation project, but my dataset is quite small. I was thinking of using a pretrained model (like DenseNet or ResNet101...) to extract features and then feed those features into a U-Net architecture.\n\nWould that make sense for improving performance with limited data?  \nAlso, should I freeze the encoder weights at first or train the whole thing end-to-end from the start?\n\nAny advice or implementation tips would be appreciated.",
    "author": "Taaaha_",
    "timestamp": "2025-10-18T04:43:14",
    "url": "https://reddit.com/r/computervision/comments/1o9tkhn/using_pretrained_densenetresnet101_as_unet/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o90c32",
    "title": "Hair counting for hair transplant industry finished project",
    "content": "Hey everyone,  \nI wanted to share one of my recent AI projects that turned into a real-world product,¬†[HairCounting.com](https://haircounting.com).\n\nIt is an AI-powered analysis system that processes microscopic scalp images and automatically counts and maps hair follicles. Dermatologists and trichologists use it to measure hair density and monitor hair-loss treatments without doing the manual work.\n\n# How it works\n\nThe pipeline is built around a YOLO-based detection model trained on thousands of annotated scalp images.  \nThe process:\n\n1. Image preprocessing: color normalization, noise removal, and scale calibration\n2. Detection and segmentation: the model identifies each visible hair shaft and follicle\n3. Post-processing: removes duplicates, merges close detections, and calculates density per cm¬≤\n4. Visualization and report generation: builds a visual map and returns counts and thickness data via API\n\nI trained the model to reach around 70%+ precision, which was actually a real medical requirement from one of the clinics. Total perfection is not needed, doctors mainly need consistent automated measurements.\n\n# Stack and integration\n\n* Frameworks: PyTorch and OpenCV\n* API backend: Laravel 11 with Sanctum authentication\n* Deployment: Nginx on Ubuntu (GPU optional)\n\n# Challenges I faced\n\n* Managing image scale calibration across different microscopes\n* Detecting extremely fine or gray hairs under varying light\n* Creating a balanced dataset for both dense and sparse hair regions\n* Returning structured JSON output fast enough for clinical software\n\n# Why I am sharing this\n\nI thought it would be useful to showcase how computer vision can be applied to a very niche but impactful problem.  \nIf anyone here is building custom AI for medical, beauty, or visual measurement use cases, I would love to compare approaches or exchange feedback.\n\nYou can test the live demo or read the technical overview here:¬†[https://haircounting.com/](https://haircounting.com/)",
    "author": "mbtonev",
    "timestamp": "2025-10-17T05:46:17",
    "url": "https://reddit.com/r/computervision/comments/1o90c32/hair_counting_for_hair_transplant_industry/",
    "score": 123,
    "num_comments": 19,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o9gzjr",
    "title": "New edge AI platform",
    "content": "Hi! If you're interested in Edge AI, this might be something for you.\n\nWe‚Äôve just created Embedl Hub, a developer platform where you can experiment with on-device AI and understand how models perform on real hardware. It allows you to optimize, benchmark, and compare models by running them on devices in the cloud, so you don‚Äôt need access to physical hardware yourself.\n\nIt currently supports phones, dev boards, and SoCs, and everything is free to use.",
    "author": "elinaembedl",
    "timestamp": "2025-10-17T16:49:32",
    "url": "https://reddit.com/r/computervision/comments/1o9gzjr/new_edge_ai_platform/",
    "score": 4,
    "num_comments": 3,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o98laf",
    "title": "I built an AI CCTV surveillance system for scale",
    "content": "There were a couple of challenges.  \n**1. Accuracy:**¬†addressed by newer AI models and VLMs for task-level understanding  \n**2. Scaling:**¬†developed an in-house workflow for deploying models for 8-10x speed gains and lower hardware requirements.  \n**3. Anonymity:**¬†face blurring for people to collect anonymized events\n\nI am building an agentic layer on top of this to help customize the workflow for different use-cases and deploy with a single click. Ask me anything about it!",
    "author": "abd297",
    "timestamp": "2025-10-17T11:05:57",
    "url": "https://reddit.com/r/computervision/comments/1o98laf/i_built_an_ai_cctv_surveillance_system_for_scale/",
    "score": 8,
    "num_comments": 25,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o8uyy4",
    "title": "Dual 3D vision | software/library - synced TEMAS modules",
    "content": "Both TEMAS units controlled through a shared Python library, or by software synchronized over PoE.\n\nOne command triggers both sensors.\n\nHow would you use this kind of swarm setup? What do you think about swarm knowledge in vision systems?",
    "author": "Big-Mulberry4600",
    "timestamp": "2025-10-17T00:35:08",
    "url": "https://reddit.com/r/computervision/comments/1o8uyy4/dual_3d_vision_softwarelibrary_synced_temas/",
    "score": 46,
    "num_comments": 2,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o9agj9",
    "title": "Exe installer with openmmlab",
    "content": "Hello, so i'm a bit stuck on a project. \nI do computer vision models for quite some time, i know how to package and dockerise my projects. However today at work a client asked for a .exe file to install the current pyqt app that runs a detection model from mmdet on CPU.\n\nAlso note that I can't onnx this model with mmdeploy (I don't know if that makes a diff√©rence or not).\n\nThe thing is, I've never created installers like that. Is there any good r√©f√©rence for this ?\nThanks",
    "author": "tanglef",
    "timestamp": "2025-10-17T12:17:21",
    "url": "https://reddit.com/r/computervision/comments/1o9agj9/exe_installer_with_openmmlab/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o8xv44",
    "title": "Can UNets train on multiple sizes?",
    "content": "So I made a UNet based on the more recent designs that enforce 2nd power scaling. So technically it works on any size image. However, I'm not sure performance-wise, if I train on random image sizes, if this will affect anything. Like will it become more accurate for all sizes I train on, or performance degrade?\n\nI never really tried this. So far I've only just been making my dataset a uniform size.",
    "author": "Affectionate_Use9936",
    "timestamp": "2025-10-17T03:39:38",
    "url": "https://reddit.com/r/computervision/comments/1o8xv44/can_unets_train_on_multiple_sizes/",
    "score": 2,
    "num_comments": 19,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o92c2k",
    "title": "Seeking advice: Automating AI product image retouching at scale (jewelry, 1000+ images)",
    "content": "I run an online jewelry shop with several hundred product photos, and I‚Äôve already improved many images using common AI tools for background removal and retouching with good results.\n\n‚ÄãMy goal now is to automate this end‚Äëto‚Äëend so I can process large batches reliably without manual steps or one‚Äëoff scripts.\n\n‚ÄãWhat I‚Äôm imagining: I upload a simple CSV/Google Sheet with image URLs and a ‚Äútask/prompt‚Äù column (e.g., background removal + natural shadow + center/crop), and the system returns 1,000 retouched images or 1,000 images with new backgrounds to a specified destination (e.g., cloud bucket or Shopify/DAM).\n\n‚ÄãQuestions for the community:\n\n\\-Which tools/APIs or hosted services would you recommend for robust batch processing of background removal, retouching, and consistent lighting/shadows for jewelry products ?\n\n¬†\\-Any suggested orchestration patterns ¬†suitable for 1k+ images per run ?\n\n\\-Cost expectations: If I rely on API credits for background removal/retouching at this volume, what ballpark per‚Äëimage costs should I expect?\n\nI‚Äôd really appreciate concrete suggestions, lessons learned, and any tutorials or threads that walk through similar setups at scale.",
    "author": "jojo-de",
    "timestamp": "2025-10-17T07:09:13",
    "url": "https://reddit.com/r/computervision/comments/1o92c2k/seeking_advice_automating_ai_product_image/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o86d2p",
    "title": "Made a CV model which detects Smoke and Fire  suing yolov8, any feedback?",
    "content": "Like its a very basic model which i made and posted to GitHub, I plan on training the [last.pt](http://last.pt) of this model on a much LARGER dataset.\n\nLike, here is the thing link to the repo, i would be really grateful to feedback i can receive as i am new to CV model training using YOLO and GitHub repos:\n\n[https://github.com/Nocluee100/Fire-and-Smoke-Detection-AI-v1](https://github.com/Nocluee100/Fire-and-Smoke-Detection-AI-v1)",
    "author": "No_Clue1000",
    "timestamp": "2025-10-16T06:37:02",
    "url": "https://reddit.com/r/computervision/comments/1o86d2p/made_a_cv_model_which_detects_smoke_and_fire/",
    "score": 77,
    "num_comments": 17,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o906uz",
    "title": "LOOKING for Remote Sensing DatasetsÔºÅÔºÅÔºÅ",
    "content": "I would like some datasets of remote sensing scene graphÔºàRSSGÔºâ. Could you tell me which ones there are? Thank you all.",
    "author": "FrontWillingness39",
    "timestamp": "2025-10-17T05:39:51",
    "url": "https://reddit.com/r/computervision/comments/1o906uz/looking_for_remote_sensing_datasets/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o8rtlt",
    "title": "I built an AI tool to generate and refine brand product images for advertising",
    "content": "https://preview.redd.it/phmogsu6nlvf1.jpg?width=2397&amp;format=pjpg&amp;auto=webp&amp;s=0cfdac0569bb6adbee624c5657a27dc83c269f18\n\n\n\n\n\nHey everyone! I recently built **BrandRefinement**, an open-source AI pipeline that helps create high-quality brand advertising images.\n\nThe Problem: When using AI to generate product placement in creative scenes, the generated products often have small inconsistencies - wrong logos, slightly off colors, or distorted details that don't match the actual brand product.\n\n**The Solution**: A 3-stage pipeline:\n\n**1. Generate** \\- Combine your creative background (character, scene) with a brand product reference  \n**2. Draw Masks** \\- Mark which parts need refinement  \n**3. Refine** \\- AI precisely adjusts the generated product to match the original brand specifications\n\n**Example workflow:**\n\n\\- **Input**: Astronaut cow character + Heineken bottle reference  \n\\- **Output**: Professional advertising image with accurate product details\n\nThe tool uses DreamO for initial generation and a custom refinement pipeline to ensure brand consistency.\n\nCheck it out: [*https://github.com/DinhLuan14/BrandRefinement*](https://github.com/DinhLuan14/BrandRefinement)\n\nWould love to hear your feedback or see what you create with i",
    "author": "ndluan2709",
    "timestamp": "2025-10-16T21:24:59",
    "url": "https://reddit.com/r/computervision/comments/1o8rtlt/i_built_an_ai_tool_to_generate_and_refine_brand/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o8ba4h",
    "title": "Do companies these days even care about DS and Leetcode style algorithmic interviews? (AI/CV job interviews)",
    "content": "For more context, few years ago I was actively interviewing for computer vision roles, and most of them were traditional computer vision jobs with focus on C++, and there used to be at least one round of interview with live coding and they used to focus on Leetcode style questions followed by DS questions.  \nNow I am planning to start job hunting again, but after AI assisted coding boom, I am wondering if I should spend any time practicing DS Algo questions, or should I just create good CV projects with AIs help and understanding math and logic?\n\nThanks!",
    "author": "absudist_robot",
    "timestamp": "2025-10-16T09:44:59",
    "url": "https://reddit.com/r/computervision/comments/1o8ba4h/do_companies_these_days_even_care_about_ds_and/",
    "score": 21,
    "num_comments": 6,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o973jk",
    "title": "Seeking Founding CV/AI Engineers for a New Tech Startup",
    "content": "Hey Reddit Community,\n\nI'm the founder of Point Ref Inc., a new venture backed by 30 years of experience leading tech and marketing programs at a major tech company. We're in the early stages of building a product to solve a major challenge in the sports officiating space, and we're looking for a couple of entrepreneurial CV/AI engineers to join as foundational members of our team.\n\nThis is a ground-floor, equity-focused opportunity to build a product from scratch and have a massive impact.\n\nIf you have a strong background in computer vision and a passion for building things that matter, send me a DM with a link to your GitHub, portfolio, or LinkedIn. I'm happy to share the full project brief with qualified candidates.",
    "author": "Massive-Letter6296",
    "timestamp": "2025-10-17T10:09:24",
    "url": "https://reddit.com/r/computervision/comments/1o973jk/seeking_founding_cvai_engineers_for_a_new_tech/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o8pfe1",
    "title": "Seamless cloning with OpenCV Python",
    "content": "Seamless cloning is a cool technique that uses Poisson Image Editing, which blends objects from one image into another, even if the lighting conditions are completely different.\n\nImagine cutting out an object lit by warm indoor light and pasting it into a cool, outdoor scene, and it just '*fits'*, as if the object was always there.\n\n\n\nLink:-  [https://youtu.be/xWvt0S93TDE](https://youtu.be/xWvt0S93TDE)",
    "author": "computervisionpro",
    "timestamp": "2025-10-16T19:23:43",
    "url": "https://reddit.com/r/computervision/comments/1o8pfe1/seamless_cloning_with_opencv_python/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o8l55r",
    "title": "How do I detect circular blobs without thresholding",
    "content": "Hello, I need to detect the coordinates of the circular blobs here. I have tried Hough Transform and Simple Blob Detector, but they have not achieved good results. I also prefer not to do thresholding as these LEDs will vary a lot in distance, therefore effecting the amplitude measured.\n\nhttps://preview.redd.it/r6kyuydb1kvf1.jpg?width=640&amp;format=pjpg&amp;auto=webp&amp;s=8c7ec99bba0fcad5c046f85ae8a17f42bd8e1d0c\n\n",
    "author": "momoisgoodforhealth",
    "timestamp": "2025-10-16T16:04:17",
    "url": "https://reddit.com/r/computervision/comments/1o8l55r/how_do_i_detect_circular_blobs_without/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o8ps87",
    "title": "Deploy YOLO model to Heroku",
    "content": "Hello everyone, Does anyone have solution for excess slug size issue when deploy YOLO model to heroku? I got an issue while heroku failed to install ultralytics package. This is my requirements.txt\n\n    setuptools==69.5.1\n    boto3==1.34.49\n    fastapi==0.111.0\n    ffmpeg-python==0.2.0\n    numpy==1.26.4\n    redis==5.0.5\n    pytesseract==0.3.9\n    opencv-python-headless==4.11.0.86\n    tesseract\n    uvicorn\n    requests\n    tensorflow\n    mediapipe\n    dlib\n    face_recognition\n    pyzbar\n    zxing\n    ultralytics==8.3.128\n\nAnd when heroku install ultralytics and its dependencies it seems like excess the slug size which is (500MB) .   \n",
    "author": "teetran39",
    "timestamp": "2025-10-16T19:40:44",
    "url": "https://reddit.com/r/computervision/comments/1o8ps87/deploy_yolo_model_to_heroku/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o8sts8",
    "title": "What's the biggest blocker you've hit using LLMs for actual, large-scale coding projects?",
    "content": "",
    "author": "Street-Lie-2584",
    "timestamp": "2025-10-16T22:21:15",
    "url": "https://reddit.com/r/computervision/comments/1o8sts8/whats_the_biggest_blocker_youve_hit_using_llms/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o8ta8i",
    "title": "What's the biggest blocker you've hit using LLMs for actual, large-scale coding projects?",
    "content": "",
    "author": "Street-Lie-2584",
    "timestamp": "2025-10-16T22:48:14",
    "url": "https://reddit.com/r/computervision/comments/1o8ta8i/whats_the_biggest_blocker_youve_hit_using_llms/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o8f57o",
    "title": "What‚Äôs the ideal workflow for sharing commercial samples?",
    "content": "",
    "author": "malctucker",
    "timestamp": "2025-10-16T12:05:47",
    "url": "https://reddit.com/r/computervision/comments/1o8f57o/whats_the_ideal_workflow_for_sharing_commercial/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o82n85",
    "title": "career advice",
    "content": "I‚Äôm a 3rd-year Computer Science Engineering student, and I‚Äôm really interested in Computer Vision ‚Äî mainly classical CV ‚Äî since I‚Äôm already learning Deep Learning in college.  \nI‚Äôm a bit confused about where to start with Computer Vision and OpenCV. Could you suggest some Udemy or free courses that cover both theory and coding, focusing mainly on classical CV and YOLO? and i want to learn by building projects not only theory.\n\nI am really  confused and scared please shed some light ",
    "author": "Monkey--D-Luffy",
    "timestamp": "2025-10-16T03:34:21",
    "url": "https://reddit.com/r/computervision/comments/1o82n85/career_advice/",
    "score": 3,
    "num_comments": 17,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o81kv7",
    "title": "Trying to create datasets for a game bot that try to recognize objects of same shape but different colors",
    "content": "So i'm trying to create a game bot, using supervised learning, and i need to create datasets for it. The game i needed is very depend on object color recognization, so no grayscale. And people said putting in raw colored image gonna make the training more consuming. So what is my best options here?",
    "author": "Kiyumaa",
    "timestamp": "2025-10-16T02:28:29",
    "url": "https://reddit.com/r/computervision/comments/1o81kv7/trying_to_create_datasets_for_a_game_bot_that_try/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o80swc",
    "title": "Local Intensity Normalization",
    "content": "I am working on a data augmentation pipeline for stroke lesions MRIs. The pipeline aims at pasting lesions from sick slices to healthy slices. In order to do so, I need to adjust the intensities of the pasted region to match those of the healthy slice.\n\n\n\nNow, I have implemented (with the help of ChatGPT as I had no clue on what was the best approach to do this), this function:\n\n    def normalize_lesion_intensity(healthy_img, lesion_img, lesion_mask):\n        if lesion_mask.dtype != torch.bool:\n            lesion_mask = lesion_mask.to(dtype=torch.bool)\n            \n        lesion_vals = lesion_img[lesion_mask]\n        healthy_vals = healthy_img[~lesion_mask]\n    \n        mean_les = lesion_vals.mean()\n        std_les  = lesion_vals.std()\n        mean_h   = healthy_vals.mean()\n        std_h    = healthy_vals.std()\n    \n        # normalize lesion region to healthy context\n        norm_lesion = ((lesion_img - mean_les) / (std_les + 1e-8)) * std_h + mean_h\n    \n        out = healthy_img.clone()\n        out[lesion_mask] = norm_lesion[lesion_mask]\n        return out\n\nHowever, I am getting pretty scarse results. For instance, If I were to perform augmentation on these slices:\n\n*Processing img jddh6mjwqfvf1...*\n\nI would get the following augmented slice:\n\nhttps://preview.redd.it/503heeeyqfvf1.png?width=1144&amp;format=png&amp;auto=webp&amp;s=dca19728dac9b73c74b8e9c040e4dca11e772322\n\n\n\nAs you can see, the pasted lesion stands out as if it were pasted from a letter collage. \n\n\n\nCan you help me out?",
    "author": "Lonely-Eye-8313",
    "timestamp": "2025-10-16T01:37:01",
    "url": "https://reddit.com/r/computervision/comments/1o80swc/local_intensity_normalization/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o7yhqd",
    "title": "Low Accuracy with Deepface (Facenet512 + RetinaFace + ChromaDB) - Need Help!",
    "content": "I'm building a simple facial recognition app and hitting a wall with accuracy. I'm using an open-source setup and the results are surprisingly bad‚Äîway below the $\\\\sim50\\\\%$ accuracy I expected.  \nMy Setup:\n\n* **Recognition Model:** **Facenet512** \n* **Face Detector:** **RetinaFace** \n* **Database &amp; Search:** **ChromaDB** for storage, using **cosine similarity** to compare the \"fingerprints\" (embeddings).\n* **Hardware:** **Tesla V100 32GB GPU** (It's fast, so hardware isn't the problem.)\n\nThe Problem:\n\nMy recognition results are poor. Lots of times it misses a match (false negative) or incorrectly matches the wrong person (false positive).\n\nIf you've built a system with **Deepface** and **Facenet512**, please share any tips or common pitfalls.",
    "author": "aavashh",
    "timestamp": "2025-10-15T23:04:23",
    "url": "https://reddit.com/r/computervision/comments/1o7yhqd/low_accuracy_with_deepface_facenet512_retinaface/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o79tce",
    "title": "Custom YOLO model",
    "content": "First of all: I used chatGPT, yes! ALOOT\n\nI asked ChatGPT how to build a YOLO model from scratch and after weeks of chatting I have a promissing setup. However I do feel hesitent to sharing the work since people seem to hate everything written by chatgpt.\n\nI do feel that the workspace built is promissing. Right now my GPU is working overtime to benchmark the models against a few of the smaller datasets from RF100 domain. The workspace utilities timm to build the backbones of the model.\n\nI also specified that I wanted a GPU and a CPU version since I often lack CPU speed when using different yolo-models.\n\nThe image below is created after training to summarize the training and how well the model did.\n\nSo my question: is it worth it to share the code or will it be frowned upon since ChatGPT did most of the heavy lifting?",
    "author": "ConferenceSavings238",
    "timestamp": "2025-10-15T05:37:16",
    "url": "https://reddit.com/r/computervision/comments/1o79tce/custom_yolo_model/",
    "score": 72,
    "num_comments": 29,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o83ttk",
    "title": "Looking for honest reviews for my bug bite app",
    "content": "",
    "author": "EnvironmentalTop9356",
    "timestamp": "2025-10-16T04:40:01",
    "url": "https://reddit.com/r/computervision/comments/1o83ttk/looking_for_honest_reviews_for_my_bug_bite_app/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o7ms6b",
    "title": "Finding Datasets and Pretrained YOLO Models Is a Hell",
    "content": "Seriously, why is it so damn hard to find *good* datasets or pretrained YOLO models for real-world tasks?\n\nRoboflow gives this illusion that everything you need is already there, but once you actually open those datasets, 80% of them are either tiny, poorly labeled, or just low quality. It feels like a meth lab of ‚Äúsemi-datasets‚Äù rather than something you can actually train from.\n\nAt this point, I think what the community needs more than faster YOLO versions is better shared datasets, clean, well-labeled, and covering practical use cases. The models are already fast and capable; data quality is what‚Äôs holding things back.\n\nAnd don‚Äôt even get me started on pretrained YOLO models. YOLO has become *the* go-to for object detection, yet somehow it‚Äôs still painful to find proper pretrained weights for specific applications beyond COCO. Why isn‚Äôt there a solid central place where people share trained weights and benchmarks for specific applications?\n\nFeels like everyone‚Äôs reinventing the wheel in their corner.",
    "author": "Choice_Committee148",
    "timestamp": "2025-10-15T13:49:28",
    "url": "https://reddit.com/r/computervision/comments/1o7ms6b/finding_datasets_and_pretrained_yolo_models_is_a/",
    "score": 15,
    "num_comments": 23,
    "upvote_ratio": 0.68,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o8203r",
    "title": "What kind of vision agents are people building specific and if any open source frameworks?",
    "content": "hey all, i am curious of agentic direction in computer vision instead of static workflows. basically systems that perceive, understand and proactively act in visual use cases be it surveillance, humanoids or visual inspection in manufacturing\n\nHow do people couple vision modules(such as yolo) with planning, control, decision logic?\n\nany tools that wrap together perception and action loops? something more than ‚Äújust‚Äù a CV library more like an agent stack for vision tasks\n\nand if so, then how are these agents being validated especially when you are sleeping and your agents are in action overnight.",
    "author": "Worth-Card9034",
    "timestamp": "2025-10-16T02:55:19",
    "url": "https://reddit.com/r/computervision/comments/1o8203r/what_kind_of_vision_agents_are_people_building/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o7xptk",
    "title": "Violence detection between kids and adults",
    "content": "Me and my friend have been developing an ai model to recognize violent activities in kindergartens like violent behavior between kids and violence from adults towards kids, like pulling hair, punching, aggressively behaving. This is crucial for us because we want the kindergartens to use this computer vision model on their cameras and run it 24/7 to detect and report violence.\n\nWe believe in this project and currently have a problem.\n\nWe connected our work station to the cameras successfully to read camera output and we successfully ran our ultralytics YOLO trained model against the camera feed but it has trouble detecting violence.\n\nWe are not sure what we are doing wrong and want to know if there are other ways of training the model, maybe through mmaction or something else.\n\nRight now we are manually annotating thousands of frames of fake aggression towards kids from the adults, we staged some aggression videos in kindergartens with the permission of parents, kindergarten and adults working in kindergarten and gathered 4000 videos with 10 seconds duration of each of these and we annotated most of them through cvat with bounding boxes then trained the model with this annotated data using yolo8. \n\nThe results are not so good, it seems like the model still cannot figure out if there is aggression on some videos.\n\nSo I want to ask you for advices or maybe you have some other approach in mind (maybe using mmaction) that can potentially help us solve this problem!\n\nA friend of mine suggested using hrnet to detect points across skeleton of a person and to use mmaction and train it to detect violence so basically using two models together to detect it.\n\nWhat do you think?\n\n\n\n",
    "author": "Comfortable-Gold-352",
    "timestamp": "2025-10-15T22:16:59",
    "url": "https://reddit.com/r/computervision/comments/1o7xptk/violence_detection_between_kids_and_adults/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o7lz7z",
    "title": "Simple/Lightweight Factor Graph project",
    "content": "I wrote a small factor graph library and open sourced it.  I wanted a small and lightweight factor graph library for some SFM / SLAM (structure from motion / simultaneous localization and mapping) projects I was working on.\n\nI like GTSAM but it was just a bit too heavy and has some Boost dependencies.  I decided to make a new library, and focus on making the interface as simple and easy-to-use as possible, while retaining the things i liked about GTSAM\n\nIt compiles down to a pretty small library (\\~400-600kb).  And uses Eigen for most of the heavy lifting - and uses Eigen sparse matrices for the full Jacobian/Hessian representation.  \n[https://github.com/steven-gilbert-az/factorama](https://github.com/steven-gilbert-az/factorama)",
    "author": "stevethatsmyname",
    "timestamp": "2025-10-15T13:18:59",
    "url": "https://reddit.com/r/computervision/comments/1o7lz7z/simplelightweight_factor_graph_project/",
    "score": 8,
    "num_comments": 9,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o78g5r",
    "title": "MegaSaM: A Breakthrough in Real-Time Depth and Camera Pose Estimation from Dynamic Monocular Videos",
    "content": "If you‚Äôre into computer vision, 3D scene reconstruction, or SLAM research, you should definitely check out the new paper ‚ÄúMegaSaM‚Äù. It introduces a system capable of extracting highly accurate and robust camera parameters and depth maps from ordinary monocular videos, even in challenging dynamic and low-parallax scenes. Traditional methods tend to fail in such real-world conditions since they rely heavily on static environments and large parallax, but MegaSaM overcomes these limitations by combining deep visual SLAM with neural network-based depth estimation. The system uses a differentiable bundle adjustment layer supported by single-frame depth predictions and object motion estimation, along with an uncertainty-aware global optimization that improves reliability and pose stability. Tested on both synthetic and real-world datasets, MegaSaM achieves remarkable gains in accuracy, speed, and robustness compared to previous methods. It‚Äôs a great read for anyone working on visual SLAM, geometric vision, or neural 3D perception. Read the paper here: [https://arxiv.org/pdf/2412.04463](https://arxiv.org/pdf/2412.04463)\n\nhttps://preview.redd.it/kmn1lss4h9vf1.png?width=1451&amp;format=png&amp;auto=webp&amp;s=6cc246a24272ef7a96c009777a68695b45c5f8e9\n\n",
    "author": "eminaruk",
    "timestamp": "2025-10-15T04:30:44",
    "url": "https://reddit.com/r/computervision/comments/1o78g5r/megasam_a_breakthrough_in_realtime_depth_and/",
    "score": 27,
    "num_comments": 5,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o7ubcx",
    "title": "Fine tuning Vertex classification model with niche data",
    "content": "TLDR; I‚Äôm a software engineer who‚Äôs been hacking together a niche dataset with 50k self taken images across 145 labels .  How can I improve accuracy within the Vertex image classification?  Vertex docs for me don‚Äôt help a newbie \n\nI‚Äôve been working on a mobile app for almost 2 years. We are using image recognition for a niche outdoor sports related product. At the very beginning, I picked Google vertex because it seemed to be easy enough to add our custom images to their model, and train, and use the output\n\nBecause of the thing we are using image recognition for his niche, the default models struggle a bit. Don‚Äôt get me wrong. It works quite well majority of the time.  But consumers don‚Äôt care about majority.  \n\nI saw recently that there is an option to fine tune the model. But honestly, I don‚Äôt understand how this works.  [docs](https://cloud.google.com/vertex-ai/docs/tutorials/custom-training-pipelines/image-classification).  \n\n\nMy cofounder and I are going back-and-forth on whether or not to try to hire a company to help build out but I thought I would try doing what I can first.\n\nWhat does fine-tuning really do?\nHow do you control? What is tuned?\nIs fine-tuning a good idea for niche data sets?\n\nMaybe I‚Äôm barking up the wrong tree‚Ä¶\n\n",
    "author": "lucksp",
    "timestamp": "2025-10-15T19:15:44",
    "url": "https://reddit.com/r/computervision/comments/1o7ubcx/fine_tuning_vertex_classification_model_with/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o82tua",
    "title": "How to make AI detect aggressive behavior in kids/adults?",
    "content": "Hey everyone, I‚Äôm working on a project to spot aggressive actions in kindergartens using computer vision. I tried YOLO8 on 4000 staged videos, but it‚Äôs not great at spotting aggression.\n\nI‚Äôm thinking of using pose estimation plus an action recognition model like MMAction2 to look at sequences of frames.\n\nHas anyone tried something like this? Any tips on making it more accurate or improving the dataset?",
    "author": "Street-Lie-2584",
    "timestamp": "2025-10-16T03:45:08",
    "url": "https://reddit.com/r/computervision/comments/1o82tua/how_to_make_ai_detect_aggressive_behavior_in/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o7qxyj",
    "title": "Pangolin issue ORB-SLAM3 Visualization on Apple Silicon Mac M1",
    "content": "Hi everyone,\n\nI‚Äôm currently running **ORB-SLAM3** on my **Apple Silicon MacBook M1**, using the **KITTI dataset**.  \nWhen I execute the program, I encounter the following error (see attached screenshot):\n\n  \n\\*\\*\\* Terminating app due to uncaught exception 'NSInternalInconsistencyException', reason: 'nextEventMatchingMask should only be called from the Main Thread!'\n\nhttps://preview.redd.it/nkh76dsy2dvf1.png?width=2292&amp;format=png&amp;auto=webp&amp;s=6046c30f68a7eb296b0d45cc6e315263e02d74e9\n\n  \nAfter some debugging, I found that this issue comes from the line in `mono_kitti.cc`:\n\nORB\\_SLAM3::System SLAM(argv\\[1\\], argv\\[2\\], ORB\\_SLAM3::System::MONOCULAR, true);\n\nIt seems that **Pangolin visualization** is enabled by default (`true`).  \nWhen I disable it by changing the flag to `false`, the crash disappears ‚Äî but of course, I lose visualization entirely.\n\nWhat I really want is to have **Pangolin visualization working properly** on macOS.  \nI‚Äôve tried asking ChatGPT multiple times and even explored alternatives like **Open3D**, but that only made things worse.\n\nHas anyone successfully run ORB-SLAM3 with Pangolin visualization on **macOS / Apple Silicon (M1)**?  \nAny advice or workaround would be greatly appreciated.\n\nThanks in advance!",
    "author": "Quocanh987",
    "timestamp": "2025-10-15T16:39:27",
    "url": "https://reddit.com/r/computervision/comments/1o7qxyj/pangolin_issue_orbslam3_visualization_on_apple/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6maim",
    "title": "Next-Gen LiDAR Powered by Neural Networks | One of the Top 2 Computer Vision Papers of 2025",
    "content": "I just came across a fantastic research paper that was selected as one of the top 2 papers in the field of Computer Vision in 2025 and it‚Äôs absolutely worth a read. The topic is a next-generation LiDAR system enhanced with neural networks. This work uses time-resolved flash LiDAR data, capturing light from multiple angles and time intervals. What‚Äôs groundbreaking is that it models not only direct reflections but also indirect reflected and scattered light paths. Using a neural-network-based approach called Neural Radiance Cache, the system precisely computes both the incoming and outgoing light rays for every point in the scene, including their temporal and directional information. This allows for a physically consistent reconstruction of both the scene geometry and its material properties. The result is a much more accurate 3D reconstruction that captures complex light interactions, something traditional LiDARs often miss. In practice, this could mean huge improvements in autonomous driving, augmented reality, and remote sensing, providing unmatched realism and precision. Unfortunately, the code hasn‚Äôt been released yet, so I couldn‚Äôt test it myself, but it‚Äôs only a matter of time before we see commercial implementations of systems like this.\n\n  \n[https://arxiv.org/pdf/2506.05347](https://arxiv.org/pdf/2506.05347)\n\nhttps://preview.redd.it/po57jm0d64vf1.png?width=1371&amp;format=png&amp;auto=webp&amp;s=53ee1e6a133ef5dafccdaa4e1ddecbc552b570ff",
    "author": "eminaruk",
    "timestamp": "2025-10-14T10:42:11",
    "url": "https://reddit.com/r/computervision/comments/1o6maim/nextgen_lidar_powered_by_neural_networks_one_of/",
    "score": 90,
    "num_comments": 2,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o7ac63",
    "title": "How to detect slight defects and nanoscale anomalies in the visual inspection tasks?",
    "content": "Even small visual defects, such as a missing hole, a tiny crack, or a slight texture inconsistency on a PCB, can have serious consequences, from electrical failure to degraded performance.\n\nIn our current research, we have been exploring an AI-driven inspection approach that combines object detection, defect classification, anomaly Inspection to identify subtle or random anomalies in large image datasets. This system processes microscope images in real time and flags areas that deviate from learned normal patterns, helping to reduce manual fatigue and bias in the inspection process.\n\nI'd really like to hear from others in this field: How do you detect defects or anomalies in complex image data?",
    "author": "Downtown_Pea_3413",
    "timestamp": "2025-10-15T06:00:23",
    "url": "https://reddit.com/r/computervision/comments/1o7ac63/how_to_detect_slight_defects_and_nanoscale/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6fkhl",
    "title": "RF-DETR vs YOLOv12: A Comprehensive Comparison of Transformer and CNN-Based Object Detection",
    "content": "Read the full blog here: [https://farukalamai.substack.com/p/rf-detr-vs-yolov12-a-comprehensive](https://farukalamai.substack.com/p/rf-detr-vs-yolov12-a-comprehensive) ",
    "author": "yourfaruk",
    "timestamp": "2025-10-14T06:30:26",
    "url": "https://reddit.com/r/computervision/comments/1o6fkhl/rfdetr_vs_yolov12_a_comprehensive_comparison_of/",
    "score": 139,
    "num_comments": 12,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o77ily",
    "title": "Parking Lot Management System",
    "content": "Hello,\n\nWe are building a Parking Lot Management System. We will show the basic details like how many slots are empty and filled.\n\nCurrently we trying to build this using [YOLO Parking Management](https://docs.ultralytics.com/guides/parking-management/), but it's not giving the desired output. \n\nOutput video1 -&gt; [https://drive.google.com/file/d/1rvQ-9OcMM47CdeHqhf0wvQj3m8nOIDzs/view?usp=sharing](https://drive.google.com/file/d/1rvQ-9OcMM47CdeHqhf0wvQj3m8nOIDzs/view?usp=sharing)\n\nOutput video2 -&gt; [https://drive.google.com/file/d/10jG6wAmnX9ZIfbsbPFlf66jjLaeZvx7n/view?usp=sharing](https://drive.google.com/file/d/10jG6wAmnX9ZIfbsbPFlf66jjLaeZvx7n/view?usp=sharing)\n\n  \nAny suggestion of how to make YOLO work?\n\nAny other libraries which give better results?\n\n  \nTIY",
    "author": "Yuvraj128",
    "timestamp": "2025-10-15T03:39:47",
    "url": "https://reddit.com/r/computervision/comments/1o77ily/parking_lot_management_system/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6osts",
    "title": "What are the job prospects for undergrads focusing on computer vision?",
    "content": "I‚Äôm an undergrad majoring in computer science and really interested in computer vision (image recognition, object detection, etc.).  \nI‚Äôd like to know how the job market looks for undergrads in this field ‚Äî are there decent entry-level roles or research assistant positions, or is a master‚Äôs usually needed to break in?",
    "author": "lazerbeamfan30000",
    "timestamp": "2025-10-14T12:14:48",
    "url": "https://reddit.com/r/computervision/comments/1o6osts/what_are_the_job_prospects_for_undergrads/",
    "score": 17,
    "num_comments": 9,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6vbf4",
    "title": "Videos Explaining Recent Computer Vision Papers",
    "content": "I am looking for a YouTube channel or something similar that explains recent CV research papers. I find it challenging at this stage to decipher those papers on my own.",
    "author": "PhD-in-Kindness",
    "timestamp": "2025-10-14T16:29:48",
    "url": "https://reddit.com/r/computervision/comments/1o6vbf4/videos_explaining_recent_computer_vision_papers/",
    "score": 4,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o7kngm",
    "title": "Does this used computer vision?",
    "content": "",
    "author": "Fantastic-Light-2925",
    "timestamp": "2025-10-15T12:28:19",
    "url": "https://reddit.com/r/computervision/comments/1o7kngm/does_this_used_computer_vision/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.13,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5rhxe",
    "title": "SLAM Camera Board",
    "content": "Hello, I have been building a compact VIO/SLAM camera module over past year.\n\nCurrently, this uses camera + IMU and outputs estimated 3d position in real-time ON-DEVICE. I am now working on adding lightweight voxel mapping all in one module.\n\nI will try to post updates here if folks are interested. Otherwise on X too: https://x.com/_asadmemon/status/1977737626951041225",
    "author": "twokiloballs",
    "timestamp": "2025-10-13T11:13:34",
    "url": "https://reddit.com/r/computervision/comments/1o5rhxe/slam_camera_board/",
    "score": 493,
    "num_comments": 46,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o69mbj",
    "title": "Looking for Modern Computer Vision book",
    "content": "**Hey everyone,**  \nI‚Äôm a computer science student trying to improve my skills in computer vision. I came across the book *Modern Computer Vision* by V. Kishore Ayyadevara and Yeshwanth Reddy, but unfortunately, I can‚Äôt afford to buy it right now.\n\nIf anyone has a PDF version of the book and can share it , I‚Äôd really appreciate it. I‚Äôm just trying to learn and grow my skills.",
    "author": "Educational_Sail_602",
    "timestamp": "2025-10-14T01:11:01",
    "url": "https://reddit.com/r/computervision/comments/1o69mbj/looking_for_modern_computer_vision_book/",
    "score": 35,
    "num_comments": 16,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6pdrf",
    "title": "Recent Turing Post article highlights Stanford‚Äôs PSI among emerging world models",
    "content": "Turing Post published a feature on ‚Äúworld models you should know‚Äù ([link](https://www.turingpost.com/p/cwm?utm_source=chatgpt.com)), covering several new approaches - including Meta‚Äôs Code World Model (CWM) and Stanford‚Äôs Probabilistic Structure Integration (PSI) from the NeuroAI (SNail) Lab.\n\nThe article notes a growing trend in self-supervised video modeling, where models aim to predict and reconstruct future frames while internally discovering mid-level structure such as optical flow, depth, and segmentation. PSI, for example, uses a probabilistic autoregressive model trained on large-scale video data and applies causal probing to extract and reintegrate those structures into training.\n\nFor practitioners in computer vision, this signals a shift from static-image pretraining toward dynamic, structure-aware representations - potentially relevant for motion understanding, robotics, and embodied perception.\n\nFull piece: [Turing Post ‚Äì ‚ÄúWorld Models You Should Know‚Äù](https://www.turingpost.com/p/cwm?utm_source=chatgpt.com)",
    "author": "Appropriate-Web2517",
    "timestamp": "2025-10-14T12:36:43",
    "url": "https://reddit.com/r/computervision/comments/1o6pdrf/recent_turing_post_article_highlights_stanfords/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6ft0f",
    "title": "Liveness Detection Project üì∑üîÑ‚úÖ",
    "content": "* üïπ Try out: [https://antal.ai/projects/liveness-detection.html](https://antal.ai/projects/liveness-detection.html)\n* üí° Learn more: [https://antal.ai/demo/livenessdetector/demo.html](https://antal.ai/demo/livenessdetector/demo.html)\n* üìñ Code documentation:¬†[https://antal.ai/demo/livenessdetector/documentation/index.html](https://antal.ai/demo/livenessdetector/documentation/index.html)\n\nThis project is designed to verify that a user in front of a camera is a live person, thereby preventing spoofing attacks that use photos or videos. It functions as a challenge-response system, periodically instructing the user to perform simple actions such as blinking or turning their head. The engine then analyzes the video feed to confirm these actions were completed successfully. I compiled the project to WebAssembly using Emscripten, so you can try it out on my website in your browser. If you like the project, you can purchase it from my website. The entire project is written in C++ and depends solely on the OpenCV library. If you purchase, you will receive the complete source code, the related neural networks, and detailed documentation. ",
    "author": "Gloomy_Recognition_4",
    "timestamp": "2025-10-14T06:40:07",
    "url": "https://reddit.com/r/computervision/comments/1o6ft0f/liveness_detection_project/",
    "score": 9,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6vs5y",
    "title": "event-based sensors/cameras/vision engineering jobs",
    "content": "",
    "author": "erTomih",
    "timestamp": "2025-10-14T16:50:27",
    "url": "https://reddit.com/r/computervision/comments/1o6vs5y/eventbased_sensorscamerasvision_engineering_jobs/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6k5c4",
    "title": "Fine-tuning real-time object detection models on a small dataset",
    "content": "Hi everyone,\n\nI'm currently looking to use real-time DETR-based models, such as RT-DETR and RF-DETR, for a task involving training on a small dataset. For each object class, I might only have about a dozen images.\n\nWould you recommend focusing on finding good hyperparameters for fine-tuning, or should I consider inserting new modules to aid the fine-tuning process?\n\nAny other suggestions or advice for this kind of task would also be greatly appreciated.\n\nThanks in advance!",
    "author": "Far_Caterpillar_1167",
    "timestamp": "2025-10-14T09:24:09",
    "url": "https://reddit.com/r/computervision/comments/1o6k5c4/finetuning_realtime_object_detection_models_on_a/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6r7kn",
    "title": "final Project ideas",
    "content": "Hey guys I'm trying to find a final project idea since it's a requirement for my grade in high school that I do project related to my course which is informatics, I know the project that I want to develop will be something that envolves mobile+computer Vision, but I can't find any good ideas, I even went to [devpost.com](http://devpost.com) for ideas but nothing crazy showed up so I came to you guys for ideas, any ideas?",
    "author": "AromaticFerret4583",
    "timestamp": "2025-10-14T13:44:26",
    "url": "https://reddit.com/r/computervision/comments/1o6r7kn/final_project_ideas/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6eg4g",
    "title": "YOLO-based image search engine: EyeInside",
    "content": "Hi everyone,\n\nI developed a software named EyeInside to search images in folders full of thousands of images. It works with YOLO. You type the object and then YOLO starts to look at images in the folder. If YOLO finds the object in an image or images , it shows them. \n\nYou can also count people in an image. Of course, this is also done by YOLO.\n\nYou can add your own-trained YOLO model and search fot images with it. One thing to remember, YOLO can't find the objects that it doesn't know, so do EyeInside. \n\nYou can download and install EyeInside from [here.](https://github.com/oguz81/EyeInsideV1/releases/tag/v1.0) You can also fork the repo to your GitHub and develop with your ideas.\n\nCheck out the EyeInside GitHub repo: [GitHub: EyeInside](https://github.com/oguz81/EyeInsideV1)\n\nhttps://preview.redd.it/j9266cmik2vf1.png?width=1252&amp;format=png&amp;auto=webp&amp;s=18d77e6380d4b6fa92c7d98cb3a38b09c0ebdca3\n\n",
    "author": "ucantegmen",
    "timestamp": "2025-10-14T05:42:58",
    "url": "https://reddit.com/r/computervision/comments/1o6eg4g/yolobased_image_search_engine_eyeinside/",
    "score": 5,
    "num_comments": 8,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6gmul",
    "title": "Face Landmark Detection with AlbumentationsX: Keypoint Label Swapping",
    "content": "In version 2.0.12 of AlbumentationsX, I've added a long awaited feature (I guess, first time it was asked about 6 years ago) of a semantic label swap.\n\n  \nThe issue is that when we perform a transform that changes the orientation of the space:  \n\\- VerticalFlip  \n\\- HorizontalFlip  \n\\- Transpose  \n\\- Some ways in D4/SquareSymmetry  \n  \nWe may have left and right eye to change coordinates, but to make the label semantically meaningful, we need to swap the labels as well. \n\n\\----  \nIt was a long awaited request in Albumentations. Finally added.\n\nLink in this post is an example notebook how to use the semantic label swapping during training.",
    "author": "ternausX",
    "timestamp": "2025-10-14T07:12:52",
    "url": "https://reddit.com/r/computervision/comments/1o6gmul/face_landmark_detection_with_albumentationsx/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6b8g7",
    "title": "Dataset release (unannotated): Real-world retail images (2014) + three full-store reference visits.",
    "content": "Happy to release some of our 1m image datasets for the wider community to work with.\n\n2014 set (full-res), unannotated, ships with manifest.csv (sha256, EXIF, dims, optional GPS). c. 6000 images across 22 retailers. These are of numerous elements in stores.\n\n‚Ä¢ Reference visits: Tesco Lincoln 2014, Tesco Express 2015, Asda Leeds 2016 (unannotated; each with manifest). These are full stores (2014 not bay by bay but the other two stores are) c. 1910 items.\n\n‚Ä¢ Purpose: robustness, domain shift, shelf complexity, spatial awareness in store alongside wider developmental work.\n\n‚Ä¢ License: research/eval only; no redistribution.\n\n‚Ä¢ Planned v2: 2014 full annotations (PriceSign, PromoBarker, ShelfLabel, ProductBlock in some cases) alongside numerous other tags around categories, retailer, promo etc.\n\nContact: [happytohelp@groceryinsight.com](mailto:happytohelp@groceryinsight.com) for access and manifests.",
    "author": "malctucker",
    "timestamp": "2025-10-14T02:55:07",
    "url": "https://reddit.com/r/computervision/comments/1o6b8g7/dataset_release_unannotated_realworld_retail/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o6ce8h",
    "title": "[D] 3DV 2026: Still showing ‚Äú0 Official Reviews Submitted‚Äù on OpenReview after the review deadline ‚Äî is this normal?",
    "content": "Hi everyone,\n\nI submitted a paper to **3DV 2026**, and according to the conference timeline, the **review deadline has already passed**. However, when I check my submission on OpenReview, it still says:\n\n&gt;\n\nDoes this mean that no reviewers have submitted their reviews yet, or is it normal for authors not to see any reviews at this stage?\n\nI checked the author guidelines, which state that:\n\n&gt;\n\nSo I‚Äôm wondering ‚Äî if there‚Äôs no rebuttal, are reviews completely hidden from authors until the final decision, or should they appear later on OpenReview?\n\nHas anyone experienced the same thing with 3DV or similar conferences that use OpenReview but don‚Äôt have a rebuttal phase?\n\nThanks in advance for your insights!",
    "author": "Equivalent_Ostrich_6",
    "timestamp": "2025-10-14T04:01:11",
    "url": "https://reddit.com/r/computervision/comments/1o6ce8h/d_3dv_2026_still_showing_0_official_reviews/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o60u8y",
    "title": "Career advice",
    "content": "Hi everyone! I was hoping to get some honest career advice in this sub so I'll get straight to the point. I hold a PhD in computational physics from a US ivy. I graduated in December 2023. My dissertation involved modern C++, Python and numerical algorithms for partial differential equations in CFD. After deciding to get out of academia, I went back to my home town in Colombia, where I did whatever industry job my technical skills could get me.\n\nAfter a boring 6-month job as a data scientist at a bank, I landed an R&amp;D job where, among other duties, I trained my first CNNs for a somewhat challenging detection problem. After almost a year in that job, last month I moved back to the US following a great career shift my American spouse was offered. Now, again, I'm currently trying to find a job.\n\nAfter my last job I got very interested in computer vision, deep learning, and even more specific stuff like nerfs. I know the basics of CV, DL, and of course I have a strong math, physics, and numerical computing background from school.\n\nHere's my question to experienced CV engineers in this sub: what would you advice a scientist with my background in order to break into this field and land a job? Is there any concrete way in which I can use my background to land a job in this current market?\n\nThank you for your honest reply!",
    "author": "Ok-Cucumber-5204",
    "timestamp": "2025-10-13T17:18:01",
    "url": "https://reddit.com/r/computervision/comments/1o60u8y/career_advice/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5p8g9",
    "title": "Last week in Multimodal AI - Vision Edition",
    "content": "I curate a weekly newsletter on multimodal AI. Here are the vision-related highlights from last week:\n\n# StreamDiffusionV2 - Real-Time Interactive Video Generation\n\n‚Ä¢Fully open-source streaming system for video diffusion.\n\n‚Ä¢Achieves 42 FPS on 4x H100s and 16.6 FPS on 2x RTX 4090s.\n\n‚Ä¢[Twitter](https://x.com/Chenfeng_X/status/1975453498197078080) | [Project Page](https://streamdiffusionv2.github.io/) | [GitHub](https://github.com/chenfengxu714/StreamDiffusionV2)\n\nhttps://reddit.com/link/1o5p8g9/video/ntlo618bswuf1/player\n\n# Meta SSDD - Efficient Image Tokenization\n\n‚Ä¢Single-step diffusion decoder for faster and better image tokenization.\n\n‚Ä¢3.8x faster sampling and superior reconstruction quality.\n\n‚Ä¢[Paper](https://arxiv.org/abs/2510.04961)\n\n[Left: Speed-quality Pareto-front for different state-of-the-art f8c4 feedforward and diffusion autoencoders. Right: Reconstructions of KL-VAE and SSDD models with similar throughput. Bottom: High-level overview of our method.](https://preview.redd.it/plbighaeswuf1.png?width=1456&amp;format=png&amp;auto=webp&amp;s=c8cc3d342bb70aabf86b42ebf3881521209d9bf9)\n\n# Character Mixing for Video Generation\n\n‚Ä¢Framework for natural cross-character interactions in video.\n\n‚Ä¢Preserves identity and style fidelity.\n\n‚Ä¢[Twitter](https://x.com/tingtin36139994/status/1975861545637953819) | [Project Page](http://tingtingliao.github.io/mimix/) | [GitHub](https://github.com/TingtingLiao/mimix) | [Paper](https://arxiv.org/pdf/2510.05093)\n\nhttps://reddit.com/link/1o5p8g9/video/pe93d9agswuf1/player\n\n# ChronoEdit - Temporal Reasoning for Image Editing\n\n‚Ä¢Reframes image editing as a video generation task for temporal consistency.\n\n‚Ä¢[Twitter](https://x.com/HuanLing6/status/1975624827261362408) | [Project Page](https://research.nvidia.com/labs/toronto-ai/chronoedit/) | [Paper](https://arxiv.org/abs/2510.04290)\n\nhttps://reddit.com/link/1o5p8g9/video/4u1axjbhswuf1/player\n\n# VLM-Lens - Interpreting Vision-Language Models\n\n‚Ä¢Toolkit for systematic benchmarking and interpretation of VLMs.\n\n‚Ä¢[Twitter](https://x.com/ziqiao_ma/status/1974183755649523939) | [GitHub](https://github.com/compling-wat/vlm-lens) | [Paper](https://arxiv.org/abs/2510.02292)\n\nSee the full newsletter for more demos, papers, more): [https://thelivingedge.substack.com/p/multimodal-monday-28-diffusion-thinks](https://thelivingedge.substack.com/p/multimodal-monday-28-diffusion-thinks)",
    "author": "Vast_Yak_4147",
    "timestamp": "2025-10-13T09:53:46",
    "url": "https://reddit.com/r/computervision/comments/1o5p8g9/last_week_in_multimodal_ai_vision_edition/",
    "score": 13,
    "num_comments": 1,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o68c3k",
    "title": "Lazyeat! A touch-free controller for use while eating!",
    "content": "https://i.redd.it/r7xln8cox8ue1.gif\n\nHere is the repo:  \n[https://github.com/lanxiuyun/lazyeat](https://github.com/lanxiuyun/lazyeat)",
    "author": "Striking_Salary_7698",
    "timestamp": "2025-10-13T23:46:35",
    "url": "https://reddit.com/r/computervision/comments/1o68c3k/lazyeat_a_touchfree_controller_for_use_while/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5p96a",
    "title": "We just produced the White Edition of TEMAS",
    "content": "Hey folks,\nafter months of focusing on the tech side, we finally produced our White Edition of the modular 3D vision kit TEMAS.\n\nIt‚Äôs the same core setup.\n\nWe‚Äôre now running sealing and durability tests to see how it performs in daily use. The black version stays our standard for robotics and industrial setups, but the white one opens up new use cases.\n\nCurious what you think ‚Äî would you ever prefer a clean white look for lab or indoor robotics gear?\n\n[Kickstarter](https://www.kickstarter.com/projects/temas/temas-powerful-modular-sensor-kit-for-robotics-and-labs)",
    "author": "Big-Mulberry4600",
    "timestamp": "2025-10-13T09:54:28",
    "url": "https://reddit.com/r/computervision/comments/1o5p96a/we_just_produced_the_white_edition_of_temas/",
    "score": 10,
    "num_comments": 2,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5o5bo",
    "title": "[Feedback] FocoosAI Computer Vision Open Source SDK and Web Platform",
    "content": "https://reddit.com/link/1o5o5bo/video/axrz6usgmwuf1/player\n\n  \nHi everyone, I‚Äôm an AI SW engineer at[ ](https://focoos.ai)[focoos.ai](https://focoos.ai).  \nWe're developing a platform and a Python SDK aiming to simplify the workflow to train, fine-tune, compare and deploy computer vision models. I'd love to hear some **honest feedback and thoughts** from the community!\n\nWe‚Äôve developed a collection of **optimized computer vision** pre-trained models, available on **MIT license**, based on:\n\n* **RTDetr** for object detection\n* **MaskFormer &amp; BisenetFormer** for semantic and instance segmentation\n* **RTMO** for keypoints estimation¬†\n* **STDC** for classification\n\nThe **Python SDK** ([GitHub](https://github.com/FocoosAI/focoos)) allows you to use, train, export pre-trained and custom models. All our models are exportable with optimized engines, such as **ONNX with TensorRT** support or TorchScript, for high performance inference.\n\nOur **web platform** ([app.focoos.ai](https://app.focoos.ai)) provides a **no-code** environment that allows users to leverage our pre-trained models, import their own datasets or use public ones to train new models, monitor training progress, compare different runs and deploy models seamlessly in the cloud or on-premises.\n\nIn this early stage we offer a **generous free tier**: 10hr of T4 cloud training, 5GB of storage and 1000 cloud inferences.\n\nThe SDK and the platform are designed to **work seamlessly together**. For instance, you can train a model locally while tracking metrics online just like wandb. You can also use a remote dataset for local training, or perform local inference with¬†models trained on the platform.\n\nWe‚Äôre aiming for high performance and simplicity: faster inference, lower compute cost, and a smoother experience.\n\nIf you‚Äôre into computer vision and want to try a new workflow, we‚Äôd really appreciate your thoughts:\n\n* How does it compare to your current setup?\n* Any blockers, missing features, or ideas for improvement?\n\nWe‚Äôre still early and actively improving things, **so your feedback really helps us build something valuable for the community**.",
    "author": "AcanthisittaOk598",
    "timestamp": "2025-10-13T09:15:00",
    "url": "https://reddit.com/r/computervision/comments/1o5o5bo/feedback_focoosai_computer_vision_open_source_sdk/",
    "score": 7,
    "num_comments": 3,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5unli",
    "title": "How to evaluate poses from a pose detection model?",
    "content": "Im starting work on my Bachelor Thesis and my subject will be pose estimation on Medieval Manuscripts, right now im drafting the actual research question with my supervisor and so far the plan is roughly to use a model like OpenPose on the dataset and then evaluate the results for poses, hand gestures etc.\n\nBut as we were talking about the evaluation of the poses, we sort of ran out of ideas for a quality focused evaluation.\n\nFirst off, the data set I'll be using doesn't have any pose estimation focused annotations, so no keypoints or bounding boxes for people. It has some basic annotations about the bible scene it depicts and also about saints etc., but nothing that could really be used for evaluating the poses themselves.  The dataset has around 12k images, so labeling it all by hand is out of the question.\n\nOur first idea is to use a segmentation/object detection model to find as many people as possible on the pages and then generate crops based on the output before then using for example OpenPose for pose recognition on these crops. But suppose all of these crops were perfect and would only depict one person, how could we validate the correctness of a pose without checking manually?\n\nMy idea was to use a measurement based on joint angles, basically ruling out impossible situations that imply abnormally twisted joints in actual humans. But so far none of us were not able to find any papers using a similar approach, which would be very helpful, since proposing an evaluation like this is quite hard to do correctly and according to scientific standard. So I was wondering if anyone here might know an already tried approach for something like this or can maybe recommend a paper.\n\nBesides that we were also talking about a quantitative evaluation, where we would use a ratio of expected keypoints vs actually detected keypoints as a 2nd measure of correctness. But this of course will have its own issues since in reality not all of our crops will contain exactly one person or a person who has all of their joints/limbs in a visible position.  Are there any other measures we could try, given that there are no proper annotations for this dataset?\n\nEdit: here's an example [https://imgur.com/a/fPkxb6m](https://imgur.com/a/fPkxb6m)",
    "author": "Fdffed",
    "timestamp": "2025-10-13T13:07:17",
    "url": "https://reddit.com/r/computervision/comments/1o5unli/how_to_evaluate_poses_from_a_pose_detection_model/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5tgux",
    "title": "Is a Raspberry Pi Zero 2W powerful enough for a vision-controlled robotic desk lamp?",
    "content": "Hey everyone,\n\nI‚Äôm planning a project where a camera detects a white sheet of paper on a desk, and a robotic arm automatically moves a small lamp so that the light always stays focused on the paper.\n\nHere‚Äôs the idea:\n\t‚Ä¢\tA Pi Camera captures live video.\n\t‚Ä¢\tOpenCV runs on the Raspberry Pi to detect the white area (the paper) and track its position.\n\t‚Ä¢\tA PCA9685 servo driver (connected via I¬≤C) generates PWM signals to control several servo motors that move the arm.\n\t‚Ä¢\tThe system continuously tracks the paper‚Äôs movement in real time and adjusts the lamp accordingly.\n\nI originally planned to use a Raspberry Pi 4, but I‚Äôm wondering if the Pi Zero 2W would be powerful enough to handle the camera input and basic OpenCV tracking (grayscale conversion, thresholding, contour detection, centroid calculation) while communicating with the PCA9685 over I¬≤C.\n\nHas anyone tried a similar vision-based tracking project on a Pi Zero 2W?\nAny tips, performance insights, or examples would be greatly appreciated ‚Äî or if you‚Äôve done something similar, I‚Äôd love to hear about your experience!\n\nThanks a lot üôå\n",
    "author": "CelestialDragon127",
    "timestamp": "2025-10-13T12:24:09",
    "url": "https://reddit.com/r/computervision/comments/1o5tgux/is_a_raspberry_pi_zero_2w_powerful_enough_for_a/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5vook",
    "title": "Training machine learning models for optical flow/depth",
    "content": "Hey guys, wanted to get feedback on the community about cropping based augmentation during training models for depth/optical flow.\n1. Cropping to a smaller resolution should speed up the training, but are there any drawbacks?\n2. Is there a ratio of crop size vs input image resolution that impacts model training?",
    "author": "Purple_Bumblebee1755",
    "timestamp": "2025-10-13T13:44:58",
    "url": "https://reddit.com/r/computervision/comments/1o5vook/training_machine_learning_models_for_optical/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5e02d",
    "title": "jax-raft: Faster Jax/Flax implementation of the RAFT optical flow estimator",
    "content": "",
    "author": "Savings-Square572",
    "timestamp": "2025-10-13T01:20:11",
    "url": "https://reddit.com/r/computervision/comments/1o5e02d/jaxraft_faster_jaxflax_implementation_of_the_raft/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5po3m",
    "title": "Need help with a basic CV project",
    "content": "",
    "author": "Half_Minded",
    "timestamp": "2025-10-13T10:08:55",
    "url": "https://reddit.com/r/computervision/comments/1o5po3m/need_help_with_a_basic_cv_project/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5ozpn",
    "title": "Using Edge AI on BeagleY-AI",
    "content": "",
    "author": "SAAAIL",
    "timestamp": "2025-10-13T09:45:15",
    "url": "https://reddit.com/r/computervision/comments/1o5ozpn/using_edge_ai_on_beagleyai/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5ka6a",
    "title": "Looking for a Polycam alternative with an API for large-scale 3D reconstruction",
    "content": "",
    "author": "shmpbr",
    "timestamp": "2025-10-13T06:51:37",
    "url": "https://reddit.com/r/computervision/comments/1o5ka6a/looking_for_a_polycam_alternative_with_an_api_for/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o4x3pg",
    "title": "What IDE to use for computer vision working with Python.",
    "content": "Hello everyone. I'm working on computer vision for my research and I'm tired of all the IDEs around. It is true that I have some constraints with each of the IDEs but I cant find a solution for prototyping with respect to working with image projects.\n\nSome background as to my constraints: I'm using Linux because of overall ease of use, and access to software. I don't want to use terminal based IDEs since image rendering is not direct in the terminal. I also would like the IDEs to be easily configurable so that I can implement the changes as per my need. \n\n- I use Jupyter notebook and I don't think I'll stop using it anytime but it's very difficult to prototype in jupyter notebook. I use it to test others' notebooks and create a final output for showcase but it's not fast enough for trial and error. \n\n- I really caught up with using Spyder as an IDE but it tends to crash a lot, with and without running it in a virtual environment. It also doesn't seem right to run an IDE in a virtual environment. I also can't easily run plug-ins such as vim plugin in spyder and it crashes a lot. The feature to run only selected parts of the code as well as the variable explorer feature is phenomenal but I hate that it crashes from time to time. Tried installing via conda forge, conda, through arch repository but to no avail. \n\n- I like emacs as an IDE but I find trouble with running images in line. The output plots and images tend to pop up outside emacs and not in line unless I use the EIN package. Also I don't know of any features like the variable explorer or separate window where all the plots are saved. \n\n- I tried pycharm but as of now I've not tried it enough to enjoy it. The plugin management is also a bit clanky afaik but it's seamless integrating plugin in emacs. \n\n- (edit:) I don't prefer using vscode due to the closed nature and the non intuitive method of customising the IDE. I know it's more of a philosophical reason but I believe it is a hindrance to the flexibility of the development environment. Also I know that Libre alternatives are there for vscode but since I can't tinker with it using literate programming minimally, I don't prefer using it, unless absolutely necessary. Let's say it's less hackable and demanding on resources. \n\nSo I would like your views and opinions on the setups and toolings used for your needs.\n\nAlso there's the python dependency hell as well as the virtual environment issue. So although this is a frequently asked question, I would like your opinions on that too as well. My first priority is minimalism over simplicity, and simplicity over abstraction. ",
    "author": "Harishnkr",
    "timestamp": "2025-10-12T11:43:58",
    "url": "https://reddit.com/r/computervision/comments/1o4x3pg/what_ide_to_use_for_computer_vision_working_with/",
    "score": 17,
    "num_comments": 27,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o4slw2",
    "title": "Real-time shooter Pose + Gun detection using YOLO",
    "content": "Here is the GitHub repo guys and let me know what you think : https://github.com/putbullet/firearms-detection-system",
    "author": "Annual_Ebb9158",
    "timestamp": "2025-10-12T08:52:03",
    "url": "https://reddit.com/r/computervision/comments/1o4slw2/realtime_shooter_pose_gun_detection_using_yolo/",
    "score": 28,
    "num_comments": 7,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o4lz3y",
    "title": "Desk bot update 0 - Mechatronic head with real-time face tracking + ROS2",
    "content": "",
    "author": "Outrageous-Bet2558",
    "timestamp": "2025-10-12T03:49:45",
    "url": "https://reddit.com/r/computervision/comments/1o4lz3y/desk_bot_update_0_mechatronic_head_with_realtime/",
    "score": 51,
    "num_comments": 0,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o4lrq1",
    "title": "I wrote some optimizers for TensorFlow",
    "content": "Hello everyone, I wrote some optimizers for TensorFlow. If you're using TensorFlow, they should be helpful to you.\n\n[https://github.com/NoteDance/optimizers](https://github.com/NoteDance/optimizers)",
    "author": "NoteDancing",
    "timestamp": "2025-10-12T03:37:08",
    "url": "https://reddit.com/r/computervision/comments/1o4lrq1/i_wrote_some_optimizers_for_tensorflow/",
    "score": 15,
    "num_comments": 3,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o54636",
    "title": "¬øBest webcam for computer vision in 2025?",
    "content": "¬øWhat is the best webcam for computer vision in 2025?, if im starting in this and i wnat to do all kind of projects, i see logitech c270 and logitech c920 very recommend it but the c270 dont have tripod so i can't put it in a specific position, i also see the raspi camera, but i dont know what to choose, ¬øwhat do you think?, thank you for reading this.",
    "author": "S0meOne3ls3",
    "timestamp": "2025-10-12T16:31:26",
    "url": "https://reddit.com/r/computervision/comments/1o54636/best_webcam_for_computer_vision_in_2025/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o4xe4y",
    "title": "Problems you have faced while designing your AV (Autonomous Vehicle)",
    "content": "Hello guys, so I am currently a CS/AI student (artificial intelligence), and for my final project I have chosen autonomous driving systems with my group of 4. We won't be implementing anything physical, but rather a system to give good performance on CARLA etc. (the focus will be on a novel ai system) We might turn it into a paper later on. I was wondering what could be the most challenging part to implement, what are the possible problems we might face and mostly what were your personal experiences like?",
    "author": "Prior_Advisor_1785",
    "timestamp": "2025-10-12T11:55:14",
    "url": "https://reddit.com/r/computervision/comments/1o4xe4y/problems_you_have_faced_while_designing_your_av/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o4vu1d",
    "title": "Any maintained Action Recognition/Classification frameworks?",
    "content": "Hi everyone, I am currently working on a project that where I would like to have some action classification/recognition models running on top the the pose estimation keypoints extracted from another model. I have been looking for frameworks that have models implemented and easy to use/fine tune (similar to what supervision/ultralytics/roboflow do fo regular CV), but without any luck. This is what I have found thus far:  \n\\- [mmaction2](https://github.com/open-mmlab/mmaction2/pulls) is completely deprecated, with some of the certificates required to install some of the dependencies, like mmcv or mmengine.  \n\\- [pyskl](https://github.com/kennymckormick/pyskl) also deprecated, followed their step-by-step but in the end cant get any of the models to train, and the documentation is very lacking.\n\nDoes anyone know of any lightweight framework that can do this? Implementing myself is an option, just trying to avoid re-doing what might already be done and optimized.\n\nThanks!",
    "author": "_nmvr_",
    "timestamp": "2025-10-12T10:55:44",
    "url": "https://reddit.com/r/computervision/comments/1o4vu1d/any_maintained_action_recognitionclassification/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o50nfy",
    "title": "Semiautomatic Image Labeling",
    "content": "Hi guys,  \nI am currently persuing my first ML vision project in which I have to train a model capable to detect dimensional defects on objects on a conveyor. On the conveyor multiple cookies with multiple shapes are processed simultaneously.   \nI have mainly two questions:\n\n1. Since a criteria for classification are also the dimnesion of the defect (chocolate &lt;1cm is good, =&gt;1 &lt;=2cm is mid and over 2 is trash) I have to resize all the images before processing and in deployment the respective video to a defined scale where 1px converts to Xmm in reality. Is this thought enough or is there more I have to look out? \n2. For the correct labeling I cannot find a usefull, free/open source, software to do all the labeling, is there anyone which has experience? I was looking into modifing an existing open source but I feel like its not something that specific not to have already a solution around and I was wondering if I was missing something.\n\nThank you very much for your help!\n\nBests  \n",
    "author": "Gold_Alarming",
    "timestamp": "2025-10-12T14:01:16",
    "url": "https://reddit.com/r/computervision/comments/1o50nfy/semiautomatic_image_labeling/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5ctg8",
    "title": "20F, looking for someone with roboflow premium",
    "content": "I need help with one of my machine learning projects but I'm unable to download model weights üòî, later found out that, that feature is only available for premium users. And I can't afford to spend 50 bucks for my tiny project...",
    "author": "Fit_Replacement_8351",
    "timestamp": "2025-10-13T00:03:57",
    "url": "https://reddit.com/r/computervision/comments/1o5ctg8/20f_looking_for_someone_with_roboflow_premium/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o5e7oi",
    "title": "Why do people still use OpenCV when there‚Äôs PyTorch/TensorFlow?",
    "content": "I‚Äôve been diving deeper into Computer Vision lately, and I‚Äôve noticed that a lot of tutorials and even production systems still rely heavily on OpenCV  even though deep learning frameworks like PyTorch and TensorFlow have tons of vision-related features built in (e.g., torchvision, tf.image, etc).\n\nIt made me wonder:\nWhy do people still use OpenCV so much in 2025?",
    "author": "That-Percentage-5798",
    "timestamp": "2025-10-13T01:34:04",
    "url": "https://reddit.com/r/computervision/comments/1o5e7oi/why_do_people_still_use_opencv_when_theres/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o431vx",
    "title": "Detecting Aggressive Drivers from a Fixed Camera View Using YOLO + OpenCV",
    "content": "",
    "author": "eminaruk",
    "timestamp": "2025-10-11T11:32:04",
    "url": "https://reddit.com/r/computervision/comments/1o431vx/detecting_aggressive_drivers_from_a_fixed_camera/",
    "score": 83,
    "num_comments": 25,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o4sowz",
    "title": "Confused",
    "content": "I am new to computer vision and building machine learning projects from scratch. I am doing a course in Computer vision and not getting how to start advanced projects from scratch and require model building. I am looking into the image generation domain. Any help would be great!",
    "author": "overthinkingMelon",
    "timestamp": "2025-10-12T08:55:26",
    "url": "https://reddit.com/r/computervision/comments/1o4sowz/confused/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o4c94v",
    "title": "Would YOLOv10 be a good choice for a retail product detection project?",
    "content": "Hey everyone,\n\nI‚Äôm working with my company on a product detection project. The goal is to:\n\n1. Detect individual products from our catalog.\n2. Detect and count all our products on a store shelf at a customer‚Äôs site.\n3. Distinguish our products from competitors‚Äô products on the same shelf.\n\nBasically, we want to automatically count how many of *our* products are present in each customer‚Äôs store display.\n\nI‚Äôm considering using YOLOv10 for this task, but I have a few questions:\n\n* Would YOLOv10 be a good fit for this type of real-world retail detection problem?\n* Roughly how large should our dataset be ( I mean the set number of images or labels) to get good accuracy?\n* What kind of hardware (GPUs, VRAM, etc.) would you recommend for training such a model?\n* How long would it typically take to train a model of this kind?\n\nAny advice or insights from people who‚Äôve done similar object detection or retail shelf analysis projects would be really appreciated!\n\nThanks in advance",
    "author": "[deleted]",
    "timestamp": "2025-10-11T18:17:22",
    "url": "https://reddit.com/r/computervision/comments/1o4c94v/would_yolov10_be_a_good_choice_for_a_retail/",
    "score": 11,
    "num_comments": 8,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o3q98i",
    "title": "Real-time athlete speed tracking using a single camera",
    "content": "We recently shared a tutorial showing how you can estimate an athlete‚Äôs speed in real time using just a regular broadcast camera.  \nNo radar, no motion sensors. Just video.\n\nWhen a player moves a few inches across the screen, the AI needs to understand how that translates into actual distance. The tricky part is that the camera‚Äôs angle and perspective distort everything. Objects that are farther away appear to move slower.\n\nIn our new tutorial, we reveal the computer vision \"trick\" that transforms a camera's distorted 2D view into a real-world map. This allows the AI to accurately measure distance and calculate speed.\n\nIf you want to try it yourself, we‚Äôve shared **resources in the comments.**\n\nThis was built using the Labellerr SDK for video annotation and tracking.\n\nAlso We‚Äôll soon be launching an **MCP integration** to make it even more accessible, so you can run and visualize results directly through your local setup or existing agent workflows.\n\nWould love to hear your thoughts and what all features would be beneficial in the MCP",
    "author": "Full_Piano_3448",
    "timestamp": "2025-10-11T01:25:49",
    "url": "https://reddit.com/r/computervision/comments/1o3q98i/realtime_athlete_speed_tracking_using_a_single/",
    "score": 175,
    "num_comments": 31,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o4mga0",
    "title": "Project Idea",
    "content": "Hi everyone!\nHope you're doing well\n\nI am Automtion and Computer Engineering Student and this is my graduation year ,\nI need an idea for the final year project \n\nI have a prior knowledge on Arduino ,Esp32 and built some robots ,\nI am learning CV rightnow started with Opencv and now looking for small projects to learn with , i am also learning react and my goal is to learn react native to build mobile applications , i will be starting with react native at the 1st of november ,\n\nIam really confused about the ideas , \n\nFirst idea is to build an application for blind people so that whenever the camera captures an object it will describe it ,\n\nAnd i am interested about a project that i don't have any idea if it will work which is scene generation for education purposes , for example in medicine will generate small video of the organs and its details and so on\n\nI would love to see your suggestions and if there is an idea you would like to share i will be thankful \n\nAnd if someone knows about scene generation and these stuff i hope if he describe how the thing will be like , \n\nAnd that's it \n\nThanks for reading ",
    "author": "No-Football8462",
    "timestamp": "2025-10-12T04:17:31",
    "url": "https://reddit.com/r/computervision/comments/1o4mga0/project_idea/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o4a4hv",
    "title": "[P] arXiv Endorsement ‚Äì Real-time Crowd Analytics using Computer Vision + VLMs",
    "content": "I‚Äôm looking for an¬†**arXiv endorsement**¬†in¬†[`cs. CV`](http://cs.cv/)¬†or¬†`eess.IV`¬†to upload a paper on¬†**vision-based crowd analytics and safety monitoring**.\n\n[https://arxiv.org/auth/endorse?x=DVFR4P](https://arxiv.org/auth/endorse?x=DVFR4P)\n\nWe‚Äôve been developing a¬†**real-time crowd analysis system**¬†that uses computer vision and vision-language models (VLMs) to detect high-density zones, flow disruptions, and potential crush conditions across large gatherings.\n\nThe system fuses heatmaps, optical flow, and descriptive VLM outputs to generate human-readable situational insights (e.g., ‚Äúno visible egress path,‚Äù ‚Äúcritical density area,‚Äù etc.) ‚Äî all in real time from multi-camera feeds.\n\nThe paper focuses on:\n\n* Large-scale CV pipelines for¬†**crowd flow and density estimation**\n* **VLM-based contextual reasoning**¬†for real-time scene interpretation\n* Deployment metrics: \\~100+ camera streams, sub-2s latency, adaptive optical flow fusion\n\nIf anyone who‚Äôs published in¬†[`cs.CV`](http://cs.cv/),¬†`eess.IV`, or¬†[`cs.AI`](http://cs.ai/)¬†could endorse my account, I‚Äôd really appreciate it\n\nHappy to share the preprint PDF or discuss technical details if interested.\n\nAlso open to collaboration with folks working on¬†**multimodal perception**,¬†**AI for public safety**, or¬†**VLMs for dynamic scene understanding**.\n\n  \n",
    "author": "alertify",
    "timestamp": "2025-10-11T16:32:58",
    "url": "https://reddit.com/r/computervision/comments/1o4a4hv/p_arxiv_endorsement_realtime_crowd_analytics/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o49h72",
    "title": "Is OpenVX Dead ?",
    "content": "At one point, there was a lot of hype about the OpenVX spec by Khronos for its cross-platform, graph-based runtime for CV and some ML projects. Digging into it, I saw there is some merit to the concepts but quite a steep learning curve.\n\nIs OpenVX still relevant in the CV world ? Is everyone just using ROS or building custom solutions ?\n\nI'm looking for a standard platform / infra I can use for a commercial project with a well-supported community. Hoping to get some feedback from the tenured experts or folks who have gone into the weeds with this!",
    "author": "Eastern-Hall-2632",
    "timestamp": "2025-10-11T16:03:13",
    "url": "https://reddit.com/r/computervision/comments/1o49h72/is_openvx_dead/",
    "score": 6,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o41k0l",
    "title": "Has anyone found a good way to handle labeling fatigue for image datasets?",
    "content": "We‚Äôve been training a CV model for object detection but labeling new data is brutal. We tried active learning loops but accuracy still dips without fresh labels. Curious if there‚Äôs a smarter workflow.",
    "author": "Miserable_Concern670",
    "timestamp": "2025-10-11T10:31:48",
    "url": "https://reddit.com/r/computervision/comments/1o41k0l/has_anyone_found_a_good_way_to_handle_labeling/",
    "score": 9,
    "num_comments": 12,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o43an5",
    "title": "Built a Production Computer Vision System for Document Understanding, 99.9% OCR Accuracy on Real-World Docs",
    "content": "https://preview.redd.it/qnsuhxni1juf1.png?width=1912&amp;format=png&amp;auto=webp&amp;s=c131dd88d7134a7633ebb63ef705b6c9ec3e7d43\n\nhttps://preview.redd.it/otxgwibj1juf1.png?width=1918&amp;format=png&amp;auto=webp&amp;s=8321f39ac82060c3f1f82210de04fa68bb2b3545\n\nhttps://preview.redd.it/jjq41x7k1juf1.png?width=1915&amp;format=png&amp;auto=webp&amp;s=a1726cdb556d4b8867601dc50504d9be6167ddbc\n\nhttps://preview.redd.it/r2c0y27l1juf1.png?width=1915&amp;format=png&amp;auto=webp&amp;s=f3b81ddc2de05a734e75e7195dd253f077a36402\n\n\n\nAfter spending years frustrated with OCR systems that fall apart on anything less than perfect scans, I built Inkscribe AI, a document processing platform using computer vision and deep learning that actually handles real-world document complexity.\n\nThis is a technical deep-dive into the CV challenges we solved and the architecture we're using in production.\n\n**The Computer Vision Problem:**\n\nMost OCR systems are trained on clean, high-resolution scans. They break on real-world documents: handwritten annotations on printed text, multi-column layouts with complex reading order, degraded scans from 20+ year old documents, mixed-language documents with script switching, documents photographed at angles with perspective distortion, low-contrast text on textured backgrounds, and complex tables with merged cells and nested structures.\n\nWe needed a system robust enough to handle all of this while maintaining 99.9% accuracy.\n\n**Our Approach:**\n\nWe built a multi-stage pipeline combining classical CV techniques with modern deep learning:\n\n**Stage 1: Document Analysis &amp; Preprocessing** \n\nPerspective correction using homography estimation, adaptive binarization accounting for uneven lighting and background noise, layout analysis with region detection (text blocks, tables, images, equations), reading order determination for complex multi-column layouts, and skew correction and dewarping for photographed documents.\n\n**Stage 2: Text Detection &amp; Recognition** \n\nCustom-trained text detection model based on efficient architecture for document layouts. Character recognition using attention-based sequence models rather than simple classification. Contextual refinement using language models to correct ambiguous characters. Specialized handling for mathematical notation, chemical formulas, and specialized symbols.\n\n**Stage 3: Document Understanding (ScribIQ)** \n\nThis is where it gets interesting. Beyond OCR, we built ScribIQ, a vision-language model that understands document structure and semantics.\n\nIt uses visual features from the CV pipeline combined with extracted text to understand document context. Identifies document type (contract, research paper, financial statement, etc.) from visual and textual cues. Extracts relationships between sections and understands hierarchical structure. Answers natural language queries about document content with spatial awareness of where information appears.\n\nFor example: \"What are the termination clauses?\" - ScribIQ doesn't just keyword search \"termination.\" It understands legal document structure, identifies clause sections, recognizes related provisions across pages, and provides spatially-aware citations.\n\n**Training Data &amp; Accuracy:**\n\nTrained on millions of real-world documents across domains: legal contracts, medical records, financial statements, academic papers, handwritten notes, forms and applications, receipts and invoices, and technical documentation.\n\n99.9% character-level accuracy across document types. 98.7% layout structure accuracy on complex multi-column documents. 97.3% table extraction accuracy maintaining cell relationships. Handles 25+ languages with script-specific optimizations.\n\n**Performance Optimization:**\n\nModel quantization reducing inference time 3x without accuracy loss. Batch processing up to 10 pages simultaneously with parallelized pipeline. GPU optimization with TensorRT for sub-2-second page processing. Adaptive resolution processing based on document quality.\n\n**Real-World Challenges We Solved:**\n\nHandwritten annotations on printed documents, dual model approach detecting and processing each separately. Mixed-orientation pages (landscape tables in portrait documents), rotation detection per region rather than per page. Faded or degraded historical documents, super-resolution preprocessing before OCR. Complex scientific notation and mathematical equations, specialized LaTeX recognition pipeline. Multilingual documents with inline script switching, language detection at word level.\n\n**ScribIQ Architecture:**\n\nVision encoder processing document images at multiple scales. Text encoder handling extracted OCR with positional embeddings. Cross-attention layers fusing visual and textual representations. Question encoder for natural language queries. Decoder generating answers with document-grounded attention.\n\nThe key insight: pure text-based document QA loses spatial information. ScribIQ maintains awareness of visual layout, enabling questions like \"What's in the table on page 3?\" or \"What does the highlighted section say?\"\n\n**What's Coming Next - Enterprise Scale:**\n\nWe're launching Inkscribe Enterprise with capabilities that push the CV system further:\n\nBatch processing 1000+ pages simultaneously with distributed inference across GPU clusters. Custom model fine-tuning on client-specific document types and terminology. Real-time processing pipelines with sub-100ms latency for high-throughput applications. Advanced table understanding with complex nested structure extraction. Handwriting recognition fine-tuned for specific handwriting styles. Multi-modal understanding combining text, images, charts, and diagrams. Form understanding with automatic field detection and value extraction.\n\n**Technical Stack:**\n\nPyTorch for model development and training. ONNX Runtime and TensorRT for optimized inference. OpenCV for classical CV preprocessing. Custom CUDA kernels for performance-critical operations. Distributed training with DDP across multiple GPUs. Model versioning and A/B testing infrastructure.\n\n**Open Questions for the CV Community:**\n\nHow do you handle reading order in extremely complex layouts (academic papers with side notes, figures, and multi-column text)? What's your approach to mixed-quality document processing where quality varies page-by-page? For document QA systems, how do you maintain visual grounding while using transformer architectures? What evaluation metrics do you use beyond character accuracy for document understanding tasks?\n\n**Available for Testing:**\n\nWeb: [https://inkscribe.ai/](https://inkscribe.ai/)\n\niOS: [https://apps.apple.com/us/app/inkscribe-ai/id6744860905](https://apps.apple.com/us/app/inkscribe-ai/id6744860905)\n\nAndroid: [https://play.google.com/store/apps/details?id=ai.inkscribe.app.twa&amp;pcampaignid=web\\_share](https://play.google.com/store/apps/details?id=ai.inkscribe.app.twa&amp;pcampaignid=web_share)\n\nCommunity: [https://www.reddit.com/r/InkscribeAI/](https://www.reddit.com/r/InkscribeAI/)\n\n**For Researchers &amp; Engineers:**\n\nInterested in discussing architecture decisions, training approaches, or optimization techniques? I'm happy to go deeper on any aspect of the system. Also looking for challenging documents that break current systems, if you have edge cases, send them my way and I'll share how our pipeline handles them.\n\n**Current Limitations &amp; Improvements:**\n\nWorking on better handling of dense mathematical notation (95% accuracy, targeting 99%). Improving layout analysis on artistic or highly stylized documents. Optimizing memory usage for very high-resolution scans (current limit \\~600 DPI). Expanding language support beyond current 25 languages.\n\n**Benchmarks:**\n\nOpen to running our system against standard benchmarks if there's interest. Currently tracking internal metrics, but happy to evaluate on public datasets for comparison.\n\n**The Bottom Line:**\n\nDocument understanding is fundamentally a computer vision problem, not just OCR. Understanding requires spatial awareness, layout comprehension, and multi-modal reasoning. We built a system that combines classical CV, modern deep learning, and vision-language models to solve real-world document processing.\n\nTry it, break it, tell me where the CV pipeline fails. Looking for feedback from people who understand the technical challenges we're tackling.\n\n**Links:** \n\nWeb: [https://inkscribe.ai/](https://inkscribe.ai/)\n\niOS: [https://apps.apple.com/us/app/inkscribe-ai/id6744860905](https://apps.apple.com/us/app/inkscribe-ai/id6744860905)\n\nAndroid: [https://play.google.com/store/apps/details?id=ai.inkscribe.app.twa&amp;pcampaignid=web\\_share](https://play.google.com/store/apps/details?id=ai.inkscribe.app.twa&amp;pcampaignid=web_share)\n\nCommunity: [https://www.reddit.com/r/InkscribeAI/](https://www.reddit.com/r/InkscribeAI/)\n\nWhat CV approaches have you found effective for document understanding? What problems are still unsolved in this space?",
    "author": "BaronofEssex",
    "timestamp": "2025-10-11T11:41:41",
    "url": "https://reddit.com/r/computervision/comments/1o43an5/built_a_production_computer_vision_system_for/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o48wr3",
    "title": "Is there any way to tune segment anything?",
    "content": "I am pretty new to computer vision, I just wrote a script to run segment anything locally to segment microscope images of microplastics (very basic). \n\n  \nthe issue is that sam2 sometimes doesn't separate the clusters of microplastics and thinks of those as one. sam2 also sometimes double segments a single microplastic, so when I want to count the number of microplastics or determine their sizes, this would be an issue. \n\n  \nIs there a way to tune sam2? I dont have a big enough dataset in order to train my own model (I only have \\~40 pictures). What do you guys think would be the best way foreword? \n\nhttps://preview.redd.it/ph5y0jz48kuf1.png?width=1350&amp;format=png&amp;auto=webp&amp;s=0a10c74fbafe41661d2acd95f6ecd2a979f1afc7\n\nhttps://preview.redd.it/dwwt0vw68kuf1.png?width=1342&amp;format=png&amp;auto=webp&amp;s=8a80d6cf270fa5cc2eb5f8de2ae8991089512187\n\n",
    "author": "Deep-Dragonfly-3342",
    "timestamp": "2025-10-11T15:38:00",
    "url": "https://reddit.com/r/computervision/comments/1o48wr3/is_there_any_way_to_tune_segment_anything/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o49asg",
    "title": "Performance averages?",
    "content": "I only kind of know what I am doing. CPU inference, yolo models, what would be considered a good processing speed?  How would one optimize it?\n\nI trained a model from scratch in pytorch on a 3080.  Exported to onnx.  \n\nI have a 64 core Ampere Altra CPU.  \n\nI wrote some C to convert image data into CHW format and am running it through the Onnx API \n\nIt works, objects are detected.  All CPU cores pegged at 100%.  \n\nI am only getting like 12 fps processing 640x640 images on CPU in FP32.  I know 10% of the performance is coming from my unoptimized image preprocessor. \n\nIf I set dynamic mode on the model and feed it large 1920x1080 images, stuff seems like it's not being detected.  Confidence tanks.  \n\nSo I am like slicing 1920x1080 images into 640x640 chunks with a little bit of overlap.  \n\nIs that required?\n\nIs the Onnx CPU math core optimized for Armv7?   I know OoenBLAS and Blis are.    \n\nIs it worth quantizing to int8?  \n\nMy onnx was compiled from scratch.  Should I try blas or blis?   I understand it uses mlas by default which is supposedly pretty good?\n\n\nShould I give up and use a GPU?  \n\n\n",
    "author": "d13f00l",
    "timestamp": "2025-10-11T15:55:29",
    "url": "https://reddit.com/r/computervision/comments/1o49asg/performance_averages/",
    "score": 1,
    "num_comments": 9,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o3n2ol",
    "title": "Built a real-time P/L dashboard that uses computer vision to scan and price booster cards",
    "content": "I was always curious if I actually made or lost money from my booster openings, so I built a tool that uses computer vision to fix that.\n\nIt scans each card image automatically, matches it against a pricing API, and pulls the latest market value in real time.\n\nYou can enter your booster cost to see instant profit/loss, plus breakdowns by rarity, daily/weekly price trends, and mini price charts per card.\n\nThe same backend can process bulk uploads (hundreds or thousands of cards) for collection tracking.\n\nHere‚Äôs a quick 55-second demo.\n\nWould love feedback from the CV/ML crowd, especially on improving scan accuracy or card-matching efficiency.",
    "author": "v1190cs",
    "timestamp": "2025-10-10T22:10:55",
    "url": "https://reddit.com/r/computervision/comments/1o3n2ol/built_a_realtime_pl_dashboard_that_uses_computer/",
    "score": 21,
    "num_comments": 2,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o41h2o",
    "title": "What are the easiest ways to calculate distance (ideally down to the mm at ranges of 1cm-20cm) in an image? Can computer vision itself do this reliably? If not, what are good options for sensors/adding points of reference to an image? Constraints in description.",
    "content": "I‚Äôll be posting this to electronics subreddits as well but thought I‚Äôd post here too because I recall hearing about pure software approaches to calculate distance, I‚Äôm just not sure if they‚Äôre reliable especially at the short distances I‚Äôm talking about. \n\nI want to point a camera at an object from as close as 1cm to as far away as 20cm and be able to calculate the distance to said object by hopefully as close as 1mm. If there‚Äôs something that won‚Äôt get me to 1mm accuracy but will definitely get me to e.g. 2mm accuracy mention it anyway. \n\nIf this is out of the realm of reliably doing with computer vision then give me your best ideas for supplemental sensors/approaches. \n\nMy constraints are the distances and accuracy as I mentioned, but also cost, ease of implementation, and size of said components (smaller is better, hoping to be able to hold in one hand). \n\nLasers are the first thing that comes to mind but would love if there are any other obvious contenders. Thanks for any help. ",
    "author": "Cixin97",
    "timestamp": "2025-10-11T10:28:30",
    "url": "https://reddit.com/r/computervision/comments/1o41h2o/what_are_the_easiest_ways_to_calculate_distance/",
    "score": 0,
    "num_comments": 14,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o3yird",
    "title": "How to handle low-light footage for night-time vehicle detection (using YOLOv11)",
    "content": "Hi everyone,\nI‚Äôve been working on a vehicle detection project using YOLOv11, and it‚Äôs performing quite well during the daytime. I‚Äôve fine-tuned the model for my specific use case, and the results are pretty solid.\n\nHowever, I‚Äôm now trying to extend it for night-time detection, and that‚Äôs where I‚Äôm facing issues. The footage at night has very low light, which makes it difficult for the model to detect vehicles accurately.\n\nMy main goal is to count the number of moving vehicles at night.\nCan anyone suggest effective ways to handle low-light conditions?\n(For example: preprocessing techniques, dataset adjustments, or model tweaks.)\n\nThanks in advance for any guidance!",
    "author": "AhmadSanjar",
    "timestamp": "2025-10-11T08:29:34",
    "url": "https://reddit.com/r/computervision/comments/1o3yird/how_to_handle_lowlight_footage_for_nighttime/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o3qtgt",
    "title": "Looking for beginner-friendly resources to learn data annotation‚Äîany recommendations?",
    "content": "What resources do you recommend for learning data annotation?",
    "author": "Stunning_Tax_9021",
    "timestamp": "2025-10-11T02:02:18",
    "url": "https://reddit.com/r/computervision/comments/1o3qtgt/looking_for_beginnerfriendly_resources_to_learn/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o3skko",
    "title": "i'm looking image tagging model",
    "content": "I am looking for image tagging model, that i can intergate into my setup\n\nI know about [Recognize Anything / Recognize Anything Plus Model ](https://recognize-anything.github.io/)\n\ni am wondering if there is anything better/newer?",
    "author": "Fit-Job9016",
    "timestamp": "2025-10-11T03:51:38",
    "url": "https://reddit.com/r/computervision/comments/1o3skko/im_looking_image_tagging_model/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o3nw1s",
    "title": "Upgrading LiDAR: every light reflection matters",
    "content": "What if the messy, noisy, scattered light that cameras usually ignore actually holds the key to sharper 3D vision? The Authors of the *Best Student Paper Award* ask: can we learn from every bounce of light to see the world more clearly?\n\nFull reference : Malik, Anagh, et al. ‚Äú[Neural Inverse Rendering from Propagating Light.](https://openaccess.thecvf.com/content/CVPR2025/papers/Malik_Neural_Inverse_Rendering_from_Propagating_Light_CVPR_2025_paper.pdf)‚Äù *Proceedings of the Computer Vision and Pattern Recognition Conference*. 2025.\n\n# Context\n\nDespite the light moving very fast, modern sensors can actually capture its journey as it bounces around a scene. The key tool here is the *flash lidar*, a type of laser camera that emits a quick pulse of light and then measures the tiny delays as it reflects off surfaces and returns to the sensor. By tracking these echoes with extreme precision, flash lidar creates detailed 3D maps of objects and spaces.\n\nNormally, lidar systems only consider the first bounce of light, i.e. the direct reflection from a surface. But in the real world, light rarely stops there. It bounces multiple times, scattering off walls, floors, and shiny objects before reaching the sensor. These additional *indirect* reflections are usually seen as a problem because they make calculations messy and complex. But they also carry additional information about the shapes, materials, and hidden corners of a scene. Until now, this valuable information was usually filtered out.\n\n# Key results\n\nThe Authors developed the first system that doesn‚Äôt just capture these complex reflections but actually models them in a physically accurate way. They created a hybrid method that blends physics and machine learning: physics provides rules about how light behaves, while the neural networks handle the complicated details efficiently. Their approach builds a kind of *cache* that stores how light spreads and scatters over time in different directions. Instead of tediously simulating every light path, the system can quickly look up these stored patterns, making the process much faster.\n\nWith this, the Authors can do several impressive things:\n\n* **Reconstruct accurate 3D geometry** even in tricky situations with lots of reflections, such as shiny or cluttered scenes.\n* **Render videos of light propagation** from entirely new viewpoints, as if you had placed your lidar somewhere else.\n* **Separate direct and indirect light** automatically, revealing how much of what we see comes from straight reflection versus multiple bounces.\n* **Relight scenes in new ways**, showing what they would look like under different light sources, even if that lighting wasn‚Äôt present during capture.\n\nThe Authors tested their system on both simulated and real-world data, comparing it against existing state-of-the-art methods. Their method consistently produced more accurate geometry and more realistic renderings, especially in scenes dominated by indirect light.\n\nOne slight hitch: the approach is computationally heavy and can take over a day to process on a high-end computer. But its potential applications are vast. It could improve self-driving cars by helping them interpret complex lighting conditions. It could assist in remote sensing of difficult environments. It could even pave the way for seeing around corners. By embracing the ‚Äúmessiness‚Äù of indirect light rather than ignoring it, this work takes an important step toward richer and more reliable 3D vision.\n\n# My take\n\nThis paper is an important step in using all the information that lidar sensors can capture, not just the first echo of light. I like this idea because it connects two strong fields ‚Äî lidar and neural rendering ‚Äî and makes them work together. Lidar is becoming central to robotics and mapping, and handling indirect reflections could reduce errors in difficult real-world scenes such as large cities or interiors with strong reflections. The only downside is the slow processing, but that‚Äôs just a question of time, right? (pun intended)\n\nStepping aside from the technology itself, this invention is another example of how *digging deeper* often yields better results. In my research, I‚Äôve frequently used principal component analysis (PCA) for dimensionality reduction. In simple terms, it‚Äôs a method that offers a new perspective on multi-channel data.\n\nConsider, for instance, a collection of audio tracks recorded simultaneously in a studio. PCA combines information from these tracks and ‚Äúsummarises‚Äù it into a new set of tracks. The first track captures most of the meaningful information (in this example, sounds), the second contains much less, and so on, until the last one holds little more than random noise. Because the first track retains most of the information, a common approach is to discard the rest (hence the dimensionality reduction).\n\nRecently, however, our team discovered that the *second* track (the second principal component) actually contained information *far more relevant* to the problem we were trying to solve.",
    "author": "PiotrAntonik",
    "timestamp": "2025-10-10T22:58:20",
    "url": "https://reddit.com/r/computervision/comments/1o3nw1s/upgrading_lidar_every_light_reflection_matters/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o3qg8i",
    "title": "Light field scale-depth space transform for dense depth estimation paper",
    "content": "Hello everyone, \nSo I‚Äôm taking computer vision course and the professor asked us to read some research papers then summarize and present them. \nFor context, it‚Äôs my first time studying CV, i mean i did but it‚Äôs was in a very high-level way (ML libraries, CNN etc).\nAfter reading the paper for the first time i understood the concept, the problem, the solution they proposed and the results but my issue is that i find it very hard to understand the heavy math part solution. \nSo i wanted to know if any of you have some resources to understand those concepts and get familiar in order to fully understand their method, i don‚Äôt wanna use chatgpt because it won‚Äôt be fun anymore and kill the scientific spirit that woke up in me. \n",
    "author": "root_rotting",
    "timestamp": "2025-10-11T01:38:31",
    "url": "https://reddit.com/r/computervision/comments/1o3qg8i/light_field_scaledepth_space_transform_for_dense/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o41uft",
    "title": "Which laptop is best for data science usecase?",
    "content": "",
    "author": "HistoricalLet9848",
    "timestamp": "2025-10-11T10:43:23",
    "url": "https://reddit.com/r/computervision/comments/1o41uft/which_laptop_is_best_for_data_science_usecase/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.13,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o3ajug",
    "title": "How to evaluate real time object detection models on video footage?",
    "content": "Greetings everyone,\n\nI‚Äôm working on a real-time object detection project, where I‚Äôm trying to detect and track multiple animals moving around in videos. I‚Äôm struggling to find an efficient and smart way to evaluate how well my models perform.\n\nSpecifically, I‚Äôm using and training RF-DETR models to perform object detection on video segments. These videos vary in length (some are just a few minutes, others are over an hour long).\n\nMy main challenge is evaluating model consistency over time. I want to know how reliably a model keeps detecting and tracking the same animals throughout a video. This is crucial because I‚Äôll later be adding trackers and using those results for further forecasting and analysis.\n\nRight now, my approach is pretty manual. I just run the model on a few videos and visually inspect whether it loses track of objects which is not ideal to draw conclusions.\n\nSo my question is:\n\nIs there a platform, framework, or workflow you use to evaluate this kind of problem?\n\nHow do you measure consistency of detections across time, not just frame-level accuracy or label correctness?\n\nAny suggestions appreciated.\n\nThanks a lot!",
    "author": "Rep_Nic",
    "timestamp": "2025-10-10T12:30:59",
    "url": "https://reddit.com/r/computervision/comments/1o3ajug/how_to_evaluate_real_time_object_detection_models/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o362fo",
    "title": "3rd Year Project Idea",
    "content": "Hey, I wanna work on a project with one of my teachers who normally teaches the image processing course, but this semester, our school left out the course from our academic schedule. I still want to pitch some project ideas to him and learn more about IP (mostly on my own), but I don't know where to begin and I couldn't come up with an idea that would make him, like i don't know, interested? Do you guys have any suggestions? I'm a CENG student btw",
    "author": "MatterStrong8612",
    "timestamp": "2025-10-10T09:42:49",
    "url": "https://reddit.com/r/computervision/comments/1o362fo/3rd_year_project_idea/",
    "score": 4,
    "num_comments": 5,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o2tgmx",
    "title": "Looking for a solid computer vision development firm",
    "content": "Hey everyone, I‚Äôm in the early stages of a project that needs some serious computer vision work. I‚Äôve been searching around and it‚Äôs hard to tell which firms actually deliver without overpromising. Anyone here had a good experience with a computer vision development firm? want something that knows what they‚Äôre doing and won‚Äôt waste time. ",
    "author": "Opening-Water227",
    "timestamp": "2025-10-09T23:28:14",
    "url": "https://reddit.com/r/computervision/comments/1o2tgmx/looking_for_a_solid_computer_vision_development/",
    "score": 26,
    "num_comments": 27,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o30i8a",
    "title": "Finding a tool to identify the distance between camera and object in a video",
    "content": "Hi guys, I am a university student and my project with professor stuck. Specifically, I have to develop a tool that should be able to identify the 3D coordinate of an object in the video (we focus on video that have one main object only), to do that, I would first have to measure the distance (depth) between the camera and the object. I find the model DepthAnythingv2 could help me to estimate the distance, and I will combines it with the model CoTracker, used for tracking the object during the video.\n\nMy main problem is to create a suitable dataset for the project. I looked for many dataset but could hardly find one that is suitable. KiTTy is quite close to my demand since they provide the 3D coordinator, depth, intrinsic of the camera and everything but they mainly works for transportation and they do not record the video base on the depth.\n\nTo be clearer, my professor said that I should find or create a dataset of about 100 video of, I guess, 10 objects (10 video each object). In the video, I will stand away from the object for 9m and then move closely to the object until the distance is 3m only. My idea now is to establish special marks of the 3m, 4.5m, 6m, 7.5m and 9m distances from the object by drawing a line on the road or attaching a color tape. I will use a depth estimation model (probably DepthAnything) (and I am looking for some other deep learning model also) to estimate the depth from these distance and compare this result to the ground truth of these distance.\n\nI have two main jobs to do now. The first is to find a suitable dataset to match my demand as I mentioned above. From the video recorded, I will cut the 3m, 4,5m, 6m, 7.5m and 9m distance in a video (which is 5 image in a video) to evaluate the performance of the depth estimation model, and I will use that depth estimation model also in every single frame in the video, to see if the distance estimated decrease continuously (as I move closer to the object), which is good, or it fluctuates, which is bad and unstable. But I gonna work on this problem later after I have established an appropriate dataset which is also my second and my priority job right now.\n\nWorking on that task, I don't know is that the most appropriate approach to help me evaluate the performance of the depth estimation model and it is kinda waste as I can only compare 5 distance in the whole video. Therefore, I am looking for some measurement tool or app that maybe could measure the depth throughout the video (like the tape measure I guess) so that I could label and use every single frame in the video. Can you guys recommend me some ideas to create the suitable dataset for my problems or maybe a tool/ app/ kit that could help me to identify the distance from the camera to the object in the video? I will attach my phone to my chest so we can cound the distance from the camera to the object as from me to the object.\n\nP/s: I am sorry for the long post and my Engligh, it might be difficult for me to express my idea and for you to read my problem. If there are any confusing information, please tell me so I can explain.\n\nP/s 2: I have attached an example of what I am working in my project. I will have an object in the video, which is a person in this example, and I would have to estimate the distance between the person and the camera, which is me standing 6m away using my phone to record. In another words, I have to estimate the distance between that person (the object) to the phone (or camera).\n\n[An example of my dataset](https://preview.redd.it/g1t5hi0nsduf1.jpg?width=339&amp;format=pjpg&amp;auto=webp&amp;s=6e60d90f78ba8931eb788be6d788244b9a70ded4)",
    "author": "Expensive_Barber9432",
    "timestamp": "2025-10-10T06:11:14",
    "url": "https://reddit.com/r/computervision/comments/1o30i8a/finding_a_tool_to_identify_the_distance_between/",
    "score": 6,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o2x0lx",
    "title": "An open-source vision agent framework for live video intelligence",
    "content": "",
    "author": "thewritingwallah",
    "timestamp": "2025-10-10T03:16:37",
    "url": "https://reddit.com/r/computervision/comments/1o2x0lx/an_opensource_vision_agent_framework_for_live/",
    "score": 7,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o2jwz0",
    "title": "Two weeks ago I shared TagiFLY, a lightweight open-source labeling tool for computer vision ‚Äî here‚Äôs v2.0.0, rebuilt from your feedback (Undo/Redo fixed, label import/export added üöÄ",
    "content": "Original post: [\\[I built TagiFLY ‚Äì a lightweight open-source labeling tool for computer vision\\]](https://www.reddit.com/r/computervision/comments/1nsyv0h/i_built_tagifly_a_lightweight_opensource_labeling/)\n\n*Two weeks ago I shared the first version of \\*\\*TagiFLY\\*\\*, and the feedback from the community was incredible ‚Äî thank you all üôè*\n\n*Now I‚Äôm excited to share TagiFLY v2.0.0 ‚Äî rebuilt entirely from your feedback.*  \n*Undo/Redo now works perfectly, Grid/List view is fixed, and label import/export is finally here üöÄ*\n\n**‚ú® What‚Äôs new in v2.0.0**  \n‚Ä¢ Fixed Undo/Redo across all annotation types  \n‚Ä¢ Grid/List view toggle now works flawlessly  \n‚Ä¢ Added label import/export (save your label sets as JSON)  \n‚Ä¢ Improved keyboard workflow (no more shortcut conflicts)  \n‚Ä¢ Dark Mode fixes, zoom improvements, and overall UI polish\n\n[Homepage](https://preview.redd.it/8ibl95o4u5uf1.png?width=2866&amp;format=png&amp;auto=webp&amp;s=02d3cbddc8b0a8b9a7210562b2bfb4ab3d42da41)\n\n[Export\\/ƒ∞mport Labels](https://preview.redd.it/pnhw8m1nu5uf1.png?width=760&amp;format=png&amp;auto=webp&amp;s=9a26d473a5ee60ccb938f2705b54f4b6d4df9676)\n\n[Labels](https://preview.redd.it/mkpmnrg5x5uf1.png?width=2856&amp;format=png&amp;auto=webp&amp;s=ac2fdc7d1691589a9216118b9c09c455e717a579)\n\n\n\n[Export](https://preview.redd.it/2l2a2yf8x5uf1.png?width=2830&amp;format=png&amp;auto=webp&amp;s=cc4dad1d547c2a54e3e1375a8afac1f982ad2ea3)\n\n**üéØ What TagiFLY does**  \nTagiFLY is a lightweight open-source labeling tool for computer-vision datasets.  \nIt‚Äôs designed for those who just want to open a folder and start labeling ‚Äî no setup, no server, no login.\n\nMain features:  \n‚Ä¢ 6 annotation types ‚Äî Box, Polygon, Point, Keypoint (17-point pose), Mask Paint, Polyline  \n‚Ä¢ 4 export formats ‚Äî JSON, YOLO, COCO, Pascal VOC  \n‚Ä¢ Cross-platform ‚Äî Windows, macOS, Linux  \n‚Ä¢ Offline-first ‚Äî runs entirely on your local machine via Electron (MIT license), ensuring full data privacy.  \nNo accounts, no cloud uploads, no telemetry ‚Äî nothing leaves your device.  \n‚Ä¢ Smart label management ‚Äî import/export configurations between projects\n\nüîπ¬†**Why TagiFLY exists ‚Äî and why v2 was built**  \nOriginally, I just wanted a simple local tool to create datasets for:  \nü§ñ Training data for ML  \nüéØ Computer vision projects  \nüìä Research or personal experiments\n\nBut after sharing the first version here, the feedback made it clear there‚Äôs a real need for a lightweight, privacy-friendly labeling app that just works ‚Äî fast, offline, and without setup.  \nSo v2 focuses on polishing that idea into something stable and reliable for everyone. üöÄ\n\nüöÄ Links  \nGitHub repo:¬†[https://github.com/dvtlab/TagiFLY](https://github.com/dvtlab/TagiFLY)  \nLatest release:¬†[https://github.com/dvtlab/TagiFLY/releases](https://github.com/dvtlab/TagiFLY/releases)\n\nThis release focuses on stability, usability, and simplicity ‚Äî keeping TagiFLY fast, local, and practical for real computer-vision workflows.  \nFeedback is gold ‚Äî if you try it, let me know what works best or what you‚Äôd love to see next üôè",
    "author": "hello_wordx",
    "timestamp": "2025-10-09T15:27:53",
    "url": "https://reddit.com/r/computervision/comments/1o2jwz0/two_weeks_ago_i_shared_tagifly_a_lightweight/",
    "score": 23,
    "num_comments": 10,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o2v6t5",
    "title": "Advice on action recognition for fencing, how to capture sequences?",
    "content": "I am working on an action recognition project for fencing and trying to analyse short video clips (around 10 s each). My goal is to detect and classify sequences of movements like *step-step-lunge*, *retreat-retreat-lunge*, etc.\n\nI have seen plenty of datasets and models for general human actions (Kinetics, FineGym, UCF-101, etc.), but nothing specific to fencing or fine-grained sports footwork.\n\nA few questions:\n\n* Are there any models or techniques well-suited for recognizing action sequences rather than single movements?\n* Since I don‚Äôt think a fencing dataset exists, does it make sense to build my own dataset from match videos (e.g., extracting 2‚Äì3 s clips and labeling action sequences)?\n* Would pose-based approaches (e.g., ST-GCN, CTR-GCN, X-CLIP, or transformer-based models) be better than video CNNs for this type of analysis?\n\nAny papers, repos, or implementation tips for fine-grained motion recognition would be really appreciated. Thanks!",
    "author": "Content-Opinion-9564",
    "timestamp": "2025-10-10T01:20:11",
    "url": "https://reddit.com/r/computervision/comments/1o2v6t5/advice_on_action_recognition_for_fencing_how_to/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o384u5",
    "title": "Need help finding an ai auto image labeling tool that I can use to quickly label my data using segmentation.",
    "content": "I am a beginner to computer vision and AI, and in my exploration process I want to use some other ai tool to segment and label data for me such that I can just glance over the labels to see if they look about good, then feed it into my model and learn how to train the model and tune parameters. I dont really want to spend time segmenting and labeling data myself. \n\nAnyone got any good free options that would work for me? ",
    "author": "Deep-Dragonfly-3342",
    "timestamp": "2025-10-10T10:59:25",
    "url": "https://reddit.com/r/computervision/comments/1o384u5/need_help_finding_an_ai_auto_image_labeling_tool/",
    "score": 0,
    "num_comments": 11,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o2i23x",
    "title": "Has anyone successful fine tuned dinov3 on 100k + images self supervised?",
    "content": "Attempting to fine tune a dinov3 backbone on a subset of images. Lightly train looks like they kind of do it but don‚Äôt give you the backbone separate. \n\nAttempting to use Dino to create SOTR VLM for subsets of data but am still working to get the back bone \n\nDino finetunes self supervised on large dataset ‚Äî&gt; dinotxt used on subset of that data (~50k images) ‚Äî&gt; then there should be great vlm model and you didn‚Äôt have to label everything ",
    "author": "SadPaint8132",
    "timestamp": "2025-10-09T14:11:59",
    "url": "https://reddit.com/r/computervision/comments/1o2i23x/has_anyone_successful_fine_tuned_dinov3_on_100k/",
    "score": 21,
    "num_comments": 18,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o3749z",
    "title": "[Research] Contributing to Facial Expressions Dataset for CV Training",
    "content": "Hi r/datasets,\n\nI'm currently working on an academic research project focused on computer vision and need help building a robust, open dataset of facial expressions.\n\nTo do this, I've built a simple web portal where contributors can record short, anonymous video clips.\n\nLink to the data collection portal: https://sochii2014.pythonanywhere.com/\n\nDisclosure: This is my own project and I am the primary researcher behind it. This post is a form of self-promotion to find contributors for this open dataset.\n\nWhat's this for?\nThe goal is to create a high-quality, ethically-sourced dataset to help train and benchmark AI models for emotion recognition and human-computer interaction systems. I believe a diverse dataset is key to building fair and effective AI.\n\nWhat would you do?\nThe process is simple and takes 3-5 minutes:\n\nYou'll be asked to record five, 5-second videos.\n\nThe tasks are simple: blink, smile, turn your head.\n\nEverything is anonymous‚Äîno personal data is collected.\n\nData &amp; Ethics:\n\nAnonymity: All participants are assigned a random ID. No facial recognition is performed.\n\nFormat: Videos are saved in WebM format with corresponding JSON metadata (task, timestamp).\n\nUsage: The resulting dataset will be intended for academic and non-commercial research purposes.\n\nIf you have a moment to contribute, it would be a huge help. I'm also very open to feedback on the data collection method itself.\n\nThank you for considering it",
    "author": "Funny-Whereas8597",
    "timestamp": "2025-10-10T10:21:25",
    "url": "https://reddit.com/r/computervision/comments/1o3749z/research_contributing_to_facial_expressions/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o301zj",
    "title": "Looking for Vietnamese or Asian Traffic Detection Data",
    "content": "Hi guys, I am a university student in Vietnam working on the project of Traffic Vehicle Detection and I need your recommendation on choosing tools and suitable approach. Talking about my project, I need to work with the Vietnamese traffic environment, with the main idea of the project is to output how many vehicles appeared in the inputted frame/ image. I need to build a dataset from scratch and I could choose to train/ finetune a model myself. I have some intuitive and I am wondering you guys can recommends me something:\n\n1. For the dataset, I am thinking about writing a code so that I could crawl/scrape or somehow collect the data of the real - time Vietnamese traffic (I already found some sites that features such as¬†[https://giaothong.hochiminhcity.gov.vn/](https://giaothong.hochiminhcity.gov.vn/)). I will captures it once every 1 minutes for examples so that I can have a dataset of, maybe, 10 000 images of daylight and 10 000 images of nightlight.\n2. After collecting the dataset composing of 20 000 images in total, I have to find a tool or maybe manually label the dataset myself. Since my project is about Vehicle Detection, I only need to bounding box the vehicles and label their bounding box coordinates and the name of the object (vehicles) (car, bus, bike, van, ...). I really need you guys to suggest me some tools or approach so that I can label my data.\n3. For the model, I am gonna finetune the model Yolo12n on my dataset only. If you guys have other specified model in Traffic Vehicle Detection, please tell me, so that I can compare the performance of the models.\n\nIn short, my priority now is to find a suitable dataset, specifically a labeled Vehicle Detection dataset of Vietnamese or Asian transportation, or to create and label a dataset myself, which involves collecting real - time traffic image then label the vehicles appeared. Can you recommend me some idea on my problem.\n\n  \n",
    "author": "Expensive_Barber9432",
    "timestamp": "2025-10-10T05:52:21",
    "url": "https://reddit.com/r/computervision/comments/1o301zj/looking_for_vietnamese_or_asian_traffic_detection/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o3fby4",
    "title": "You update apps constantly, your mind deserves the same upgrade",
    "content": "You update apps constantly.\nYour mind deserves the same upgrade.\n\nMost people treat their phones\nbetter than their minds.\n\nYour brain processes 11 million bits\nof information per second.\nBut you're only conscious of 40.\n\nThe rest runs on autopilot.\nOld programs. Old patterns.\nOld stories you've outgrown.\n\nEvery day you choose:\nOld software vs new updates\n\nA sherpa in Nepal who guided \nexpeditions¬†for 40 years, said,\n\n\"Your mind is like base camp.\nYou must prepare it daily.\nOr the mountain wins.\"\n\nHe wasn't talking about Everest.\nHe was talking about life.\n\nBest ways to update your software:\n\n1. Books feed new perspectives.\nNot just any books.¬†\nThe ones that challenge you.\n\n2. Podcasts plant seeds while you move.\nWalking. Driving. Living.\nKnowledge compounds in motion.\n\n3. Experience writes the deepest code.\nTry. Fail. Learn. Repeat.\nYour mistakes become your wisdom.\n\nProtect your battery:\nEight hours of sleep is maintenance.\nYour brain clears toxins while you dream.\n\nNature doesn't just calm you.\nIt recalibrates your frequency.\n\nDigital detox isn't avoiding technology.\nIt's about choosing when it serves you.\n\nClean your hard drive:\n\nMeditation isn't emptying your mind.\nIt's watching your thoughts\nwithout becoming them.\n\nThe Bhutanese have a practice.\nEvery morning, they sit in silence.\n\"We dust our minds,\" they say.\n\nYour brain isn't just along for the ride.\nIt's the driver, the engine, the GPS.\n\nTreat it like the miracle it is.\n\nWhat's one upgrade you can make?\nLook forward to reading your comments.",
    "author": "Fav_bud_nikkib420",
    "timestamp": "2025-10-10T15:40:33",
    "url": "https://reddit.com/r/computervision/comments/1o3fby4/you_update_apps_constantly_your_mind_deserves_the/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.28,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o2x3r6",
    "title": "What are some current research directions in Variational Auto-encoders?",
    "content": "Please also share the current SOTA techniques.",
    "author": "Aromatic_Eye_6268",
    "timestamp": "2025-10-10T03:21:43",
    "url": "https://reddit.com/r/computervision/comments/1o2x3r6/what_are_some_current_research_directions_in/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o2qjsh",
    "title": "Mood swings - Hand driven animation",
    "content": "concept made with mediapipe and ball physics. You can find more experiments at https://www.instagram.com/sante.isaac",
    "author": "curryboi99",
    "timestamp": "2025-10-09T20:42:57",
    "url": "https://reddit.com/r/computervision/comments/1o2qjsh/mood_swings_hand_driven_animation/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o25f6z",
    "title": "Help: Project Cloud Diffusion Chamber",
    "content": "I‚Äôm working with images from a cloud (diffusion) chamber to make particle tracks (alpha / beta, occasionally muons) visible and usable in a digital pipeline. My goal is to automatically extract clean track polylines (and later classify by basic geometry), so I can analyze lengths/curvatures etc. Downstream tasks need vectorized tracks rather than raw pixels.\n\nSo Basically I want to extract the sharper white lines of the image with their respective thickness, length and direction.\n\nhttps://preview.redd.it/ae8u6iz743uf1.jpg?width=2049&amp;format=pjpg&amp;auto=webp&amp;s=a2ffd1b248cee81f29c3d24fe25de94327250b6d\n\n**Data**\n\n* Single images or short videos, grayscale, uneven illumination, diffuse ‚Äúfog‚Äù.\n* Tracks are thin, low-contrast, often wavy (Œ≤), sometimes short &amp; thick (Œ±), occasionally long &amp; straight (Œº).\n* many soft edges; background speckle.\n* Labeling is hard even for me (no crisp boundaries; drawing accurate masks/polylines is slow and subjective).\n\n**What I tried**\n\n1. **Background flattening**: Gaussian large-œÉ subtraction to remove smooth gradients.\n2. **Denoise w/o killing ridges**: light bilateral / NLM + 3√ó3 median.\n3. **Shape filtering**: keep components with high elongation/excentricity; discard round blobs.\n4. I have trained a **YOLO** model earlier on a different project with good results, but here performance is weak due to fuzzy boundaries and ambiguous labels.\n\n**Where I‚Äôm stuck**\n\n* Robustly separating faint tracks from ‚Äúfog‚Äù without erasing thin Œ≤ segments.\n* Consistent, low-effort labeling: drawing precise polylines or masks is slow and noisy.\n* Generalization across sessions (lighting, vapor density) without re-tuning thresholds every time.\n\n**My Questions**\n\n1. **Preprocessing:** Are there any better ridge/line detectors or illumination-correction methods for very faint, fuzzy lines?\n2. **Training ML:** Is there a better way than a YOLO modell for this specific task ? Or is ML even the correct approach for this Project ?\n\nThanks for any pointers, references, or minimal working examples!\n\nEdit: As far as its not obvious I am very new to Image PreProcessing and Computer Vision",
    "author": "RoundScore2820",
    "timestamp": "2025-10-09T06:06:21",
    "url": "https://reddit.com/r/computervision/comments/1o25f6z/help_project_cloud_diffusion_chamber/",
    "score": 10,
    "num_comments": 11,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o2hcbk",
    "title": "3D CT reports generation : advices and ressources ?",
    "content": "  \nHi !   \nI'm working on 3D medical imaging AI research and I'm looking for some advices and ressources  \nMy goal is to make an MLLM for 3D brain CT. Im currently making a Multitask learning (MTL) for several tasks ( prediction , classification,segmentation). The model architecture consist of a shared encoder and different heads (outputs ) for each task. Then I would like to ¬†take the trained 3D Vision shared encoder and align its feature vectors with a¬†Text Encoder/LLM¬†to generate reports based on the CT volume\n\nDo you know good ressources or repo I can look to help me with my project ? The problem is I'm working alone on the project and I don't really know how to make something useful for ML community.",
    "author": "LavishnessUnlikely72",
    "timestamp": "2025-10-09T13:44:22",
    "url": "https://reddit.com/r/computervision/comments/1o2hcbk/3d_ct_reports_generation_advices_and_ressources/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o29lpr",
    "title": "Medical images Datasets recommendations?",
    "content": "Hey guys! I'm kinda new to medical images and I want to practice low level difficulty datasets of medical images. I'm aiming towards classification and segmentation problems. \n\nI've asked chatgpt for recommendations for begginers, but maybe I am too beginer or I didn't know how to properly make the prompt or maybe just chatgpt-things, the point is I wasn't really satisfied with its response, so would you please recommend me some medical image datasets (CT, MRI, histopathology, ultrasound) to start in this? (and perhaps some prompt tips lol)",
    "author": "Xerath69420",
    "timestamp": "2025-10-09T08:51:23",
    "url": "https://reddit.com/r/computervision/comments/1o29lpr/medical_images_datasets_recommendations/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o25xie",
    "title": "Help: Startup Team Infrastructure/Workflow Decision",
    "content": "Greetings,\n\nWe are a small team of 6 people that work on a startup project in our free time (mainly computer vision + some algorithms etc.). So far, we have been using the roboflow platform for labelling, training models etc. However, this is very costly and we cannot justify 60 bucks / month for labelling and limited credits for model training with limited flexibility.\n\n  \nWe are looking to see where it is worthwhile to migrate to, without needing too much time to do so and without it being too costly.\n\n  \nCurrently, this is our situation: \n\n\\- We have a small grant of 500 euros that we can utilize. Aside from that we can also spend from our own money if it's justified. The project produces no revenue yet, we are going to have a demo within this month to see the interest of people and from there see how much time and money we will invest moving forward. In any case we want to have a migration from roboflow set-up to not have delays.\n\n\\- We have setup an S3 bucket where we keep our datasets (so far approx. 40GB space) which are constantly growing since we are also doing data collection. We also are renting a VPS where we are hosting CVAT for labelling. These come around 4-7 euros / month. We have set up some basic repositories for drawing data, some basic training workflows which we are trying to figure out, mainly revolving around YOLO, RF-DETR, object detection and segmentation models, some timeseries forecasting, trackers etc. We are playing around with different frameworks so we want to be a bit flexible.\n\n\\- We are looking into renting VMs and just using our repos to train models but we also want some easy way to compare runs etc. so we thought something like MLFlow. We tried these a bit but it has an initial learning process and it is time consuming to setup your whole pipeline at first.\n\n  \n\\-&gt; What would you guys advice in our case? Is there a specific platform you would recommend us going towards? Do you suggest just running in any VM on the cloud ? If yes, where and what frameworks would you suggest we use for our pipeline? Any suggestions are appreciated and I would be interested to see what computer vision companies use etc. Of course in our case the budget would ideally be less than 500 euros for the next 6 months in costs since we have no revenue and no funding, at least currently. \n\nTL;DR - Which are the most pain-free frameworks/platforms/ways to setup a full pipeline of data gathering -&gt; data labelling -&gt; data storage -&gt; different types of model training/pre-training -&gt; evaluation -&gt; comparison of models -&gt; deployment on our product etc. when we have a 500 euro budget for next 6 months making our lives as much as possible easy while being very flexible and able to  train different models, mess with backbones, transfer learning etc. without issues.\n\n  \nFeel free to ask for any additional information.\n\n  \nThanks!",
    "author": "Rep_Nic",
    "timestamp": "2025-10-09T06:27:56",
    "url": "https://reddit.com/r/computervision/comments/1o25xie/help_startup_team_infrastructureworkflow_decision/",
    "score": 4,
    "num_comments": 12,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o27w01",
    "title": "When Should I Have Stopped Training?",
    "content": "Hi,\n\nAccording to the graphs, is 100 epochs too many? Or just right?\n\nNot sure what other information you might need.\n\n\n\nhttps://preview.redd.it/b2tdexz8m3uf1.png?width=2400&amp;format=png&amp;auto=webp&amp;s=cd21b38d602dc2769127d160afe2448c02dcdd84\n\nThanks for the feedback! \n\nExtra info:\n\nCreating new Ultralytics Settings v0.0.6 file ‚úÖ   \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'  \nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs\\_dir=path/to/dir'. For help see [https://docs.ultralytics.com/quickstart/#ultralytics-settings](https://docs.ultralytics.com/quickstart/#ultralytics-settings).  \nDownloading [https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt) to 'yolo11m.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 38.8MB 155.0MB/s 0.3s  \nUltralytics 8.3.208 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)  \n**engine/trainer:** agnostic\\_nms=False, amp=True, augment=False, auto\\_augment=randaugment, batch=80, bgr=0.0, box=7.5, cache=True, cfg=None, classes=\\[0, 1, 2, 3, 4, 5, 6, 11, 14, 15, 16, 20, 22, 31, 33, 35, 60, 61\\], close\\_mosaic=10, cls=0.5, compile=False, conf=None, copy\\_paste=0.0, copy\\_paste\\_mode=flip, cos\\_lr=False, cutmix=0.0, data=/content/wildlife.yaml, degrees=8.0, deterministic=False, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=105, erasing=0.4, exist\\_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv\\_h=0.015, hsv\\_s=0.7, hsv\\_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line\\_width=None, lr0=0.01, lrf=0.01, mask\\_ratio=4, max\\_det=300, mixup=0.4, mode=train, [model=yolo11m.pt](http://model=yolo11m.pt), momentum=0.937, mosaic=1, multi\\_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap\\_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/AI Training, rect=False, resume=False, retina\\_masks=False, save=True, save\\_conf=False, save\\_crop=False, save\\_dir=/content/drive/MyDrive/AI Training/train, save\\_frames=False, save\\_json=False, save\\_period=-1, save\\_txt=False, scale=0.5, seed=0, shear=2.0, show=False, show\\_boxes=True, show\\_conf=True, show\\_labels=True, simplify=True, single\\_cls=False, source=None, split=val, stream\\_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid\\_stride=1, visualize=False, warmup\\_bias\\_lr=0.1, warmup\\_epochs=3.0, warmup\\_momentum=0.8, weight\\_decay=0.0005, workers=32, workspace=None  \nDownloading [https://ultralytics.com/assets/Arial.ttf](https://ultralytics.com/assets/Arial.ttf) to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 110.3MB/s 0.0s  \nOverriding model.yaml nc=80 with nc=63  \n  \nfrom  n    params  module                                       arguments                       \n  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             \\[3, 64, 3, 2\\]                   \n  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             \\[64, 128, 3, 2\\]                 \n  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            \\[128, 256, 1, True, 0.25\\]       \n  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             \\[256, 256, 3, 2\\]                \n  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            \\[256, 512, 1, True, 0.25\\]       \n  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             \\[512, 512, 3, 2\\]                \n  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            \\[512, 512, 1, True\\]             \n  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             \\[512, 512, 3, 2\\]                \n  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            \\[512, 512, 1, True\\]             \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            \\[512, 512, 5\\]                   \n 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           \\[512, 512, 1\\]                   \n 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         \\[None, 2, 'nearest'\\]            \n 12             \\[-1, 6\\]  1         0  ultralytics.nn.modules.conv.Concat           \\[1\\]                             \n 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            \\[1024, 512, 1, True\\]            \n 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         \\[None, 2, 'nearest'\\]            \n 15             \\[-1, 4\\]  1         0  ultralytics.nn.modules.conv.Concat           \\[1\\]                             \n 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            \\[1024, 256, 1, True\\]            \n 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             \\[256, 256, 3, 2\\]                \n 18            \\[-1, 13\\]  1         0  ultralytics.nn.modules.conv.Concat           \\[1\\]                             \n 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            \\[768, 512, 1, True\\]             \n 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             \\[512, 512, 3, 2\\]                \n 21            \\[-1, 10\\]  1         0  ultralytics.nn.modules.conv.Concat           \\[1\\]                             \n 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            \\[1024, 512, 1, True\\]            \n 23        \\[16, 19, 22\\]  1   1459597  ultralytics.nn.modules.head.Detect           \\[63, \\[256, 512, 512\\]\\]           \nYOLO11m summary: 231 layers, 20,101,581 parameters, 20,101,565 gradients, 68.5 GFLOPs  \n  \nTransferred 643/649 items from pretrained weights  \nFreezing layer 'model.23.dfl.conv.weight'  \n**AMP:** running Automatic Mixed Precision (AMP) checks...  \nDownloading [https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt) to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 302.4MB/s 0.0s  \n**AMP:** checks passed ‚úÖ  \n**train:** Fast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 3819.3¬±1121.8 MB/s, size: 529.3 KB)  \n**train:** Scanning /content/data/train/labels... 7186 images, 750 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7936/7936 1.5Kit/s 5.5s  \n**train:** /content/data/train/images/75286\\_1a2242d93eb9c64d4869e62b875ed65a\\_763b34.jpg: corrupt JPEG restored and saved  \n**train:** /content/data/train/images/IMG\\_20201004\\_130233\\_62f8e7.jpg: corrupt JPEG restored and saved  \n**train:** New cache created: /content/data/train/labels.cache  \n**train:** Caching images (5.7GB RAM): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7936/7936 283.6it/s 28.0s  \n**albumentations:** Blur(p=0.01, blur\\_limit=(3, 7)), MedianBlur(p=0.01, blur\\_limit=(3, 7)), ToGray(p=0.01, method='weighted\\_average', num\\_output\\_channels=3), CLAHE(p=0.01, clip\\_limit=(1.0, 4.0), tile\\_grid\\_size=(8, 8))  \n**val:** Fast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1527.6¬±1838.9 MB/s, size: 676.1 KB)  \n**val:** Scanning /content/data/val/labels... 1796 images, 186 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1982/1982 379.0it/s 5.2s  \n**val:** New cache created: /content/data/val/labels.cache  \n**val:** Caching images (1.4GB RAM): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1982/1982 197.8it/s 10.0s  \nPlotting labels to /content/drive/MyDrive/AI Training/train/labels.jpg...   \n**optimizer:** 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically...   \n**optimizer:** SGD(lr=0.01, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.000625), 112 bias(decay=0.0)  \nImage sizes 640 train, 640 val  \nUsing 12 dataloader workers  \nLogging results to **/content/drive/MyDrive/AI Training/train**  \nStarting training for 105 epochs...\n\n  \n",
    "author": "Ambitious_Ad4186",
    "timestamp": "2025-10-09T07:46:48",
    "url": "https://reddit.com/r/computervision/comments/1o27w01/when_should_i_have_stopped_training/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o2f2se",
    "title": "Active 3D Vision on a robotic vehicle ‚Äî TEMAS as the eye in motion",
    "content": "Our project TEMAS has evolved from a static 3D Vision module into an active robotic component.\n\nWatch the short demo\n",
    "author": "Big-Mulberry4600",
    "timestamp": "2025-10-09T12:17:29",
    "url": "https://reddit.com/r/computervision/comments/1o2f2se/active_3d_vision_on_a_robotic_vehicle_temas_as/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o20xav",
    "title": "OCR on user-generated content. Thoughts on Florence2?",
    "content": "Hi all! I‚Äôm a researcher working with a large dataset of social media posts and need to transcribe text that appears in images and video frames. I'm considering Florence-2, mostly because it is free and open source. It is important that the model has support for Indian languages.\n\nWould really appreciate advice on:\n\n\\- Is Florence2 a good choice for OCR at this scale? (\\~400k media files)\n\n\\- What alternatives should I consider that are multilingual, good for messy user-generated content and not too expensive ?\n\n(FYI: I have access to the high-performance computing cluster of my research institution. Accuracy is more important than speed).\n\nThank you!",
    "author": "Much_Golf_1808",
    "timestamp": "2025-10-09T01:58:12",
    "url": "https://reddit.com/r/computervision/comments/1o20xav/ocr_on_usergenerated_content_thoughts_on_florence2/",
    "score": 6,
    "num_comments": 6,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o21ngh",
    "title": "2d projection visualziation with 3d point cloud using 3d gaussian splatting",
    "content": "github link : [genji970/pointclip-gaussain\\_splatting-: Using multivariate gaussian splatting, visualizing 2d object from 3d point cloud dataset.](https://github.com/genji970/pointclip-gaussain_splatting-)\n\nhttps://preview.redd.it/ln8qgpox42uf1.png?width=493&amp;format=png&amp;auto=webp&amp;s=ba826787b6e67b4020ef5078e2f0a430ebde7a74\n\n   \n\n\nhttps://preview.redd.it/q28x00ey42uf1.png?width=367&amp;format=png&amp;auto=webp&amp;s=7196fd3438a0643c40acd00c66f701b57ae8b1fb\n\n \n\nhttps://preview.redd.it/u10o4d5z42uf1.png?width=494&amp;format=png&amp;auto=webp&amp;s=69e97ea1ccba3ee0186900c7594c193261a18b58\n\n \n\nhttps://preview.redd.it/3rihkcrz42uf1.png?width=367&amp;format=png&amp;auto=webp&amp;s=f66ea76e7efe29f071a9e2a0e94169ee51184378\n\n",
    "author": "Round_Apple2573",
    "timestamp": "2025-10-09T02:45:57",
    "url": "https://reddit.com/r/computervision/comments/1o21ngh/2d_projection_visualziation_with_3d_point_cloud/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o28cnr",
    "title": "Can anyone help me with the person Re-identification and tracking using DeepSort and Osnet?",
    "content": "Hi everyone,\nI'm currently working on a person re-identification and tracking project using DeepSort and OSNet.\nI'm having some trouble tracking and Re-identification and would appreciate any guidance or example implementations.\nHas anyone worked on something similar or can point me to good resources?",
    "author": "L1onSynth",
    "timestamp": "2025-10-09T08:04:25",
    "url": "https://reddit.com/r/computervision/comments/1o28cnr/can_anyone_help_me_with_the_person/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o222hi",
    "title": "Need help with forward and backward motion detection using optical flow?",
    "content": "I'm using a monocular system for estimating camera motion in forward/ backward direction. The camera is installed on a forklift working in warehouse, where there's a lot of relative motion, even when the forklift is standing still. I have built this initial approach using gemini, since I didn't knew this topic too well.\n\nMy current approach is as follows:  \n1. Grab keypoints from initial frame. (shitomasi method)  \n2. Track them across subsequent frames using Lucas Kannade algorithm.  \n3. Using the radial vectors, I calculate whether the camera is moving forward or backward: (explained in detail using gemini)\n\nDivergence Score Calculation\n\nThe script mathematically checks if the flow is radiating outward or contracting inward by using the **dot product**.\n\n1. **Center-to-Feature Vectors:** The script calculates a vector from the image center to each feature point (`center_to_feature_vectors = good_old - center`). This vector is the radial line from the center to the feature.\n2. **Dot Product:** It calculates the dot product between the radial vector and the feature's actual flow vector: Dot¬†Product=Radial¬†Vector‚ãÖFlow¬†Vector\n3. **Interpretation:**\n   * **Positive Dot Product:** The flow vector is moving **in the same direction** as the radial vector (i.e., **outward** from the center). This indicates **Expansion (Forward Motion)**.\n   * **Negative Dot Product:** The flow vector is moving **in the opposite direction** of the radial vector (i.e., **inward** toward the center). This indicates **Contraction (Backward Motion)**.\n4. **Mean Divergence Score:** By taking the mean of the signs of all these dot products (`np.mean(np.sign(dot_products))`), the script gets a single, normalized score:\n   * A score close to +1 means almost all features are expanding (strong forward motion).\n   * A score close to ‚àí1 means almost all features are contracting (strong backward motion).\n5. I reinitialize the keypoints if they are lost due to strong movement.\n\n  \nThe issue is that it's not robust enough. In the scene, there are people walking towards/ away from the camera. And there are other forklifts in the scene as well. \n\nHow can I improve on my approach? What are some algorithms that I can use in this case (traditional CV and deep learning based approaches)? Also, This solution has to run on raspberry pi/ Jetson Nano SBC.\n\n\n\n ",
    "author": "atmadeep_2104",
    "timestamp": "2025-10-09T03:12:40",
    "url": "https://reddit.com/r/computervision/comments/1o222hi/need_help_with_forward_and_backward_motion/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1t8sf",
    "title": "Extracting data from consumer product images: OCR vs multimodal vision models",
    "content": "Hey everyone \n\nI‚Äôm working on a project where I need to **extract product information from consumer goods** (name, weight, brand, flavor, etc.) **from real-world photos**, not scans.\n\nThe images come with several challenges:\n\n* **angle variations**,\n* **light reflections and glare**,\n* **curved or partially visible text**,\n* and **distorted edges** due to packaging shape.\n\nI‚Äôve considered tools like **DocStrange** coupled with **Nanonets-OCR/Granite**, but they seem more suited for **flat or structured documents** (invoices, PDFs, forms).\n\nIn my case, photos are taken by regular users, so lighting and perspective can‚Äôt be controlled.  \nThe goal is to build a **robust pipeline** that can handle those real-world conditions and output structured data like:\n\n{\n\n  \"product\": \"Galletas Ducales\",\n\n  \"weight\": \"220g\",\n\n  \"brand\": \"Noel\",\n\n  \"flavor\": \"Original\"\n\n}\n\nIf anyone has worked on consumer product recognition, retail datasets, or real-world labeling, I‚Äôd love to hear what kind of approach worked best for you ‚Äî or how you combined OCR, vision, and language models to get consistent results.",
    "author": "kmuentez",
    "timestamp": "2025-10-08T18:27:53",
    "url": "https://reddit.com/r/computervision/comments/1o1t8sf/extracting_data_from_consumer_product_images_ocr/",
    "score": 3,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1jx9w",
    "title": "Faster RCNN explained using PyTorch",
    "content": "A Simple tutorial on Faster RCNN and how one can implement it with Pytorch  \n  \nLink: [https://youtu.be/YHv6\\_YpzRTI](https://youtu.be/YHv6_YpzRTI)",
    "author": "computervisionpro",
    "timestamp": "2025-10-08T12:11:10",
    "url": "https://reddit.com/r/computervision/comments/1o1jx9w/faster_rcnn_explained_using_pytorch/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1k9k4",
    "title": "Food images recognition",
    "content": "I will work on training my first ai model that can recognize food images and then display nutrition facts using Roboflow. Can you suggest me a good food dataset? Did anyone try something like that?üò¨",
    "author": "lasxavier",
    "timestamp": "2025-10-08T12:23:47",
    "url": "https://reddit.com/r/computervision/comments/1o1k9k4/food_images_recognition/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1j1m3",
    "title": "Looking for a modern alternative to MMAction2 for spatiotemporal action detection",
    "content": "I‚Äôve been experimenting with MMAction2 for spatiotemporal / video-based human action detection, but it looks like the project has been discontinued or at least not actively maintained anymore. The latest releases don‚Äôt build cleanly under recent PyTorch + CUDA versions, and the mmcv/mmcv-full dependency chain keeps breaking.\n\nBefore I spend more time patching the build, I‚Äôd like to know what people are using instead for *spatiotemporal action detection or video understanding*.\n\nRequirements:\n\n* Actively maintained\n* Works with the latest libs\n* Supports real-time or near-real-time inference (ideally webcam input)\n* Open-source or free for research use\n\nIf you‚Äôve migrated away from MMAction2, which frameworks or model hubs have worked best for you?\n",
    "author": "BrilliantWill1234",
    "timestamp": "2025-10-08T11:39:06",
    "url": "https://reddit.com/r/computervision/comments/1o1j1m3/looking_for_a_modern_alternative_to_mmaction2_for/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1crj3",
    "title": "Pixel-to-Pixel alignment on DJI Matrice 4T",
    "content": "I am working on a project where I need to gather a dataset using this drone. I need both IR and optic (regular camera) pictures to fuse them and train a model. I am not an expert on this matter and this project is merely just curiosity. What I need to find out right now is if the DJI Matrice 4T alinges them automatically. And if it does, my problem is pretty much solved. But if it is not, I need to find a way to align them. Or maybe, since the distance between the cameras are in the milimeters, it wont even cause a problem when training. \n\n",
    "author": "seboidagoat",
    "timestamp": "2025-10-08T07:54:56",
    "url": "https://reddit.com/r/computervision/comments/1o1crj3/pixeltopixel_alignment_on_dji_matrice_4t/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1bydo",
    "title": "Hardware Requirements (+model suggestion)",
    "content": "Hi! I am doing a project where we are performing object detection in a drone. The drone itself is big (+4m wingspan) and has a big airframe and battery capacity. We want to be able to perform object detection over  RGB and infrarred cameras (at 30 FPS? i guess 15 would also be okay). Me and my team are debating between a Raspberry pi 5 with an accelerator and a Jetson model. For the model we will most probably be using a YOLO. I know the Jetson is enough for the task, but would the raspberry pi also be an option?\n\nEDIT: team went with on-ground computing",
    "author": "Otherwise-Warthog551",
    "timestamp": "2025-10-08T07:24:51",
    "url": "https://reddit.com/r/computervision/comments/1o1bydo/hardware_requirements_model_suggestion/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o12ww8",
    "title": "I stumbled on Meta's Perception Encoder and language Model launched in Apr 2025 but not sure about it from the AI community.",
    "content": "Meta AI research team introduced the key backbone behind this model which is Perception encoder which is a large-scale vision encoder that excels across several vision tasks for images and video. So many downstream image recognition tasks can be achieved with this right from image captioning to classification to retrieval to segmentation and grounding!\n\nHas anyone tried this till now and what has been the experience?",
    "author": "Worth-Card9034",
    "timestamp": "2025-10-07T23:21:39",
    "url": "https://reddit.com/r/computervision/comments/1o12ww8/i_stumbled_on_metas_perception_encoder_and/",
    "score": 14,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1dxd5",
    "title": "4 Cameras Object Detection",
    "content": "I originally had a plan to use the 2 CSI ports and 2 USB on a jetson orin nano to have 4 cameras. the 2nd CSI port seems to never want to work so I might have to do 1CSI 3 USB.\n\nIs it fast enough to use USB cameras for real time object detection? I looked online and for CSI cameras you can buy the IMX519 but for USB cameras they seem to be more expensive and way lower quality. I am using cpp and yolo11 for inference.\n\nAny suggestions on cameras to buy that you really recommend or any other resources that would be useful?",
    "author": "Micnasr",
    "timestamp": "2025-10-08T08:37:19",
    "url": "https://reddit.com/r/computervision/comments/1o1dxd5/4_cameras_object_detection/",
    "score": 2,
    "num_comments": 20,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1078k",
    "title": "What‚Äôs ‚Äúproduction‚Äù look like for you?",
    "content": "Looking to up my game when it comes to working in production versus in research mode. For example by ‚Äúproduction mode‚Äù I‚Äôm talking about the codebase and standard operating procedures you go to when your boss says to get a new model up and running next week alongside the two dozen other models you‚Äôve already developed and are now maintaining.  Whereas ‚Äúresearch mode‚Äù is more like a pile of half-working notebooks held together with duct tape. \n\nWhat are people‚Äôs setups like? How are you organizing things? Level of abstraction? Do you store all artifacts or just certain things? Are you utilizing a lot of open-source libraries or mostly rolling your own stuff? Fully automated or human in the loop?\n\nReally just prompting you guys to talk about how you handle this important aspect of the job!\n\n",
    "author": "InternationalMany6",
    "timestamp": "2025-10-07T20:46:13",
    "url": "https://reddit.com/r/computervision/comments/1o1078k/whats_production_look_like_for_you/",
    "score": 18,
    "num_comments": 4,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o11q90",
    "title": "Practicality of using CV2 on getting dimensions of Objects",
    "content": "Hello everyone,\n\nI‚Äôm planning to work on a proof of concept (POC) to determine the dimensions of logistics packages from images. The idea is to use computer vision techniques potentially with OpenCV to automatically measure package length, width, and height based on visual input captured by a camera system.\n\nHowever, I‚Äôm concerned about the practicality and reliability of using OpenCV for this kind of core business application. Since logistics operations require precise and consistent measurements, even small inaccuracies could lead to significant downstream issues such as incorrect shipping costs or storage allocation errors.\n\nI‚Äôd appreciate any insights or experiences you might have regarding the feasibility of this approach, the limitations of OpenCV for high-accuracy measurement tasks, and whether integrating it with other technologies (like depth cameras or AI-based vision models) could improve performance and reliability.",
    "author": "Mean_Mongoose_7404",
    "timestamp": "2025-10-07T22:09:44",
    "url": "https://reddit.com/r/computervision/comments/1o11q90/practicality_of_using_cv2_on_getting_dimensions/",
    "score": 11,
    "num_comments": 6,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0fv3n",
    "title": "Fun with YOLO object detection and RealSense depth powered 3D bounding boxes!",
    "content": "GitHub: [https://github.com/chrismatthieu/realsense-yolo-3d](https://github.com/chrismatthieu/realsense-yolo-3d) ",
    "author": "Chemical-Hunter-5479",
    "timestamp": "2025-10-07T07:10:55",
    "url": "https://reddit.com/r/computervision/comments/1o0fv3n/fun_with_yolo_object_detection_and_realsense/",
    "score": 169,
    "num_comments": 29,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o17o9j",
    "title": "Has anyone converted RT-DETR to NCNN (for mobile)? ONNX / PNNX hit unsupported torch ops",
    "content": "Hey all \n\n\n\nI‚Äôm trying to get **RT-DETR (from Ultralytics)** running on **mobile (via NCNN)**. My conversion pipeline so far:\n\n1. Export model to ONNX\n2. Use ONNX to NCNN (via onnx2ncnn / pnnx)\n\nBut I keep running into unsupported operators / Torch layers that NCNN (or PNNX) can‚Äôt handle.\n\n\n\n# What I‚Äôve attempted &amp; the issues encountered\n\n* I tried directly converting the Ultralytics RT-DETR (PyTorch) to ONNX to NCNN. But ONNX contains some Torch-derived ops / custom ops that NCNN can‚Äôt map.\n* I also tried **PNNX** (PyTorch / ONNX to NCNN converter), but that also fails on RT-DETR (e.g. handling of higher-rank tensors, ‚Äúbinaryop‚Äù with rank-6 tensors) per issue logs.\n* On the Ultralytics repo, there is an issue where export to NCNN or TFLite fails.¬†\n* On the Tencent/ncnn repo, there is an open issue ‚ÄúImpossible to convert RTDetr model‚Äù ‚Äî people recommend using the latest PNNX tool but no confirmed success.¬†\n* Also Ultralytics issue #10306 mentions problems in the export pipeline, e.g. ops with rank 6 tensors that NCNN doesn‚Äôt support.¬†\n\nSo far I‚Äôm stuck ‚Äî the converter chokes on intermediate ops (e.g. binaryop on high-rank tensors, etc.).\n\n\n\n# What I‚Äôm hoping someone here might know / share\n\n* Has **anyone successfully converted an RT-DETR (or variant) model to NCNN** and run inference on mobile?\n* What workarounds or ‚Äúfixes‚Äù did you apply to unsupported ops? (e.g. rewriting parts of the model, operator fusion, patching PNNX, custom plugins)\n* Did you simplify parts of the model (e.g., removing or approximating troublesome layers) to make it ‚ÄúNCNN-friendly‚Äù?\n* Any insights on which RT-DETR variant (small, lite, trimmed) is easier to convert?\n* If you used an alternative backend (e.g. TensorRT, TFLite, MNN, etc.) instead and why you chose it over NCNN.\n\n\n\n# Additional context &amp; constraints\n\n* I need this to run **on-device (mobile / embedded)**\n* I prefer to stay within open-source toolchains (PNNX, NCNN)\n* If needed, I‚Äôm open to modifying model architecture / pruning / reimplementing layers in a ‚ÄúNCNN-compatible‚Äù style\n\nIf you‚Äôve done this before ‚Äî or even attempted partial conversion ‚Äî I‚Äôd deeply appreciate any pointers, code snippets, patches, or caveats you ran into.\n\nThanks in advance!\n\n",
    "author": "iem-saad",
    "timestamp": "2025-10-08T04:20:00",
    "url": "https://reddit.com/r/computervision/comments/1o17o9j/has_anyone_converted_rtdetr_to_ncnn_for_mobile/",
    "score": 3,
    "num_comments": 9,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1fwcq",
    "title": "FastVLM n FastViTHD in action!",
    "content": "",
    "author": "TextDeep",
    "timestamp": "2025-10-08T09:49:16",
    "url": "https://reddit.com/r/computervision/comments/1o1fwcq/fastvlm_n_fastvithd_in_action/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1f6ge",
    "title": "Restormer - Experience and Challenges",
    "content": "I'm getting started on working on a CI/CV project for which I was looking at potential state of the art models to compare my work to. Does anyone have any experience working with Restormer in any context? What were some challenges you faced and what would you do differently? \nOne thing that I have seen is that it is computationally expensive. \n\nLink: https://arxiv.org/abs/2111.09881",
    "author": "Gayarmy",
    "timestamp": "2025-10-08T09:22:41",
    "url": "https://reddit.com/r/computervision/comments/1o1f6ge/restormer_experience_and_challenges/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1ahrs",
    "title": "i need references pls",
    "content": "Hey everyone, how‚Äôs it going?\n\nI wanted to ask something just for reference.\n\n\n\nI‚Äôm about to start a project that I already have a working prototype for ‚Äî it involves using YOLOv11 with object tracking to count items moving in and out of a certain area in real time, using a camera mounted above a doorway.\n\nThe idea is to display the counts and some stats on a dashboard or simple graphical interface.\n\n\n\nThe hardware would be something like a Jetson Orin Nano or a ReComputer Jetson, with a connected camera and screen, and it would require traveling on-site for installation and calibration.\n\nThere‚Äôs also some dataset labeling and model training involved to fine-tune detection accuracy for the specific environment.\n\n\n\nMy question is: what would you say is the minimum reasonable amount you‚Äôd charge for a project like this, considering the development, dataset work, hardware integration, and travel?\n\n\n\nI‚Äôm just trying to get a general sense of the ballpark for this kind of work.",
    "author": "aiduc",
    "timestamp": "2025-10-08T06:27:36",
    "url": "https://reddit.com/r/computervision/comments/1o1ahrs/i_need_references_pls/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o1542r",
    "title": "Anyone here tried RTMaps with ROS for development ?",
    "content": "Hi I came across this linkedin Post from Enzo : [https://www.linkedin.com/posts/enzo-ghisoni-robotics\\_ros2-robotics-computervision-activity-7347958048675495936-F4b0?utm\\_source=share&amp;utm\\_medium=member\\_desktop&amp;rcm=ACoAAA8GTEMBtl3EqVfpXcVphtJ-QEPW4sxfaL8](https://www.linkedin.com/posts/enzo-ghisoni-robotics_ros2-robotics-computervision-activity-7347958048675495936-F4b0?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA8GTEMBtl3EqVfpXcVphtJ-QEPW4sxfaL8)\n\nIt is block-based interface for building ROS 2 pipelines and perception pipelines. \nHas anyone here tried it? ",
    "author": "gauti1311",
    "timestamp": "2025-10-08T01:43:37",
    "url": "https://reddit.com/r/computervision/comments/1o1542r/anyone_here_tried_rtmaps_with_ros_for_development/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0e88e",
    "title": "Face Reidentification Project  üë§üîçüÜî",
    "content": "* üïπ Try out: [https://antal.ai/demo/facerecognition/demo.html](https://antal.ai/demo/facerecognition/demo.html)\n* üí° Learn more: [https://antal.ai/projects/face\\_recognition.html](https://antal.ai/projects/face_recognition.html)\n* üìñ Code documentation: [https://antal.ai/demo/facerecognition/documentation/index.html](https://antal.ai/demo/facerecognition/documentation/index.html)\n\nThis project is designed to perform face re-identification and assign IDs to new faces. The system uses OpenCV and neural network models to detect faces in an image, extract unique feature vectors from them, and compare these features to identify individuals.\n\nYou can try it out firsthand on my website. Try this: If you move out of the camera's view and then step back in, the system will recognize you again, displaying the same \"faceID\". When a new person appears in front of the camera, they will receive their own unique \"faceID\".\n\nI compiled the project to WebAssembly using Emscripten, so you can try it out on my website in your browser. If you like the project, you can purchase it from my website. The entire project is written in C++ and depends solely on the OpenCV library. If you purchase, you will receive the complete source code, the related neural networks, and detailed documentation. \n\n",
    "author": "Gloomy_Recognition_4",
    "timestamp": "2025-10-07T06:05:04",
    "url": "https://reddit.com/r/computervision/comments/1o0e88e/face_reidentification_project/",
    "score": 50,
    "num_comments": 4,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0mzzi",
    "title": "Real-Time Object Detection on edge devices without Ultralytics",
    "content": "Hello guys üëã,\n\nI've been trying to build a project with cctv cameras footage and need to create an app that can detect people in real time and the hardware is a simple laptop with no gpu, so need to find an alternative to Ultralytics license free object detection model that can work on real-time on cpu, I've tested Mmdetection and paddlepaddle and it is very hard to implement so are there any other solution?",
    "author": "Esi_ai_engineer2322",
    "timestamp": "2025-10-07T11:29:31",
    "url": "https://reddit.com/r/computervision/comments/1o0mzzi/realtime_object_detection_on_edge_devices_without/",
    "score": 12,
    "num_comments": 32,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0kuwq",
    "title": "Best practices for annotating basketball court keypoints for homography with YOLOv8 Pose?",
    "content": "I'm working on project to create a tactical 2d map from nba2k game footage. Currently my pipeline is to use a YOLOv8 pose model to detect court keypoints, and then use OpenCV to calculate a homography matrix to map everything onto a top-down view of the court.\n\nI'm struggling to get an accurate keypoint detection model. I've trained a model on about 50 manually annotated frames in roboflow but the predictions are consistently inaccurate, often with a systematic offset. I suspect I'm annotating in a wrong way. There's not too much variation in the images because the camera angle from the footage has a fixed position. It zooms in and out slightly but the keypoints always remain in view. \n\nWhat I've done so far:\n\n* Dataset Structure: I'm using a single object class called court.\n* Bounding Box Strategy: I'm trying to be very consistent with my bounding boxes, anchoring them tightly to specific court landmarks (the baseline, the top of the 3pt arc, and the 3pt corners) on every frame.\n* Keypoint Placement: I'm aiming for high precision, placing keypoints on the exact centre of line intersections.\n\nDespite this, my model is still not performing well and I'm wondering if I'm missing something key.\n\nHow can I improve my annotations? Is there a better way to define the bounding box or select the keypoints to build a more robust and accurate model?\n\nI've attached three images to show my process:\n\n1. My Target 2D Map: This is the simple, top-down court I want to map the coordinates onto.\n2. My Annotation Example: This shows how I'm currently drawing the tight bounding box and placing the keypoints.\n3. My Model's Inaccurate Output: This shows the predictions from my current model on a test frame. You can see how the points are consistently offset.\n\nAny tips or resources from those who have worked on similar sports analytics or homography projects would be greatly appreciated.",
    "author": "Big-Professional2635",
    "timestamp": "2025-10-07T10:13:55",
    "url": "https://reddit.com/r/computervision/comments/1o0kuwq/best_practices_for_annotating_basketball_court/",
    "score": 9,
    "num_comments": 6,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0ytbw",
    "title": "Medical Graph detection from lab reports.",
    "content": "Hello everyone, \n\nA part of my project is to detect whether graphs like ECG is present in the lab report or not. Do I train my own model or are there any models published for this specific use case. \n\nI'm quite new to this whole thing, so forgive me if the options I put forward are blunders and please suggest a light weight solution. ",
    "author": "Equivalent_Ad393",
    "timestamp": "2025-10-07T19:35:28",
    "url": "https://reddit.com/r/computervision/comments/1o0ytbw/medical_graph_detection_from_lab_reports/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0pd3v",
    "title": "Advice on collecting data for oral histopathology image classification",
    "content": "I‚Äôm currently working on a research project involving oral cancer histopathological image classification, and I could really use some advice from people who‚Äôve worked with similar data.\n\nI‚Äôm trying to decide whether it‚Äôs better to collect whole slide images (WSIs) or to use captured images (smaller regions captured from slides).\n\nIf I go with captured images, I‚Äôll likely have multiple captures containing cancerous tissues from different parts of the same slide (or even multiple slides from the same patient).\n\nMy question is: should I treat those captures as one data point (since they‚Äôre from the same case) or as separate data points for training?\n\nI‚Äôd really appreciate any advice, papers, or dataset references that could help guide my approach.",
    "author": "DryHat3296",
    "timestamp": "2025-10-07T12:55:33",
    "url": "https://reddit.com/r/computervision/comments/1o0pd3v/advice_on_collecting_data_for_oral_histopathology/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0gxvs",
    "title": "Benchmarking vision models",
    "content": "Hello everyone,\n\nI would like to know what are the best practices you apply while comparing different models on different tasks that are trained on different domain specific datasets. \n\nAs far as I know running models multiple times with different seeds, reporting metrics, then some statistical calculations (mean, std, etc.)\n\nBut I would like to know the standards when we want compare A architecture with B with same hyperparameters on same dataset for example.\n\nDo you know any papers, sources to read ? Thanks.",
    "author": "raufatali",
    "timestamp": "2025-10-07T07:51:25",
    "url": "https://reddit.com/r/computervision/comments/1o0gxvs/benchmarking_vision_models/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o097ag",
    "title": "First-class 3D Pose Estimation",
    "content": "I was looking into pose estimation and extraction from a given video file.\n\nAnd I find current research to initially extract 2D frames, before proceeding to extrapolate from the 2D keypoints.\n\nAre there any first-class single-shot video to pose models available ?\n\n  \nPreferably Open Source.\n\nReference: [https://github.com/facebookresearch/VideoPose3D/blob/main/INFERENCE.md](https://github.com/facebookresearch/VideoPose3D/blob/main/INFERENCE.md)",
    "author": "WinMassive5748",
    "timestamp": "2025-10-07T01:36:31",
    "url": "https://reddit.com/r/computervision/comments/1o097ag/firstclass_3d_pose_estimation/",
    "score": 15,
    "num_comments": 5,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0rzmt",
    "title": "Computer Vision PhD in Neuroimaging vs Agriculture",
    "content": "",
    "author": "bellwetherlk",
    "timestamp": "2025-10-07T14:33:23",
    "url": "https://reddit.com/r/computervision/comments/1o0rzmt/computer_vision_phd_in_neuroimaging_vs_agriculture/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0rszz",
    "title": "Reconhecimento visual para identificar bocas",
    "content": "Hello everyone,\n\nI'm nearing the end of my Computer Science degree and have been assigned a project to identify mouth types. Basically, I need the model (I'm using YOLO, but suggestions are welcome) to identify what a mouth is in the image.\n\nIn the second step, I need it to categorize whether the identified mouth is type A, B, or C. I'll post an example of a type A mouth. \n\nhttps://preview.redd.it/1gtay6gdcrtf1.png?width=427&amp;format=png&amp;auto=webp&amp;s=550d773818ff2208bd9fabe496f53dbe14c669e4\n\nAny suggestions on how I can do this?\n\nThank you in advance if you've read this far &lt;3",
    "author": "Anxious_Anteater3258",
    "timestamp": "2025-10-07T14:26:14",
    "url": "https://reddit.com/r/computervision/comments/1o0rszz/reconhecimento_visual_para_identificar_bocas/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nzztj3",
    "title": "Last week in Multimodal AI - Vision Edition",
    "content": "I curate a weekly newsletter on multimodal AI, here are vision related highlights from last week:\n\n**Tencent DA2 - Depth in any direction**\n\n* First depth model working in ANY direction\n* Sphere-aware ViT with 10x more training data\n* Zero-shot generalization for 3D scenes\n* [Paper](https://arxiv.org/pdf/2509.26618) | [Project Page](https://depth-any-in-any-dir.github.io/)\n\n**Ovi - Synchronized audio-video generation**\n\n* Twin backbone generates both simultaneously\n* 5-second 720√ó720 @ 24 FPS with matched audio\n* Supports 9:16, 16:9, 1:1 aspect ratios\n* [HuggingFace](https://huggingface.co/chetwinlow1/Ovi) | [Paper](https://arxiv.org/pdf/2510.01284)\n\nhttps://reddit.com/link/1nzztj3/video/w5lra44yzktf1/player\n\n**HunyuanImage-3.0**\n\n* Better prompt understanding and consistency\n* Handles complex scenes and detailed characters\n* [HuggingFace](https://huggingface.co/tencent/HunyuanImage-3.0) | [Paper](https://arxiv.org/pdf/2509.23951)\n\n**Fast Avatar Reconstruction**\n\n* Personal avatars from random photos\n* No controlled capture needed\n* [Project Page](https://zcai0612.github.io/UP2You/)\n\nhttps://reddit.com/link/1nzztj3/video/if88hogozktf1/player\n\n**ModernVBERT - Efficient document retrieval**\n\n* 250M params matches 2.5B models\n* Cross-modal transfer fixes data scarcity\n* 7x faster CPU inference\n* [Paper](https://arxiv.org/pdf/2510.01149)¬†|¬†[HuggingFace](https://huggingface.co/ModernVBERT)\n\nhttps://preview.redd.it/vf3q0rrqzktf1.png?width=1170&amp;format=png&amp;auto=webp&amp;s=247d1582dd660068ebcb8c49d67a1f0338562dfa\n\nAlso covered: VLM-Lens benchmarking toolkit, LongLive interactive video generation, visual encoder alignment for diffusion\n\nFree newsletter(demos,papers,more):¬†[https://thelivingedge.substack.com/p/multimodal-monday-27-small-models](https://thelivingedge.substack.com/p/multimodal-monday-27-small-models)",
    "author": "Vast_Yak_4147",
    "timestamp": "2025-10-06T17:08:08",
    "url": "https://reddit.com/r/computervision/comments/1nzztj3/last_week_in_multimodal_ai_vision_edition/",
    "score": 25,
    "num_comments": 3,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nzdmiu",
    "title": "Synthetic endoscopy data for cancer differentiation",
    "content": "This is a 3D clip composed of synthetic images of the human intestine.\n\nOne of the biggest challenges in medical computer vision is getting balanced and well-labeled datasets. Cancer cases are relatively rare compared to non-cancer cases in the general population. Synthetic data allows you to generate a dataset with any proportion of cases. We generated synthetic datasets that support a broad range of simulated modalities: colonoscopy, capsule endoscopy, hysteroscopy.¬†\n\nDuring acceptance testing with a customer, we benchmarked classification performance for detecting two lesion types:\n\n* **Synthetic data results:** Recall 95%, Precision 94%  \n* **Real data results:** Recall 85%, Precision 83%\n\nBeyond performance, synthetic datasets eliminate privacy concerns and allow tailoring for rare or underrepresented lesion classes.\n\nCurious to hear what others think ‚Äî especially about broader applications of synthetic data in clinical imaging. Would you consider training or pretraining with synthetic endoscopy data before moving to real datasets?",
    "author": "SKY_ENGINE_AI",
    "timestamp": "2025-10-06T01:37:32",
    "url": "https://reddit.com/r/computervision/comments/1nzdmiu/synthetic_endoscopy_data_for_cancer/",
    "score": 237,
    "num_comments": 36,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nztztc",
    "title": "Visual AI for Agricultural Use Cases - Free Virtual and In-Person Events",
    "content": "Registration info in the comments. Join us for these free virtual and in-person events to hear talks from experts on the latest developments at the intersection of visual AI and agriculture. ",
    "author": "sickeythecat",
    "timestamp": "2025-10-06T13:14:04",
    "url": "https://reddit.com/r/computervision/comments/1nztztc/visual_ai_for_agricultural_use_cases_free_virtual/",
    "score": 21,
    "num_comments": 2,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0c107",
    "title": "I just built a CNN model that recognizes handwritten numbers at midnight",
    "content": "",
    "author": "eminaruk",
    "timestamp": "2025-10-07T04:24:29",
    "url": "https://reddit.com/r/computervision/comments/1o0c107/i_just_built_a_cnn_model_that_recognizes/",
    "score": 0,
    "num_comments": 15,
    "upvote_ratio": 0.49,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0bdr4",
    "title": "Colmap bad results",
    "content": "",
    "author": "Unhappy-Print8574",
    "timestamp": "2025-10-07T03:50:29",
    "url": "https://reddit.com/r/computervision/comments/1o0bdr4/colmap_bad_results/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o0k5dx",
    "title": "RECOMENDACIONES PARA LA SEGMENTACI√ìN DE FALLAS (GRIETAS Y HUECOS) PEQUE√ëAS OBTENIDAS DE IM√ÅGENES AEREAS",
    "content": "¬°Buen d√≠a!\n\n  \nEstoy trabajando en un proyecto de la carrera de ingenier√≠a civil (pregrado). Que b√°sicamente consiste en la segmentaci√≥n de instancias multiclase para identificar grietas y huecos (fallas) en pavimentos de ciclov√≠as usando im√°genes obtenidas mediante fotogrametr√≠a con Drone UAV.\n\nAl principio me fue bastante bien con el manejo de obtenci√≥n de datos y entender la arquitectura YOLO11-seg (no a gran detalle), pero al entrenar el modelo con mi propio dataset (im√°genes ortogonales obtenidas desde mi celular a 2m de altura + im√°genes a√©reas de dron a 5m de altura con una resolucion menor a 0.5 cm/pixel) he presentado dificultades para lograr m√©tricas de deteccion aceptables al predecir im√°genes no entrenada. Siendo uno de los principales problemas el hecho de que el modelo segmenta fallas que no son. Vease IMG01\n\n[IMG01](https://preview.redd.it/ca4as2q1sptf1.png?width=990&amp;format=png&amp;auto=webp&amp;s=a416e84878a415ac17cf62bbed3f1f70681752a9)\n\n\n\nOtro de los problemas es con respecto al arduo trabajo de etiquetado manual de grietas para mi dataset en Roboflow, debido que esta etapa la considero muy trabajosa. \n\nQu√© alternativas se encuentran m√°s accesibles en t√©rminos de tiempo para reducir este proceso y obtener resultados prometedores. \n\n[IMG02](https://preview.redd.it/f8wggxonqptf1.png?width=1347&amp;format=png&amp;auto=webp&amp;s=91a143fa3bc4c3100a8bb28aeb0fcb129e9055de)\n\n  \nEn base a estas principales inquietudes, qu√© me podr√≠an sugerir en base a su arduo conocimiento en visi√≥n artificial, puesto a que he encontrado miles de papers en sitios como google scholar, sciencedirect, etc. M√°s no encuentro guias completas que expliquen problem√°ticas puntuales basadas en enfoque de segmentaci√≥n para im√°genes a√©reas y de mediana resolucion. \n\nPsdt: Si pueden brindarme material audiovisual/textual o una recomendaci√≥n para mejorar el enfoque de mi proyecto, se los agradecer√≠a, ya que realmente estoy muy interesado en aprender sobre visi√≥n artificial, pero el hecho de encontrarme limitado a la informaci√≥n y consecuentemente al conocimiento, me desanima mucho y no quiero tirar la toalla con este lindo proyecto. \n\nEspero sus comentarios y cr√≠ticas constructivas, gracias!",
    "author": "Maximum_Candidate830",
    "timestamp": "2025-10-07T09:48:58",
    "url": "https://reddit.com/r/computervision/comments/1o0k5dx/recomendaciones_para_la_segmentaci√≥n_de_fallas/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1o038uc",
    "title": "Cognex ViDi EL Classify tool - what's the secret sauce?",
    "content": "Hello, we use Cognex Insight2800 cameras at work and the 'Classify' tool is sort of amazing for how quickly it's able to effectively classify a OK/NG condition. Also, the ability to update it with new frames/captures at any point and see the confidence factor go up or down is really neat. \n\nAll the compute for this is local on the camera, which is not very powerful computer-wise. What's the secret sauce here? What do you guys think is going on behind the scenes that allows this tool to get decent classification results with only a handful of user-classified examples? ",
    "author": "LukeDuke",
    "timestamp": "2025-10-06T19:47:58",
    "url": "https://reddit.com/r/computervision/comments/1o038uc/cognex_vidi_el_classify_tool_whats_the_secret/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nzvehy",
    "title": "YOLO12 Object Segmentation with OAK D Pro Camera?",
    "content": "I am trying to use my weights from my trained YOLO12n and s model on my OAK D Pro Camera. This works seamlessly on my YOLOv11 models but it seems that it's not yet supporting YOLO12. Can there be a workaround which still allows me to use it on the cameras chip? Normally I would just deploy it on my device but to make it more comparable on my thesis, I wanted to try it once again.",
    "author": "FragrantPassenger891",
    "timestamp": "2025-10-06T14:05:01",
    "url": "https://reddit.com/r/computervision/comments/1nzvehy/yolo12_object_segmentation_with_oak_d_pro_camera/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nztna2",
    "title": "Structural distractions in edge detection",
    "content": "Currently working on a vision project for some videos. The issue is qualities within the video vary greatly. Initially we were just detecting all edges and then picking the upper and lowermost continuous edges. This worked for maybe 75% of our images. But the other 25% have large structural distractions that cause false edges (generally above the uppermost edge). Obviously the aforementioned approach fails on this.\n\nI‚Äôve tried several things at this point, some in combination with eachother. Fitting a polynomial via RANSAC (edge should form a parabola), curvature based path finding, slope based path finding, and more. I‚Äôm tempted to try random sampling but this is a performance constrained system.\n\nAny ideas/help?",
    "author": "GanachePutrid2911",
    "timestamp": "2025-10-06T13:01:28",
    "url": "https://reddit.com/r/computervision/comments/1nztna2/structural_distractions_in_edge_detection/",
    "score": 2,
    "num_comments": 9,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nzhpc9",
    "title": "Jetson Orin Nano Vs. Raspberry pi 5 with an A.I. Hat 13 or 26 TOPS",
    "content": "I'm thinking about trying a sensor-fusion project and I'm having a lot of trouble choosing an Orin Nano and a Raspberry pi 5. The amounnt is a concern as I'm trying to keep it budget friendly. Would Raspberry pi 5 be enough to run a sensor-fusion?",
    "author": "Mochiert",
    "timestamp": "2025-10-06T05:30:23",
    "url": "https://reddit.com/r/computervision/comments/1nzhpc9/jetson_orin_nano_vs_raspberry_pi_5_with_an_ai_hat/",
    "score": 6,
    "num_comments": 10,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nzrg4b",
    "title": "Prints defect detection problem",
    "content": "Hello, newbie in computer vision.\n\nI want to create a vision system to control the quality of prints on paper and I want to verify here my approach.\n\nMain goals:\n\n* **to find a graphic on the captured picture** \\- i thought here about using a template matching with the perfect image on captured image and cutting the region of interest, but there is a problem that if the captured image won't allign perectly, it won't analyze the whole image and there will be some deviations due to unability of template matching to capture the rotated images. What's the best approach here, to catch the rotated image? Shall I use some kind of DL models, or are there any classic CV approaches?\n* **to find a deffects caused by printing heads**:\n   * Printing head has nozzles, that sometimes are being plugged. The result is the line on the print, which I want to detect\n   * Changes in the color of the image relative to the original digital image - I thought of creating some kind of mask, which will analyze the colors of the image if they have a right value. The problem here is that I print with CMYK color range, but the camera captures image with RGB.\n\nSo tl;dr I want to create a program that is able to:  \n\\- check if the printed pattern on the paper matches the original digital design  \n\\- finds deffects on the printed pattern, like lines, or any other defects  \n\\- checks if the color saturation is ok\n\nPhysical setup:\n\nThere will be a linear camera (meaning the image can be infinitely long), and the analyzed printout will travel on a conveyor belt. Image collection will simply be integrated with the conveyor belt's movement, ensuring the image is the correct size. I'm aware that lighting will be crucial, but for now, I'm assuming the light intensity will remain constant. All prints will be with the same image. I assume the lighting will be perfect.\n\nAny tips, papers, or code examples would be really appreciated",
    "author": "Longjumping-Low-4716",
    "timestamp": "2025-10-06T11:40:36",
    "url": "https://reddit.com/r/computervision/comments/1nzrg4b/prints_defect_detection_problem/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nzdqp8",
    "title": "VLMs on Edge Devices",
    "content": "Has anyone tried running VLMs on edge devices (e.g. cctv's) for object detection? If so, are there latency issues? How's the accuracy like?",
    "author": "jingieboy",
    "timestamp": "2025-10-06T01:45:07",
    "url": "https://reddit.com/r/computervision/comments/1nzdqp8/vlms_on_edge_devices/",
    "score": 5,
    "num_comments": 5,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nzqkd7",
    "title": "How to make SwinUNETR (3D MRI Segmentation) train faster on Colab T4 ‚Äî currently too slow, runtime disconnects",
    "content": "I‚Äôm training a¬†**3D SwinUNETR**¬†model for MRI lesion segmentation (MSLesSeg dataset) using¬†**PyTorch/MONAI components**¬†on¬†**Google Colab Free (T4 GPU)**.  \nDespite using¬†**small patches (64√ó64√ó64)**¬†and¬†**batch size = 1**, training is¬†**extremely slow**, and the Colab session¬†**disconnects before completing epochs**.\n\n**Setup summary:**\n\n* Framework: PyTorch transforms\n* Model: SwinUNETR (3D transformer-based UNet)\n* Dataset: MSLesSeg (3D MR volumes \\~182√ó218√ó182)\n* Input: 64¬≥ patches via TorchIO¬†`Queue`¬†\\+¬†`UniformSampler`\n* Batch size: 1\n* GPU: Colab Free (T4, 16 GB VRAM)\n* Dataset loader: TorchIO¬†`Queue`¬†(not using CacheDataset/PersistentDataset)\n* AMP: not currently used (no autocast / GradScaler in final script)\n* Symptom: slow training ‚Üí Colab runtime disconnects before finishing\n* Approx. epoch time: unclear (probably several minutes)\n\nWhat‚Äôs the most effective way to¬†**reduce training time or memory pressure**¬†for SwinUNETR on a limited T4 (Free Colab)? Any insights or working configs from people who‚Äôve run¬†**SwinUNETR or 3D UNet models on small GPUs (T4 / 8‚Äì16 GB)**¬†would be really valuable.",
    "author": "SuperSwordfish1537",
    "timestamp": "2025-10-06T11:07:42",
    "url": "https://reddit.com/r/computervision/comments/1nzqkd7/how_to_make_swinunetr_3d_mri_segmentation_train/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nzkqld",
    "title": "help me to resolve this error",
    "content": "Even after installing the latest version of the bitsandbytes  library i am still getting Import error to install the latest version . tried solutions from chatgpt and online but cant solve this issue.  \ni am using collab and trying to finetune VLM\n\nError - **ImportError: Using \\`bitsandbytes\\` 4-bit quantization requires the latest version of bitsandbytes: \\`pip install -U bitsandbytes\\`**\n\n  \nCode-\n\n    import torch\n    MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n    from transformers import BitsAndBytesConfig, Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n    \n    \n    \n    if torch.cuda.is_available():\n        device = \"cuda\"\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16\n        )\n        model = Qwen2VLForConditionalGeneration.from_pretrained(\n            MODEL_ID,\n            device_map=\"auto\",\n            quantization_config=bnb_config,\n            use_cache=False\n        )\n    else:\n        device = \"cpu\"\n        model = Qwen2VLForConditionalGeneration.from_pretrained(MODEL_ID, use_cache=False)\n    \n    processor = Qwen2VLProcessor.from_pretrained(MODEL_ID)\n    processor.tokenizer.padding_side = 'right'",
    "author": "Monkey--D-Luffy",
    "timestamp": "2025-10-06T07:33:44",
    "url": "https://reddit.com/r/computervision/comments/1nzkqld/help_me_to_resolve_this_error/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nzkfz1",
    "title": "Best Approach for Open-Ended VQA: Fine-tuning a VL Model vs. Using an Agentic Framework (LangChain)?",
    "content": "Hi everyone,  \nI'm working on a project that requires answering complex, open-ended questions about images, and I'm trying to determine the most effective architectural approach to maximize accuracy. I have a custom dataset of (image, question, answer) pairs ready.\n\nI'm currently considering two main paths:\n\n1. **Fine-tuning a Vision-Language (VL) Model:** This involves taking a strong base model and fine-tuning it directly on my dataset.\n2. **Agentic Approach using LangChain/LangGraph:** This involves using a powerful, general-purpose VL model as a \"tool\" within a larger agentic system. The agent, built with a framework like LangChain or LangGraph, could decompose a complex question, use the VL model to perform specific visual perception tasks, and then synthesize a final answer based on the results.\n\nMy primary goal is to achieve the highest possible accuracy and robustness. Which of these two paths would you generally recommend, and what are the key trade-offs I should be aware of?\n\nAdditionally, I would be extremely grateful for any pointers to helpful resources:\n\n* **GitHub Repositories or Libraries:** Any examples or tools you've found useful, especially for implementing the agentic VQA approach.\n* **Reference Materials:** Key research papers, tutorials, or blog posts that compare these strategies or provide guidance.\n* **Alternative Methods:** Any other state-of-the-art models or techniques I might be overlooking for this kind of task.\n\nThanks in advance for your time and insights\n\n",
    "author": "Fit-Musician-8969",
    "timestamp": "2025-10-06T07:22:31",
    "url": "https://reddit.com/r/computervision/comments/1nzkfz1/best_approach_for_openended_vqa_finetuning_a_vl/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nz27st",
    "title": "Multisensor rig for computer vision v2",
    "content": "I have posted earlier about the same project:\n\n[Multisensor rig for computer vision](https://www.reddit.com/r/computervision/comments/1l4mo2a/multisensor_rig_for_computer_vision/) and [Computer for a multisensor rig](https://www.reddit.com/r/computervision/comments/1lufxwc/computer_for_a_multisensor_rig/)\n\nHere it is now integrated on a vehicle. Now, there are still many open questions and I will try to collect them in a separate post soon, but now I would like to see if there is some community interest about it and let you drill me a bit with your questions. So, go ahead and ask!",
    "author": "super_koza",
    "timestamp": "2025-10-05T15:32:24",
    "url": "https://reddit.com/r/computervision/comments/1nz27st/multisensor_rig_for_computer_vision_v2/",
    "score": 19,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nzgluc",
    "title": "Tooth Segmentation  Annotation",
    "content": "I'm working on post-processing a dental image where I've annotated the dentin (blue) using a polygon mask and the pulp (red) using the brush tool in Label Studio. My goal is to subtract the pulp area from the dentin region to generate the correct annotation.\n\nHere's what I've tried so far:\n\n* Vector subtraction with¬†`shapely.difference()`\n* Raster-to-vector conversion (decode RLE ‚Üí contours ‚Üí Shapely subtraction)\n* Mask subtraction with NumPy (`dentin_mask &amp; ~pulp_mask`)\n* Repairing geometry with¬†`polygon.buffer(0)`¬†before subtraction\n* Filtering valid, external contours with OpenCV\n* A hybrid approach (converting pupil mask to polygon, fixing geometry, and subtracting)\n\nI've exported the annotations in both JSON and COCO formats. I also tried using libraries like¬†`label_studio_tools`¬†and¬†`pycocotools`, but ran into module errors.\n\nHas anyone dealt with a similar issue or found reliable processing techniques to resolve this type of annotation subtraction problem? Any advice or workflow recommendations would be appreciated!\n\nhttps://preview.redd.it/epbc7659ahtf1.png?width=445&amp;format=png&amp;auto=webp&amp;s=07ae2cffe72510281e7b633a7cd17d2c059b5b45\n\n",
    "author": "calculussucksperiod",
    "timestamp": "2025-10-06T04:37:36",
    "url": "https://reddit.com/r/computervision/comments/1nzgluc/tooth_segmentation_annotation/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nz0cae",
    "title": "A scalable inference platform that provides multi-node management and control for CV inference workloads.",
    "content": "I shared this side project a couple of weeks ago [https://www.reddit.com/r/computervision/comments/1nn5gw6/cv\\_inference\\_pipeline\\_builder/?utm\\_source=share&amp;utm\\_medium=web3x&amp;utm\\_name=web3xcss&amp;utm\\_term=1&amp;utm\\_content=share\\_button](https://www.reddit.com/r/computervision/comments/1nn5gw6/cv_inference_pipeline_builder/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button)\n\nFinally got round to tidying up some bits *(still a lot to do... thanks Claude for the spaghetti code)* and making it public.\n\n[https://github.com/olkham/inference\\_node](https://github.com/olkham/inference_node)\n\nIf you give it a try, let me know what breaks first üòÖ",
    "author": "dr_hamilton",
    "timestamp": "2025-10-05T14:14:24",
    "url": "https://reddit.com/r/computervision/comments/1nz0cae/a_scalable_inference_platform_that_provides/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nzcnl1",
    "title": "running DM-VIO",
    "content": "helllo everyone,  if someone has expirence in running DM-VIO on custom dataset, something tat you made yourself, plese contact me, i need help fast",
    "author": "nmam_adeep",
    "timestamp": "2025-10-06T00:32:37",
    "url": "https://reddit.com/r/computervision/comments/1nzcnl1/running_dmvio/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nyl7g2",
    "title": "Struggling in my final PhD year ‚Äî need guidance on producing quality research in VLMs",
    "content": "Hi everyone,\n\nI‚Äôm a final-year PhD student working alone without much guidance. So far, I‚Äôve published one paper ‚Äî a fine-tuned CNN for brain tumor classification. For the past year, I‚Äôve been fine-tuning vision-language models (like Gemma, LLaMA, and Qwen) using Unsloth for brain tumor VQA and image captioning tasks.\n\nHowever, I feel stuck and frustrated. I lack a deep understanding of pretraining and modern VLM architectures, and I‚Äôm not confident in producing high-quality research on my own.\n\nCould anyone please suggest how I can:\n\n1. Develop a deeper understanding of VLMs and their pretraining process\n\n\n2. Plan a solid research direction to produce meaningful, publishable work\n\n\n\nAny advice, resources, or guidance would mean a lot.\n\nThanks in advance.",
    "author": "Ahmadai96",
    "timestamp": "2025-10-05T03:58:34",
    "url": "https://reddit.com/r/computervision/comments/1nyl7g2/struggling_in_my_final_phd_year_need_guidance_on/",
    "score": 28,
    "num_comments": 12,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nyqcyu",
    "title": "Object detection under the hood including yolo and modern archs like DETR.",
    "content": "I am finding it really hard to find a good blog or youtube video that really explains the theory of how object detection models work what is going on under the hood and how does the architecture actually work especially yolo. Any blog or youtube video or book that really breaks down every pice of the architecture and breaks abstractions as well.",
    "author": "NeuralNoble",
    "timestamp": "2025-10-05T07:55:48",
    "url": "https://reddit.com/r/computervision/comments/1nyqcyu/object_detection_under_the_hood_including_yolo/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nz0h7o",
    "title": "Has anyone already used Radxa ROCK 4D and/or Cubie A7A ?",
    "content": "",
    "author": "Al_GoRythm_",
    "timestamp": "2025-10-05T14:19:52",
    "url": "https://reddit.com/r/computervision/comments/1nz0h7o/has_anyone_already_used_radxa_rock_4d_andor_cubie/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nyq9cf",
    "title": "Help me build a simple Android/iOS app that runs YOLO (for defect detection demo)",
    "content": "Hey everyone,\n\nI‚Äôve been working on a computer vision project using **YOLOv7** to detect defects on industrial parts. The model is trained and works pretty well ‚Äî nothing fancy, but it gets the job done.\n\nNow I‚Äôd like to **showcase it to my company** (and maybe open a few doors), so I‚Äôm thinking of building a **very simple mobile app** ‚Äî basically something that can show live detection results from the camera feed.\n\nHere‚Äôs the problem: I‚Äôm **not really a developer**, and my attempts so far have been... bad üòÖ (ultralytics hub). I‚Äôm considering hiring someone on **Fiverr/Upwork** to put this together, but I have **no idea what to ask for or how much it should cost**.\n\nSo:\n\n* What‚Äôs a realistic budget for a basic YOLO-based demo app (Android and/or iOS, wichever is easier)?\n* What should I ask or specify when posting a job for this? Expecially considering I don't want anything fancy.\n\nAnd if there‚Äôs a **straightforward guide or repo** that shows how to do this myself, I‚Äôd love to give it a try too.\n\nThanks in advance for any pointers üôè",
    "author": "TheRealCaso",
    "timestamp": "2025-10-05T07:51:49",
    "url": "https://reddit.com/r/computervision/comments/1nyq9cf/help_me_build_a_simple_androidios_app_that_runs/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ny3de5",
    "title": "Improving small, fast-moving object detection/tracking at 240 fps (sports)",
    "content": "Hitting a wall with this detection and tracking problem for small, fast objects in outdoor sports video. We're talking baseballs, golf balls. It's 240fps with mixed lighting, and the performance just tanks with any clutter, motion blur, or partial occlusions.\n\nThe setup is a YOLO-family backbone, training imgsz is around 1280 cause of VRAM limits. Tried the usual stuff. Higher imgsz, class-aware sampling, copy-paste, mosaic, some HSV and blur augs. Also ran some experiments with slicing like SAHI, but the results are mixed. In a lot of clips, blur is a way bigger problem than object scale.\n\nLooking for thoughts on a few things.\n\nP2 head vs SAHI for these tiny targets, what's the actual accuracy and latency trade-off you've seen? Any good starter YAMLs? What loss and NMS settings are people using? Any preferred Focal/Varifocal settings or box loss that boosts recall without spiking the FPs? For augs, anything beyond mosaic that actually helps with motion blur or rolling shutter on 240fps footage? Also trying to figure out the best way to handle the hard examples without overfitting. Any lightweight deblur pre-processing that plays nice with detectors at this frame rate?\n\nFor tracking, what's the go-to for tiny, fast objects with momentary occlusions? BYTE, OC-SORT, BoT-SORT? What params are you guys using? Has anyone tried training a larger teacher model and distilling down? Wondering if it gives a noticeable bump in recall for tiny objects.\n\nAlso, how are you evaluating this stuff beyond mAP50/95? Need a way to make sure we're not getting fooled by all the easy scenes. Any recs would be awesome.",
    "author": "tomsoundz",
    "timestamp": "2025-10-04T12:38:58",
    "url": "https://reddit.com/r/computervision/comments/1ny3de5/improving_small_fastmoving_object/",
    "score": 18,
    "num_comments": 6,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nyfyw1",
    "title": "Multi Modal Input",
    "content": "Hey all,\n\nSpecifically related to medical imaging:\n\nLet‚Äôs say that I have some combination of medical imaging modalities (X-rays, CT/MRI, live intra-operative digital intra-operative imaging):\n\n1) Obvious some modalities provide much more information than others, but how accurately can one in real time segment specific anatomic structures by incorporating previously obtained data (ie - recognizing an appendix as distinct from a diverticulosis of the colon)\n2) Can real time human image annotation significantly improve said segmentation? For example, while a surgeon is viewing the abdomen through a laparoscope, can an assistant ‚Äúcircle‚Äù an area of interest on a screen, and have this provide enhanced improvement of the CV evaluation of that region?\n\nBasically trying to create a HUD for real time medical imaging based on static previously obtained imaging, augmented by real time human input ",
    "author": "CrookedCasts",
    "timestamp": "2025-10-04T22:36:09",
    "url": "https://reddit.com/r/computervision/comments/1nyfyw1/multi_modal_input/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ny47a8",
    "title": "How to get camera intrinsics and depth maps?",
    "content": "I am trying to use FoundationPose to get the 6 DOF pose of objects in my dataset. My dataset contains 3d point cloud, 200 images per model and masks. However, it seems like FoundationPose also need depth maps and camera intrinsics which I don't have. The broader task involves multiple neural networks so I am avoiding using AI to generate them just to minimize compound error of the overall pipeline. Are there some really good packages that I can use to calculate camera intrinsics and depth maps with only using images, 3d object and masks?",
    "author": "ExcellentFile6873",
    "timestamp": "2025-10-04T13:12:45",
    "url": "https://reddit.com/r/computervision/comments/1ny47a8/how_to_get_camera_intrinsics_and_depth_maps/",
    "score": 7,
    "num_comments": 7,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ny4su4",
    "title": "Handball model (kids sports)",
    "content": "So, my son plays u13 handball, and I have taken up filming the matches (using xbotgo) for the team, it gets me involved in the team and I get to be a bit nerdy.\nWhat I would love is to have a few models that:\n could use kinematics to give me a top down  view of the players on each team (I've been thinking that since the goal is almost always in frame  and is striped red/white it should be doable)\nShot analysis model that could show where shots were taken from (and whether they were saved/blocked/missed/goal could be entered by me)\n\nIt would be great with stats per team/jersey number (player)\n\nSo models would need to recognize \nBall, team1, team2 (including goalkeeper), goal, and preferably jersey number\n\nThat is as far as I have come, I think I am in too deep with trying to create models, tried some roboflow models with stills from my games, and it isn't really filling me with confidence that I could use a model from there.\n\nIs there a history for people wanting to do something like this for \"fun\" if the credits are paid for? Or something similar, I don't have a huge amount of money to throw at it, but it would be so useful to have for the kids, and I would love to play with something like this \n\n[this is some of the inspiration](https://www.spiideo.com/autodata/autodata-handball/)",
    "author": "create4drawing",
    "timestamp": "2025-10-04T13:36:38",
    "url": "https://reddit.com/r/computervision/comments/1ny4su4/handball_model_kids_sports/",
    "score": 5,
    "num_comments": 8,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ny23z2",
    "title": "Suggestion",
    "content": "I'm almost well versed with open cv now, what do I learn or do next??",
    "author": "LifeguardStraight819",
    "timestamp": "2025-10-04T11:49:40",
    "url": "https://reddit.com/r/computervision/comments/1ny23z2/suggestion/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nx2gjs",
    "title": "Heat maps extraction for Ultralytics YOLO",
    "content": "Hi everybody. I would like to ask how this kind of heat map extraction can be done? \n\nI know feature or attention map extraction (transformer specific) can be done, but how they (image taken from yolov12 paper) can get that much perfect feature maps? \n\nOr am I missing something in the context of heat maps?\n\nAny clarification highly appreciated. Thx.",
    "author": "raufatali",
    "timestamp": "2025-10-03T08:35:38",
    "url": "https://reddit.com/r/computervision/comments/1nx2gjs/heat_maps_extraction_for_ultralytics_yolo/",
    "score": 98,
    "num_comments": 9,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nxe3z2",
    "title": "[HIRING] Member of Technical Staff ‚Äì Computer Vision @ ProSights (YC)",
    "content": "I‚Äôm building ProSights (YC W24), where investment and data science teams rely on our proprietary data extraction + orchestration tech to turn messy docs (PDFs, images, spreadsheets, JSON) into structured insights.\n\nIn the past 6 months, we‚Äôve sold into over half of the 25 largest private equity firms and became cash flow positive.\n\nHappy to answer questions in the comments or DMs!\n\n‚Äî‚Äî‚Äî\n\nAs a Member of Technical Staff, you‚Äôll own our extraction domain end-to-end:\n- Advance document understanding (OCR, CV, LLM-based tagging, layout analysis)\n- Transform real-world inputs into structured data (tables, charts, headers, sentences)\n- Ship research ‚Üí production systems that 1000s of enterprise users depend on\n\nQualifications\n- 3+ years in computer vision, OCR, or document understanding\n- Strong Python + full-stack data fluency (datasets ‚Üí models ‚Üí APIs ‚Üí pipelines)\n- Experience with OCR pipelines + LLM-based programming is a big plus\n\nWhat We Offer\n- Ownership of our core CV/LLM extraction stack\n- Freedom to experiment with cutting-edge models + tools\n- Direct collaboration with the founding team (NYC-based, YC community)",
    "author": "jw00zy",
    "timestamp": "2025-10-03T16:08:38",
    "url": "https://reddit.com/r/computervision/comments/1nxe3z2/hiring_member_of_technical_staff_computer_vision/",
    "score": 9,
    "num_comments": 13,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nxp058",
    "title": "ROS 2 Integration for TEMAS Sensors ‚Äì Your Feedback Matters!",
    "content": "Hi everyone, \n\nWe‚Äôre excited to share that we‚Äôre currently developing a ROS 2 package for TEMAS!\n\nThis will make it possible to integrate TEMAS sensors directly into ROS 2-based robotics projects ‚Äî perfect for research, education, and rapid prototyping. \n\nOur goal is to make the package as flexible and useful as possible for different applications. \n\nThat‚Äôs why we‚Äôd love to get your input: Which features or integrations would be most valuable for you in a ROS 2 package? \n\nYour feedback will help us shape the ROS 2 package to better fit the needs of the community. Thank you for your amazing support ‚Äî \n\nwe can‚Äôt wait to show you more soon! \n\nRubu Team",
    "author": "Big-Mulberry4600",
    "timestamp": "2025-10-04T01:58:34",
    "url": "https://reddit.com/r/computervision/comments/1nxp058/ros_2_integration_for_temas_sensors_your_feedback/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nwl9ub",
    "title": "RF-DETR Segmentation Preview: Real-Time, SOTA, Apache 2.0",
    "content": "We [just launched](https://blog.roboflow.com/rf-detr-segmentation-preview/) an instance segmentation head for [RF-DETR](https://github.com/roboflow/rf-detr), our permissively licensed, real-time detection transformer. It achieves SOTA results for realtime segmentation models on COCO, is designed for fine-tuning, and runs at up to 300fps (in fp16 at 312x312 resolution with TensorRT on a T4 GPU).\n\nDetails in [our announcement post](https://blog.roboflow.com/rf-detr-segmentation-preview/), fine-tuning and deployment code is available both in [our repo](https://github.com/roboflow/rf-detr) and on [the Roboflow Platform](https://app.roboflow.com).\n\nThis is a preview release derived from a pre-training checkpoint that is still converging, but the results were too good to keep to ourselves. If the remaining pre-training improves its performance we'll release updated weights alongside the RF-DETR paper (which is planned to be released by the end of October).\n\nGive it a try on your dataset and let us know how it goes!",
    "author": "aloser",
    "timestamp": "2025-10-02T17:50:22",
    "url": "https://reddit.com/r/computervision/comments/1nwl9ub/rfdetr_segmentation_preview_realtime_sota_apache/",
    "score": 252,
    "num_comments": 14,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nx44hp",
    "title": "Using a HomeAssistant powered bridge between my Blink outdoor cameras and my bird spotter model",
    "content": "Long term goal is to auto populate a webpage when a particular species is detected. ",
    "author": "bigjobbyx",
    "timestamp": "2025-10-03T09:36:48",
    "url": "https://reddit.com/r/computervision/comments/1nx44hp/using_a_homeassistant_powered_bridge_between_my/",
    "score": 10,
    "num_comments": 1,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nx1m69",
    "title": "Depth Estimation Model won't train properly",
    "content": "https://preview.redd.it/b2gqrpn3wwsf1.png?width=405&amp;format=png&amp;auto=webp&amp;s=44c400e54f28908520b7b1f1e754173c52a31624\n\nhello everyone. I have been trying to implement a light weight depth estimation model from a paper. The top part is my prediction and botton one is the GT. Idk where the training is going wrong but the loss plateau's and it doesn't seem to learn. also the prediction is very noisy. I have tried adding other loss functions but they don't seem to make a difference.\n\nThis is the paper: [https://ieeexplore.ieee.org/document/9411998](https://ieeexplore.ieee.org/document/9411998)\n\ncode: [https://github.com/Utsab-2010/Depth-Estimation-Task/blob/main/mobilenetv2.pytorch/test\\_v3.ipynb](https://github.com/Utsab-2010/Depth-Estimation-Task/blob/main/mobilenetv2.pytorch/test_v3.ipynb)\n\nany help will be appreciated",
    "author": "Otaku_boi1833",
    "timestamp": "2025-10-03T08:04:07",
    "url": "https://reddit.com/r/computervision/comments/1nx1m69/depth_estimation_model_wont_train_properly/",
    "score": 9,
    "num_comments": 11,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nx552i",
    "title": "Looking for Camera/Sensor Recommendations for Optical Dimensional Inspection Project",
    "content": "I want to design a device to inspect and sort small, 2d-ish components like the ones shown. Checking things like if the diameter is in tolerance, the ‚Äúteeth‚Äù, etc. The max part size would be 2 inches (50.8mm) in diameter. I was originally going to use a telecentric lens mounted over a small conveyor belt, but I haven‚Äôt been able to find one for less than $2,000. I will have a calibration/reference image at the same height as the part, and the camera will be in a fixed position. Ideally I‚Äôll be able to measure the parts with an accuracy of +/-0.001 in (0.025mm). Are there any cheaper camera/lens options available? ",
    "author": "helpmeowo",
    "timestamp": "2025-10-03T10:14:05",
    "url": "https://reddit.com/r/computervision/comments/1nx552i/looking_for_camerasensor_recommendations_for/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nwxg6o",
    "title": "SAMv2 video/camera segmentation FPS?",
    "content": "How fast should it be? On their Github, 91.2 FPS is mentioned for the tiny checkpoint. However, I feel like there are some workarounds or unexplained things in the picture. When I run a 60 FPS video on drastically downsampled res (640x360), I still get barely 6 FPS on a single object being segmented (this is for instance segmentation).\n\nOf course I understand it wouldn't increase its FPS but there's no way the inference step supports 90 FPS without some major workarounds.\n\nEdit: also, I have a RTX3060, soooo...",
    "author": "regista-space",
    "timestamp": "2025-10-03T05:14:17",
    "url": "https://reddit.com/r/computervision/comments/1nwxg6o/samv2_videocamera_segmentation_fps/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nxc3rc",
    "title": "OpenCV framegrab doesnt reach maximum possible Camera FPS",
    "content": "My camera's max fps is 210 as listed below. But I can only get 120 fps on opencv, how do i get higher fps  \n`v4l2-ctl -d /dev/video0 --list-formats-ext`\n\n`ioctl: VIDIOC_ENUM_FMT`\n\n`Type: Video Capture`\n\n\n\n`[0]: 'MJPG' (Motion-JPEG, compressed)`\n\n`Size: Discrete 2560x800`\n\n`Interval: Discrete 0.008s (120.000 fps)`\n\n`Interval: Discrete 0.017s (60.000 fps)`\n\n`Interval: Discrete 0.040s (25.000 fps)`\n\n`Interval: Discrete 0.067s (15.000 fps)`\n\n`Interval: Discrete 0.100s (10.000 fps)`\n\n`Interval: Discrete 0.200s (5.000 fps)`\n\n`Size: Discrete 2560x720`\n\n`Interval: Discrete 0.008s (120.000 fps)`\n\n`Interval: Discrete 0.017s (60.000 fps)`\n\n`Interval: Discrete 0.040s (25.000 fps)`\n\n`Interval: Discrete 0.067s (15.000 fps)`\n\n`Interval: Discrete 0.100s (10.000 fps)`\n\n`Interval: Discrete 0.200s (5.000 fps)`\n\n`Size: Discrete 1600x600`\n\n`Interval: Discrete 0.008s (120.000 fps)`\n\n`Interval: Discrete 0.017s (60.000 fps)`\n\n`Interval: Discrete 0.067s (15.000 fps)`\n\n`Interval: Discrete 0.100s (10.000 fps)`\n\n`Interval: Discrete 0.200s (5.000 fps)`\n\n`Size: Discrete 1280x480`\n\n`Interval: Discrete 0.008s (120.000 fps)`\n\n`Interval: Discrete 0.017s (60.000 fps)`\n\n`Interval: Discrete 0.040s (25.000 fps)`\n\n`Interval: Discrete 0.067s (15.000 fps)`\n\n`Interval: Discrete 0.100s (10.000 fps)`\n\n`Interval: Discrete 0.200s (5.000 fps)`\n\n`Size: Discrete 640x240`\n\n`Interval: Discrete 0.005s (210.000 fps)`\n\n`Interval: Discrete 0.007s (150.000 fps)`\n\n`Interval: Discrete 0.008s (120.000 fps)`\n\n`Interval: Discrete 0.017s (60.000 fps)`\n\n`Interval: Discrete 0.040s (25.000 fps)`\n\n`Interval: Discrete 0.067s (15.000 fps)`\n\n`Interval: Discrete 0.100s (10.000 fps)`\n\n`Interval: Discrete 0.200s (5.000 fps)`\n\nBut when i set OpenCV FPS to 210, it just reaches 120 on both window and headless test.\n\n    int main() {¬† ¬† \n    int deviceID = 0;¬† ¬† cv::VideoCapture cap(deviceID, cv::CAP_V4L2);\n    \n    ¬† ¬† if (!cap.isOpened()) {\n    ¬† ¬† ¬† ¬† std::cerr &lt;&lt; \"ERROR: Could not open camera on device \" &lt;&lt; deviceID &lt;&lt; std::endl;\n    ¬† ¬† ¬† ¬† return 1;\n    ¬† ¬† }\n    \n    ¬† ¬† cap.set(cv::CAP_PROP_FOURCC, cv::VideoWriter::fourcc('M', 'J', 'P', 'G'));\n    ¬† ¬† cap.set(cv::CAP_PROP_FRAME_WIDTH, 640);\n    ¬† ¬† cap.set(cv::CAP_PROP_FRAME_HEIGHT, 240);\n    ¬† ¬† cap.set(cv::CAP_PROP_FPS, 210);\n    \n\n  \n",
    "author": "momoisgoodforhealth",
    "timestamp": "2025-10-03T14:43:25",
    "url": "https://reddit.com/r/computervision/comments/1nxc3rc/opencv_framegrab_doesnt_reach_maximum_possible/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nwwb86",
    "title": "Showcasing TEMAS: Modular 3D sensor platform (RGB + LiDAR + ToF) ‚Äì calibrated &amp; synchronized out of the box",
    "content": "Hey everyone,\nwe‚Äôre on our Road to Kickstarter and recently showcased TEMAS at KI Palooza (AI conference in Germany).\n\nWhat TEMAS is:\n\nModular 3D sensor platform combining RGB camera + LiDAR + ToF\n\nAll sensors are pre-calibrated and synchronized, so you get reliable data right away\n\nPowered by Raspberry Pi 5 and scalable with AI accelerators like Jetson or Hailo for advanced machine learning tasks.\n\nDelivers colorized 3D point clouds\n\nAccessible via PyPi Lib(pip install rubu)\n\nWe‚Äôd love your thoughts:\n\nWhich computer vision use cases would benefit most from an all-in-one, pre-calibrated sensor platform like this?",
    "author": "Big-Mulberry4600",
    "timestamp": "2025-10-03T04:18:51",
    "url": "https://reddit.com/r/computervision/comments/1nwwb86/showcasing_temas_modular_3d_sensor_platform_rgb/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nwayaf",
    "title": "I turned a hotel room at HILTON ISTANBUL into 3D using the VGGT model!",
    "content": "",
    "author": "eminaruk",
    "timestamp": "2025-10-02T10:57:00",
    "url": "https://reddit.com/r/computervision/comments/1nwayaf/i_turned_a_hotel_room_at_hilton_istanbul_into_3d/",
    "score": 112,
    "num_comments": 13,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nx6xw5",
    "title": "Help with identifying cloud from a NASA texture",
    "content": "Hello! I'm completely new to computer vision or image matching whatever you might call it, and I don't really know much about programming but I was wondering if someone could help me with this. I have a cropped image of a cloud from a game trailer and I know exactly what texture was used for it, the only thing is I don't know where on the texture it is. I tried manually looking for it and have found some success with other clouds but this cropped one eludes me. Is there a website I could go that would let me upload my 2 images and have it search one of them for the other? Or is there a program I can download that does this? I spent a little bit of time searching online for information about this and it seems that any application is done by manually running some code, which I don't want to say is beyond me but It seems a bit complicated for what I'm trying to do.\n\nLink to cloud texture for higher rez versions:  \n[https://visibleearth.nasa.gov/images/57747/blue-marble-clouds](https://visibleearth.nasa.gov/images/57747/blue-marble-clouds)\n\nAlso if this is not the right subreddit for this please let me know.\n\nEdit: I found a method that is somewhat working for me. ",
    "author": "UNSCfighter",
    "timestamp": "2025-10-03T11:21:31",
    "url": "https://reddit.com/r/computervision/comments/1nx6xw5/help_with_identifying_cloud_from_a_nasa_texture/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nw4qrg",
    "title": "How is this possible?",
    "content": "I was trying to do template matching with OpenCV, the cross correlation confidence is 0.48 for these two images. Isn't that insanely high?? How to make this algorithm more robust and reliable and reduce the false positives?",
    "author": "OkRestaurant9285",
    "timestamp": "2025-10-02T07:05:50",
    "url": "https://reddit.com/r/computervision/comments/1nw4qrg/how_is_this_possible/",
    "score": 71,
    "num_comments": 17,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nwwu2c",
    "title": "AI- Invoice/ Bill parser ( Ocr &amp; DocAI Proj)",
    "content": "Good Evening Everyone!\n\nHas anyone worked on OCR / Invoice/ bill parser¬† project? I needed advice.\n\nI have got a project where I have to extract data from the uploaded bill whether it's png or pdf to json format. It should not be AI api calling. I am working on some but no break through... Thanks in advance!",
    "author": "Putrid-Use-4955",
    "timestamp": "2025-10-03T04:44:52",
    "url": "https://reddit.com/r/computervision/comments/1nwwu2c/ai_invoice_bill_parser_ocr_docai_proj/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nw66bu",
    "title": "Preparing for an interview: C++ and industrial computer vision ‚Äì what should I focus on in 6 days?",
    "content": "Hi everyone,\n\nI have an interview next week for a working student position in **software development for computer vision**. The focus seems to be on **C++ development with industrial cameras (GenICam / GigE Vision)** rather than consumer-level libraries like OpenCV.\n\nHere‚Äôs my situation:\n\n* Strong C++ basics from robotics/embedded projects, but haven‚Äôt used it for image processing yet.\n* Familiar with ROS 2, microcontrollers, sensor integration, etc.\n* 6 days to prepare as effectively as possible.\n\nMy main questions:\n\n1. For industrial vision, what are the *essential concepts* I should understand (beyond OpenCV)?\n2. Which C++ techniques or patterns are critical when working with image buffers / real-time processing?\n3. Any recommended resources, tutorials, or SDKs (Basler Pylon, Allied Vision Vimba, etc.) that can give me a quick but solid overview?\n\nThe goal isn‚Äôt to become an expert in a week, but to demonstrate a strong foundation, quick learning curve, and awareness of industry standards.\n\nAny advice, resources, or personal experience would be greatly appreciated üôè",
    "author": "GenoTheSecond02",
    "timestamp": "2025-10-02T08:00:44",
    "url": "https://reddit.com/r/computervision/comments/1nw66bu/preparing_for_an_interview_c_and_industrial/",
    "score": 36,
    "num_comments": 25,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nwgylq",
    "title": "Is UNET v2 a good drop-in for UNET?",
    "content": "I have a workflow which I've been using a UNET in. I don't know if UNET v2 is better in every way or there's some costs associated to using it compared to a traditional UNET.",
    "author": "Affectionate_Use9936",
    "timestamp": "2025-10-02T14:41:27",
    "url": "https://reddit.com/r/computervision/comments/1nwgylq/is_unet_v2_a_good_dropin_for_unet/",
    "score": 5,
    "num_comments": 11,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nwoedq",
    "title": "Fast-Livo2",
    "content": "",
    "author": "Furai69",
    "timestamp": "2025-10-02T20:23:54",
    "url": "https://reddit.com/r/computervision/comments/1nwoedq/fastlivo2/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvwt2c",
    "title": "How to improve YOLOv11 detection on small objects?",
    "content": "Hi everyone,\n\nI‚Äôm training a YOLOv11 (nano) model to detect golf balls. Since golf balls are small objects, I‚Äôm running into performance issues ‚Äî especially on ‚Äúhard‚Äù categories (balls in bushes, on flat ground with clutter, or partially occluded).\n\n**Setup:**\n\n* Dataset: \\~10k images (8.5k train, 1.5k val), collected in diverse scenes (bushes, flat ground, short trees).\n* Training: 200 epochs, batch size 16, image size 1280.\n* Validation mAP50: **0.92**.\n\nI tried the Train Model on separate Test dataset  for validation and below are results we got .  \n**Test dataset** have **9 categories** and each have approx ---&gt;**30 images**\n\n**Test results:**\n\n    Category        Difficulty   F1_score   mAP50     Precision   Recall\n    short_trees     hard         0.836241   0.845406  0.926651    0.761905\n    bushes          easy         0.914080   0.970213  0.858431    0.977444\n    short_trees     easy         0.908943   0.962312  0.932166    0.886849\n    bushes          hard         0.337149   0.285672  0.314258    0.363636\n    flat            hard         0.611736   0.634058  0.534935    0.714286\n    short_trees     medium       0.810720   0.884026  0.747054    0.886250\n    bushes          medium       0.697399   0.737571  0.634874    0.773585\n    flat            medium       0.746910   0.743843  0.753674    0.740266\n    flat            easy         0.878607   0.937294  0.876042    0.881188\n\nThe **easy** and **medium** categories are fine but we want to make **F1** above **80**, and for  the **hard categories (especially bushes hard, F1=0.33, mAP50=0.28)** perform very poorly.\n\nMy main question: **What‚Äôs the best way to improve YOLOv11 performance ?**\n\nWould love to hear what worked for you when tackling small object detection.\n\nThanks!\n\nImages from Hard Category \n\nhttps://preview.redd.it/mduad1lshnsf1.png?width=427&amp;format=png&amp;auto=webp&amp;s=7031c807097ba282ccec6a198f047cbde7281263\n\nhttps://preview.redd.it/z3089asxhnsf1.png?width=427&amp;format=png&amp;auto=webp&amp;s=4232e7f525a900abcfd1150c46ca5d2da18b61ff\n\nhttps://preview.redd.it/3hqq8kd0insf1.png?width=427&amp;format=png&amp;auto=webp&amp;s=f9156d2b031127c100627ab3fb11c58c288a1906\n\nhttps://preview.redd.it/2p80crz1insf1.png?width=902&amp;format=png&amp;auto=webp&amp;s=5432c9f601cf456d8bd718564de29e53b18546c7\n\n",
    "author": "zaynst",
    "timestamp": "2025-10-02T00:01:17",
    "url": "https://reddit.com/r/computervision/comments/1nvwt2c/how_to_improve_yolov11_detection_on_small_objects/",
    "score": 14,
    "num_comments": 31,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nv4d8u",
    "title": "basketball players recognition with RF-DETR, SAM2, SigLIP and ResNet",
    "content": "Models I used:\n\n\\- RF-DETR ‚Äì a DETR-style real-time object detector. We fine-tuned it to detect players, jersey numbers, referees, the ball, and even shot types.\n\n\\- SAM2 ‚Äì a segmentation and tracking. It re-identifies players after occlusions and keeps IDs stable through contact plays.\n\n\\- SigLIP + UMAP + K-means ‚Äì vision-language embeddings plus unsupervised clustering. This separates players into teams using uniform colors and textures, without manual labels.\n\n\\- SmolVLM2 ‚Äì a compact vision-language model originally trained on OCR. After fine-tuning on NBA jersey crops, it jumped from 56% to 86% accuracy.\n\n\\- ResNet-32 ‚Äì a classic CNN fine-tuned for jersey number classification. It reached 93% test accuracy, outperforming the fine-tuned SmolVLM2.\n\nLinks:\n\n\\- code:¬†[https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb)\n\n\\- blogpost:¬†[https://blog.roboflow.com/identify-basketball-players](https://blog.roboflow.com/identify-basketball-players)\n\n\\- detection dataset:¬†[https://universe.roboflow.com/roboflow-jvuqo/basketball-player-detection-3-ycjdo/dataset/6](https://universe.roboflow.com/roboflow-jvuqo/basketball-player-detection-3-ycjdo/dataset/6)\n\n\\- numbers OCR dataset:¬†[https://universe.roboflow.com/roboflow-jvuqo/basketball-jersey-numbers-ocr/dataset/3](https://universe.roboflow.com/roboflow-jvuqo/basketball-jersey-numbers-ocr/dataset/3)",
    "author": "RandomForests92",
    "timestamp": "2025-10-01T03:17:32",
    "url": "https://reddit.com/r/computervision/comments/1nv4d8u/basketball_players_recognition_with_rfdetr_sam2/",
    "score": 510,
    "num_comments": 45,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvmih9",
    "title": "Multi-Location Object Counting Web App ‚Äî ASP.NET Core + RF-DETR / YOLO + Angular",
    "content": "I created this web app by prompting Gemini 2.5 Pro. It uses RTSP cameras (like regular IP surveillance cameras) to count objects.\n\nYou can use RF-DETR or YOLO.\n\nMore details in this GitHub repository:\n\n[Object Counting System](https://github.com/m0hamed23/Object-Counting-System)",
    "author": "Mohamed_ar2311",
    "timestamp": "2025-10-01T15:23:11",
    "url": "https://reddit.com/r/computervision/comments/1nvmih9/multilocation_object_counting_web_app_aspnet_core/",
    "score": 30,
    "num_comments": 5,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nw93e2",
    "title": "Alien vs Predator Image Classification with ResNet50 | Complete Tutorial [project]",
    "content": "https://preview.redd.it/0jniwo02aqsf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=1696b711c1b546d40ba40d80a1866c5d21abb921\n\n**I‚Äôve been experimenting with ResNet-50 for a small Alien vs Predator image classification exercise. (Educational)**\n\n**I wrote a short article with the code and explanation here:** [**https://eranfeit.net/alien-vs-predator-image-classification-with-resnet50-complete-tutorial**](https://eranfeit.net/alien-vs-predator-image-classification-with-resnet50-complete-tutorial)\n\n**I also recorded a walkthrough on YouTube here:** [**https://youtu.be/5SJAPmQy7xs**](https://youtu.be/5SJAPmQy7xs)\n\n**This is purely educational ‚Äî happy to answer technical questions on the setup, data organization, or training details.**\n\n¬†\n\n**Eran**",
    "author": "Feitgemel",
    "timestamp": "2025-10-02T09:49:08",
    "url": "https://reddit.com/r/computervision/comments/1nw93e2/alien_vs_predator_image_classification_with/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvfoby",
    "title": "Demo: transforming an archery target to a top-down-view",
    "content": "This video demonstrates my solution to a question that was asked here a few weeks ago. I had to cut about 7 minutes of the original video to fit Reddit time limits, so if you want a little more detail throughout the video, plus the part at the end about masking off the part of the image around the target, check my YouTube channel.",
    "author": "AntoneRoundyIE",
    "timestamp": "2025-10-01T11:06:38",
    "url": "https://reddit.com/r/computervision/comments/1nvfoby/demo_transforming_an_archery_target_to_a/",
    "score": 53,
    "num_comments": 6,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvi3hf",
    "title": "[Paper] Convolutional Set Transformer (CST) ‚Äî a new architecture for image-set processing",
    "content": "We introduce the **Convolutional Set Transformer**, a novel deep learning architecture for processing **image sets** that are visually heterogeneous yet share high-level semantics (e.g. a common category, scene, or concept). Our paper is available on [ArXiv](https://arxiv.org/abs/2509.22889) üëà\n\n# üîë Highlights\n\n* **General-purpose**: CST supports a broad range of tasks, including Contextualized Image Classification and Set Anomaly Detection.\n* Outperforms existing set-learning methods such as [Deep Sets](https://arxiv.org/abs/1703.06114) and [Set Transformer](https://arxiv.org/abs/1810.00825) in image-set processing.\n* Natively compatible with **CNN explainability tools** (e.g., Grad-CAM), unlike competing approaches.\n* First set-learning architecture with demonstrated **Transfer Learning support** ‚Äî we release **CST-15**, pre-trained on ImageNet.\n\n# üíª Code and Pre-trained Models (cstmodels)\n\nWe release the `cstmodels` Python package (`pip install cstmodels`) which provides reusable Keras 3 layers for building CST architectures, and an easy interface to load CST-15 pre-trained on ImageNet in just two lines of code:\n\n    from cstmodels import CST15\n    model = CST15(pretrained=True)\n\nüìë [API Docs](https://chinefed.github.io/convolutional-set-transformer/)  \nüñ• [GitHub Repo](https://github.com/chinefed/convolutional-set-transformer)\n\n# üß™ Tutorial Notebooks\n\n* [Training a toy CST from scratch on the CIFAR-10 dataset](https://github.com/chinefed/convolutional-set-transformer/blob/main/tutorial_notebooks/cst_from_scratch.ipynb)\n* [Transfer Learning with CST-15 on colorectal histology images ](https://github.com/chinefed/convolutional-set-transformer/blob/main/tutorial_notebooks/cst15_transfer_learning.ipynb)\n\n# üåü Application Example: Set Anomaly Detection\n\n**Set Anomaly Detection** is a binary classification task meant to identify images in a set that are **anomalous or inconsistent with the majority of the set**.\n\nThe Figure below shows two sets from CelebA. In each, most images share two attributes (*‚Äúwearing hat &amp; smiling‚Äù* in the first, *‚Äúno beard &amp; attractive‚Äù* in the second), while a minority lack both of them and are thus anomalous.\n\nAfter training a CST and a Set Transformer (Lee et al., 2019) on CelebA for Set Anomaly Detection, we evaluate the explainability of their predictions by overlaying Grad-CAMs on anomalous images.\n\n‚úÖ **CST highlights the anomalous regions correctly**  \n‚ö†Ô∏è **Set Transformer fails to provide meaningful explanations**\n\nhttps://preview.redd.it/am9yyfhgxjsf1.png?width=702&amp;format=png&amp;auto=webp&amp;s=4380db5d19858af31697538807c04a2ec4981155\n\nWant to dive deeper? Check out our paper!",
    "author": "chinefed",
    "timestamp": "2025-10-01T12:34:14",
    "url": "https://reddit.com/r/computervision/comments/1nvi3hf/paper_convolutional_set_transformer_cst_a_new/",
    "score": 28,
    "num_comments": 9,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvngxs",
    "title": "Help me improve my object segmentation UX",
    "content": "My app accepts a drawn bounding box and segments salient objects for design mockups.  See video...how can I make this sequence more satisfying for my users?",
    "author": "w0nx",
    "timestamp": "2025-10-01T16:03:47",
    "url": "https://reddit.com/r/computervision/comments/1nvngxs/help_me_improve_my_object_segmentation_ux/",
    "score": 10,
    "num_comments": 2,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nw8z25",
    "title": "Need to start my learning journey as a beginner, could use your insight. Thankyou.",
    "content": "(forgive me the above image has no relevance to my cry for help)\n\n\n\nI had studied image processing subject in my university, aced it well, but it was all theoretical and no practical, it was my fault too but I had to change my priorities back then.\n\nI want to start again, but not sure where to begin to re-learn and what research papers i should read to keep myself updated and how to get practical, because I don't want to make the same mistakes again.\n\n\nI have understanding of python and it's libraries. And I'm good at calculus and matrices, but don't know where to start. I intend to ask the gpt the same thing, but I thought before I did that, i should consult you guys (real and experienced) before. Thank you. \n\n\nMy college senior recommended I try the enrolling the free courses of opencv university, could use your insight. Thankyou.",
    "author": "Equity_Harbinger",
    "timestamp": "2025-10-02T09:44:34",
    "url": "https://reddit.com/r/computervision/comments/1nw8z25/need_to_start_my_learning_journey_as_a_beginner/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nveml3",
    "title": "Best of ICCV 2025 - Four Days of Virtual Events",
    "content": "Can't make it to ICCV 2025? Catch the highlights at these free virtual events! Registration info in the comments.",
    "author": "sickeythecat",
    "timestamp": "2025-10-01T10:29:10",
    "url": "https://reddit.com/r/computervision/comments/1nveml3/best_of_iccv_2025_four_days_of_virtual_events/",
    "score": 22,
    "num_comments": 3,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nw31vi",
    "title": "anti-shoplifting computer vision solution",
    "content": "How useful is an anti-shoplifting computer vision solution? Does this really help to detect shoplifting or headache for a shop owner with false alarms?",
    "author": "Sea-Manufacturer-646",
    "timestamp": "2025-10-02T05:57:42",
    "url": "https://reddit.com/r/computervision/comments/1nw31vi/antishoplifting_computer_vision_solution/",
    "score": 0,
    "num_comments": 19,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvdt7g",
    "title": "Does this video really show a breakthrough in airborne object detection with cameras?",
    "content": "I don‚Äôt have a strong background in computer vision, so I‚Äôd love to hear opinions from people with more expertise:\n\n[video](https://www.youtube.com/watch?v=YZkLQsv3huo)",
    "author": "0Kbruh1",
    "timestamp": "2025-10-01T09:59:51",
    "url": "https://reddit.com/r/computervision/comments/1nvdt7g/does_this_video_really_show_a_breakthrough_in/",
    "score": 8,
    "num_comments": 20,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nv9twa",
    "title": "Image classification tool using Google's sigLIP 2 So400m (naflex)",
    "content": "Hey everyone! I built a tool to search for images and videos locally using [Google's sigLIP 2 model](https://huggingface.co/google/siglip2-so400m-patch16-naflex).\n\nI'm looking for people to test it and share feedback, especially about how it runs on different hardware.\n\nDon't mind the ugly GUI, I just wanted to make it as simple and accessible as possible, but you can still use it as a command line tool anyway if you want to. You can find the repository here: [https://github.com/Gabrjiele/siglip2-naflex-search](https://github.com/Gabrjiele/siglip2-naflex-search)\n\n\n\n",
    "author": "AnywhereTypical5677",
    "timestamp": "2025-10-01T07:32:29",
    "url": "https://reddit.com/r/computervision/comments/1nv9twa/image_classification_tool_using_googles_siglip_2/",
    "score": 8,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvmkq8",
    "title": "Problem with understanding YOLOv8 loss function",
    "content": "I want to create my own YOLOv8 loss function to tailor it to my very specific usecase (for academic purposes). To do that, I need access to bounding boxes and their corresponding classes. I'm using Ultralytics implementation (https://github.com/ultralytics/ultralytics). I know the loss function is defined in ultralytics/utils/loss.py in class v8DetectionLoss. I've read the code and found two tensors: target\\_scores and target\\_bboxes. The first one is of size e.g. 12x8400x12 (I think it's batch size by number of bboxes by number of classes) and the second one of size 12x8400x4 (probably batch size by number of bboxes by number of coordinates). The numbers in target\\_scores are between 0 and 1 (so I guess it's probability) and the numbers in the second one are probably coordinates in pixels.\n\nTo be sure what they represent, I took my fine-tuned model, segmented an image and then started training the model with a debugger with only one element in the training set which is the image I segmented earlier (I put a breakpoint inside the loss function). I wanted to compare what the debugger sees during training in the first epoch with the image segmented with the same model. I took 15 elements with highest probability of belonging to some class (by searching through target\\_scores with something similar to argmax) and looked at what class they are predicted to belong to and their corresponding bboxes. I expected it to match the segmented image. The problem is that they don't match at all. The elements with the highest probabilities are of completely different classes than the elements with the highest probabilities in the segmented image. The bboxes seen through debugger don't make sense at all as well (although they seem to be bboxes because their coordinates are between 0 and 640, which is the resolution I trained the model with). I know that it's a very specific question but maybe you can see something wrong with my approach.",
    "author": "Astaemir",
    "timestamp": "2025-10-01T15:25:42",
    "url": "https://reddit.com/r/computervision/comments/1nvmkq8/problem_with_understanding_yolov8_loss_function/",
    "score": 1,
    "num_comments": 7,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvme4z",
    "title": "VLM for detailed description of text images?",
    "content": "Hi, what are the best VLMs, local and proprietary, for such a case. I've pasted an example image from ICDAR, I want it to be able to generate a response that describes every single property of a text image, from things like the blur/quality to the exact colors to the style of the font. It's unrealistic probably but figured I'd ask.\n\nhttps://preview.redd.it/sl3xkp9krksf1.png?width=430&amp;format=png&amp;auto=webp&amp;s=d0f7cc851916e402d4951255b7976eb0ab1e78cd\n\n",
    "author": "Relative-Pace-2923",
    "timestamp": "2025-10-01T15:18:29",
    "url": "https://reddit.com/r/computervision/comments/1nvme4z/vlm_for_detailed_description_of_text_images/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvlncr",
    "title": "$10,000 for B200s for cool project ideas",
    "content": "",
    "author": "tensorpool_tycho",
    "timestamp": "2025-10-01T14:49:02",
    "url": "https://reddit.com/r/computervision/comments/1nvlncr/10000_for_b200s_for_cool_project_ideas/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvl6s9",
    "title": "Advice on distinguishing phone vs landline use with YOLO",
    "content": "Hi all,\n\nI‚Äôm working on a project to detect whether a person is using a mobile phone or a landline phone. The challenge is making a reliable distinction between the two in real time.\n\nMy current approach:\n\n* Use¬†**YOLO11l-pose**¬†for person detection (it seems more reliable on near-view people than yolo11l).\n* For each detected person, run a¬†**YOLO11l-cls**¬†classifier (trained on a custom dataset) with three classes:¬†`no_phone`,¬†`phone`, and¬†`landline_phone`.\n\nThis should let me flag phone vs landline usage, but the issue is dataset size, right now I only have \\~5 videos each (1‚Äì2 people talking for about a minute). As you can guess, my first training runs haven‚Äôt been great. I‚Äôll also most likely end up with a very large \\`no\\_phone\\` class compared to the others.\n\nI‚Äôd like to know:\n\n* Does this seem like a solid approach, or are there better alternatives?\n* Any tips for improving YOLO classification training (dataset prep, augmentations, loss tuning, etc.)?\n* Would a different pipeline (e.g., two-stage detection vs. end-to-end training) work better here?",
    "author": "Choice_Committee148",
    "timestamp": "2025-10-01T14:30:50",
    "url": "https://reddit.com/r/computervision/comments/1nvl6s9/advice_on_distinguishing_phone_vs_landline_use/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nux45k",
    "title": "Whom should we hire? Traditional image processing person or deep learning",
    "content": "I am part of a company that deals in automation of data pipelines for Vision AI. Now we need to bring in a mindset to improve  benchmark in the current product engineering team where there is already someone who has worked at the intersection of Vision and machine learning but relatively lesser experience . He is more of a software engineering person than someone who brings new algos or improvements to automation on the table. He can code things but he is not able to move the real needle. He needs someone who can fill this gap with experience in vision but I see that there are 2 types of folks in the market. One who are quite senior and done traditional vision processing and others relatively younger who has been using neural networks as the key component and less of vision AI.\n\nMay be my search is limited but it seems like ideal is to hire both types of folks and have them work together but it‚Äôs hard to afford that budget.\n\nGuide me pls!",
    "author": "Worth-Card9034",
    "timestamp": "2025-09-30T19:59:25",
    "url": "https://reddit.com/r/computervision/comments/1nux45k/whom_should_we_hire_traditional_image_processing/",
    "score": 21,
    "num_comments": 40,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvgk1i",
    "title": "OpenFilter Hub",
    "content": "Hi folks -- Plainsight CEO here. We open-sourced 20 new computer vision \"filters\" based on OpenFilter. They are all listed on [hub.openfilter.io](http://hub.openfilter.io) with links to the code, documentation, and pypi/docker download links. \n\n  \nYou may remember we released OpenFilter back in May and posted about it here.\n\nPlease let us know what you think! More links are on [openfilter.io](http://openfilter.io)",
    "author": "AdFair8076",
    "timestamp": "2025-10-01T11:38:14",
    "url": "https://reddit.com/r/computervision/comments/1nvgk1i/openfilter_hub/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nv4is7",
    "title": "Exploring Semantic Kernel: A Deep Dive into Microsoft's AI SDK for Intelligent Applications",
    "content": "If you're delving into Microsoft's Semantic Kernel (SK) and seeking a comprehensive understanding, Valorem Reply's recent blog post offers valuable insights. They share their experiences and key learnings from utilizing SK to build Generative AI applications.\n\n**Key Highlights:**\n\n* **Orchestration Capabilities:** SK enables the creation of automated AI function chains or \"plans,\" allowing for complex tasks without predefining the sequence of steps.\n* **Semantic Functions:** These are essentially prompt templates that facilitate a more structured interaction with AI models, enhancing the efficiency of AI applications.\n* **Planner Integration:** SK's planners, such as the SequentialPlanner, assist in determining the order of function executions, crucial for tasks requiring multiple steps.\n* **Multi-Model Support:** SK supports various AI providers, including Azure OpenAI, OpenAI, Hugging Face, and custom models, offering flexibility in AI integration.",
    "author": "Sanny_fuz",
    "timestamp": "2025-10-01T03:26:50",
    "url": "https://reddit.com/r/computervision/comments/1nv4is7/exploring_semantic_kernel_a_deep_dive_into/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nv54ex",
    "title": "Why are the GFLOPS and Parameters not the same?",
    "content": "Hi! Im currently trying to train this exacty model of this paper ([OBC-YOLOv8: an improved road damage detection model based on YOLOv8 - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11784705/)). However, when I finished training the model I got these results:\n\nmAP50 = 85.6\n\nmAP50-90 = 58.8\n\nF1-score = 81.6\n\nParameters = 4.96\n\nGFLOPS = 9.3\n\nIt is our task to have the exact same results and I was wondering why I am not getting the same results.\n\nI edited the channels as well as when I trained the model at first I got an error that it was expecting a lower channel at the CoordAttention.",
    "author": "WorkingSurround5133",
    "timestamp": "2025-10-01T04:01:44",
    "url": "https://reddit.com/r/computervision/comments/1nv54ex/why_are_the_gflops_and_parameters_not_the_same/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nv9sc2",
    "title": "Need advice/help: AI system to detect behaviours on security cameras (Argentina-based)",
    "content": "Hi everyone,\n\nI‚Äôm from Argentina and I have an idea I‚Äôd like to explore.\nSecurity companies here use operators who monitor many buildings through cameras. It‚Äôs costly because humans need to watch all screens.\n\nWhat I‚Äôd like to build is an AI assistant for CCTV that can detect certain behaviors like:\n\n- Loitering (someone staying too long in a common area)\n\n- Entering restricted areas at the wrong time\n\n- Abandoned objects (bags/packages)\n\n- Unusual events (falls, fights, etc.)\n\nThe AI wouldn‚Äôt replace humans, just alert them so one operator can cover more buildings.\n\nI don‚Äôt know how to build this, how long it takes, or how much it might cost. I‚Äôm looking for guidance or maybe someone who would like to help me prototype something. Spanish speakers would be a plus, but not required.\n\nAny advice or help is appreciated!",
    "author": "Interesting-Post8260",
    "timestamp": "2025-10-01T07:30:46",
    "url": "https://reddit.com/r/computervision/comments/1nv9sc2/need_advicehelp_ai_system_to_detect_behaviours_on/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nv9o26",
    "title": "Contagem de caixas em paletas (YOLOv8n), problemas com paletas fracionadas",
    "content": "Ol√°, pessoal estou desenvolvendo uma solu√ß√£o de vis√£o computacional para **contar caixas em paletas fracionadas**\n\nResumo do que j√° fiz:\n\n**Estrutura:** Ultralytics / YOLOv8 (nano) , Python 3.12, PyTorch.\n\n`requirements.txt`(principais bibliotecas): ultralytics, opencv, torch&gt;=2.0, torchvision, numpy, pandas, matplotlib, etc.\n\nHardware: **i3-10100 + GTX1650 4 GB + 16 GB de RAM** .\n\nConjunto de dados: **488 imagens** anotadas no MakeSense; imagens tiradas com iPhone 15 (4284√ó5712), fotos laterais das paletas, varia√ß√µes de brilho e √¢ngulo.\n\nExemplo de como as imagens foram anotadas ultilzando o makesense.ia\n\nhttps://preview.redd.it/ygounwnndisf1.jpg?width=539&amp;format=pjpg&amp;auto=webp&amp;s=ddb8062e1da3c17b94a223d196365790eb93ad4e\n\n\n\nEstrutura:\n\n‚îú‚îÄ‚îÄ üìÅ datasets/  \n\n‚îÇ   ‚îú‚îÄ‚îÄ üìÅ pallet\\_boxes/        # Dataset para treinamento  \n\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ images/  \n\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ train/       # Imagens de treinamento  \n\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ val/         # Imagens de valida√ß√£o  \n\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ test/        # Imagens de teste  \n\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ labels/  \n\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÅ train/       # Labels de treinamento  \n\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÅ val/         # Labels de valida√ß√£o  \n\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÅ test/        # Labels de tes\n\n\n\nArgumento de treino que deu ‚Äúmelhor resultado‚Äù:\n\n`train_args = {`\n\n  `'data': 'datasets/dataset_config.yaml',`\n\n  `'epochs': 50,`\n\n  `'batch': 4,`\n\n  `'imgsz': 640,`\n\n  `'patience': 10,`\n\n  `'device': device,`\n\n  `'project': 'models/trained_models',`\n\n  `'name': 'pallet_detection_v2',`\n\n  `'workers': 2,`\n\n`}`\n\n\n\n**Testei:**    \n\\- mais √©pocas (+100),   \n\\- resolu√ß√£o maior,    \n\\- paci√™ncia maior     \nsem melhoria significativa.\n\n\n\n**Problema:** detec√ß√µes inconsistentes, n√£o sei se h√° falta de dados, anota√ß√µes, arquitetura ou hiperpar√¢metros ou se esta acontecendo overfiting.\n\nhttps://preview.redd.it/l4vt0y04fisf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=a9f1a92ea8cdc2197ed87fc729809c5cfcc31120\n\nhttps://preview.redd.it/qy0plx04fisf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=07071c2288cd69071dc80377596d56649c3f40c8\n\nhttps://preview.redd.it/hpitsx04fisf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=c7860ea4d2e19fae3426e46f361b708df132fafc\n\nhttps://preview.redd.it/qx8c3y04fisf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=faf6ea742f9d9162595560e5db6cf6488a48aae9\n\n",
    "author": "lucasanael",
    "timestamp": "2025-10-01T07:26:08",
    "url": "https://reddit.com/r/computervision/comments/1nv9o26/contagem_de_caixas_em_paletas_yolov8n_problemas/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nui7pd",
    "title": "a lot of things don't live up to their hype. moondream3 is NOT one of those things. it's actually kinda dope",
    "content": "Check out the integration in FiftyOne here: https://github.com/harpreetsahota204/moondream3\n\nOr, to see the results already parsed to a [FiftyOne Dataset](https://docs.voxel51.com/user_guide/using_datasets.html) you can download this dataset: https://huggingface.co/datasets/harpreetsahota/moondream3_on_images\n\nYou can evaluate the model performance in FiftyOne as well. Checkout the docs here: https://docs.voxel51.com/user_guide/evaluation.html",
    "author": "datascienceharp",
    "timestamp": "2025-09-30T09:41:43",
    "url": "https://reddit.com/r/computervision/comments/1nui7pd/a_lot_of_things_dont_live_up_to_their_hype/",
    "score": 48,
    "num_comments": 11,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nuz9ye",
    "title": "Roboflow for training YOLO or RF-DETR???",
    "content": "Hi all!  \nI am trying to generate a model that I can run ***WITHOUT INTERNET*** on an Nvidia Jetson Orin NX.  \nI started using Roboflow and was able to train a YOLO model, and I gotta say, it SUCKS! I was thinking I am really bad at this.\n\nThen I tried to train everything just the way it was with the YOLO model on RF-DETR, and wow.... that is accurate. Like, scary accurate.\n\nBut, I can't find a way to run RF-DETR on my JETSON without a connection to their service?  \nOr am i not actually married to roboflow and can run without internet. I ask because InferenceHTTPClient requires an api\\_key, if it is local, why require an api\\_key?\n\nPlease help, I really want to run without internet in the woods!\n\n\\[Edit\\]  \n\\-I am on the paid version  \n\\-I can download the RF-DETR .pt file, but can't figure out how to usse it :(",
    "author": "ConfectionForward",
    "timestamp": "2025-09-30T21:52:49",
    "url": "https://reddit.com/r/computervision/comments/1nuz9ye/roboflow_for_training_yolo_or_rfdetr/",
    "score": 4,
    "num_comments": 11,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nvgp21",
    "title": "the last ai edge device we need.  bleeding edge life",
    "content": "[https://ccnphfhqs21z.feishu.cn/wiki/F5krwD16viZoF0kKkvDcrZNYnhb](https://ccnphfhqs21z.feishu.cn/wiki/F5krwD16viZoF0kKkvDcrZNYnhb)\n\n[https://github.com/78/xiaozhi-esp32](https://github.com/78/xiaozhi-esp32)\n\nprivate, open source, edge ai, mcp server compatible. gpio sensors compatible. multi model. multi model.  \n  \nall projects should be built ontop of this in my opinion. ai first approach to solutions on the edge. \n\nive already built a few, i highly recommend if you read this, building a ton of these devices, they can be and do anything. in time. and that time is now.   \n\n\nhttps://preview.redd.it/fhjiznw6kjsf1.png?width=803&amp;format=png&amp;auto=webp&amp;s=d584dd0f6fdb58f62fa834f716fd550ec97cefe2\n\n",
    "author": "Drjonesxxx-",
    "timestamp": "2025-10-01T11:43:10",
    "url": "https://reddit.com/r/computervision/comments/1nvgp21/the_last_ai_edge_device_we_need_bleeding_edge_life/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.13,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nux760",
    "title": "best object detection in terms of efficiency/speed",
    "content": "i have a mid tier laptop  that runs yolo v8 to connect to an external camera and wanted to know if there are more efficient and faster A.I. models i can use\n\n",
    "author": "Worldly_Gold9169",
    "timestamp": "2025-09-30T20:03:29",
    "url": "https://reddit.com/r/computervision/comments/1nux760/best_object_detection_in_terms_of_efficiencyspeed/",
    "score": 3,
    "num_comments": 7,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nv2wmf",
    "title": "Help for Roboflow version updating",
    "content": "I have my version 1 of raw images dataset. Then after that I uploaded version 2 of the processed versions. I wanted both raw and processed to be kept. But after I uploaded the processed images it's the raw ones that appear instead in the new version. I've uploaded twice already around 8 GB. Does anyone have the same problem or can someone help me with this?",
    "author": "GTGA2004",
    "timestamp": "2025-10-01T01:43:02",
    "url": "https://reddit.com/r/computervision/comments/1nv2wmf/help_for_roboflow_version_updating/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nv5sfi",
    "title": "What are best practices for writing annotation guidelines for computer vision detection projects ?",
    "content": "When i asked Reddit about this query it provided me very generic version of the answer.\n\n* Structured and Organized Content\n* Explicit Instructions\n* Consistent Terminology\n* Quality Control and Feedback\n\nBut what i want to understand the community here to highlight the challenges faced due to unclear guidelines in their respective actual experiences in data annotation labeling initiatives?\n\n  \nThere must be scenarios which are domain/use case specific which should be kept in mind and might be generalizable to some extent",
    "author": "Worth-Card9034",
    "timestamp": "2025-10-01T04:37:45",
    "url": "https://reddit.com/r/computervision/comments/1nv5sfi/what_are_best_practices_for_writing_annotation/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nu9iut",
    "title": "Detecting small and specific movements in noisy radar, doable?",
    "content": "We're working with quite some videos of radar movements like the above. We are interested in the flight paths of birds. In the above example, I indicated with a red arrow an example of birds flying. Sadly, we are not working with the direct logs, rather the output images/videos.  \n\n\nAs you can see, there is quite a bit of noise, as well as that birds and their flights are small and are difficult to detect. \n\nIdeally, we would like to have a model that automatically detects the birds, and is able to connect flight paths (the radar is georeferenced). In our eyes, the model should also be temporal (e.g., with tracking or such a temporal model such as LSTM) to learn the characteristics of a bird flight and to discern bird movement from static (like the noise) and clouds. \n\nBut my expertise is lacking, and something is telling me that this use case is too difficult. Is it? If not, what would be a solid methodology, and what models are potentially suited? When I think of an LSTM (in combination with CNN for example), I think it looks at a time trajectory of a single pixel, when in fact a bird movement takes place over multiple of pixels.\n\n  \nThanks in advance!",
    "author": "augustcs",
    "timestamp": "2025-09-30T03:26:07",
    "url": "https://reddit.com/r/computervision/comments/1nu9iut/detecting_small_and_specific_movements_in_noisy/",
    "score": 43,
    "num_comments": 9,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1numouf",
    "title": "Oct 2 - Women in AI Virtual Meetup",
    "content": "Join us on Oct 2 for the monthly Women in AI virtual Meetup. [Register for the Zoom.](https://voxel51.com/events/women-in-ai-october-2-2025)",
    "author": "sickeythecat",
    "timestamp": "2025-09-30T12:29:01",
    "url": "https://reddit.com/r/computervision/comments/1numouf/oct_2_women_in_ai_virtual_meetup/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nu79dq",
    "title": "I am making an app to learn about 3D Computer Vision",
    "content": "Hello everyone,\n\nJust wanted to share an idea which I am currently working on. The backstory is that I am trying to finish my PhD in Visual SLAM and I am struggling to find proper educational materials on the internet. Therefore I started to create my own app which summarizes the main insights I am gaining during my research and learning process. The app is continously updated. I did not share the idea anywhere yet and in the r/appideas subreddit I just read the suggestion to talk about your idea *before* actually implementing it.\n\nNow I am curious what the CV community thinks about my project. I know it is unusual to post the app here and I was considering posting it in the appideas subreddit instead. But I think you are the right community to show it to, as you may have the same struggle as I do. Or maybe you do not see any value in such an app? Would you mind sharing your opinion? What do you really need to improve your knowledge or what would bring you the most benefit?\n\nLooking forward to reading your valuable feedback. Thank you!",
    "author": "Interesting-Net-7057",
    "timestamp": "2025-09-30T00:59:16",
    "url": "https://reddit.com/r/computervision/comments/1nu79dq/i_am_making_an_app_to_learn_about_3d_computer/",
    "score": 21,
    "num_comments": 20,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nukd44",
    "title": "Suggestions on vision research containing multi-level datasets",
    "content": "I have the following datasets:   \n  \n1. A large dataset of different bumblebee species (more than 400k images with 166 classes)   \n2. A small annotated dataset of bumblebee body masks (8,033 images)   \n3. A small annotated dataset of bumblebee body part masks (4,687 images of head, thorax and abodmen masks)\n\nNow I want to leverage these dataset for improving performance on bee classification. Does multimodal approach (segmentation+classification) seems a good idea? If not what approach do you suggest?\n\nMoreover, please let me know if there already exists multi-modal classification and segmentation model which can detect the \"head\" of species \"x\" in an image. The approach in my mind is train EfficientNetV2 for classification, and then YOLOv11-seg for segmenting different body parts (I tried the basic UNet model but it has poor results, YOLOv11-seg has good results, what other segmentation models should I use?). Use both models separately for species and body part labeling. But is there any better approach?",
    "author": "Alternative_Mine7051",
    "timestamp": "2025-09-30T11:01:40",
    "url": "https://reddit.com/r/computervision/comments/1nukd44/suggestions_on_vision_research_containing/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nue8ym",
    "title": "1M+ retail interior images. multi market, temporally organised (UK/US/EU)",
    "content": "All taken for our consulting work, we have ended up with 1m images going back to 2010, they're all owned by us and the majority are taken by me also. We appear to have created a superb archive of imagery, unwittingly, perhaps. \n\nThus we have compiled a comprehensive retail image dataset that might be useful for the community:\n\n**Our Dataset Overview:**\n\n* **Size:** 1M total images, 280K highly structured/curated by event.\n* **Coverage:** UK, US, Netherlands, Ireland retail environments. Predominantly UK.\n* **Organisation:** Categorised by year/month, retailer, season, product category (down to SKU level for organised subset of imagery).\n* **Range:** Multi year coverage including seasonal merchandising patterns (Christmas, Easter, Diwali, Valentine's Day etc, over 60 events)\n* **Use cases:** Planogram compliance, shelf monitoring, inventory management, out of stock detection, product recognition, autonomous checkout systems, signage, all images are used for our consulting work so these do not feature people and images are detailed and not simply random images in stores. \n\n**What makes this unique:**\n\n* Multi market data (different retail formats, lighting, merchandising across 4 countries and thousands of store locations and hundreds of banners)\n* Temporal dimension showing how displays evolve seasonally and generally (IE general store development) across the years and locations. \n* Professional curation (not just raw dumps) by year / month / retailer / type etc. \n* Implementation support and custom sorting is available, we can offer further support to aid model training and other elements.  \n\n**Availability:** We're making this available for commercial and research use. Academic researchers can inquire about discounted licensing, it's a brave new world for us so we are testing the water to see what interest there is, and how we may be able to market this. It's a new world entirely. We think there are use cases that we would develop (IE how has value for shoppers changed, inflation tracking, shrinkflation, best practice and showcasing what happened, when etc from a trade plan perspective).\n\nThis dataset addresses a common pain point we've observed: retail CV models struggling to see and visualise across different store environments and international markets. The temporal component is particularly valuable for understanding seasonal variations, especially as time has progressed in food retail, good / bad etc. \n\n**Interested?**\n\n* Please send me a DM for sample images, detailed specifications, and pricing, we have worked up a sample and have manifests and readme etc. \n* Looking for feedback from researchers on what additional annotations would be most valuable.\n* Open to partnerships with serious ML teams. \n\nHappy to answer questions in the comments about collection methodology, image quality, or specific use cases too. It's fully owned by us as a dataset and de-duplication has taken place on the seasonal aspect (280k) images already, folder names need to be harmonised though..... The bigger dataset is organised by month / week / retailer. ",
    "author": "malctucker",
    "timestamp": "2025-09-30T07:10:42",
    "url": "https://reddit.com/r/computervision/comments/1nue8ym/1m_retail_interior_images_multi_market_temporally/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ntgedn",
    "title": "The dumbest part of getting GPU compute is‚Ä¶",
    "content": "Seriously. I‚Äôve been losing sleep over this. I need compute for AI &amp; simulations, and every time I spin something up, it‚Äôs like a fresh boss fight:\n\n‚ÄúYour job is in queue‚Äù - cool, guess I‚Äôll check back in 3 hours  \nSpot instance disappeared mid-run - love that for me  \nDevOps guy says ‚ÄúJust configure Slurm‚Äù - yeah, let me Google that for the 50th time\n\nBill arrives - why am I being charged for a GPU I never used?  \nI feel like I‚Äôve tried every platform, and so far the three best have been Modal, Lyceum, and RunPod. They‚Äôre all great  but how is it that so many people are still on AWS/etc.?\n\nSo tell me, what‚Äôs the dumbest, most infuriating thing about getting HPC resources?",
    "author": "Connect_Gas4868",
    "timestamp": "2025-09-29T05:16:01",
    "url": "https://reddit.com/r/computervision/comments/1ntgedn/the_dumbest_part_of_getting_gpu_compute_is/",
    "score": 98,
    "num_comments": 20,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nu3qyf",
    "title": "3d object detection using CAD models in Unity",
    "content": "Does anyone know any open source software or SDK (non Vuforia,since it's too expensive) for detecting 3d objects given a CAD model file for that object. We are developing on Unity and currently the target device is iPad Pro. We can use ARKit 3d detection, however I am looking for ways to detect 3d object given its CAD model.\n\n",
    "author": "TypicalSeaweed5378",
    "timestamp": "2025-09-29T21:24:35",
    "url": "https://reddit.com/r/computervision/comments/1nu3qyf/3d_object_detection_using_cad_models_in_unity/",
    "score": 4,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nuiwgy",
    "title": "Emotion Dataset",
    "content": "I need to find video dataset  labeled with human emotions. Could you share the source?",
    "author": "Sea-Celebration2780",
    "timestamp": "2025-09-30T10:07:42",
    "url": "https://reddit.com/r/computervision/comments/1nuiwgy/emotion_dataset/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nu2j25",
    "title": "I need someone to review my profile and give me concrete steps to move further.",
    "content": "Pretty much the title. I need someone to review my profile and see what's needed to land a better job/organization/team.\n\nIn summary:\n\n1. I'm working professional with five years of industry experience, but I don't know what to do next. Currently working as CV engineer in a startup. Pretty much isolated from the rest of the CV world.\n2. I find myself constantly looking for interesting jobs but most interesting jobs either require a lot more experience or a higher degree (I don't have masters/PhD). Or at least that's what I found.\n3. I'm looking for interesting problems to work on, but also to make some money, so can't do open source all the time.\n4. I feel like \"I know nothing\" almost 99% of the time. And without guidance I don't think I will ever know anything. Because there's just a lot to this field and it feels overwhelming.\n5. Interesting problems for me: something related to geometry not just black box neural net training (although I do like it). Something which I've not done before. But tbh, I don't know where my interests are. I tend to like everything at first.\n\nHere's my profile: [GitHub](https://github.com/Arshad221b).\n\nBe brutally honest.",
    "author": "Amazing_Life_221",
    "timestamp": "2025-09-29T20:22:03",
    "url": "https://reddit.com/r/computervision/comments/1nu2j25/i_need_someone_to_review_my_profile_and_give_me/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ntmmgs",
    "title": "Last week in Multimodal AI - Vision Edition",
    "content": "I curate a weekly newsletter on multimodal AI, here are this week's vision highlights:\n\n**Veo3 Analysis From DeepMind - Video models learn to reason**\n\n* Spontaneously learned maze solving, symmetry recognition\n* Zero-shot object segmentation, edge detection\n* Emergent visual reasoning without explicit training\n* [Paper](https://arxiv.org/abs/2509.20328) | [Project Page](https://video-zero-shot.github.io/)\n\n**WorldExplorer - Fully navigable 3D from text**\n\n* Generates explorable 3D scenes that don't fall apart\n* Consistent quality across all viewpoints\n* Uses collision detection to prevent degenerate results\n* [Paper](https://arxiv.org/abs/2506.01799) | [Project](https://mschneider456.github.io/world-explorer/)\n\nhttps://reddit.com/link/1ntmmgs/video/pl3q59d5r4sf1/player\n\n**NVIDIA Lyra - 3D scenes without multi-view data**\n\n* Self-distillation from video diffusion models\n* Real-time 3D from text or single image\n* No expensive capture setups needed\n* [Paper](https://arxiv.org/abs/2509.19296) | [Project](https://research.nvidia.com/labs/toronto-ai/lyra/) | [GitHub](https://github.com/nv-tlabs/lyra)\n\nhttps://reddit.com/link/1ntmmgs/video/r6i6xrq6r4sf1/player\n\n**ByteDance Lynx - Personalized video**\n\n* Single photo to video with 0.779 face resemblance\n* Beats competitors (0.575-0.715)\n* [Project](https://byteaigc.github.io/Lynx/) | [GitHub](https://github.com/bytedance/lynx)\n\nhttps://reddit.com/link/1ntmmgs/video/u1ona3n7r4sf1/player\n\nAlso covered: HDMI robot learning from YouTube, OmniInsert maskless insertion, Hunyuan3D part-level generation\n\nhttps://reddit.com/link/1ntmmgs/video/gil7evpjr4sf1/player\n\nFree newsletter(demos,papers,more): [https://thelivingedge.substack.com/p/multimodal-monday-26-adaptive-retrieval](https://thelivingedge.substack.com/p/multimodal-monday-26-adaptive-retrieval)",
    "author": "Vast_Yak_4147",
    "timestamp": "2025-09-29T09:26:43",
    "url": "https://reddit.com/r/computervision/comments/1ntmmgs/last_week_in_multimodal_ai_vision_edition/",
    "score": 13,
    "num_comments": 2,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nuf3mb",
    "title": "Facial Spoofing Detector ‚úÖ/‚ùå",
    "content": "* üïπ Try out: [https://antal.ai/demo/spoofingdetector/demo.html](https://antal.ai/demo/spoofingdetector/demo.html)\n* üìñLearn more: [https://antal.ai/projects/face-anti-spoofing-detector.html](https://antal.ai/projects/face-anti-spoofing-detector.html)\n\nThis project can spots video presentation attacks to secure face authentication. I compiled the project to WebAssembly using Emscripten, so you can try it out on my website in your browser. If you like the project, you can purchase it from my website. The entire project is written in C++ and depends solely on the OpenCV library. If you purchase, you will receive the complete source code, the related neural networks, and detailed documentation. ",
    "author": "Gloomy_Recognition_4",
    "timestamp": "2025-09-30T07:43:58",
    "url": "https://reddit.com/r/computervision/comments/1nuf3mb/facial_spoofing_detector/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ntuzjx",
    "title": "[Project Update] TraceML ‚Äî Real-time PyTorch Memory Tracing",
    "content": "",
    "author": "traceml-ai",
    "timestamp": "2025-09-29T14:42:11",
    "url": "https://reddit.com/r/computervision/comments/1ntuzjx/project_update_traceml_realtime_pytorch_memory/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ntzw7s",
    "title": "Getting start with YOLO in general and YOLOv5 in specific",
    "content": "Hi all, I'm quite new to YOLO and I want to ask where should I start with YOLO.\nCould u recommend good starting points (books, papers, tutorials, or videos) that explain both the theory (anchors, loss functions, model structure) and the practical side (training on custom datasets, evaluation, deployment)?\nAny learning path, advice, or sources will be great.",
    "author": "Glass_Map5003",
    "timestamp": "2025-09-29T18:16:37",
    "url": "https://reddit.com/r/computervision/comments/1ntzw7s/getting_start_with_yolo_in_general_and_yolov5_in/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ntoggd",
    "title": "Lung CT datasets with segmentation annotations",
    "content": "I put together a GitHub repo that collects Lung CT datasets with segmentation annotations .  \nIt includes the popular ones (LIDC-IDRI, LUNA16, MSD) and also recent challenges like ATM‚Äô22, AeroPath‚Äô23, AIIB23 all in one place.\n\nThe idea is to save researchers/students some time and have a central hub that the community can expand.\n\n[https://github.com/noureddinekhiati/Awesome-Lung-CT-Datasets/tree/main](https://github.com/noureddinekhiati/Awesome-Lung-CT-Datasets/tree/main)",
    "author": "noureddinekhiati",
    "timestamp": "2025-09-29T10:34:49",
    "url": "https://reddit.com/r/computervision/comments/1ntoggd/lung_ct_datasets_with_segmentation_annotations/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ntm623",
    "title": "Help in people ReID from CCTV footage",
    "content": "Hey, redditors \nI am relativeky new to computer vision and currently working on a project that needs accurate ReID for people. \n\nWhat do you think is the most accurate way of doing that?\n\nEspecially for cases like the one in the video. I could make some progress on the video above by using cos similarity and tuning the threshold. But it is obviously not generalizable. \n\nSource: https://github.com/kevinlin311tw/ABODA/blob/master/video1.avi",
    "author": "kareem_fofo2005",
    "timestamp": "2025-09-29T09:09:22",
    "url": "https://reddit.com/r/computervision/comments/1ntm623/help_in_people_reid_from_cctv_footage/",
    "score": 1,
    "num_comments": 10,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ntluw9",
    "title": "Extracting overlaid text from videos",
    "content": "Hey everyone,\n\nI‚Äôm working on an offline system to extract overlaid text from videos (like captions/titles in fitness/tutorial clips with people moving in the background).\n\nWhat I‚Äôve tried so far\n\nFrame extraction ‚Üí text detection with EAST and DBNet50 ‚Üí OCR (Tesseract)\n\nResults: not very accurate, especially when text overlaps with complex backgrounds or uses stylized fonts\n\nMy main question\n\nShould I:\n\nKeep optimizing this traditional pipeline (better preprocessing, fine-tuned text detection + OCR models, etc.), or\n\nExplore a more modern multimodal/video-text model approach (e.g. Gemini) (e.g. what‚Äôs described here:¬†[https://www.sievedata.com/blog/video-ocr-guide](https://www.sievedata.com/blog/video-ocr-guide)¬†), even though it‚Äôs costlier?\n\nThe videos I‚Äôll process are very diverse (different fonts, colors, backgrounds). The system will run offline.\n\nCurious to hear your thoughts on which path is more promising for this type of problem",
    "author": "cabesahuevo",
    "timestamp": "2025-09-29T08:57:36",
    "url": "https://reddit.com/r/computervision/comments/1ntluw9/extracting_overlaid_text_from_videos/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nsyv0h",
    "title": "I built TagiFLY ‚Äì a lightweight open-source labeling tool for computer vision (feedback welcome!)",
    "content": "Hi everyone,\n\nMost annotation tools I‚Äôve used felt too heavy or cluttered for small projects. So I created¬†**TagiFLY**¬†‚Äì a lightweight, open-source labeling app focused only on what you need.\n\nüîπ¬†**What it does**\n\n* 6 annotation tools (box, polygon, point, line, mask, keypoints)\n* 4 export formats: JSON, YOLO, COCO, Pascal VOC\n* Light &amp; dark theme, keyboard shortcuts, multiple image formats (JPG, PNG)\n\nüîπ¬†**Why I built it**  \nI wanted a simple tool to create datasets for:\n\n* ü§ñ Training data for ML\n* üéØ Computer vision projects\n* üìä Research or personal experiments\n\nhttps://preview.redd.it/y28zj1mftyrf1.png?width=1748&amp;format=png&amp;auto=webp&amp;s=6aeb618cdfd4276c07c1e94c79763a4074ff8334\n\n\n\n[Export Window](https://preview.redd.it/u54mf25styrf1.png?width=1428&amp;format=png&amp;auto=webp&amp;s=80565f9207c2c140e8e1a305048c50d02e017c1b)\n\n\n\nüîπ¬†**Demo &amp; Code**  \nüëâ GitHub repo:¬†[https://github.com/dvtlab/tagiFLY](https://github.com/dvtlab/tagiFLY)  \n\n\n‚ö†Ô∏è It‚Äôs still in¬†**beta**¬†‚Äì so it may have bugs or missing features.  \nI‚Äôd love to hear your thoughts:\n\n* Which features do you think are most useful?\n* What would you like to see added in future versions?\n\nThanks a lot üöÄ",
    "author": "hello_wordx",
    "timestamp": "2025-09-28T13:33:11",
    "url": "https://reddit.com/r/computervision/comments/1nsyv0h/i_built_tagifly_a_lightweight_opensource_labeling/",
    "score": 29,
    "num_comments": 13,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ntdtap",
    "title": "Object detection using Raspberry pi 4 and camera module v3",
    "content": "How to use Raspberry pi 4 with raspberry pi camera module v3 for simple object detection (real time) with Roboflow for dataset. Anyone help i have no knowledge in setting up the os of raspberry pi, help!!! ",
    "author": "Low-Principle9222",
    "timestamp": "2025-09-29T02:50:21",
    "url": "https://reddit.com/r/computervision/comments/1ntdtap/object_detection_using_raspberry_pi_4_and_camera/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nswnoq",
    "title": "Seeking for teammate for soccerNet 2026",
    "content": "Is anyone interested to work together for soccerNet challenge 2026? This year they have bring a new challenge \n\nhttps://www.soccer-net.org/challenges/2026",
    "author": "return_my_name",
    "timestamp": "2025-09-28T12:05:32",
    "url": "https://reddit.com/r/computervision/comments/1nswnoq/seeking_for_teammate_for_soccernet_2026/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nsoihe",
    "title": "Still can't find a VLM that can count how many squats in a 90s video",
    "content": "For how far some tech has come, it's shocking how bad video understanding still is. I've been testing various models against various videos of myself exercising and they almost all perform poorly even when I'm making a concerted effort to have clean form that any human could easily understand.\n\nAI is 1000x better at Geo guesser than me but worse than a small  child when it comes to video (provided image alone isn't enough).\n\nThis area seems to be a bottle neck so would love to see it improved, I'm kinda shocked it's so bad considering how much it matters to e.g. self driving cars. But also just robotics in general, can a robot that can't count squats then reliably flip burgers?\n\nFWIW best result I got is 30 squats when I actually did 43, with Qwen's newest VLM, basically tied or did better than Gemini 2.5 pro in my testing, but a lot of that could be luck.",
    "author": "JoelMahon",
    "timestamp": "2025-09-28T06:35:58",
    "url": "https://reddit.com/r/computervision/comments/1nsoihe/still_cant_find_a_vlm_that_can_count_how_many/",
    "score": 11,
    "num_comments": 33,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nt0iqp",
    "title": "Identifying exterior door gaps in floor plan using cv2 and pytorch",
    "content": "I'm working on building a model that take an apartment floor plan and identifies walls, windows and the exterior door gap. Using cv2 with pytorch right now and have gotten it so it is pretty good at identifying the walls and windows, but struggles to identify the front door. (this is tricky because the door is often just a blank break in the exterior line. I need to calculate the width of the entrance door relative to the rest of the rest of the apartment so that I can estimate square footage of the interior space based on the assumed width of the door.  Currently making masks in CVAT to train, attached is an example (base image + mask + output) - door in light blue. Whenever i run it on a non training model it misses the entrance door. Has anyone done something similar or have an idea how I should approach this problem? I just started my journey learning this stuff so any advice would be great. Thanks!",
    "author": "Marble_Hill_Analytic",
    "timestamp": "2025-09-28T14:41:05",
    "url": "https://reddit.com/r/computervision/comments/1nt0iqp/identifying_exterior_door_gaps_in_floor_plan/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nsulph",
    "title": "seeking for teammates for the Kaggle competition ‚ÄúGreat Daxinzhuang Pottery Puzzle Challenge.",
    "content": "Hey everyone,\n\nI‚Äôm **noob** in **computer vision** but really excited to dive in and learn through the **Kaggle competition ‚ÄúGreat Daxinzhuang Pottery Puzzle Challenge.‚Äù** The goal is to reassemble 20,000+ ancient pottery fragments using AI ‚Äî basically turning broken shards into reconstructed vessels.\n\nI‚Äôm looking for teammates who have experience or interest in:\n\n* **Computer Vision basics** (OpenCV, contour detection, feature matching)\n* **Deep Learning / Metric Learning** (Siamese nets, CNNs, etc.)\n* **3D Reconstruction** (Open3D, mesh generation, point clouds)\n* Or anyone curious about archaeology + AI crossover\n\nI aim to get experience and win is not first goal. If you are interested let's team up",
    "author": "Single-Entertainer13",
    "timestamp": "2025-09-28T10:45:02",
    "url": "https://reddit.com/r/computervision/comments/1nsulph/seeking_for_teammates_for_the_kaggle_competition/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nsutbt",
    "title": "Facial Recognition and Tracking on Videos",
    "content": "Hello,\n\nI am learning computer vision and facial recognition. I want to track person‚Äôs movement in a recorded video using facial recognition. How can I do so? Any suggestions?\n\n[ I have been able to track movement through object detection and tracking - want to know how can I implement facial recognition on top of this tracking - thank you! ]\n",
    "author": "arafmustavi",
    "timestamp": "2025-09-28T10:53:37",
    "url": "https://reddit.com/r/computervision/comments/1nsutbt/facial_recognition_and_tracking_on_videos/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nske2b",
    "title": "DinoV3 based segmentation",
    "content": "Any good references for DinoV3 segmentation a bit more advanced than patch-level PCA or clustering?\nThanks!",
    "author": "Ok_Pie3284",
    "timestamp": "2025-09-28T02:53:47",
    "url": "https://reddit.com/r/computervision/comments/1nske2b/dinov3_based_segmentation/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nsaqxj",
    "title": "Your Opinion on a PhD Opportunity in Maritime Computer Vision",
    "content": "My professor (i am european) secured funding and offered me a PhD on computer vision / signal processing / sensor fusion in the maritime domain. I‚Äôd appreciate your take on the field‚Äôs potential‚Äîespecially where CV + multisensor fusion can make a real impact at sea.  \nOne concern : papers in this niche seem to get relatively few citations. Does that meaningfully affect career prospects or signal limited research impact?\n\nHe‚Äôs asked for my decision within a week.\n\nthanks",
    "author": "No-Cut2077",
    "timestamp": "2025-09-27T17:31:20",
    "url": "https://reddit.com/r/computervision/comments/1nsaqxj/your_opinion_on_a_phd_opportunity_in_maritime/",
    "score": 26,
    "num_comments": 11,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nstth3",
    "title": "Need Guidance: Embedded Systems in India &amp; Abroad ‚Äì Job Market, Pay &amp; Future",
    "content": "Hey everyone,\n\nI‚Äôm an ECE student exploring a career in¬†**Embedded Systems**. I‚Äôve been hearing mixed things about the field, especially in India. Some say the job market here is already¬†**saturated and low-paying**, which makes me a bit worried about long-term growth.\n\nI did some online research and found that adding¬†**TinyML (Machine Learning on Microcontrollers) and Edge AI**¬†to embedded systems is being considered the¬†*future of this field*. Apparently, companies are moving toward smarter, AI-enabled embedded devices, so it seems like the career path could shift in that direction.\n\nI‚Äôd love to get input from people already working in the industry (both in India and abroad):\n\n* How is the¬†**embedded systems job market**¬†right now in India vs other countries?\n* Is it true that salaries in India are¬†**quite low compared to the difficulty of the work**?\n* Do skills like¬†**TinyML and Edge AI**¬†really open better opportunities?\n* What‚Äôs the¬†**future scope**¬†of embedded systems if I commit to it for the next 5‚Äì10 years?\n* Would it be smarter to build my career in India first or try to move abroad early on?\n\nAny personal experiences, advice, or even roadmap suggestions would mean a lot üôè",
    "author": "Sea_Pirate_8477",
    "timestamp": "2025-09-28T10:14:33",
    "url": "https://reddit.com/r/computervision/comments/1nstth3/need_guidance_embedded_systems_in_india_abroad/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nt2ept",
    "title": "I need help!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",
    "content": "I want to build a model that can detect both objects and human bodies using YOLO models, then draw the relations between each person and the detected objects, and finally export the results to a CSV file.\n\nBut honestly, I feel a bit lost right now. Could someone please give me a clear roadmap on how to achieve this?",
    "author": "Low_Art_2216",
    "timestamp": "2025-09-28T16:04:15",
    "url": "https://reddit.com/r/computervision/comments/1nt2ept/i_need_help/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nsn2fe",
    "title": "Ocr",
    "content": "",
    "author": "swarley_0901",
    "timestamp": "2025-09-28T05:26:50",
    "url": "https://reddit.com/r/computervision/comments/1nsn2fe/ocr/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nsml0t",
    "title": "New to Computer Vision: Beginner project. Automating Sky and White Value Detection in Images",
    "content": "Hey everyone! I‚Äôm completely new to computer vision and programming. My interest started through my work as an artist, which eventually led me to discover the fascinating world of computer vision.\n\nA while back, I started a project where I manually calculated the color values of the sky and the white values in images using data from my camera. But since the number of pictures I deal with is huge, I‚Äôm now trying to figure out how to automate the process.\n\nI‚Äôve tried using OpenCV to create color masks, but that approach feels pretty limited (at least the way I‚Äôve managed to do it). Since you have to set values for the colors, other parts of the image that fall within the same range show up too, not just the sky.\n\nIs there a better way to mask out specific parts of an image (like only the sky) or to automatically calculate how warm/cold the white values are?\n\nSorry if this sounds super basic I‚Äôm just starting out and this is my first attempt at diving into computer vision.",
    "author": "Thermo_sifonas",
    "timestamp": "2025-09-28T05:02:21",
    "url": "https://reddit.com/r/computervision/comments/1nsml0t/new_to_computer_vision_beginner_project/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nrr50r",
    "title": "Object Tracking: A Comprehensive Survey From Classical Approaches to Large Vision-Language and Foundation Models",
    "content": "Found a a new survey + resource repo on¬†**object tracking**, spanning from¬†**classical Single Object Tracking (SOT)**¬†and¬†**Multi-Object Tracking (MOT)**¬†to the latest¬†**vision-language and foundation model based trackers**.\n\nüîó GitHub:¬†[Awesome-Object-Tracking](https://github.com/rahulrj/Awesome-Object-Tracking)\n\n‚ú® What makes this unique:\n\n* First survey to systematically cover¬†**VLMs &amp; foundation models**¬†in tracking.\n* Covers¬†**SOT, MOT, LTT**, benchmarks, datasets, and code links.\n* Organized for both researchers and practitioners.\n* Authored by researchers at Carnegie Mellon University¬†**(CMU)**¬†, Boston University and Mohamed bin Zayed University of Artificial Intelligence(**MBZUAI).**\n\nFeel free to ‚≠ê star and fork this repository to keep up with the latest advancements and contribute to the community.",
    "author": "Downtown_Ambition662",
    "timestamp": "2025-09-27T02:44:43",
    "url": "https://reddit.com/r/computervision/comments/1nrr50r/object_tracking_a_comprehensive_survey_from/",
    "score": 51,
    "num_comments": 2,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nrzi4k",
    "title": "What can we do nowÔºü",
    "content": "Hey everyone, we‚Äôre in the post-AI era now. The big models these days are really mature‚Äîthey can handle all sorts of tasks, like GPT and Gemini. But for grad students studying computer science, a lot of research feels pointless. ‚ÄòCause using those advanced big models can get great results, even better ones, in the same areas. \n\nI‚Äôm a grad student focusing on computer vision, so I wanna ask: are there any meaningful tasks left to do now? What are some tasks that are actually worth working on?",
    "author": "FrontWillingness39",
    "timestamp": "2025-09-27T09:27:05",
    "url": "https://reddit.com/r/computervision/comments/1nrzi4k/what_can_we_do_now/",
    "score": 12,
    "num_comments": 28,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nsfe07",
    "title": "Is this camera good for air hockey robot",
    "content": "OV4689 4MP 2K USB Camera Module for Face Recognition https://share.google/7SAHYHNwQvbRm6qVd\n\nMainly focusing on frame rate as quality doesn't matter much.\nIs this option viable or can I get something better for my use case.",
    "author": "Average_discord_guy",
    "timestamp": "2025-09-27T21:38:07",
    "url": "https://reddit.com/r/computervision/comments/1nsfe07/is_this_camera_good_for_air_hockey_robot/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ns3u6u",
    "title": "Want to build a project to detect unhealthy plants‚Äîlearn OpenCV first or dive into image processing?",
    "content": "Hey seniors,  \nI‚Äôm a 2nd-year undergrad and planning to make a hackathon project where I detect unhealthy plants using OpenCV and image processing. I‚Äôm good with C++ and C, and I know the basics of Python. Just a bit confused‚Äîshould I start with OpenCV first or directly learn image processing concepts?\n\nMy bigger goal is to get into ML + finance, so I‚Äôll have to dive into machine learning at some point anyway. I‚Äôm fine if it takes time, just want to start in the right direction and resources.",
    "author": "Bubbly_Ad5559",
    "timestamp": "2025-09-27T12:21:33",
    "url": "https://reddit.com/r/computervision/comments/1ns3u6u/want_to_build_a_project_to_detect_unhealthy/",
    "score": 5,
    "num_comments": 6,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nsng86",
    "title": "Seeking Guidance: Step-by-Step Roadmap to Advance in Computer Vision ‚Äì Is Multimodal/Agentic AI Essential?",
    "content": "Hi everyone!\n\nI‚Äôve been seriously exploring computer vision and have a solid foundation in CNN-based models and some experience with medical image segmentation. I‚Äôve also been learning about Vision Transformers and newer models like SAM, CLIP, DINOv2, etc.\n\nLately, I‚Äôve been hearing a lot about **multimodal AI** and **agentic AI**, and I‚Äôm curious:\n\n# üß† What I Want to Understand:\n\n1. Is it **necessary or strategic** to shift toward **multimodal or agentic AI** to stay relevant in the future of computer vision?\n2. What **algorithms/concepts** should I focus on *beyond CNNs and ViTs*?\n3. Could anyone recommend a **step-by-step learning roadmap** (from fundamentals to state-of-the-art) for someone wanting to become excellent in computer vision?\n4. What would be the **ideal learning pipeline** (courses, topics, projects) to follow in 2025‚Äì2026?\n\nThanks in advance!",
    "author": "tasnimjahan",
    "timestamp": "2025-09-28T05:45:56",
    "url": "https://reddit.com/r/computervision/comments/1nsng86/seeking_guidance_stepbystep_roadmap_to_advance_in/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nsej8k",
    "title": "Need guidance in my final year project",
    "content": "",
    "author": "barryallenx16",
    "timestamp": "2025-09-27T20:50:24",
    "url": "https://reddit.com/r/computervision/comments/1nsej8k/need_guidance_in_my_final_year_project/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nsem5v",
    "title": "Need help in my final year project",
    "content": "I am trying to build a AI based outfit recommendation system app as my final year project. Where users upload there clothes and ai works in-house to suggest outfits from their existing clothes. My projects value proposition, I am focusing on Indian ethnic wear . I am currently in the stage of data collecting for model creation . And I have doubt if I am going on the right path or not. \nThis is how I am collecting data :\n- I have created a website where users can swipe right or left to approve or reject randomly shown outfit pieces. Like in the tinder app. I have attached the photo too. The images are ai generated. \n- the dresses are shuffled using fisher yates shuffle algorithm.\n- I am only storing info about them like top red shirt , bottom black jeans, gender male , with created timestamp, status like approve or reject . In supabase \n- I have attached the image showing the the clothes I currently have in the website right now . Both for male and female.\n\nNow I will come to the doubts and questions I have . \n- I thought I could just fintune a model . now I am just confused on what and how to do it. \n- I also need to integrate other features like weather based recommendation like wear this as it is sunny or this as it is rainy . \n- I also have to recommend for the occasion. Like for college wear this. According to their daily commute. Atleast that's the vague idea I have . That is what I proposed. \n- there is Polyvore Dataset but I don't know how to train a model with it . I thought I can create a base model with this and then add indian ethnic outfits later.  \n- I don't know anyother dataset for my project. Is there is any . Please do tell \n- my teacher has told me that I need to create a bitmoji like feature when showing the outfit recommendation. I don't know how . Also I don't how possible it will be when I can going to the outfits are created from users existing clothes.\n- all this has to happen inhouse. Atleast that's what I wish for. Due to privacy concerns.\n\nCorrect me and guide me in all ways possible. I am entrusting everything to the people of reddit.\n",
    "author": "barryallenx16",
    "timestamp": "2025-09-27T20:54:57",
    "url": "https://reddit.com/r/computervision/comments/1nsem5v/need_help_in_my_final_year_project/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nrknj5",
    "title": "What slows you down most when reproducing ML research repos?",
    "content": "I have been working as a freelance computer vision engineer for past couple years . When I try to get new papers running, I often find little things that cost me hours ‚Äî missing hyperparams, preprocessing steps buried in the code, or undocumented configs.\n\nFor those who do this regularly:\n\n* **what‚Äôs the biggest time sink in your workflow?**\n* **how do you usually track fixes (personal notes, Slack, GitHub issues, spreadsheets)?**\n* **do you have a process for deciding if a repo is ‚Äúready‚Äù to use in production?**\n\nI‚Äôd love to learn how others handle this, since I imagine teams and solo engineers approach it very differently.",
    "author": "AsadShibli",
    "timestamp": "2025-09-26T20:10:50",
    "url": "https://reddit.com/r/computervision/comments/1nrknj5/what_slows_you_down_most_when_reproducing_ml/",
    "score": 22,
    "num_comments": 7,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nrzt2r",
    "title": "OBC online Computer Vision MSc",
    "content": "Does anyone have experience with the online MSc in Computer Vision offered by Universitat Oberta de Catalunya? I'm looking for an online MSc at the moment and I'm interesting in anything that is related to robotics. I have a BSc in Computer Science, so this MSc seems like a good fit in terms of courseware. \n\nI'm wondering though if anyone has actual experience with it and can share whether they find it worth it. ",
    "author": "myndrift",
    "timestamp": "2025-09-27T09:39:18",
    "url": "https://reddit.com/r/computervision/comments/1nrzt2r/obc_online_computer_vision_msc/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nry102",
    "title": "Do remote CV jobs for Africans really exist or l'm just wasting my time searching?",
    "content": "",
    "author": "Adventurous_Being747",
    "timestamp": "2025-09-27T08:26:19",
    "url": "https://reddit.com/r/computervision/comments/1nry102/do_remote_cv_jobs_for_africans_really_exist_or_lm/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nrvgeg",
    "title": "Voice assist for FastVLM",
    "content": "Requesting some feedback please!",
    "author": "TextDeep",
    "timestamp": "2025-09-27T06:37:13",
    "url": "https://reddit.com/r/computervision/comments/1nrvgeg/voice_assist_for_fastvlm/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nrq2x1",
    "title": "Image reconstruction",
    "content": "Hello, first time publishing. I would like your expertise on something. My work consists of dividing the image into blocks, process them then reassemble them. However, blocks after processing thend to have different values by the extermeties thus my blocks are not compatible. How can I get rid of this problem? Any suggestions? ",
    "author": "Deathfighter2017",
    "timestamp": "2025-09-27T01:35:48",
    "url": "https://reddit.com/r/computervision/comments/1nrq2x1/image_reconstruction/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nrn7rt",
    "title": "Who have taken vizuara course on vision transformer? The pro version please dm",
    "content": "",
    "author": "Frosty-Career1086",
    "timestamp": "2025-09-26T22:35:31",
    "url": "https://reddit.com/r/computervision/comments/1nrn7rt/who_have_taken_vizuara_course_on_vision/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nrur09",
    "title": "üî• YOLO26 is coming soon",
    "content": "YOLO26 introduces major improvements‚Äîit‚Äôs designed for edge and low-power devices, features a NMS-free end-to-end architecture for faster inference, and brings the new MuSGD optimizer for more stable, efficient training. Performance is especially strong for small object detection and real-time tasks like robotics and manufacturing.  ",
    "author": "yourfaruk",
    "timestamp": "2025-09-27T06:05:24",
    "url": "https://reddit.com/r/computervision/comments/1nrur09/yolo26_is_coming_soon/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nr4iko",
    "title": "I think Google lens has finally supported Sanskrit i have tried it before like 2 or 3 years ago or was not as good as it is now",
    "content": "",
    "author": "Business-Bottle-8283",
    "timestamp": "2025-09-26T08:34:11",
    "url": "https://reddit.com/r/computervision/comments/1nr4iko/i_think_google_lens_has_finally_supported/",
    "score": 7,
    "num_comments": 3,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nq4rb0",
    "title": "YOLO Model Announced at YOLO Vision 2025",
    "content": "",
    "author": "Ultralytics_Burhan",
    "timestamp": "2025-09-25T04:57:10",
    "url": "https://reddit.com/r/computervision/comments/1nq4rb0/yolo_model_announced_at_yolo_vision_2025/",
    "score": 290,
    "num_comments": 59,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nr7lku",
    "title": "Measuring Segmented Objects",
    "content": "I have a Yolo model that does object segmentation. I want to take the mask of these objects and calculate the height and diameter (it's a model that finds the stem of some plant seedlings). The problem is that each time the mask comes out differently for the same object... so if the seedling is passed through the camera twice, it generates different results (which obviously breaks the accuracy of my project). I'm not sure if Yolo is the best option or if the camera is the most suitable. Any help? I'm kind of at a loss for what to do, or where to look.\n\nhttps://preview.redd.it/8ubyeocvv4sf1.jpg?width=2592&amp;format=pjpg&amp;auto=webp&amp;s=b4bd6898b2700e77bf1fd7033ae738b27e0a7e5f\n\nhttps://preview.redd.it/btdmw4evv4sf1.jpg?width=26&amp;format=pjpg&amp;auto=webp&amp;s=6d8df3ee7415af57e20df9ed3aff4d6dd333693e\n\n\\* EDIT: I've added an image of the mask that is being detected by YOLO, as well as an example of the seedling reading. I created this colored division on the conveyors, but YOLO is run on the clean frame.",
    "author": "Easy_Ad_7888",
    "timestamp": "2025-09-26T10:32:42",
    "url": "https://reddit.com/r/computervision/comments/1nr7lku/measuring_segmented_objects/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nqrea9",
    "title": "Mobile App Size Reality Check: Multiple YOLOv8 Models + TFLite for Offline Use",
    "content": "Hi everyone,\n\nI'm in the planning stages of a mobile application (targeting Android first, then iOS) and I'm trying to get a reality check on the final APK size before I get too deep into development. My goal is to keep the total application size¬†**under 150 MB**.\n\n**The Core Functionality:**  \nThe app needs to run several different detection tasks offline (e.g., body detection, specific object tracking, etc.). My plan is to use separate, pre-trained YOLOv8 models for each task, converted to¬†**TensorFlow Lite**¬†for on-device inference.\n\n**My Current Technical Assumptions:**\n\n* **Framework:**¬†TensorFlow Lite for offline inference.\n* **Models:**¬†I'll start with the smallest possible models (e.g., YOLOv8n-nano) for each task.\n* **Optimization:**¬†I plan to use post-training quantization (likely INT8) during the TFLite conversion to minimize model sizes.\n\n**My Size Estimate Breakdown:**\n\n* TFLite Runtime Library: \\~3-5 MB\n* App Code &amp; Basic UI: \\~10-15 MB\n* **Remaining Budget for Models: \\~130 MB**\n\n**My Specific Questions for the Community:**\n\n1. **Is my overall approach sound?**¬†Does using multiple, specialized TFLite models seem like the right way to handle multiple detection types offline?\n2. **Model Size Experience:**¬†For those who've deployed YOLOv8n/s as TFLite models, what final file sizes are you seeing after quantization? (e.g., Is a quantized YOLOv8n for a single class around \\~2-3 MB?).\n3. **Hidden Overheads:**¬†Are there any significant size overheads I might be missing? For example, does using the TFLite GPU delegate add considerable size? Or are there large native libraries for image pre-processing I should account for?\n4. **Optimization Tips:**¬†Beyond basic quantization, are there other TFLite conversion tricks or model pruning techniques specific to YOLO that can shave off crucial megabytes without killing accuracy?\n\nI'm especially interested in hearing from anyone who has actually shipped an app with a similar multi-model, offline detection setup. Thanks in advance for any insights‚Äîit will really help me validate the project's feasibility!",
    "author": "SoilProper4327",
    "timestamp": "2025-09-25T20:54:10",
    "url": "https://reddit.com/r/computervision/comments/1nqrea9/mobile_app_size_reality_check_multiple_yolov8/",
    "score": 12,
    "num_comments": 6,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nqzr2g",
    "title": "Models keep overfitting despite using regularization e.t.c",
    "content": "I have tried data augmentation, regularization, penalty loss, normalization, dropout, learning rate schedulers, etc., but my models still tend to overfit. Sometimes I get good results in the very first epoch, but then the performance keeps dropping afterward. In longer trainings (e.g., 200 epochs), the best validation loss only appears in 2‚Äì3 epochs.\n\nI encounter this problem not only with one specific setup but also across different datasets, different loss functions, and different model architectures. It feels like a persistent issue rather than a case-specific one.\n\nWhere might I be making a mistake?",
    "author": "Swimming-Ad2908",
    "timestamp": "2025-09-26T05:15:01",
    "url": "https://reddit.com/r/computervision/comments/1nqzr2g/models_keep_overfitting_despite_using/",
    "score": 3,
    "num_comments": 20,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nqz4wy",
    "title": "Anyone here who worked on shuttleset?",
    "content": "Hey folks I need .pkl files of shuttleset but they are not mentioned in the original dataset paper. \nHas anyone worked on shuttleset. ?",
    "author": "Nothing769",
    "timestamp": "2025-09-26T04:44:50",
    "url": "https://reddit.com/r/computervision/comments/1nqz4wy/anyone_here_who_worked_on_shuttleset/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nr1p34",
    "title": "YOLO specs help for a Project",
    "content": "Hello, Me and my group decided to go for a project where we will use cctv to scan employees if they wear ppe or not through an entrance. Now we will use YOLO, but i wanna ask what is the proper correct specs we should plan to buy? we are open to optimization and use the best minimum just enough to detect if a person is wearing this PPE or not. ",
    "author": "NoSleepMan69",
    "timestamp": "2025-09-26T06:42:25",
    "url": "https://reddit.com/r/computervision/comments/1nr1p34/yolo_specs_help_for_a_project/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nqgy0w",
    "title": "üöÄ Automating Abandoned Object Detection Alerts with n8n + WhatsApp ‚Äì Version 3.0 üöÄ",
    "content": "üö® **No More Manual CCTV Monitoring!** üö®\n\nI‚Äôve built a **fully automated abandoned object detection system** using **YOLOv11 + ByteTrack**, seamlessly integrated with **n8n** and **Twilio WhatsApp API**.\n\nKey highlights of Version 3.0:  \n‚úÖ **Real-time detection** of abandoned objects in video streams.  \n‚úÖ **Instant WhatsApp notifications** ‚Äî no human monitoring required.  \n‚úÖ **Detected frames saved to Google Drive** for demo or record-keeping purposes.  \n‚úÖ **n8n workflow** connects Google Colab detection to Twilio for automated alerts.  \n‚úÖ Alerts include optional **image snapshots** to see exactly what was detected.\n\nThis pipeline demonstrates how **AI + automation** can make public spaces, offices, and retail safer while reducing human overhead.\n\nüí° Imagine deploying this in airports, malls, or offices ‚Äî instantly notifying staff when a suspicious object is left unattended.\n\n\\#Automation #AI #MachineLearning #ObjectDetection #YOLOv11 #n8n #Twilio #WhatsAppAPI #SmartSecurity #RealTimeAlerts",
    "author": "DaaniDev",
    "timestamp": "2025-09-25T13:00:03",
    "url": "https://reddit.com/r/computervision/comments/1nqgy0w/automating_abandoned_object_detection_alerts_with/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nqeuyj",
    "title": "Is Object Detection with Frozen DinoV3 with YOLO head possible?",
    "content": "In the DinoV3 paper they're using PlainDETR to perform object detection. They extract 4 levels of features from the dino backbone and feed it to the transformer to generate detections. \n\nI'm wondering if the same idea could be applied to a YOLO style head with FPNs. After all, the 4 levels of features would be similar to FPN inputs. Maybe I'd need to downsample the downstream features?",
    "author": "Lethandralis",
    "timestamp": "2025-09-25T11:39:38",
    "url": "https://reddit.com/r/computervision/comments/1nqeuyj/is_object_detection_with_frozen_dinov3_with_yolo/",
    "score": 5,
    "num_comments": 5,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nqhu8l",
    "title": "Mosquitto vs ZeroMQ: Send Android to Server real-time video frame streaming, 10 FPS",
    "content": "",
    "author": "Early_Ad4023",
    "timestamp": "2025-09-25T13:34:18",
    "url": "https://reddit.com/r/computervision/comments/1nqhu8l/mosquitto_vs_zeromq_send_android_to_server/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nqnc8c",
    "title": "Background Replacement Using BiRefNet",
    "content": "Background Replacement Using BiRefNet\n\n[https://debuggercafe.com/background-replacement-using-birefnet/](https://debuggercafe.com/background-replacement-using-birefnet/)\n\nIn this article, we will create a simple¬†***background replacement application using BiRefNet***.\n\nhttps://preview.redd.it/sf8e7rhylerf1.png?width=768&amp;format=png&amp;auto=webp&amp;s=a0c222b0c1d261ebb2508dd00664bd06ac3d91ae\n\n",
    "author": "sovit-123",
    "timestamp": "2025-09-25T17:31:07",
    "url": "https://reddit.com/r/computervision/comments/1nqnc8c/background_replacement_using_birefnet/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nqlglm",
    "title": "Using Rust to run the most powerful AI models for Camera Trap processing",
    "content": "",
    "author": "PatagonianCowboy",
    "timestamp": "2025-09-25T16:04:36",
    "url": "https://reddit.com/r/computervision/comments/1nqlglm/using_rust_to_run_the_most_powerful_ai_models_for/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1npmw68",
    "title": "I made a Morse code translator that uses facial gestures as input; It is my first computer vision project",
    "content": "Hey guys, I have been a silent enjoyer of this subreddit for a while; and thanks to some of the awesome posts on here; creating something with computer vision has been on my bucket list and so as soon as I started wondering about how hard it would be to blink in Morse Code; I decided to start my computer vision coding adventure.\n\nBuilding this took a lot of work; mostly to figure out how to detect blinks vs long blinks, nods and head turns. However, I had soo much fun building it. To be honest it has been a while since I had that much fun coding anything!\n\nI made a video showing how I made this if you would like to watch it:  \n[https://youtu.be/LB8nHcPoW-g](https://youtu.be/LB8nHcPoW-g)\n\nI can't wait to hear your thoughts and any suggestions you have for me!",
    "author": "Piko8Blue",
    "timestamp": "2025-09-24T13:20:35",
    "url": "https://reddit.com/r/computervision/comments/1npmw68/i_made_a_morse_code_translator_that_uses_facial/",
    "score": 90,
    "num_comments": 6,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nq6wu4",
    "title": "Looking for referrals/opportunities in AI/ML research roles (diffusion, segmentation, multimodal",
    "content": "Hey everyone,\n\nI‚Äôm a Master‚Äôs student in CS from a Tier-1 institute in India. While our campus placements are quite strong, they are primarily geared towards software development/engineering roles. My career interests, however, are more aligned with **AI/ML research**, so I‚Äôm looking for advice and possible referrals for opportunities that better match my background.\n\n**A bit about me:**\n\n* Bachelor‚Äôs in Electronics and Communication Engineering, now pursuing Master‚Äôs in CS.\n* \\~2 years of experience working on **deep learning, computer vision, and generative models** in academia.\n* Research spans **medical image segmentation, diffusion models, and multimodal learning**.\n* Implemented and analyzed architectures like **U-Net, ResNets, Faster R-CNN, Vision Transformers, CLIP, and diffusion models**.\n* Led multiple projects end-to-end: designing novel model variants, running experiments, and writing up work for publication.\n* Currently have papers **under review at top-tier venues** (as main author), awaiting decisions.\n\nI‚Äôm particularly interested in teams/roles that involve:\n\n* Applied research in **computer vision, generative modeling, or multimodal learning**\n* Opportunities to collaborate on **diffusion, foundation models, or segmentation problems**\n* Labs or companies that value research contributions and allow publishing.\n\nI‚Äôd really appreciate:\n\n1. **Referrals** to companies/labs/startups that are hiring in this space\n2. **Suggestions** for companies (both big tech and smaller research-focused startups) that I should target\n3. **Guidance** from people who have taken a similar path (moving from academia in India into ML research roles either in India or internationally).",
    "author": "Ok-Employ-4957",
    "timestamp": "2025-09-25T06:33:43",
    "url": "https://reddit.com/r/computervision/comments/1nq6wu4/looking_for_referralsopportunities_in_aiml/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nq943u",
    "title": "Suggestions for detecting atypical neurons in microscopic images",
    "content": "Hi everyone,\n\nI‚Äôm working on a project and my dataset consists of high-resolution microscopic images of neurons (average resolution ~2560x1920). Each image contains numerous neurons, and I have bounding box annotations (from Labelbox) for atypical neurons (those with abnormal morphology). The dataset has around 595 images.\n\nA previous study on the same dataset applied Faster R-CNN and achieved very strong results (90%+ accuracy). For my project, I need to compare alternative models (detection-based CNNs or other approaches) to see how they perform on this task. I would really like to achieve 90% accuracy too.\n\nI‚Äôve tried setting up some architectures (EfficientDet, YOLO, etc.), but I‚Äôm running into implementation issues and would love suggestions from the community.\n\nüëâ Which architectures or techniques would you recommend for detecting these atypical neurons?\nüëâ Any tips for handling large, high-resolution images with many objects per image?\nüëâ Are there references or example projects (preferably with code) that might be close to my problem domain?\n\nAny pointers would be super helpful. Thanks!\n",
    "author": "Drakkarys_",
    "timestamp": "2025-09-25T08:01:12",
    "url": "https://reddit.com/r/computervision/comments/1nq943u/suggestions_for_detecting_atypical_neurons_in/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nq3wsi",
    "title": "Recommendation for state of the art zero shot object detection model with fine-tuning and ONNX export?",
    "content": "Hey all,\n\n  \nfor a project where I have very small amount of training images (between 30 and 180 depending on use case) I am looking for a state of the art zero shot object detection model with fine-tuning and ONNX export. \n\nSo far I have experimented with a few and the out of the box performance without any training was bad to okayish so I want to try to fine-tune them on the data I have. Also I will probably have more data in the future but not thousands of images unfortunately. \n\nI know some models also include segmentation but I just need the detected objects, doesn't matter if bounding box or boundaries.\n\nHere are my findings:\n\n* YOLOE \n   * initial results were okayish\n   * fine-tuning works but was a little tricky to set up (https://docs.ultralytics.com/models/yoloe/#fine-tuning-on-custom-dataset) \n      * IIRC to get it to work I needed to include 80 classes in the dataset.yaml even though only trained on a few (I think because it was trained on 80 classes and expects this for the dataset.yaml somehow)\n      * ability to choose how many layers to freeze during fine-tuning\n   * ONNX export is included out of the box\n* OWLViT/OWLv2\n   * best out of the box performance\n   * no official fine-tuning code but few GitHub issues exist addressing this with one possible code example:\n      * code: [https://github.com/stevebottos/owl-vit-object-detection](https://github.com/stevebottos/owl-vit-object-detection) (haven't tried yet)\n      * [https://github.com/huggingface/transformers/issues/20091](https://github.com/huggingface/transformers/issues/20091)\n      * [https://github.com/huggingface/transformers/issues/28778](https://github.com/huggingface/transformers/issues/28778)\n      * [https://github.com/huggingface/transformers/issues/33664](https://github.com/huggingface/transformers/issues/33664)\n   * ONNX models available on huggingface but not sure if fine-tuned models could also be easily exported as ONNX (https://github.com/huggingface/optimum/issues/1713)\n* Grounding Dino\n   * initial results were okayish but it's comparatively slow\n   * fine-tuning via mmdetection (https://github.com/IDEA-Research/GroundingDINO/issues/228)\n   * ONNX export might be supported by mmdetection but apart from that only found a drive link in GitHub comments (https://github.com/IDEA-Research/GroundingDINO/issues/156)\n* DETIC\n   * initial results were okayish \n   * have not found a way yet to fine-tune\n   * ONNX export via long script here: [https://github.com/facebookresearch/Detic/issues/113](https://github.com/facebookresearch/Detic/issues/113)\n\n  \nRecently, I looked a little bit at DINOv3¬†but so far couldn't get it to run for object detection and have no idea about ONNX export and fine-tuning. Just read that it is supposed to have really good performance.\n\n  \nAre there any other models you know of that fulfill my criteria (zero shot object detection + fine-tuning + ONNX export) and you would recommend trying?\n\n  \nThank you :)",
    "author": "R1P4",
    "timestamp": "2025-09-25T04:12:21",
    "url": "https://reddit.com/r/computervision/comments/1nq3wsi/recommendation_for_state_of_the_art_zero_shot/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1npeogt",
    "title": "Alternative to NAS: A New Approach for Finding Neural Network Architectures",
    "content": "Over the past two years, we have been working at One Ware on a project that provides an alternative to classical Neural Architecture Search. So far, it has shown the best results for image classification and object detection tasks with one or multiple images as input.\n\nThe idea: Instead of testing thousands of architectures, the existing dataset is analyzed (for example, image sizes, object types, or hardware constraints), and from this analysis, a suitable network architecture is predicted.\n\nCurrently, foundation models like YOLO or ResNet are often used and then fine-tuned with NAS. However, for many specific use cases with tailored datasets, these models are vastly oversized from an information-theoretic perspective. Unless the network is allowed to learn irrelevant information, which harms both inference efficiency and speed. Furthermore, there are architectural elements such as Siamese networks or the support for multiple sub-models that NAS typically cannot support. The more specific the task, the harder it becomes to find a suitable universal model.\n\n**How our method works**  \nOur approach combines two steps. First, the dataset and application context are automatically analyzed. For example, the number of images, typical object sizes, or the required FPS on the target hardware. This analysis is then linked with knowledge from existing research and already optimized neural networks. The result is a prediction of which architectural elements make sense: for instance, how deep the network should be or whether specific structural elements are needed. A suitable model is then generated and trained, learning only the relevant structures and information. This leads to much faster and more efficient networks with less overfitting.\n\n**First results**  \nIn our first whitepaper, our neural network was able to improve accuracy from 88% to 99.5% by reducing overfitting. At the same time, inference speed increased by several factors, making it possible to deploy the model on a small FPGA instead of requiring an NVIDIA GPU. If you already have a dataset for a specific application, you can test our solution yourself and in many cases you should see significant improvements in a very short time. The model generation is done in 0.7 seconds and further optimization is not needed.",
    "author": "leonbeier",
    "timestamp": "2025-09-24T08:08:34",
    "url": "https://reddit.com/r/computervision/comments/1npeogt/alternative_to_nas_a_new_approach_for_finding/",
    "score": 64,
    "num_comments": 17,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nqknnq",
    "title": "How to change design of 3500 images fast,easy and extremely accurate?",
    "content": "How to change the design of 3500 football training exercise images, fast, easily, and extremely accurately? It's not necessary to be 3500 at once; 50 by 50 is totally fine as well, but only if it's extremely accurate.\n\nI was thinking of using the OpenAI API in my custom project and with a prompt to modify a large number of exercises at once (from .png to create a new .png with the Image creator), but the problem is that ChatGPT 5's vision capabilities and image generation were not accurate enough. It was always missing some of the balls, lines, and arrows; some of the arrows were not accurate enough. For example, when I ask ChatGPT to explain how many balls there are in an exercise image and to make it in JSON, instead of hitting the correct number, 22, it hits 5-10 instead, which is pretty terrible if I want perfect or almost perfect results. Seems like it's bad at counting.\n\nGuys how to change design of 3500 images fast,easy and extremely accurate?\n\nhttps://preview.redd.it/el7e42u6eerf1.png?width=2351&amp;format=png&amp;auto=webp&amp;s=f9bb879018de364654ded3aa4ed68a865a8444a4\n\nThat's what OpenAI image generator generated. On the left side is the generated image and on the right side is the original:  \n",
    "author": "Real_Investment_3726",
    "timestamp": "2025-09-25T15:29:35",
    "url": "https://reddit.com/r/computervision/comments/1nqknnq/how_to_change_design_of_3500_images_fasteasy_and/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nq3rx9",
    "title": "Drawing person orientation from pose estimation",
    "content": "So I have a bunch of videos from overhead cameras in a store and I'm trying to determine in which direction is the person looking. I'm currently using yolopose to get the pose keypoints but I'm struggling to get the person orientation. This is my current method: I run a pose model on each frame and grab the torso joints, primarily the shoulders, with hips or knees as backups. From those points I compute the torso‚Äôs left‚Äëto‚Äëright axis, take its perpendicular to get a facing direction, and smooth that vector over time so sudden keypoint jitter doesn‚Äôt flip the arrow. This works ookayish, sometimes it's correct and sometimes is completely wrong. Has anyone done anything similar and do you have any advice? Any help is welcome. ",
    "author": "Doodle_98",
    "timestamp": "2025-09-25T04:04:49",
    "url": "https://reddit.com/r/computervision/comments/1nq3rx9/drawing_person_orientation_from_pose_estimation/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nqhjg5",
    "title": "How to change design of 3500 images fast,easy and extremely accurate?",
    "content": "Hi, I have 3500 football training exercise images, and I'm looking for a tool/AI tool that's going to be able to create a new design of those 3500 images fast, easily, and extremely accurately. It's not necessary to be 3500 at once; 50 by 50 is totally fine as well, but only if it's extremely accurate.\n\nI was thinking of using the OpenAI API in my custom project and with a prompt to modify a large number of exercises at once (from .png to create a new .png with the Image creator), but the problem is that ChatGPT 5's vision capabilities and image generation were not accurate enough. It was always missing some of the balls, lines, and arrows; some of the arrows were not accurate enough. For example, when I ask ChatGPT to explain how many balls there are in an exercise image and to make it in JSON, instead of hitting the correct number, 22, it hits 5-10 instead, which is pretty terrible if I want perfect or almost perfect results. Seems like it's bad at counting.\n\nGuys do you have any suggestion how to change the design of 3500 images fast,easy and extremely accurate?\n\nFrom the left is from OpenAI image generation and from the right is the original. As you can see some arrows are wrong,some figures are missing and better prompt can't really fix that. Maybe it's just a bad vision/image generation capabilities.\n\nhttps://preview.redd.it/8aq96xnuddrf1.png?width=2351&amp;format=png&amp;auto=webp&amp;s=ad89791b8d63c7dbfdff3a2c281fa6fc3ddbf726",
    "author": "Real_Investment_3726",
    "timestamp": "2025-09-25T13:23:05",
    "url": "https://reddit.com/r/computervision/comments/1nqhjg5/how_to_change_design_of_3500_images_fasteasy_and/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nqhexd",
    "title": "How to change design of 3500 images fast,easy and extremely accurate?",
    "content": "Hi, I have 3500 football training exercise images, and I'm looking for a tool/AI tool that's going to be able to create a new design of those 3500 images fast, easily, and extremely accurately. It's not necessary to be 3500 at once; 50 by 50 is totally fine as well, but only if it's extremely accurate.\n\nI was thinking of using the OpenAI API in my custom project and with a prompt to modify a large number of exercises at once (from .png to create a new .png with the Image creator), but the problem is that ChatGPT 5's vision capabilities and image generation were not accurate enough. It was always missing some of the balls, lines, and arrows; some of the arrows were not accurate enough. For example, when I ask ChatGPT to explain how many balls there are in an exercise image and to make it in JSON, instead of hitting the correct number, 22, it hits 5-10 instead, which is pretty terrible if I want perfect or almost perfect results. I tried AI to explain the image in json and the idea was to give that json to AI image generation model,but seems like Gemini and GPT are bad at counting with their Vision capabilities.\n\nGuys do you have any suggestion how to change the design of 3500 images fast,easy and extremely accurate?\n\nFrom the left is from OpenAI image generation and from the right is the original. As you can see some arrows are wrong,some figures are missing and better prompt can't really fix that. Maybe it's just a bad vision/image generation capabilities.\n\nhttps://preview.redd.it/ec5s640qbdrf1.png?width=2351&amp;format=png&amp;auto=webp&amp;s=ba462444c71333a7fc186c0800410db0adf0aba6",
    "author": "Real_Investment_3726",
    "timestamp": "2025-09-25T13:18:15",
    "url": "https://reddit.com/r/computervision/comments/1nqhexd/how_to_change_design_of_3500_images_fasteasy_and/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.11,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1np8rho",
    "title": "Kickup detection",
    "content": "My current implementation for the detection and counting breaks when the person starts getting more creative with their movements but I wanted to share the demo anyway.\n\nThis directly references work from another post in this sub a few weeks back [@Willing-Arugula3238]. (Not sure how to tag people)\n\nOriginal video is from @khreestyle on insta\n",
    "author": "Rurouni-dev-11",
    "timestamp": "2025-09-24T03:41:09",
    "url": "https://reddit.com/r/computervision/comments/1np8rho/kickup_detection/",
    "score": 61,
    "num_comments": 15,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nplcir",
    "title": "I built an open-source llm agent that controls your OS without computer vision",
    "content": "[github link](https://github.com/iBz-04/raya)¬†I looked into automations and built raya, an ai agent that lives in the GUI layer of the operating system, although its now at its basic form im looking forward to expanding its use cases\n\nthe¬†[github link](https://github.com/iBz-04/raya)¬†is attached",
    "author": "Ibz04",
    "timestamp": "2025-09-24T12:20:19",
    "url": "https://reddit.com/r/computervision/comments/1nplcir/i_built_an_opensource_llm_agent_that_controls/",
    "score": 11,
    "num_comments": 11,
    "upvote_ratio": 0.61,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1np8a34",
    "title": "Algorithmically how can I more accurately mask the areas containing text?",
    "content": "I am essentially trying to create a create a mask around areas that have some textual content. Currently this is how I am trying to achieve it:\n\n    import cv2\n    \n    def create_mask(filepath):\n      img    = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n      edges  = cv2.Canny(img, 100, 200)\n      kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,3))\n      dilate = cv2.dilate(edges, kernel, iterations=5)\n    \n      return dilate\n    \n    mask = create_mask(\"input.png\")\n    cv2.imwrite(\"output.png\", mask)\n\nEssentially I am converting the image to gray scale, Then performing canny edge detection on it, Then I am dilating the image.\n\nThe goal is to create a mask on a word-level, So that I can get the bounding box for each word &amp; Then feed it into an OCR system. I can't use AI/ML because this will be running on a powerful microcontroller but due to limited storage (64 MB) &amp; limited ram (upto 64 MB) I can't fit an EAST model or something similar on it.\n\nWhat are some other ways to achieve this more accurately? What are some preprocessing steps that I can do to reduce image noise? Is there maybe a paper I can read on the topic? Any other related resources?",
    "author": "FoundationOk3176",
    "timestamp": "2025-09-24T03:11:13",
    "url": "https://reddit.com/r/computervision/comments/1np8a34/algorithmically_how_can_i_more_accurately_mask/",
    "score": 37,
    "num_comments": 17,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1npkd5x",
    "title": "Drone-to-Satellite Image Matching for the Forest area",
    "content": "\nI am working on Drone-to-Satellite image matching process where I take the nadir view of drone image and try to match it with the Satellite view of the forest region. Due to repetitive patterns, dense area, my models aren't effective. I already tried Superpoint-lightglue as well as LoFTR, but the accuracy is still not enough. \n\nCan anyone suggest me some good approaches to go with?? ",
    "author": "[deleted]",
    "timestamp": "2025-09-24T11:43:26",
    "url": "https://reddit.com/r/computervision/comments/1npkd5x/dronetosatellite_image_matching_for_the_forest/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nplkqx",
    "title": "Symmetrical faces generated by Google Banana model - is there an academic justification?",
    "content": "I've noticed that AI generated faces by Gemini 2.5 Flash Image are often symmetrical and it's almost impossible to generate non symmetrical features. Is there any particular reason for that in the architecture / training in this or similar models or it's just correlation on a small sample that I've seen?",
    "author": "new_stuff_builder",
    "timestamp": "2025-09-24T12:29:21",
    "url": "https://reddit.com/r/computervision/comments/1nplkqx/symmetrical_faces_generated_by_google_banana/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1npji9j",
    "title": "Tips on Building My Own Dataset",
    "content": "I‚Äôm pretty new to Computer Vision, I‚Äôve seen YOLO mentioned a bunch and I think I have a basic understanding of how it works. From what I‚Äôve read, it seems like I can create my own dataset using pictures I take myself, then annotate and train YOLO on it.\n\nI'm having more trouble with the practical side of actually making my own dataset.\n\n* How many pictures would I need to get decent results? 100? 1000? 10000?\n* Is it better to have fewer pictures of many different scenarios, or more pictures of a few controlled setups?\n* Is there a better alternative than YOLO?\n\n",
    "author": "dontshitonmylaptop",
    "timestamp": "2025-09-24T11:11:04",
    "url": "https://reddit.com/r/computervision/comments/1npji9j/tips_on_building_my_own_dataset/",
    "score": 3,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1npl1jx",
    "title": "Instance Segmentation Models",
    "content": "Hey, I am working on a project where I need to get the count of one type of object from images. My idea is to train an instance segmentation model on a large data set of that object, then use that to get the count. I wanted to see if you guys have any advice on what SOTA is for Instance Segmentation Models. I was thinking of something where I could use Dino v3 as the backbone and then train an instance segmentation head on that would be good. Some that I was looking at are:   \n\\- MaskDINO  \n\\- DI-MaskDINO  \n\\- Mask2Former\n\nI know where others are also out there, like sam2.1 and RF-DETR. \n\nWould love any advice on this!",
    "author": "DrJurt",
    "timestamp": "2025-09-24T12:08:31",
    "url": "https://reddit.com/r/computervision/comments/1npl1jx/instance_segmentation_models/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1npdkbo",
    "title": "Alien vs Predator Image Classification with ResNet50 | Complete Tutorial [project]",
    "content": "https://preview.redd.it/zv5a85x7h4rf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=57b0d439825ad22c48a9dfa8fde84abdca1bfa18\n\nI just published a complete step-by-step guide on building an Alien vs Predator image classifier using ResNet50 with TensorFlow.\n\nResNet50 is one of the most powerful architectures in deep learning, thanks to its residual connections that solve the vanishing gradient problem.\n\nIn this tutorial, I explain everything from scratch, with code breakdowns and visualizations so you can follow along.\n\n¬†\n\nWatch the video tutorial here : [https://youtu.be/5SJAPmQy7xs](https://youtu.be/5SJAPmQy7xs)\n\n¬†\n\nRead the full post here: [https://eranfeit.net/alien-vs-predator-image-classification-with-resnet50-complete-tutorial/](https://eranfeit.net/alien-vs-predator-image-classification-with-resnet50-complete-tutorial/)\n\n¬†\n\nEnjoy\n\nEran",
    "author": "Feitgemel",
    "timestamp": "2025-09-24T07:25:48",
    "url": "https://reddit.com/r/computervision/comments/1npdkbo/alien_vs_predator_image_classification_with/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1npcsse",
    "title": "What's the best vision model for checking truck damage?",
    "content": "Hey all, I'm working at a shipping company and we're trying to set up an automated system.\n\nWe have a gate where trucks drive through slowly, and 8 wide-angle cameras are recording them from every angle. The goal is to automatically log every scratch, dent, or piece of damage as the truck passes.\n\nThe big challenge is the follow-up: when the same truck comes back, the system needs to ignore the old damage it already logged and only flag new damage.\n\nAny tips on models what can detect small things would be awesome.",
    "author": "jolvan_amigo",
    "timestamp": "2025-09-24T06:55:15",
    "url": "https://reddit.com/r/computervision/comments/1npcsse/whats_the_best_vision_model_for_checking_truck/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1npaddj",
    "title": "How to label multi part instance segmentation objects in Roboflow?",
    "content": "So I'm dealing with partially occluded objects in my dataset and I'd like to train my model to recognize all these disjointed parts as one instance. Examples of this could be electrical utility poles partially obstructed by trees.  \nBefore I switched to roboflow I used LabelStudio which had a neat relationship flag that I could use to tag these disjointed polygons and then later used a post processor script that converted these multi polygon annotations into single instances that a model like YOLO would understand.  \nAs far as I understand, roboflow doesn't really have any feature to connect these objects so I'd be stuck trying to manually connect them with thin connecting lines. That would also mean that I couldn't use the SAM2 integration which would really suck.",
    "author": "United_Highway2583",
    "timestamp": "2025-09-24T05:08:11",
    "url": "https://reddit.com/r/computervision/comments/1npaddj/how_to_label_multi_part_instance_segmentation/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1npf7pk",
    "title": "[Discussion] How client feedback shaped our video annotation timeline",
    "content": "We‚Äôre a small team based in Chandigarh, working on annotation tools, but always trying to think globally.\n\nLast week, a client asked us something simple but important:  \n\"I want to quickly jump to, add, and review keyframes on the video timeline without lag, just like scrubbing through YouTube\"\n\nWe sat down, re-thought the design, and ended up building a smoother timeline experience:\n\n* Visual keyframe pins with hover tooltips\n* Keyboard shortcuts (K to add, Del to delete)\n* Context menus for fast actions\n* Accessibility baked in (‚ÄúKeyframe at {timecode}‚Äù)\n* Performance tuned to handle thousands of pins smoothly\n\nWhat we have achieved? Now reviewing annotations feels seamless, and annotators can move much faster.\n\nFor us, the real win was seeing how **a small piece of feedback turned into a feature that feels globally relevant**.\n\nCurious to know:  \nüëâ How do you handle similar feedback loops in your own projects? Do you try to ship quickly, or wait for patterns before building?\n\nIf anyone‚Äôs working on video annotation and wants to test this kind of flow, happy to share more details about how we approached it.",
    "author": "Full_Piano_3448",
    "timestamp": "2025-09-24T08:28:48",
    "url": "https://reddit.com/r/computervision/comments/1npf7pk/discussion_how_client_feedback_shaped_our_video/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1np6hpd",
    "title": "Any useful computer vision events taking place this year in the UK?",
    "content": "...that aren't just money-making events for the organisers and speakers?",
    "author": "Mplus479",
    "timestamp": "2025-09-24T01:11:01",
    "url": "https://reddit.com/r/computervision/comments/1np6hpd/any_useful_computer_vision_events_taking_place/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nobmr3",
    "title": "Gaze vector estimation for driver monitoring system trained on 100% synthetic data",
    "content": "I‚Äôve built a real-time gaze estimation pipeline for driver distraction detection using entirely synthetic training data.\n\nI used a two-stage inference:  \n1. Face Detection: FastRCNNPredictor (torchvision) for facial ROI extraction  \n2. Gaze Estimation: L2CS implementation for 3D gaze vector regression\n\nApplications: driver attention monitoring, distraction detection, gaze-based UI",
    "author": "SKY_ENGINE_AI",
    "timestamp": "2025-09-23T01:12:10",
    "url": "https://reddit.com/r/computervision/comments/1nobmr3/gaze_vector_estimation_for_driver_monitoring/",
    "score": 221,
    "num_comments": 25,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1np7u52",
    "title": "How can I export custom Pytorch CUDA ops into ONNX and TensorRT?",
    "content": "I tried to solve this problem, but I was not able to find the documentation. ",
    "author": "Maximum-Bat-3722",
    "timestamp": "2025-09-24T02:43:30",
    "url": "https://reddit.com/r/computervision/comments/1np7u52/how_can_i_export_custom_pytorch_cuda_ops_into/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1npa4cv",
    "title": "Grad CAM class activation explained with Pytorch",
    "content": "**Link:-**   [https://youtu.be/lA39JpxTZxM](https://youtu.be/lA39JpxTZxM)\n\n[Class Activation Maps](https://preview.redd.it/r8835m7fq3rf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=30dea35e087e76fd850a7185f4120c15056b1847)",
    "author": "computervisionpro",
    "timestamp": "2025-09-24T04:56:13",
    "url": "https://reddit.com/r/computervision/comments/1npa4cv/grad_cam_class_activation_explained_with_pytorch/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1np9li2",
    "title": "Tesseract ocr+ auto hot key",
    "content": "Hey everyone,\nI‚Äôm new to OCR and AutoHotkey tools. I‚Äôve been using an AHK script along with the Capture2Text app to extract data and paste it into the right columns (basically for data entry).\n\nThe problem is that I‚Äôm running into accuracy issues with Capture2Text. I found out it‚Äôs actually using Tesseract OCR in the background, and I‚Äôve heard that Tesseract itself is what I should be using directly. The issue is, I have no idea how to properly run Tesseract. When I tried opening it, it only let me upload sample images, and the results came out inaccurate.\n\nSo my question is: how do I use Tesseract with AHK to reliably capture text with high accuracy? Is there a way to improve the results? Any advice from experts here would be really appreciated ..!",
    "author": "guywithlotofthings",
    "timestamp": "2025-09-24T04:28:09",
    "url": "https://reddit.com/r/computervision/comments/1np9li2/tesseract_ocr_auto_hot_key/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1np8tfx",
    "title": "GOT OCR 2.0 help",
    "content": "Hi All, would like some help from users who have used GOT OCR V2.0 before.\n\nI\\`m trying to extract text from an document and it was working fine (raw model).\n\nPre-process of the document which only indicate area of interest, which includes cropping and reducing the image size, lead to poor detection of the text running in GOT OCR --ocr mode. \n\nThe difference is quite big, is there something that I have missed out such as resizing requirements etc?",
    "author": "lofan92",
    "timestamp": "2025-09-24T03:44:22",
    "url": "https://reddit.com/r/computervision/comments/1np8tfx/got_ocr_20_help/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1np7r86",
    "title": "Help needed for MMI facial expression dataset",
    "content": "Dear colleagues in Vision research field, especially on facial expressions,\n\nThe MMI facial expression site is down ([http://mmifacedb.eu/](https://web.archive.org/web/2/http://mmifacedb.eu/), [http://www.mmifacedb.com/](http://www.mmifacedb.com/) ), Although I have EULA approval, no way to download dataset. Unfortunately, some data is crucial for finishing current project.\n\nAnybody downloaded it in somewhere of your HDD? Please would you help me?",
    "author": "i_psych",
    "timestamp": "2025-09-24T02:38:10",
    "url": "https://reddit.com/r/computervision/comments/1np7r86/help_needed_for_mmi_facial_expression_dataset/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nohhnb",
    "title": "Facial Expression Recognition üé≠",
    "content": "* üïπ Try out: [https://antal.ai/demo/facialexpressionrecognition/demo.html](https://antal.ai/demo/facialexpressionrecognition/demo.html)\n* üìñLearn more: [https://antal.ai/projects/facial-expression-recognition.html](https://antal.ai/projects/facial-expression-recognition.html)\n\nThis project can recognize facial expressions. I compiled the project to WebAssembly using Emscripten, so you can try it out on my website in your browser. If you like the project, you can purchase it from my website. The entire project is written in C++ and depends solely on the OpenCV library. If you purchase, you will receive the complete source code, the related neural networks, and detailed documentation. ",
    "author": "Gloomy_Recognition_4",
    "timestamp": "2025-09-23T06:31:58",
    "url": "https://reddit.com/r/computervision/comments/1nohhnb/facial_expression_recognition/",
    "score": 13,
    "num_comments": 2,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1no0w22",
    "title": "Homebrew Bird Buddy",
    "content": "The beginnings of my own bird spotter. CV applied to footage coming from my Blink cameras. ",
    "author": "bigjobbyx",
    "timestamp": "2025-09-22T15:40:54",
    "url": "https://reddit.com/r/computervision/comments/1no0w22/homebrew_bird_buddy/",
    "score": 109,
    "num_comments": 18,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1novd3t",
    "title": "Is it standard practice to create manual coco annotations within python? Or are there tools?",
    "content": "Most of the annotation tools for images I see are webuis. However I'm trying to do a custom annotation through python (for an algorithm I wrote). Is there a tool that's standard through python that I can register annotations through?",
    "author": "Affectionate_Use9936",
    "timestamp": "2025-09-23T15:24:35",
    "url": "https://reddit.com/r/computervision/comments/1novd3t/is_it_standard_practice_to_create_manual_coco/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1not66c",
    "title": "TEMAS + Jetson Orin Nano Super ‚Äî real-time person &amp; object tracking",
    "content": "hey folks ‚Äî tiny clip. Temas + jetson orin nano super. tracks people + objects at the same time in real time.\n\nwhat you‚Äôll see:\n\nmulti-object tracking\n\nlatency low enough to feel ‚Äúlive‚Äù on embedded\n\nhttps://youtube.com/shorts/IQmHPo1TKgE?si=vyIfLtWMVoewWvrg\n\nwhat would you optimize first here: stability, fps/latency, or robustness with messy backgrounds?\n\nany lightweight tricks you like for smoothing id switches on edge devices?\n\nthanks for watching!",
    "author": "Big-Mulberry4600",
    "timestamp": "2025-09-23T13:56:29",
    "url": "https://reddit.com/r/computervision/comments/1not66c/temas_jetson_orin_nano_super_realtime_person/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nospu1",
    "title": "[D] What‚Äôs your tech stack as researchers?",
    "content": "",
    "author": "Entrepreneur7962",
    "timestamp": "2025-09-23T13:38:55",
    "url": "https://reddit.com/r/computervision/comments/1nospu1/d_whats_your_tech_stack_as_researchers/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1noof8x",
    "title": "Follow-up on PSI (Probabilistic Structure Integration) - new video explainer",
    "content": "Hey all, I shared the PSI paper here a little while ago: [\"World Modeling with Probabilistic Structure Integration\"](https://arxiv.org/abs/2509.09737?utm_source=chatgpt.com).\n\nBeen thinking about it ever since, and today a video breakdown of the paper popped up in my feed - figured I‚Äôd share in case it‚Äôs helpful: [YouTube link](https://www.youtube.com/watch?v=YEHxRnkSBLQ).\n\nFor those who haven‚Äôt read the full paper, the video covers the highlights really well:\n\n* How PSI integrates depth, motion, and segmentation directly into the world model backbone (instead of relying on separate supervised probes).\n* Why its probabilistic approach lets it generalize in zero-shot settings.\n* Examples of applications in robotics, AR, and video editing.\n\nhttps://preview.redd.it/f4bcpxj1eyqf1.png?width=1584&amp;format=png&amp;auto=webp&amp;s=2a085394eb02bd33e992954903fd2039d3c448cc\n\nWhat stands out to me as a vision enthusiast is that PSI isn‚Äôt just predicting pixels - it‚Äôs actually extracting structure from raw video. That feels like a shift for CV models, where instead of training separate depth/flow/segmentation networks, you get those ‚Äúfor free‚Äù from the same world model.\n\nWould love to hear others‚Äô thoughts: could this be a step toward more general-purpose CV backbones, or just another specialized world model?",
    "author": "Appropriate-Web2517",
    "timestamp": "2025-09-23T10:56:21",
    "url": "https://reddit.com/r/computervision/comments/1noof8x/followup_on_psi_probabilistic_structure/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1no6nbp",
    "title": "Last week in Multimodal AI - Vision Edition",
    "content": "I curate a weekly newsletter on multimodal AI, here are the computer vision highlights from today's edition:\n\nTheory-of-Mind Video Understanding\n\n* First system understanding beliefs/intentions in video\n* Moves beyond action recognition to \"why\" understanding\n* Pipeline processes real-time video for social dynamics\n* [Paper](https://arxiv.org/pdf/2406.13763)\n\nOmniSegmentor (NeurIPS 2025)\n\n* Unified segmentation across RGB, depth, thermal, event, and more\n* Sets records on NYU Depthv2, EventScape, MFNet\n* One model replaces five specialized ones\n* [Paper](https://arxiv.org/pdf/2509.15096v1)\n\nMoondream 3 Preview\n\n* 9B params (2B active) matching GPT-4V performance\n* Visual grounding shows attention maps\n* 32k context window for complex scenes\n* [HuggingFace](https://huggingface.co/moondream/moondream3-preview)\n\nEye, Robot Framework\n\n* Teaches robots visual attention coordination\n* Learn where to look for effective manipulation\n* Human-like visual-motor coordination\n* [Paper](https://www.arxiv.org/pdf/2506.10968) | [Website](https://robotics.berkeley.edu/eye-robot)\n\nOther highlights\n\n* AToken: Unified tokenizer for images/videos/3D in 4D space\n* LumaLabs Ray3: First reasoning video generation model\n* Meta Hyperscape: Instant 3D scene capture\n* Zero-shot spatio-temporal video grounding\n\nhttps://reddit.com/link/1no6nbp/video/nhotl9f60uqf1/player\n\nhttps://reddit.com/link/1no6nbp/video/02apkde60uqf1/player\n\nhttps://reddit.com/link/1no6nbp/video/kbk5how90uqf1/player\n\nhttps://reddit.com/link/1no6nbp/video/xleox3z90uqf1/player\n\nFull newsletter:¬†[https://thelivingedge.substack.com/p/multimodal-monday-25-mind-reading](https://thelivingedge.substack.com/p/multimodal-monday-25-mind-reading)¬†(links to code/demos/models)",
    "author": "Vast_Yak_4147",
    "timestamp": "2025-09-22T20:14:31",
    "url": "https://reddit.com/r/computervision/comments/1no6nbp/last_week_in_multimodal_ai_vision_edition/",
    "score": 15,
    "num_comments": 6,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1noncip",
    "title": "Where do commercial Text2Image models fail? A reproducible thread (ChatGPT5.0, Qwen variants, NanoBanana, etc) to identify \"Failure Patterns\"",
    "content": "",
    "author": "Weird-Ad-7790",
    "timestamp": "2025-09-23T10:15:31",
    "url": "https://reddit.com/r/computervision/comments/1noncip/where_do_commercial_text2image_models_fail_a/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1noh4uh",
    "title": "How Can I Do Scene Text Detection Without AI/ML?",
    "content": "I want to detect the regions in an image containing text. The text itself is handwritten &amp; Often blue/black text on white background, With not alot of visual noise apart from shadows.\n\nHow can I do scene text detection without using any sort of AI/ML as the hardware this will be done on is a 400 MHz microcontroller with limited storage &amp; ram, Thus I can't fit an EAST or DB model on it.",
    "author": "FoundationOk3176",
    "timestamp": "2025-09-23T06:17:01",
    "url": "https://reddit.com/r/computervision/comments/1noh4uh/how_can_i_do_scene_text_detection_without_aiml/",
    "score": 2,
    "num_comments": 11,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nomsno",
    "title": "In search of external committee member",
    "content": "Mods, apologies in advance if this isn't allowed!\n\n  \nHey all! I'm a current part time US PhD student while working full time as a software engineer. My original background was in embedded work, then a stint as an AI/ML engineer, and now currently I work in the modeling/simulation realm. It has gotten to the time for me to start thinking about getting my committee together, and I need one external member. I had reached out at work, but the couple people I talked to wanted to give me their project to do for their specific organization/team, which I'm not interested in doing (for a multitude of reasons, the biggest being my work not being mine and having to be turned over to that organization/team). As I work full time, my job \"pays\" for my PhD, and so I'm not tethered to a grant or specific project, and have the freedom to direct my research however I see fit with my advisor, and that's one of the biggest benefits in my opinion.\n\n  \nThat being said, we have not tacked down specifically the problem I will be working towards for my dissertation, but rather the general area thus far. I am working in the space of 3D reconstruction from raw video only, without any additional sensors or camera pose information, specifically in dense, kinetic outdoor scenes (with things like someone videoing them touring a city). I have been tinkering with Dust3r/Mast3r and most recently Nvidia's ViPE, as an example. We have some ideas for improvements we have brainstormed, but that's about as far as we've gotten.\n\n  \nSo, if any of you who would be considered \"professionals\" (this is a loose term, my advisor says basically you'd just need to submit a CV and he's the determining authority on whether or not someone qualifies, you do NOT need a PhD) and might be interested in being my external committee member, please feel free to DM me and we can set up a time to chat and discuss further!\n\nEdit: after fielding some questions, here is some additional info:\n- You do NOT need to be from/in the US\n- Responsibilities include: attending the depth exam, proposal defense, and dissertation defense (can be remotely, about 1.5-2 hours apiece, just the 3 occurrences), and be willing to review my writings when I get there, though my advisor is primarily responsible for that. Any other involvement above and beyond that is greatly appreciated, but certainly not required!",
    "author": "rogueleader12345",
    "timestamp": "2025-09-23T09:54:59",
    "url": "https://reddit.com/r/computervision/comments/1nomsno/in_search_of_external_committee_member/",
    "score": 2,
    "num_comments": 8,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1no93cz",
    "title": "How do you handle inconsistent bounding boxes across your team?",
    "content": "we‚Äôre a small team working on computer vision projects and one challenge we keep hitting is annotation consistency. when different people label the same dataset, some draw really tight boxes and others leave extra space.\n\nfor those of you who‚Äôve done large-scale labeling, what approaches have helped you keep bounding boxes consistent? do you rely more on detailed guidelines, review loops, automated checks, or something else, open to discussion?",
    "author": "Loose-Ad-9956",
    "timestamp": "2025-09-22T22:29:08",
    "url": "https://reddit.com/r/computervision/comments/1no93cz/how_do_you_handle_inconsistent_bounding_boxes/",
    "score": 8,
    "num_comments": 14,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnr3tz",
    "title": "Auto-Labeling with Moondream 3",
    "content": "Set up this auto labeler with the new Moondream 3 preview.\n\nIn both examples, no guidance was given. It‚Äôs just asked to label everything.\n\nFirst step:\nUse the query end point to get a list of objects.\n\nSecond step:\nRun detect for each object.\n\nThird step:\nOverlay with the bounding box &amp; label data.\n\nWill be especially useful for removing all the unnecessary work in labeling for RL but also think it could be useful for AR &amp; robotics.\n\n",
    "author": "catdotgif",
    "timestamp": "2025-09-22T09:24:31",
    "url": "https://reddit.com/r/computervision/comments/1nnr3tz/autolabeling_with_moondream_3/",
    "score": 73,
    "num_comments": 24,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nogde4",
    "title": "Classify images",
    "content": "I have built a classification system that categorizes images into three classes: Good, Medium, or Bad. In this system, each image is evaluated based on three criteria: tilt (tilted or not), visibility (fully visible or not), and blur (blurred or not). Each criterion is assigned a score, and the total score ranges from 0 to 100. If the total score is above 70, the image is classified as Good, and the same logic applies to the other categories based on their scores.\n\nI want to automatically classify images into these three categories without manually labeling them. Could you suggest some free methods or tools to achieve this?",
    "author": "ConfectionOk730",
    "timestamp": "2025-09-23T05:44:02",
    "url": "https://reddit.com/r/computervision/comments/1nogde4/classify_images/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nof5wm",
    "title": "Headpose estimation and web spatial audio?",
    "content": "Hello  I wanted to know if any one has tried exploring spatial audio that tracks the headpose . I am wondering if one could experience or implement using mediapipe and p5js. My aim is to make a very small experiment to see how or if we can experience spatial audio with just the head pose tracking . ",
    "author": "Fluid-Beyond3878",
    "timestamp": "2025-09-23T04:47:07",
    "url": "https://reddit.com/r/computervision/comments/1nof5wm/headpose_estimation_and_web_spatial_audio/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnvw2i",
    "title": "Built an OCR+OpenCV system to read binary messages from camera into text.",
    "content": "",
    "author": "eminaruk",
    "timestamp": "2025-09-22T12:22:08",
    "url": "https://reddit.com/r/computervision/comments/1nnvw2i/built_an_ocropencv_system_to_read_binary_messages/",
    "score": 19,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nns9zi",
    "title": "crops3d dataset in case you don't want to go outside and touch grass, you can touch point clouds in fiftyone instead",
    "content": "Dataset on HuggingFace: https://huggingface.co/datasets/Voxel51/crops3d\n\nHow to parse into FO: https://github.com/harpreetsahota204/crops3d_to_fiftyone",
    "author": "datascienceharp",
    "timestamp": "2025-09-22T10:07:48",
    "url": "https://reddit.com/r/computervision/comments/1nns9zi/crops3d_dataset_in_case_you_dont_want_to_go/",
    "score": 23,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nodbxi",
    "title": "Advice for leveling up core programming skills during a 6-month CV/3D internship (solo in the lab)",
    "content": "Hello everyone!\n\nI‚Äôm an electronics engineer student (image &amp; signal processing) currently finishing a double degree in computer science (AI). I enjoy computer vision, so my first internship was in a university lab (worked on drivers behavior). Now I‚Äôm doing a 6-month internship in computer vision working on **3D mechanical data** (industrial context) in order to validate my degree. I‚Äôm the only CS/AI person in the team so it‚Äôs very autonomous.\n\nDespite these experiences, I feel my **core programming skills** aren‚Äôt strong enough . I want to dedicate **2‚Äì3 hours per day** to structured self-study alongside the internship.\n\nI‚Äôd really appreciate suggestions on a simple weekly structure I can follow to strengthen Python fundamentals, testing, and clean code, plus a couple of practical mini-project ideas in CV/3D that go beyond tutorials. If you also have a short list of resources that genuinely improved your coding and debugging, I‚Äôm all ears. Thanks for reading !!",
    "author": "Own-Dig3693",
    "timestamp": "2025-09-23T03:04:17",
    "url": "https://reddit.com/r/computervision/comments/1nodbxi/advice_for_leveling_up_core_programming_skills/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nod29m",
    "title": "Multiple Receipt Detection on Scanned receipts on white background",
    "content": "Hey folks, I‚Äôm new to CV and ran into a problem. I‚Äôm trying to figure out how many receipts are on a scanned page, but the borders usually just blend in with the white background. I tried using OpenCV to detect the receipts by their edges, but some of the scans were done using phone apps that ‚Äúprettify‚Äù the images, and that makes the receipt borders disappear.",
    "author": "zacpar546",
    "timestamp": "2025-09-23T02:47:56",
    "url": "https://reddit.com/r/computervision/comments/1nod29m/multiple_receipt_detection_on_scanned_receipts_on/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnz7ra",
    "title": "üöÄ Excited to share Version 2.0 of my Abandoned Object Detection system using YOLOv11 + ByteTrack! üé•üß≥",
    "content": "https://reddit.com/link/1nnz7ra/video/nhtyxqwyasqf1/player\n\nIn this update, I focused on making the solution smarter, more reliable, and closer to real-world deployment.üîë Key Enhancements in v2.0:‚úÖ Stable Bag IDs with IoU matching ‚Äì ensures consistent tracking even when IDs change ‚úÖ Owner locked forever ‚Äì once a bag has an owner, it remains tied to them ‚úÖ Robust against ByteTrack ID reuse ‚Äì time-based logic prevents ID recycling issues ‚úÖ \"No Owner\" state ‚Äì clearly identifies when a bag is unattended ‚úÖ Owner left ROI detection ‚Äì raises an alert if the original owner exits the Region of Interest ‚úÖ Improved alerting system ‚Äì more accurate and context-aware abandoned object warnings‚ö° Why this matters:Public safety in airports, train stations, and crowded areas often depends on the ability to spot unattended baggage quickly and accurately. By combining detection, tracking, and temporal logic, this system moves beyond simple object detection into practical surveillance intelligence.üéØ Next steps:Real-time CCTV integrationOn-device optimizations for edge deploymentExpanding logic for group behavior and suspicious movement patternsYou can follow me on Youtube as well:üëâ [youtube.com/@daanidev](http://youtube.com/@daanidev)üí° This project blends computer vision + tracking + smart rules to make AI-powered surveillance more effective.Would love to hear your thoughts! üëâ How else do you think we can extend this for real-world deployment?[hashtag#YOLOv11](https://www.linkedin.com/search/results/all/?keywords=%23yolov11&amp;origin=HASH_TAG_FROM_FEED) [hashtag#ComputerVision](https://www.linkedin.com/search/results/all/?keywords=%23computervision&amp;origin=HASH_TAG_FROM_FEED) [hashtag#ByteTrack](https://www.linkedin.com/search/results/all/?keywords=%23bytetrack&amp;origin=HASH_TAG_FROM_FEED) [hashtag#AI](https://www.linkedin.com/search/results/all/?keywords=%23ai&amp;origin=HASH_TAG_FROM_FEED) [hashtag#DeepLearning](https://www.linkedin.com/search/results/all/?keywords=%23deeplearning&amp;origin=HASH_TAG_FROM_FEED) [hashtag#Surveillance](https://www.linkedin.com/search/results/all/?keywords=%23surveillance&amp;origin=HASH_TAG_FROM_FEED) [hashtag#Security](https://www.linkedin.com/search/results/all/?keywords=%23security&amp;origin=HASH_TAG_FROM_FEED) [hashtag#OpenCV](https://www.linkedin.com/search/results/all/?keywords=%23opencv&amp;origin=HASH_TAG_FROM_FEED)",
    "author": "DaaniDev",
    "timestamp": "2025-09-22T14:30:27",
    "url": "https://reddit.com/r/computervision/comments/1nnz7ra/excited_to_share_version_20_of_my_abandoned/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1noig7j",
    "title": "Lessons from applying ML to noisy, non-stationary time-series data",
    "content": "I‚Äôve been experimenting with applying ML models to trading data (personal side project), and wanted to share a few things I‚Äôve learned + get input from others who‚Äôve worked with similar problems.\n\nMain challenges so far:\n\t‚Ä¢\tRegime shifts / distribution drift: Models trained on one period often fail badly when market conditions flip.\n\t‚Ä¢\tLabel sparsity: True ‚Äúevents‚Äù (entry/exit signals) are extremely rare relative to the size of the dataset.\n\t‚Ä¢\tOverfitting: Backtests that look strong often collapse once replayed on fresh or slightly shifted data.\n\t‚Ä¢\tInterpretability: End users want to understand why a model makes a call, but ML pipelines are usually opaque.\n\nRight now I‚Äôve found better luck with ensembles + reinforcement-style feedback loops rather than a single end-to-end model.\n\nQuestion for the group:\nFor those working on ML with highly noisy, real-world time-series data (finance, sensors, etc.), what techniques have you found useful for:\n\t‚Ä¢\tHandling label sparsity?\n\t‚Ä¢\tImproving model robustness across distribution shifts?\n\nNot looking for financial advice here ‚Äî just hoping to compare notes on how to make ML pipelines more resilient to noise and drift in real-world domains.",
    "author": "Powerful_Fudge_5999",
    "timestamp": "2025-09-23T07:10:35",
    "url": "https://reddit.com/r/computervision/comments/1noig7j/lessons_from_applying_ml_to_noisy_nonstationary/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1no8vk6",
    "title": "When using albumentations transforms for train and val dataloaders do I have to use them for prediction transform as well or can I use torchvision.transforms ?",
    "content": "For context I'm inexperienced in this field, and mostly do google search + use llms to eventually train a model for my task. Unfortunately when it came to this topic, I couldn't find an answer that I felt is reliable.\n\nCurrently following this guide [https://albumentations.ai/docs/3-basic-usage/image-classification/](https://albumentations.ai/docs/3-basic-usage/image-classification/) because I thought it'll be good to use since I have a very small dataset. My understanding is that prediction transforms should look like the val transforms in the guide:\n\n    val_transforms = A.Compose([\n        A.Resize(28, 28),\n        A.Normalize(mean=[0.1307], std=[0.3081]),\n        A.ToTensorV2(),\n    ])\n\nbut since albumentations is an augmentation library I thought it's probably not meant for use in predictions and I probably should use something like this instead:\n\n    pred_transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize((28, 28)),\n        torchvision.transforms.Normalize(mean=[0.1307], std=[0.3081]),\n        torchvision.transforms.ToTensor(),\n    ])\n\nin which case I should also use this for `val_transforms` and only use `albumentations` for `train_transforms`, no?\n\n",
    "author": "structured-bs",
    "timestamp": "2025-09-22T22:16:26",
    "url": "https://reddit.com/r/computervision/comments/1no8vk6/when_using_albumentations_transforms_for_train/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1no402t",
    "title": "Nvidia and Abu Dhabi institute launch joint AI and robotics lab in the UAE",
    "content": "A couple questions\n\nDo you guys think this is gonna lead to a genuine shift in vision?\n\nHow well will this lab handle the data &amp; environment diversity challenges for real-world robotics? Vision in controlled labs is one thing. generalization is p hard.",
    "author": "gpu_mamba",
    "timestamp": "2025-09-22T18:04:59",
    "url": "https://reddit.com/r/computervision/comments/1no402t/nvidia_and_abu_dhabi_institute_launch_joint_ai/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1no3vls",
    "title": "Image text vectorization?",
    "content": "Hi, needed to make this for a very specific part of my project, but just figure I'd ask if maybe anyone else could use it: would it ever be useful for someone to take an image of text and turn it into its SVG outlines (lines and bezier curves)?",
    "author": "Relative-Pace-2923",
    "timestamp": "2025-09-22T17:59:12",
    "url": "https://reddit.com/r/computervision/comments/1no3vls/image_text_vectorization/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1noduzi",
    "title": "What the CV equivalent of 99.1% pure blue meth?",
    "content": "As in if you achieve this and can prove it, you don‚Äôt need to show your resume to anyone ever again?",
    "author": "Proof-Bed-6928",
    "timestamp": "2025-09-23T03:35:37",
    "url": "https://reddit.com/r/computervision/comments/1noduzi/what_the_cv_equivalent_of_991_pure_blue_meth/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.13,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnk3x5",
    "title": "Using Opendatabay Datasets to Train a YOLOv8 Model for Industrial Object Detection",
    "content": "Hi everyone,\n\nI‚Äôve been working with datasets from [**Opendatabay.com**](https://opendatabay.com) to train a YOLOv8 model for detecting industrial parts. The dataset I used had \\~1,500 labeled images across 3 classes.\n\nHere‚Äôs what I‚Äôve tried so far:\n\n* **Augmentation:** Albumentations (rotation, brightness, flips) ‚Üí modest accuracy improvement (\\~+2%).\n* **Transfer Learning:** Initialized with COCO weights ‚Üí still struggling with false positives.\n* **Hyperparameter Tuning:** Adjusted learning rate &amp; batch size ‚Üí training loss improves, but validation mAP stagnates around 0.45.\n\n**Current Challenges:**\n\n* False positives on background clutter.\n* Poor generalization when switching to slightly different camera setups.\n\n**Questions for the community:**\n\n1. Would techniques like **domain adaptation** or **synthetic data generation** be worth exploring here?\n2. Any recommendations on handling **class imbalance** in small datasets (1 class dominates \\~70% of labels)?\n3. Are there specific **evaluation strategies** you‚Äôd recommend beyond mAP for industrial vision tasks?\n\nI‚Äôd love feedback and also happy to share more details if anyone else is exploring similar industrial use cases.\n\nThanks!",
    "author": "Winter-Lake-589",
    "timestamp": "2025-09-22T04:39:51",
    "url": "https://reddit.com/r/computervision/comments/1nnk3x5/using_opendatabay_datasets_to_train_a_yolov8/",
    "score": 7,
    "num_comments": 8,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nn5gw6",
    "title": "CV inference pipeline builder",
    "content": "I decided to replace all my random python scripts (that run various models for my weird and wonderful computer vision projects) with a single application that would let me create and manage my inference pipelines in a super easy way. Here's a quick demo. \n\nCode coming soon!",
    "author": "dr_hamilton",
    "timestamp": "2025-09-21T15:20:59",
    "url": "https://reddit.com/r/computervision/comments/1nn5gw6/cv_inference_pipeline_builder/",
    "score": 67,
    "num_comments": 20,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnk4bb",
    "title": "Struggling to move from simple computer vision tasks to real-world projects ‚Äì need advice",
    "content": "Hi everyone,\nI‚Äôm a junior in computer vision. So far, I‚Äôve worked on basic projects like image classification, face detection/recognition, and even estimating car speed.\n\nBut I‚Äôm struggling when it comes to real-world, practical projects. For example, I want to build something where AI guides a human during a task ‚Äî like installing a light bulb. I can detect the bulb and the person, but I don‚Äôt know how to:\n\nTrack the person‚Äôs hand during the process\n\nDetect mistakes in real-time\n\nProvide corrective feedback\n\n\nHas anyone here worked on similar ‚ÄúAI as a guide/assistant‚Äù type of projects?\nWhat would be a good starting point or resources to learn how to approach this?\n\nThanks in advance!",
    "author": "husaynShawer",
    "timestamp": "2025-09-22T04:40:25",
    "url": "https://reddit.com/r/computervision/comments/1nnk4bb/struggling_to_move_from_simple_computer_vision/",
    "score": 5,
    "num_comments": 9,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnwboc",
    "title": "Handwritten Mathematical OCR",
    "content": "Hello everyone I‚Äôm working on a project and needed some guidance, I need a model where I can upload any document which has english sentences  plus mathematical equations  and it should output the corresponding latex code, what could be a good starting point for me? Any pre trained models already out there? I tried pix2text, it works well when there is a single equation in the image but performs drops when I scan and upload a whole handwritten page \nAlso does anyone know about any research papers which talk about this? ",
    "author": "Snowysecret1811",
    "timestamp": "2025-09-22T12:38:53",
    "url": "https://reddit.com/r/computervision/comments/1nnwboc/handwritten_mathematical_ocr/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnvm7b",
    "title": "FS - RealSense Depth Cams D435 and SR305",
    "content": "I have some real sense depth cams, if anyone is interested. Feel free to PM. thx\n\nx5 D435s  [https://www.ebay.com/itm/336192352914](https://www.ebay.com/itm/336192352914)\n\nx6 SR305 - [https://www.ebay.com/itm/336191269856](https://www.ebay.com/itm/336191269856)",
    "author": "moneymatters666",
    "timestamp": "2025-09-22T12:11:31",
    "url": "https://reddit.com/r/computervision/comments/1nnvm7b/fs_realsense_depth_cams_d435_and_sr305/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnswkh",
    "title": "Landing remote computer vision job",
    "content": "Hi all! I have been trying to find remote job in computer vision. I have almost 3 years as computer vision engineer. When looking job  online every opening I see is of senior computer vision engineer with 5+ years experience. Do you guys have any tips or tricks for getting a job? Or are there any job openings  where you work? I have experience working with international client. I can dm my resume if needed. Any help is appreciated. Thank you!",
    "author": "-goldeneyez",
    "timestamp": "2025-09-22T10:31:16",
    "url": "https://reddit.com/r/computervision/comments/1nnswkh/landing_remote_computer_vision_job/",
    "score": 1,
    "num_comments": 12,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnspfp",
    "title": "We‚Äôre a small team building labellerr (image + video annotation platform). AMA!",
    "content": "Hi everyone,\n\nwe‚Äôre a small team based out of chandigarh, india trying to make a dent in the AI ecosystem by tackling one of the most boring but critical parts of the pipeline: **data annotation**.\n\nOver the past couple of years we‚Äôve been building **labellerr** ‚Äì a platform that helps ML teams label images, videos, pdfs, and audio faster with ai-assisted tools. we‚Äôve shipped things like:\n\n* **video annotation workflows** (frame-level, tracking, QA loops)\n* **image annotation toolkit** (bbox, polygons, segmentation, dicom support for medical)\n* **ai-assists** (segment anything, auto pre-labeling, smart feedback loop)\n* **multi-modality** (pdf, text, audio transcription with generative assists)\n* Labellerr SDK so you can plug into your ml pipeline directly\n\nwe‚Äôre still a small crew, and we know communities like this can be brutal but fair. so here‚Äôs an **AMA** ‚Äì ask us about annotation, vision data pipelines, or just building an ML tool as a tiny startup from India.\n\nif you‚Äôve tried tools like ours or want to, we‚Äôd also love your guidance:\n\n* what features matter most for you?\n* what pain points in annotation remain unsolved?\n* where can we improve to be genuinely useful to researchers/devs like you?\n\nthanks for reading, and we‚Äôd love to hear your thoughts!\n\n‚Äî the labellerr team",
    "author": "Fluffy_Sheepherder76",
    "timestamp": "2025-09-22T10:23:54",
    "url": "https://reddit.com/r/computervision/comments/1nnspfp/were_a_small_team_building_labellerr_image_video/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnrgyc",
    "title": "Handwritten OCR GOAT?",
    "content": "Hello! :)\n\nI have a dataset of handwritten email addresses that I need to transcribe. The challenge is that many of them are poorly written and not very clear.\n\nWhat do you think would be the best tools/models for this?\n\nThanks in advance for any insights!",
    "author": "Deep_Main9815",
    "timestamp": "2025-09-22T09:37:50",
    "url": "https://reddit.com/r/computervision/comments/1nnrgyc/handwritten_ocr_goat/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nmz4kv",
    "title": "How a String Library Beat OpenCV at Image Processing by 4x",
    "content": "",
    "author": "ternausX",
    "timestamp": "2025-09-21T11:10:48",
    "url": "https://reddit.com/r/computervision/comments/1nmz4kv/how_a_string_library_beat_opencv_at_image/",
    "score": 57,
    "num_comments": 9,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nni5l1",
    "title": "Pimeyes not working",
    "content": "I am looking for an old friend but I don't have a good photo of her..\nI tried looking her on pimeyes but due to the photo being grainy and also in the photo she not looking directly into the camera...\nSo the pimeyes won't start searching it( I use the free version) I want to know if updating it to premium will work or I need some better photos ",
    "author": "SpErMman69",
    "timestamp": "2025-09-22T02:49:15",
    "url": "https://reddit.com/r/computervision/comments/1nni5l1/pimeyes_not_working/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnkhwy",
    "title": "How to learn JAX?",
    "content": "Just came across this user on X where he wrote some model in pure JAX. I just wanted to know why you should learn JAX? and what are its benefits over others. Also share some resources and basic project ideas that i can work on while learning the basics.",
    "author": "Pure_Long_3504",
    "timestamp": "2025-09-22T04:59:41",
    "url": "https://reddit.com/r/computervision/comments/1nnkhwy/how_to_learn_jax/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnkggm",
    "title": "When developing an active vision system, do you consider its certification?",
    "content": "Hey everyone,  \nI‚Äôm curious ‚Äî if you build an assembly line with active vision to reduce defects, do you actually need to get some kind of certification to make sure the system is ‚Äúdefended‚Äù (or officially approved)?\n\nOr is this not really a big deal, especially for smaller assembly lines?\n\nWould love to hear your thoughts or experiences.",
    "author": "Commercial_Slice_254",
    "timestamp": "2025-09-22T04:57:31",
    "url": "https://reddit.com/r/computervision/comments/1nnkggm/when_developing_an_active_vision_system_do_you/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nngnk2",
    "title": "Pretrained model for building damage assessment and segmentation",
    "content": "im doing a project where im going to use a UAV to take a top down view picture and it will assess the damages of buildings and segment them. I tried training using the xview2 dataset but I keep getting bad results because of it having too much background images. Is there a ready to use pretrained model for this project? I cant seem to figure out how to train it properly. the results I get is like the one attached. \n\nedit: when I train it, I get 0 loss due to it having alot of background images so its not learning anything. im not sure if im doing something wrong",
    "author": "Anekinnn",
    "timestamp": "2025-09-22T01:10:45",
    "url": "https://reddit.com/r/computervision/comments/1nngnk2/pretrained_model_for_building_damage_assessment/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nng58c",
    "title": "Read LCD/LED or 7 segments digits",
    "content": "Hello, I'm not an AI engineer, but what I want is to extract numbers from different screens like LCD, LED, and seven-segment digits.\n\nI downloaded about 2000 photos, labeled them, and trained them with YOLOv8. Sometimes it misses easy numbers that are clear to me.\n\nI also tried with my iPhone, and it easily extracted the numbers, but I think that‚Äôs not the right approach.\n\nI chose YOLOv8n because it‚Äôs a small model and I can run it easily on Android without problems.\n\nSo, is there anything better?",
    "author": "kamelsayed",
    "timestamp": "2025-09-22T00:36:25",
    "url": "https://reddit.com/r/computervision/comments/1nng58c/read_lcdled_or_7_segments_digits/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnj937",
    "title": "SLM suggestion for complex vision tasks.",
    "content": "",
    "author": "CoolCucumberRK",
    "timestamp": "2025-09-22T03:53:43",
    "url": "https://reddit.com/r/computervision/comments/1nnj937/slm_suggestion_for_complex_vision_tasks/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nmalix",
    "title": "Real-time Abandoned Object Detection using YOLOv11n!",
    "content": "üöÄ Excited to share my latest project: Real-time Abandoned Object Detection using YOLOv11n! üé•üß≥\n\nI implemented YOLOv11n to automatically detect and track abandoned objects (like bags, backpacks, and suitcases) within a Region of Interest (ROI) in a video stream. This system is designed with public safety and surveillance in mind.\n\nKey highlights of the workflow: \n\n‚úÖ Detection of persons and bags using YOLOv11n \n\n‚úÖ Tracking objects within a defined ROI for smarter monitoring \n\n‚úÖ Proximity-based logic to check if a bag is left unattended \n\n‚úÖ Automatic alert system with blinking warnings when an abandoned object is detected\n\n‚úÖ Optimized pipeline tested on real surveillance footage‚ö° \n\nA crucial step here: combining object detection with temporal logic (tracking how long an item stays unattended) is what makes this solution practical for real-world security use cases.üí° \n\nNext step: extending this into a real-time deployment-ready system with live CCTV integration and mobile-friendly optimizations for on-device inference.",
    "author": "DaaniDev",
    "timestamp": "2025-09-20T14:49:16",
    "url": "https://reddit.com/r/computervision/comments/1nmalix/realtime_abandoned_object_detection_using_yolov11n/",
    "score": 771,
    "num_comments": 45,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nnf6wn",
    "title": "Help me out folks. Its a bit urgent. Pose extraction using yolo pose",
    "content": "https://preview.redd.it/g32k5bb4vnqf1.png?width=640&amp;format=png&amp;auto=webp&amp;s=7f9028c802a5c756762bd3ccde54c1ca227dc79f\n\nit needs to detect only 2 people (the players)\n\nProblem is its detecting wrong ones.\n\nAny heuristics?\n\nmost are failing\n\ncurrent model yolo8n-pose\n\nshould i use a different model?\n\nGPT is complicating it by figuring out the court coordinates using homography etc etc",
    "author": "Nothing769",
    "timestamp": "2025-09-21T23:35:56",
    "url": "https://reddit.com/r/computervision/comments/1nnf6wn/help_me_out_folks_its_a_bit_urgent_pose/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nn40ji",
    "title": "Rubbish Classifier Web App",
    "content": "Hi guys, i have been building a rubbish classifier that runs on device, once you download the model first but inference happens in the browser.\n\nSince the idea is for it to run on device, the quality of the database should be improved to get better results.\n\nSo I built a quick page within the classifier where anyone can contribute by uploading images/photos of rubbish and assign a label to it.\n\nI would be grateful if you guys could contribute, the images will be used to train a better model using a pre-trained one.\n\nAlso, for on device image classification, what pre trained model you guys recommend? I haven‚Äôt updated mines for a while but when i trained them (a couple of years ago) i used EfficientNet B0 and B2, so i am not up to date.\n",
    "author": "TheLastMate",
    "timestamp": "2025-09-21T14:19:17",
    "url": "https://reddit.com/r/computervision/comments/1nn40ji/rubbish_classifier_web_app/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nmx3dr",
    "title": "Tried on device VLM at grocery store üëå",
    "content": "https://youtube.com/shorts/ZbzUC3-0EVo?feature=share",
    "author": "TextDeep",
    "timestamp": "2025-09-21T09:52:45",
    "url": "https://reddit.com/r/computervision/comments/1nmx3dr/tried_on_device_vlm_at_grocery_store/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nmuo7w",
    "title": "First time training YOLO: Dataset not found",
    "content": "Hi,\n\nAs title describe, i'm trying to train a \"YOLO\" model for classification purpose for the first time, for a school project.\n\nI'm running the notebook in a Colab instance.\n\nWhenever i try to run \"model.train()\" method, i receive the error\n\n\"WARNING ‚ö†Ô∏è Dataset not found, missing path /content/data.yaml, attempting download...\"\n\nEven if the file is placed correctly in the path mentioned above\n\nWhat am i doing wrong?\n\nhttps://preview.redd.it/cn1parscbjqf1.png?width=1502&amp;format=png&amp;auto=webp&amp;s=70b6f7f6297ae3d167304baa52988a92b07ee328\n\nThanks in advance for your help!  \n  \nPS: i'm using \"cpu\" as device cause i didn't want to waste GPU quotas during the troubleshooting",
    "author": "therealdodrio",
    "timestamp": "2025-09-21T08:17:50",
    "url": "https://reddit.com/r/computervision/comments/1nmuo7w/first_time_training_yolo_dataset_not_found/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nmk4th",
    "title": "Wanted to get some insights regarding Style Transfer",
    "content": "I was working on a course project, and the overall task is to consider two images;  \na content image (call it C) and a style image (call it S). Our model should be able to generate an image which captures the content of C and the style of S.   \nFor example we give a random image (of some building or anything) and the second image is of the Starry Night (by Van Gogh). The final output should be the first image in the style of the Starry Night.   \nNow our task asks us to specifically focus on a set of shifted domains (which mainly includes environmental shifts, such as foggy, rainy, snowy, misty etc.)  \nSo the content image that we provide (can be anything) needs to capture these environmental styles and generate the final image appropriately.  \nNeeded some insights so as to how I can start working on this. I have researched about the workings of Diffusion models, while my other team mate is focusing on GANs, and later we would combine our findings. \n\nHere is the word to word description of the task incase you want to have a read :- \n\n1. Team needs to consider a set of shifted domains (based on the discussion with allotted TAs) and natural environment based domain. 2. Team should explore the StyleGAN and Diffusion Models to come up with a mechanism which takes the input as the clean image (for content) and the reference shifted image (from set of shifted domains) and gives output as an image that has the content of clean image while mimicing the style of reference shifted image. 3. Team may need to develop generic shifted domain based samples. This must be verified by the concerned TAs. 4. Team should investigate what type of metrics can be considered to make sure that the output image mimics the distribution of the shifted image as much as possible. 5. Semantic characteristics of the clean input image must be present in the output style transferred image.\n\n",
    "author": "ndstab23",
    "timestamp": "2025-09-20T22:56:55",
    "url": "https://reddit.com/r/computervision/comments/1nmk4th/wanted_to_get_some_insights_regarding_style/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nmc19w",
    "title": "Follow-up: great YouTube explainer on PSI (world models with structure integration)",
    "content": "A few days ago I shared the new PSI paper (Probabilistic Structure Integration) here and the discussion was awesome. Since then I stumbled on this YouTube breakdown that just dropped into my feed - and it‚Äôs all about the same paper:\n\nvideo link: [https://www.youtube.com/watch?v=YEHxRnkSBLQ](https://www.youtube.com/watch?v=YEHxRnkSBLQ)\n\nThe video does a solid job walking through the architecture, why PSI integrates structure (depth, motion, segmentation, flow), and how that leads to things like zero-shot depth/segmentation and probabilistic rollouts.\n\nFigured I‚Äôd share for anyone who wanted a more visual/step-by-step walkthrough of the ideas. I found it helpful to see the concepts explained in another format alongside the paper!",
    "author": "Appropriate-Web2517",
    "timestamp": "2025-09-20T15:52:47",
    "url": "https://reddit.com/r/computervision/comments/1nmc19w/followup_great_youtube_explainer_on_psi_world/",
    "score": 6,
    "num_comments": 4,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nm4yn3",
    "title": "Uni-CoT: A Unified CoT Framework that Integrates Text+Image reasoning!",
    "content": "# We introduce Uni-CoT, the first unified Chain-of-Thought framework that handles both image understanding + generation to enable coherent visual reasoning [as shown in Figure 1]. Our model even can supports NanoBanana‚Äìstyle geography reasoning [as shown in Figure 2]!\n\nSpecifically, we use **one unified architecture** (inspired by Bagel/Omni/Janus) to support multi-modal reasoning. This minimizes discrepancy between reasoning trajectories and visual state transitions, enabling coherent cross-modal reasoning. However, the multi-modal reasoning with unified model raise a large burden on computation and model training.\n\n# To solve it, we propose a hierarchical Macro‚ÄìMicro CoT:\n\n* **Macro-Level CoT** ‚Üí global planning, decomposing a task into subtasks.\n* **Micro-Level CoT** ‚Üí executes subtasks as a **Markov Decision Process (MDP)**, reducing token complexity and improving efficiency.\n\nThis **structured decomposition** shortens reasoning trajectories and lowers cognitive (and computational) load.\n\n# With this desigin, we build a novel training strategy for our Uni-CoT:\n\n* **Macro-level modeling**: refined on interleaved text‚Äìimage sequences for global planning.\n* **Micro-level modeling**: auxiliary tasks (action generation, reward estimation, etc.) to guide efficient learning.\n* **Node-based reinforcement learning** to stabilize optimization across modalities.\n\n# Results:\n\n* Training efficiently only on **8 √ó A100 GPUs**\n* Inference efficiently only on 1 **√ó A100 GPU**\n* Achieves **state-of-the-art performance** on reasoning-driven benchmarks for image generation &amp; editing.\n\n# Resource:\n\nOur paperÔºöhttps://arxiv.org/abs/2508.05606\n\nGithub repo:¬†[https://github.com/Fr0zenCrane/UniCoT](https://github.com/Fr0zenCrane/UniCoT)\n\nProject page:¬†[https://sais-fuxi.github.io/projects/uni-cot/](https://sais-fuxi.github.io/projects/uni-cot/)",
    "author": "GONG_JIA",
    "timestamp": "2025-09-20T10:59:15",
    "url": "https://reddit.com/r/computervision/comments/1nm4yn3/unicot_a_unified_cot_framework_that_integrates/",
    "score": 14,
    "num_comments": 0,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nlu279",
    "title": "RF-DETR Segmentation Releasing Soon",
    "content": "https://preview.redd.it/7q8rpj0dkaqf1.png?width=2188&amp;format=png&amp;auto=webp&amp;s=c47d98ae146a16240313e813749e0ede3ca5a072\n\n[https://github.com/roboflow/single\\_artifact\\_benchmarking/blob/main/sab/models/benchmark\\_rfdetr\\_seg.py](https://github.com/roboflow/single_artifact_benchmarking/blob/main/sab/models/benchmark_rfdetr_seg.py)\n\n  \nWas going through some benchmarking code and came across this commit from just three hours ago that has RFDETRSeg available as a new model for benchmarking. Roboflow might be releasing it soon, perhaps even with a DINOV3 backbone. ",
    "author": "Mammoth-Photo7135",
    "timestamp": "2025-09-20T02:51:43",
    "url": "https://reddit.com/r/computervision/comments/1nlu279/rfdetr_segmentation_releasing_soon/",
    "score": 62,
    "num_comments": 17,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nmap3z",
    "title": "Real time Inswapper paint shop",
    "content": "",
    "author": "redlitegreenlite456",
    "timestamp": "2025-09-20T14:53:21",
    "url": "https://reddit.com/r/computervision/comments/1nmap3z/real_time_inswapper_paint_shop/",
    "score": 6,
    "num_comments": 4,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nmlfn6",
    "title": "MiniCPM on Jetson Nano/Orin 8Gb",
    "content": "",
    "author": "techie_msp",
    "timestamp": "2025-09-21T00:16:35",
    "url": "https://reddit.com/r/computervision/comments/1nmlfn6/minicpm_on_jetson_nanoorin_8gb/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nmg2sa",
    "title": "Ultralytics_YOLO_Object_Detection_Testing_GUI",
    "content": "Built a simple GUI for testing Y OLO Object Detection models with [Ultralytics](https://www.linkedin.com/company/ultralytics/)!With this app you can: -&gt;Load your trained YOLO model -&gt; Run detection on images, videos, or live feed -&gt; Save results with bounding boxes &amp; class infoCheck it out here ",
    "author": "Kuldeep0909",
    "timestamp": "2025-09-20T19:10:38",
    "url": "https://reddit.com/r/computervision/comments/1nmg2sa/ultralytics_yolo_object_detection_testing_gui/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nmnwpp",
    "title": "As AI can do most of the things, do you still train your own models?",
    "content": "For those of you who works in model training, as the title says, do you still train your own models when AI can also do it without you needing to train it? If so, what's your reasons for that?\n\nI'm working on object detection and have some trained datasets. However, using AI, it can detect object and generate mask for object correctly without me needing to train it.\n\nThanks!",
    "author": "alaska-salmon-avocad",
    "timestamp": "2025-09-21T02:52:01",
    "url": "https://reddit.com/r/computervision/comments/1nmnwpp/as_ai_can_do_most_of_the_things_do_you_still/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nlrvt3",
    "title": "üöó Demo: Autonomous Vehicle Dodging Adversarial Traffic on Narrow Roads üöó",
    "content": "This demo shows an autonomous vehicle navigating a really tough scenario: a **single-lane road with muddy sides**, while random traffic deliberately cuts across its path.\n\nTo make things challenging, people on a **bicycle, motorbike, and even an SUV** randomly overtook and cut in front of the car. The entire responsibility of collision avoidance and safe navigation was left to the autonomous system.\n\nWhat makes this interesting:\n\n* The same vehicle had earlier done a low-speed demo on a wide road for visitors from Japan.\n* In this run, the difficulty was raised ‚Äî the car had to handle **adversarial traffic, cone negotiation, and even bi-directional traffic on a single lane** at much higher speeds.\n* All maneuvers (like the SUV cutting in at speed, the bike and cycle crossing suddenly, etc.) were done by the engineers themselves to test the system‚Äôs limits.\n\nThe decision-making framework behind this uses a **reinforcement learning policy**, which is being scaled towards full Level-5 autonomy.\n\nThe coolest part for me: watching the car calmly negotiate traffic that was actively trying to throw it off balance. Real-world, messy driving conditions are so much harder than clean test tracks ‚Äî and that‚Äôs exactly the kind of robustness autonomous vehicles need.",
    "author": "shani_786",
    "timestamp": "2025-09-20T00:32:10",
    "url": "https://reddit.com/r/computervision/comments/1nlrvt3/demo_autonomous_vehicle_dodging_adversarial/",
    "score": 18,
    "num_comments": 9,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nm5yph",
    "title": "Optical flow (pose estimation) using forward pointing camera",
    "content": "Hello guys,\n\nI have a forward facing camera on a drone that I want to use to estimate its pose instead of using an optical flow sensor. Any recommendations of projects that already do this? I am running DepthAnything V2 (metric) in real time anyway, FYI, if this is of any use. \n\nThanks in advance!",
    "author": "ComedianOpening2004",
    "timestamp": "2025-09-20T11:38:47",
    "url": "https://reddit.com/r/computervision/comments/1nm5yph/optical_flow_pose_estimation_using_forward/",
    "score": 2,
    "num_comments": 10,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nlycbb",
    "title": "need advice on Learning CV to be a Researcher?",
    "content": "I am starting my uni soon for undergrad and after exploring a bunch of stuffs i think this is where i belong.i just need some advice how do i study cv to be a researcher in this field? i have little knowledge of image handling, some ml theories, intermediate pythons, numpy, intermediate dsa? How would you do if you have to start this again.  \n  \nI am especially confused since there are a lot of resources. I thought cv was niche field. Would you recommend me books and sources if possible.  \nPlease please your help would mean a lot to me. ",
    "author": "Longjumping_Arm_3061",
    "timestamp": "2025-09-20T06:34:48",
    "url": "https://reddit.com/r/computervision/comments/1nlycbb/need_advice_on_learning_cv_to_be_a_researcher/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nlpkdc",
    "title": "I benchmarked the free vision models ‚Äî who‚Äôs fastest at image-to-text?",
    "content": "https://preview.redd.it/o9027k4n49qf1.png?width=1842&amp;format=png&amp;auto=webp&amp;s=ac2ac5e9de2b75a0464b6adc55c7b7cbcc7bdce0\n\n\n\n* Which free vision model is fastest? My latency-only leaderboard (Sep 2025)",
    "author": "buryhuang",
    "timestamp": "2025-09-19T22:13:59",
    "url": "https://reddit.com/r/computervision/comments/1nlpkdc/i_benchmarked_the_free_vision_models_whos_fastest/",
    "score": 11,
    "num_comments": 5,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nma2jx",
    "title": "Need guidance to learn VLM",
    "content": "My thesis is on Vision language model. I have basics on CNN &amp; CV. Suggest some resources to understand VLM in depth.",
    "author": "Illustrious-Wind7175",
    "timestamp": "2025-09-20T14:26:41",
    "url": "https://reddit.com/r/computervision/comments/1nma2jx/need_guidance_to_learn_vlm/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nlxshh",
    "title": "How to convert a SSD MobileNet V3 model to TFLite/LiteRT",
    "content": "Hi guys , I am a junior computer engineer and thought to reach out to the community to help me on that matter yet to help others who might also tackled same obstacles , I wanted to know how I can convert my ssd mobilenet v3 to TFLite/LiteRT without going to the hassle of conflict dependencies and errors .\n\nI would like to know what packages to install (( requirments.txt )) , and how I make sure that the conversion itself won't generate a dummy model , but rather keep as much properties as possible to my original model especially the classes to maintain high accurate inference process  \n  \nAny small comment is so so much appreciated :) ",
    "author": "PlusBass6686",
    "timestamp": "2025-09-20T06:10:14",
    "url": "https://reddit.com/r/computervision/comments/1nlxshh/how_to_convert_a_ssd_mobilenet_v3_model_to/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nl3nqu",
    "title": "Computer Vision Learning Resources",
    "content": "Hey, I‚Äôm looking to build a solid foundation in computer vision. Any suggestions for high-quality practical resources, maybe from top university labs or similar?",
    "author": "OkLion2068",
    "timestamp": "2025-09-19T06:31:13",
    "url": "https://reddit.com/r/computervision/comments/1nl3nqu/computer_vision_learning_resources/",
    "score": 30,
    "num_comments": 12,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nlewq5",
    "title": "TEMAS modular 3D vision kit (RGB + ToF + LiDAR, Raspberry Pi 5) ‚Äì would love your thoughts",
    "content": "Hey everyone,\n\nwe just put together a 10-second short of our modular 3D vision kit TEMAS. It combines an RGB camera, ToF, and optional LiDAR on a Pan/Tilt gimbal, running on a Raspberry Pi 5 with a Hailo AI Hat (26 TOPS). Everything can be accessed through an open Python API.\n\nhttps://youtu.be/_KPBp5rdCOM?si=tIcC9Ekb42me9i3J\n\nI‚Äôd really value your input:\n\nFrom your perspective, which kind of demo would be most interesting to see next? (point cloud, object tracking, mapping, SLAM?)\n\nIf you had this kit on your desk, what‚Äôs the first thing you‚Äôd try to build with it?\n\nAre there specific datasets or benchmarks you‚Äôd recommend we test against?\n\nWe‚Äôre still shaping things and your feedback would mean a lot",
    "author": "Big-Mulberry4600",
    "timestamp": "2025-09-19T13:42:52",
    "url": "https://reddit.com/r/computervision/comments/1nlewq5/temas_modular_3d_vision_kit_rgb_tof_lidar/",
    "score": 7,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nl3y4p",
    "title": "Drone simulates honey bee navigation",
    "content": "Below is the result of drone footage processed to extract a map. This is done with only optic flow: no stereopsis, compass, or active rangers. It is described at https://tomrearick.substack.com/p/honey-bee-dead-reckoning. This lightweight algorithm will next be integrated into my Holybro X650 (see https://tomrearick.substack.com/p/beyond-ai). I am seeking like-minded researchers/hobbyists.\n\nhttps://reddit.com/link/1nl3y4p/video/d9ff4ytuk4qf1/player\n\n",
    "author": "tomrearick",
    "timestamp": "2025-09-19T06:43:04",
    "url": "https://reddit.com/r/computervision/comments/1nl3y4p/drone_simulates_honey_bee_navigation/",
    "score": 15,
    "num_comments": 6,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nlelqx",
    "title": "Pose Estimation of a Planar Square from Multiple Calibrated Cameras",
    "content": "I'm trying to estimate the 3D pose of a known-edge planar square using multiple calibrated cameras. In each view, the four corners of the square are detected. Rather than triangulating each point independently, I want to treat the square as a single rigid object and estimate its global pose. All camera intrinsics and extrinsics are known and fixed.\n\nI‚Äôve seen [algorithms for plane-based pose estimation](https://inria.hal.science/inria-00525671v1/document), but they treat the camera extrinsics as unknowns and focus on recovering them as well as the pose. In my case, the cameras are already calibrated and fixed in space.\n\nAny suggestions for approaches, relevant research papers, or libraries that handle this kind of setup?",
    "author": "LorenzoDeSa",
    "timestamp": "2025-09-19T13:30:49",
    "url": "https://reddit.com/r/computervision/comments/1nlelqx/pose_estimation_of_a_planar_square_from_multiple/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nllpxl",
    "title": "hardware list for AI-heavy camera",
    "content": "Looking for a hardware list to have the following features: \n\n\\- Run AI models: Computer Vision + Audio Deep learning algos \n\n\\- Two Way Talk \n\n\\- 4k Camera 30FPS \n\n\\- battery powered - wired connection/other \n\n\\- onboard wifi or ethernet\n\n\\- needs to have RTSP (or other) cloud messaging. An app needs to be able to connect to it.\n\nPrice is not a concern at the moment. Looking to make a doorbell camera. If someone could suggest me hardware components (or would like to collaborate on this!) please let me know - I almost have all the AI algorithms done. \n\n  \nregards ",
    "author": "Apart_Situation972",
    "timestamp": "2025-09-19T18:48:30",
    "url": "https://reddit.com/r/computervision/comments/1nllpxl/hardware_list_for_aiheavy_camera/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nlh7zv",
    "title": "Good papers on Street View Imagery Object Detection",
    "content": "Hi everyone, I‚Äôm working on a project trying to detect all sorts of objects from the street environments from geolocated Street View Imagery, especially for rare objects and scenes. I wanted to ask if anyone has any recent good papers or resources on the topic? ",
    "author": "Far-Air9800",
    "timestamp": "2025-09-19T15:19:51",
    "url": "https://reddit.com/r/computervision/comments/1nlh7zv/good_papers_on_street_view_imagery_object/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nl53zz",
    "title": "Training loss",
    "content": "Should i stop training here and change hyperparameters and should wait for completion of epoch?\n\ni have added more context below the image.\n\ncheck my code here : [https://github.com/CheeseFly/new/blob/main/one-checkpoint.ipynb](https://github.com/CheeseFly/new/blob/main/one-checkpoint.ipynb)\n\nhttps://preview.redd.it/r18ck67gt4qf1.png?width=553&amp;format=png&amp;auto=webp&amp;s=e6e7dafa7951c4f62205cbc54efafb225caeb75a\n\nadding more context :\n\n    NUM_EPOCHS = 40\n    BATCH_SIZE = 32\n    LEARNING_RATE = 0.0001\n    MARGIN = 0.7  -- these are my configurations\n    \n    also i am using constrative loss function for metric learning , i am using mini-imagenet dataset, and using resnet18 pretrained model.\n    \n    initally i trained it using margin =2 and learning rate 0.0005 but the loss was stagnated around 1 after 5 epoches , then i changes margin to 0.5 and then reduced batch size to 16 then the loss suddenly dropped to 0.06 and then i still reduced the margin to 0.2 then the loss also dropped to 0.02 but now it is stagnated at 0.2 and the accuracy is 0.57.\n    \n    i am using siamese twin model.",
    "author": "Substantial-Pop470",
    "timestamp": "2025-09-19T07:29:26",
    "url": "https://reddit.com/r/computervision/comments/1nl53zz/training_loss/",
    "score": 3,
    "num_comments": 14,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nlaw8l",
    "title": "How to draw a \"stuck-to-the-ground\" trajectory with a moving camera?",
    "content": "Hello visionaries,\n\nI'm a computer science student doing computer vision internship. Currently, I'm working on a soccer analytics project where I'm tracking a ball's movement using CoTracker3. I want to visualize the ball's historical path on the video, but the key challenge is that the camera is moving (panning and zooming).\n\nMy goal is to make the trajectory line look like it's \"painted\" or \"stuck\" to the field itself, not just an overlay on the screen.\n\nHere's a quick video of what my current naive implementation looks like:\n\n[I generated this using a modified version of official CoTracker3 repo](https://reddit.com/link/1nlaw8l/video/lo5pcadjs5qf1/player)\n\nYou can see the line slides around with the camera instead of staying fixed to the pitch.  I believe the solution involves using **Homography**, but I'm unsure of the best way to implement it.\n\nI also have a separate keypoint detection model on hand that can find soccer pitch markers (like penalty box corners) on a given frame.",
    "author": "Bahamalar",
    "timestamp": "2025-09-19T11:07:25",
    "url": "https://reddit.com/r/computervision/comments/1nlaw8l/how_to_draw_a_stucktotheground_trajectory_with_a/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nl8sf5",
    "title": "Best Courses to Learn Computer Vision for Automatic Target Tracking FYP",
    "content": "Hi Everyone,\n\nI‚Äôm a 7th-semester Electrical Engineering student with a growing interest in Python and computer vision. I‚Äôve completed Coursera courses like *Crash Course on Python*, *Introduction to Computer Vision*, and *Advanced Computer Vision with TensorFlow*.\n\nI can implement YOLO for object detection and apply image filters, but I want to deepen my skills and write my own codes.\n\nMy FYP is *Automatic Target Tracking and Recognition*. Could anyone suggest the best Coursera courses or resources to strengthen my knowledge for this project?",
    "author": "Tahzeeb_97",
    "timestamp": "2025-09-19T09:48:16",
    "url": "https://reddit.com/r/computervision/comments/1nl8sf5/best_courses_to_learn_computer_vision_for/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nknk5u",
    "title": "What would you do a computer vision project on for a master‚Äôs program?",
    "content": "Hey folks, I‚Äôm starting a computer vision course as part of my master‚Äôs at NYU and I‚Äôm brainstorming potential project ideas. I‚Äôm curious‚Äîif you were in my shoes, what kind of project would you take on?\n\nI‚Äôm aiming for something that‚Äôs not just academic, but also practical and relevant to industry (so it could carry weight outside the classroom too). Open to all directions‚Äîhealthcare, robotics, AR/VR, sports, finance, you name it. Guidance on benchmarking projects would be fantastic, too!\n\nWhat‚Äôs something you‚Äôd be excited to build, test, or explore?",
    "author": "eLuda",
    "timestamp": "2025-09-18T16:18:09",
    "url": "https://reddit.com/r/computervision/comments/1nknk5u/what_would_you_do_a_computer_vision_project_on/",
    "score": 17,
    "num_comments": 8,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nk417z",
    "title": "Gaze Tracker üëÅ",
    "content": "* üïπ Try out: [https://www.antal.ai/demo/gazetracker/demo.html](https://www.antal.ai/demo/gazetracker/demo.html)\n* üìñLearn more: [https://antal.ai/projects/gaze-tracker.html](https://antal.ai/projects/gaze-tracker.html)\n\nThis project is capable to estimate and visualize a person's gaze direction in camera images. I compiled the project using emscripten to webassembly, so you can try it out on my website in your browser. If you like the project, you can purchase it from my website. The entire project is written in C++ and depends solely on the opencv library. If you purchase you will you receive the complete source code, the related neural networks, and detailed documentation.",
    "author": "Gloomy_Recognition_4",
    "timestamp": "2025-09-18T02:53:56",
    "url": "https://reddit.com/r/computervision/comments/1nk417z/gaze_tracker/",
    "score": 123,
    "num_comments": 15,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nkzbkx",
    "title": "Paper resubmission",
    "content": "My paper got rejected in AAAI, reviews didn't make sense, whatever points they pointed out were already clearly explained in the paper, clearly they didn't read my paper properly. Just for info - It is a paper on one of the CV tasks.\n\nWhere do you think I should resubmit the paper - is TMLR a good option? I have no idea how it is viewed in the industry.. Can anyone please share their suggestion",
    "author": "Ok-Employ-4957",
    "timestamp": "2025-09-19T02:54:02",
    "url": "https://reddit.com/r/computervision/comments/1nkzbkx/paper_resubmission/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nkvh0w",
    "title": "[VoxelNet] [3D-Object-Detection] [PointCloud] Question about different voxel ranges and anchor sizes per class",
    "content": "I've been studying VoxelNet for point-cloud-based 3D object detection, and I ran into something that's confusing me.\n\nIn the implementation details, I noticed that they use different voxel ranges for different object categories. For example:\n\n- Car: Z, Y, X range = [-3, 1] x [-40, 40] x [0, 70.4]\n\n- Pedestrian / Cyclist: Z, Y, X range = [-3, 1] x [-20, 20] x [0, 48]\n\nSimilarly, they also use different anchor sizes for car detection vs. pedestrian/cyclist detection.\n\nMy question is:\n\n- We design only one model, and it needs a fixed voxel grid as input.\n\n- How are they choosing different voxel ranges for different categories if the grid must be fixed?\n\n- Are they running multiple voxelization pipelines per class, or using a shared backbone with class-specific heads?\n\nWould appreciate any clarification or pointers to papers / code where this is explained!\n\nThanks!",
    "author": "AsparagusBackground8",
    "timestamp": "2025-09-18T22:49:17",
    "url": "https://reddit.com/r/computervision/comments/1nkvh0w/voxelnet_3dobjectdetection_pointcloud_question/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nkp2tj",
    "title": "Introduction to BiRefNet",
    "content": "Introduction to BiRefNet\n\n[https://debuggercafe.com/introduction-to-birefnet/](https://debuggercafe.com/introduction-to-birefnet/)\n\nIn recent years, the need for high-resolution segmentation has increased. Starting from photo editing apps to medical image segmentation, the real-life use cases are non-trivial and important. In such cases, the quality of dichotomous segmentation maps is a necessity.¬†***The BiRefNet***¬†*segmentation*¬†*model solves exactly this*. In this article, we will cover an introduction to BiRefNet and how we can use it for¬†***high-resolution dichotomous segmentation***.\n\nhttps://preview.redd.it/zrzgru83n0qf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=d3b7f19f30513dac834dd22222336fe0a0b7a140\n\n",
    "author": "sovit-123",
    "timestamp": "2025-09-18T17:28:10",
    "url": "https://reddit.com/r/computervision/comments/1nkp2tj/introduction_to_birefnet/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nkuw7t",
    "title": "Camera Calibration Help",
    "content": "I am trying to calibrate the below camera using opencvs camera calibrate functionality. The issue is , it has 2 motors and they gave me a gui to adjust the zoom and focus on scale of 16 bits (0 to 65535) but I do not know the actual focal length. When I run the opencvs calibrateCamera method, my distortion coefficents k1,k2 are too large 173... smtg and even p1,p2 tangential distortion is large in negative. How do I verify these 2 matrices , as when I had used a normal webcam from zebronics, everything was getting calibrated properly and I got the desired results?\n\nC1 PRO X3 | Kurokesu https://share.google/XMaAk2eV9g2HDjz6q\n\nPS: I am sorry if this is a newbie question , but I have been recently shifted to cv department in our startup with me being the only one person in the department.",
    "author": "Robusttequilla007",
    "timestamp": "2025-09-18T22:15:32",
    "url": "https://reddit.com/r/computervision/comments/1nkuw7t/camera_calibration_help/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nl8d1k",
    "title": "Between computer Vision and data science,which one is good please ?",
    "content": "Between computer Vision and data science,which one is good please ?\n\nI was accepted in both masters . Now I am confused which one I should study especially regarding the job opportunities. Thank you \n\nYour advice is appreciated ",
    "author": "Quiet-Drawer-8896",
    "timestamp": "2025-09-19T09:31:48",
    "url": "https://reddit.com/r/computervision/comments/1nl8d1k/between_computer_vision_and_data_sciencewhich_one/",
    "score": 0,
    "num_comments": 11,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nkdhj4",
    "title": "Need help with Face detection project",
    "content": "Hi all, this semester I have a project about \"face detection\" in the course Digital image processing and computer vision. This is my first time doing something AI related so I don't know where to start (what steps should I do and what model should I use) so I really hope that u guys can show me how u would approach this problem. Thanks in advance.",
    "author": "Glass_Map5003",
    "timestamp": "2025-09-18T09:46:04",
    "url": "https://reddit.com/r/computervision/comments/1nkdhj4/need_help_with_face_detection_project/",
    "score": 9,
    "num_comments": 23,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nkj2bs",
    "title": "Automatic motion plot from videos",
    "content": "Hi everyone,\n\nI want to create motion plots like [this motorbike example](https://www.splung.com/kinematics/images/projectiles/motorbike-parabola.jpg)\n\nI‚Äôve recorded some videos of my robot experiments, but I need to make these plots for several of them, so doing it manually in an image editor isn‚Äôt practical. So far, with the help of a friend, I tried the following approach in Python/OpenCV:\n\n```\n\n       while ret:\n       # Read the next frame\n       ret, frame = cap.read()\n        \n        # Process every (frame_skip + 1)th frame\n        if frame_count % (frame_skip + 1) == 0:\n            # Convert current frame to float32 for precise computation\n            frame_float = frame.astype(np.float32)\n\n            # Compute absolute difference between current and previous frame\n            frame_diff = np.abs(frame_float - prev_frame)\n\n            # Create a motion mask where the difference exceeds the threshold\n            motion_mask = np.max(frame_diff, axis=2) &gt; motion_threshold\n\n            # Accumulate only the areas where motion is detected\n            accumulator += frame_float * motion_mask[..., None]\n            cnt += 1 * motion_mask[..., None]\n\n            # Normalize and display the accumulated result\n            motion_frame = accumulator / (cnt + 1e-4)\n\n            cv2.imshow('Motion Effect', motion_frame.astype(np.uint8))\n\n            # Update the previous frame\n            prev_frame = frame_float\n\n            # Break if 'q' is pressed\n            if cv2.waitKey(30) &amp; 0xFF == ord('q'):\n                break\n\n        frame_count += 1\n\n    # Normalize the final accumulated frame and save it\n    final_frame = (accumulator / (cnt + 1e-4)).astype(np.uint8)\n    cv2.imwrite('final_motion_image.png', final_frame)\n\nThis works to some extent, but the resulting plot is too ‚Äútransparent‚Äù. With [this video](https://drive.google.com/file/d/1XzlHOUiufd76ZPJNbH8qL-eJSuSjWc51/view?usp=sharing) I got [this image](https://drive.google.com/file/d/1f0-qITs04NFx7YiXC5FDS6mZj8JRF5RS/view?usp=sharing).\n\nDoes anyone know how to improve this code, or a better way to generate these motion plots automatically? Are there apps designed for this?",
    "author": "guarda-chuva",
    "timestamp": "2025-09-18T13:15:33",
    "url": "https://reddit.com/r/computervision/comments/1nkj2bs/automatic_motion_plot_from_videos/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nk1v8b",
    "title": "I still think about this a lot",
    "content": "One of the concepts that took my dumb ass an eternity to understand\n\nhttps://preview.redd.it/dueqvamnlvpf1.png?width=244&amp;format=png&amp;auto=webp&amp;s=ac26002b465abfbf55f308284726682a1f19cfcf\n\n",
    "author": "Pure_Long_3504",
    "timestamp": "2025-09-18T00:30:37",
    "url": "https://reddit.com/r/computervision/comments/1nk1v8b/i_still_think_about_this_a_lot/",
    "score": 19,
    "num_comments": 11,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nk8qeo",
    "title": "Help building a rotation/scale/tilt invariant ‚Äúfingerprint‚Äù from a reference image (pattern matching app idea)",
    "content": "Hey folks, I‚Äôm working on a side project and would love some guidance.\n\nI have a reference image of a pattern (example attached). The idea is to use a smartphone camera to take another picture of the same object and then compare the new image against the reference to check how much it matches.\n\nThink of it like fingerprint matching, but instead of fingerprints, it‚Äôs small circular bead-like structures arranged randomly.\n\n**What I need:**\n\n* Extract a \"fingerprint\" from the reference image.\n* Later, when a new image is captured (possibly rotated, tilted, or at a different scale), compare it to the reference.\n* Output a match score (e.g., 85% match).\n* The system should be robust to camera angle, lighting changes, etc.\n\n**What I‚Äôve looked into:**\n\n* ORB / SIFT / SURF for keypoint matching.\n* Homography estimation for alignment.\n* Perceptual hashing (but it fails under rotation).\n* CNN/Siamese networks (but maybe overkill for a first version).\n\n**Questions:**\n\n1. What‚Äôs the best way to create a ‚Äústable fingerprint‚Äù of the reference pattern?\n2. Should I stick to feature-based approaches (SIFT/ORB) or jump into deep learning?\n3. Any suggestions for quantifying similarity (distance metric, % match)?\n4. Are there existing projects/libraries I should look at before reinventing the wheel?\n\nThe end goal is to make this into a lightweight smartphone app that can validate whether a given seal/pattern matches the registered reference.\n\nWould love to hear how you‚Äôd approach this.",
    "author": "lemmescrewher",
    "timestamp": "2025-09-18T06:45:49",
    "url": "https://reddit.com/r/computervision/comments/1nk8qeo/help_building_a_rotationscaletilt_invariant/",
    "score": 4,
    "num_comments": 10,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nkb4cv",
    "title": "Looking for the most reliable AI model for product image moderation (watermarks, blur, text, etc.)",
    "content": "I run an e-commerce site and we‚Äôre using AI to check whether product images follow marketplace regulations. The checks include things like:\n\n\\- Matching and suggesting related category of the image\n\n\\- No watermark\n\n\\- No promotional/sales text like ‚ÄúHot sell‚Äù or ‚ÄúCall now‚Äù\n\n\\- No distracting background (hands, clutter etc.)\n\n\\- No blurry or pixelated images\n\nRight now, I‚Äôm using Gemini 2.5 Flash to handle both OCR and general image analysis. It works most of the time, but sometimes fails to catch subtle cases (like for pixelated images and blurry images).\n\nI‚Äôm looking for recommendations on models (open-source or closed source API-based) that are better at combined OCR + image compliance checking.\n\nDetect watermarks reliably (even faint ones)\n\nDistinguish between promotional text vs product/packaging text\n\nHandle blur/pixelation detection\n\nBe consistent across large batches of product images\n\nAny advice, benchmarks, or model suggestions would be awesome üôè",
    "author": "sub_hez",
    "timestamp": "2025-09-18T08:18:14",
    "url": "https://reddit.com/r/computervision/comments/1nkb4cv/looking_for_the_most_reliable_ai_model_for/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1njpky3",
    "title": "Built a tool that moves furniture",
    "content": "Been tinkering with segmentation and background removal. Here‚Äôs a demo where I captured my couch and dragged it across the room to see how it looks on the other side. Basically trying to ‚Äúre-arrange reality‚Äù with computer vision.\n\nJust wanted to share. Curious if anyone else here has played with object manipulation like this in a saas product?",
    "author": "w0nx",
    "timestamp": "2025-09-17T14:17:37",
    "url": "https://reddit.com/r/computervision/comments/1njpky3/built_a_tool_that_moves_furniture/",
    "score": 75,
    "num_comments": 15,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nj8l1d",
    "title": "Computer Vison Prototypes üëÅ",
    "content": "I‚Äôm Antal Zsiros, a senior computer vision specialist. Through my website,¬†[antal.ai](http://antal.ai/), I sell my personal side projects which are professionally-built prototypes for computer vision applications, designed to save you from the costly process of building from scratch.\n\nAll solutions are coded purely in C++ using OpenCV for maximum efficiency. Every purchase includes the complete source code, detailed documentation, and build guides.\n\nYou can test every solution instantly in your browser to evaluate its capabilities and ensure it fits your needs before you buy:¬†[https://www.antal.ai/demo.html](https://www.antal.ai/demo.html)",
    "author": "Gloomy_Recognition_4",
    "timestamp": "2025-09-17T02:38:56",
    "url": "https://reddit.com/r/computervision/comments/1nj8l1d/computer_vison_prototypes/",
    "score": 354,
    "num_comments": 18,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nkefoq",
    "title": "OCR Database Resources?",
    "content": "Hello,\n\nDoes anyone have any good resources they could point me towards to learn more about reading and writing OCR data?\n\nI'm a software engineer who is hopefully going to be working on a team that does a lot of OCR  processing soon. I was hoping to learn more about the way that the data is stored/accessed, but I'm struggling to find some good resources discussing the pros and cons of storing OCR data in SQL vs. NoSQL, or whether its better to use Geospatial databases like PostGIS etc. etc.",
    "author": "whatacad",
    "timestamp": "2025-09-18T10:21:14",
    "url": "https://reddit.com/r/computervision/comments/1nkefoq/ocr_database_resources/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nk11zh",
    "title": "Few-shot learning with pre-trained YOLO",
    "content": "Hi,\n\nI have trained a Ultralytics YOLO detector on a relatively large dataset. \n\nI would like to run the detector on a slightly different dataset, where only a small number of labels is available. The dataset is from the same domain, as the large dataset.\n\nSo this sounds like a few-shot learning problem, with a given feature extractor.\n\nNaturally, I've tried freezing most of the weights of the pre-trained detector and it didn't work too well...\n\nAny other suggestions? Anything specific to Ultralytics YOLO perhaps? I'm using YOLO11...",
    "author": "Ok_Pie3284",
    "timestamp": "2025-09-17T23:39:45",
    "url": "https://reddit.com/r/computervision/comments/1nk11zh/fewshot_learning_with_pretrained_yolo/",
    "score": 5,
    "num_comments": 7,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nk4jyr",
    "title": "Is the current SOTA VLM Gemini 2.5 Pro? Or are there better open source options?",
    "content": "Is the current SOTA VLM Gemini 2.5 Pro? Or are there better open source options?",
    "author": "LivingMNML",
    "timestamp": "2025-09-18T03:25:03",
    "url": "https://reddit.com/r/computervision/comments/1nk4jyr/is_the_current_sota_vlm_gemini_25_pro_or_are/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nk4j4v",
    "title": "Is fine-tuning a VLM just like fine-tuning any other model?",
    "content": "I am new to computer vision and building an app that gets sports highlights from videos. The accuracy of Gemini 2.5 Flash is ok but I would like to make it even better. Does fine-tuning a VLM work just like fine-tuning any other model?",
    "author": "LivingMNML",
    "timestamp": "2025-09-18T03:23:41",
    "url": "https://reddit.com/r/computervision/comments/1nk4j4v/is_finetuning_a_vlm_just_like_finetuning_any/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1njxfif",
    "title": "Free or inexpensive bounding box video tool",
    "content": "Hey all, I‚Äôm looking for an ideally free tool that will add bounding boxes around objects I select in a video I input. I‚Äôm an artist and am curious about using the bounding boxes as part of a project. Any insights are helpful!",
    "author": "thedavidweaver",
    "timestamp": "2025-09-17T20:12:39",
    "url": "https://reddit.com/r/computervision/comments/1njxfif/free_or_inexpensive_bounding_box_video_tool/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1njezfd",
    "title": "This AI Hunts Grunts in Deep Rock Galactic",
    "content": "I used Machine learning to train Yolov9 to Track Grunts in Deep Rock Galactic.  \nI haven't hooked up any targeting code but I had a bunch of fun making this!",
    "author": "SpoodlyPoofs",
    "timestamp": "2025-09-17T07:40:18",
    "url": "https://reddit.com/r/computervision/comments/1njezfd/this_ai_hunts_grunts_in_deep_rock_galactic/",
    "score": 11,
    "num_comments": 1,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1njlok4",
    "title": "SOTA pose estimator",
    "content": "Hi guys,\n\nWhat would you say is SOTA human pose/skeleton estimator for 2D images of people right now?",
    "author": "Ok_Shoulder_83",
    "timestamp": "2025-09-17T11:46:40",
    "url": "https://reddit.com/r/computervision/comments/1njlok4/sota_pose_estimator/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1njt1o9",
    "title": "Question for the CV experts.",
    "content": "I have this idea for an ai estimating quote for the skilled trades. In my mind it would generate real time quotes say for like interior painting or flooring from pictures or video. Can this realistically be done? What about more complicated trades like plumbing, how would you approach this problem? How big would the models have to be, data etc? Thanks for any insight. ",
    "author": "CriticalCommand6115",
    "timestamp": "2025-09-17T16:43:49",
    "url": "https://reddit.com/r/computervision/comments/1njt1o9/question_for_the_cv_experts/",
    "score": 0,
    "num_comments": 11,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nj9u1x",
    "title": "How to Clean Up a French Book?",
    "content": "Theres a famous French course from back in the day. [Le FrancÃßais Par La MeÃÅthode Nature](https://archive.org/details/jensen-arthur-le-francais-par-la-methode-nature/page/n3/mode/2up)\n\nby¬†[Arthur Jensen](https://archive.org/search.php?query=creator%3A%22Arthur+Jensen%22). There's audiobook versions of it [made online still as it is so popular.](https://www.youtube.com/watch?v=0uS5WSeH8iM&amp;list=PLf8XN5kNFkhdIS7NMcdUdxibD1UyzNFTP) \n\nIt is pretty regular. Odd number lines French. Even number lines the pronunciation guide.   \nNew words in a margin in odd numbered pages on the left on the right on even numbered pages. Images in the margin that go right up to the margin line. Occasional big line images in the main text.\n\nThe problem is the existing versions have a photocopy looking text. And they include the pronunciation guide that is not needed now the audio is easy to get. Also these doubles+ the size of the text to be print out. How would you remove the pronunciation lines, rewrite the french text to make it look like properly typed words. And recombine the result into a shorter book?\n\nI have tried Label Studio to mark up the images, margin and main but its time consuming and the combine these back into a book that looks pretty much the same but is shorter i cannot get to look right. \n\nAny suggestions for tools or similar projects you did would be really interesting. Normal pdf extraction of text works but it mixes up margin and main text and freaks out about the pronunciation lines.\n\n",
    "author": "cavedave",
    "timestamp": "2025-09-17T03:52:27",
    "url": "https://reddit.com/r/computervision/comments/1nj9u1x/how_to_clean_up_a_french_book/",
    "score": 7,
    "num_comments": 13,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1njcc6s",
    "title": "How to detect eye blink and occlusion in Mediapipe?",
    "content": "I'm trying to develop a mobile application using Google Mediapipe (Face Landmark Detection Model). The idea is to detect the face of the human and prove the liveliness by blinking twice. However, I'm unable to do so and stuck for the last 7 days. I tried following things so far:\n\n* I extract landmark values for open vs. closed eyes and check the difference. If the change crosses a threshold twice, liveness is confirmed.\n* For occlusion checks, I measure distances between jawline, lips, and nose landmarks. If it crosses a threshold, occlusion detected.\n* I also need to ensure the user isn‚Äôt wearing glasses, but detecting that via landmarks hasn‚Äôt been reliable, especially with rimless glasses.\n\nthis ‚Äúlandmark math‚Äù approach isn‚Äôt giving consistent results, and I‚Äôm new to ML. Since the solution needs to run **on-device** for speed and better UX, Mediapipe seemed the right choice, but I‚Äôm getting failed consistently.\n\nCan anyone please help me how can I accomplish this?",
    "author": "abhijee00",
    "timestamp": "2025-09-17T05:53:33",
    "url": "https://reddit.com/r/computervision/comments/1njcc6s/how_to_detect_eye_blink_and_occlusion_in_mediapipe/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1njhkw3",
    "title": "Need help regarding a project using Jetson nano orin",
    "content": "Hi all,\n\n1. I need to perform object detection from a height of a 12 feet in a square area which is 15x15feet. \n2. I'll have to install 6 camera 4 at each vertex and 2 in between. \n3. Jetson orin will be placed in between and max distance of any camera will be approx 12 to 15 feet from orin.\n4. The data of object detection needs to be sent to PLC (allen bradley)  from Orin.\n5. Ill be using this [Carrier Board](https://robu.in/product/a205e-carrier-board-for-nvidia-jetson-nano-xavier-nx-tx2-nx-module/)\n\nAll in all these are the only requirements. My issues are :-\n\n1. Shall I go for USB cameras and connect them all to an external USB hub to Jetson board USB port? Or any other camera ? [HUB1](https://zbotic.in/product/waveshare-industrial-grade-usb-hub-extending-4x-usb-2-0-ports/?gad_source=4&amp;gad_campaignid=20903026778&amp;gbraid=0AAAAACTNRdU_1m-h6qb3e9zgA9vpASRyA&amp;gclid=Cj0KCQjwuKnGBhD5ARIsAD19Rsb6kcU7HjDJ1gIyJN_GxkCGPdIXvKcuWfOtKMVVn_Pqh_idCabXVd4aAqXTEALw_wcB) [HUB2](https://robu.in/product/waveshare-industrial-grade-usb-hub-extending-7x-usb-2-0-ports/?gad_source=1&amp;gad_campaignid=17427802559&amp;gbraid=0AAAAADvLFWeCTF5qpSWsJtBawEGj2t2IP&amp;gclid=Cj0KCQjwuKnGBhD5ARIsAD19RsbRKFWL2NrOLzYc1yj-IdCU6wINfiO-R2Vq_YN1KBUnnv7EpgtgoVsaApnFEALw_wcB)\n2. Will USB camera be good enough for 12 to 15 feet transmission or shall I go for Gige cameras. If Gige then how will I connect 6 cams to orin ?  \n\n",
    "author": "Careful_Island_2120",
    "timestamp": "2025-09-17T09:16:42",
    "url": "https://reddit.com/r/computervision/comments/1njhkw3/need_help_regarding_a_project_using_jetson_nano/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1njhjdv",
    "title": "Gestures controlling robotic hand and LEDs with computer vision using OpenCV and Mediapipe python AI libraries connection to Raspberry Pi Pico",
    "content": "My webcam delivers video images of my hand to a Python code using OpenCV and Mediapipe AI libraries. The code sends an array of 5 integer values for the states of each finger (up or down) to the serial port of a Raspberry Pi Pico.\n\nA Micropython script receives array values for my Raspberry Pi Pico and activates 5 servo motors that move the corresponding fingers to an up or down position. It also activates any of 5 LEDs corresponding to the fingers raised.\n\nAll source code is provided at my GitHub repo: [Python and Micropython codes](https://github.com/hib-1/OCV_to_Robot_hand)\n\n video:   [Youtube video](https://youtu.be/E0OxI8d-kKI)\n\nhttps://preview.redd.it/rfjmf04p1rpf1.jpg?width=871&amp;format=pjpg&amp;auto=webp&amp;s=749ef4b6daddcc9e1933370d1895c44ff7184de2\n\n",
    "author": "hred2",
    "timestamp": "2025-09-17T09:15:11",
    "url": "https://reddit.com/r/computervision/comments/1njhjdv/gestures_controlling_robotic_hand_and_leds_with/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1njbnbg",
    "title": "Impact of near-duplicate samples for datasets from video",
    "content": "Hey folks!\n\nI have some relatively static Full-Motion-Videos that I‚Äôm looking to generate a dataset out of. Even if I extract every N frames, there are a lot of near duplicates since the videos are temporally continuous.\n\nOn the one hand, ‚Äúmore data is better‚Äù so I could just use all of the frames, but inspecting the data it really seems like I could use less than 20% of the frames and still capture all the information because there isn‚Äôt a ton of variation. I also feel like I could just train longer with the smaller, but still representative data to achieve the same affect as using the whole dataset anyways, especially with good augmentation?\n\nWondering if anyone has theoretical &amp; quantitative knowledge about how adjusting the dataset size in this setting affects model performance. I‚Äôd appreciate if you guys could share insight into this issue!",
    "author": "askiiikl",
    "timestamp": "2025-09-17T05:23:03",
    "url": "https://reddit.com/r/computervision/comments/1njbnbg/impact_of_nearduplicate_samples_for_datasets_from/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nin6qk",
    "title": "What optimizer are you guys using in 2025",
    "content": "So both for work and research for standard tasks like classification, action recognition, semantic segmentation, object detection...\n\nI've been using the adamw optimizer with light weight decay and a cosine annealing schedule with warmup epochs to the base learning rate.\n\nI'm wondering for any deep learning gurus out there have you found anything more modern that can give me faster convergence speed? Just thought I'd check in with the hive mind to see if this is worth investigating.",
    "author": "Relative_Goal_9640",
    "timestamp": "2025-09-16T10:02:27",
    "url": "https://reddit.com/r/computervision/comments/1nin6qk/what_optimizer_are_you_guys_using_in_2025/",
    "score": 43,
    "num_comments": 21,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1njbbn0",
    "title": "How to annotate big objects for object detection",
    "content": "https://preview.redd.it/nop3lfocuppf1.png?width=462&amp;format=png&amp;auto=webp&amp;s=445d0d2b0b899c23b964d076ff28e931bfdc40d8\n\nHi everyone, I want to train a model on detection scaffolding ( and i want it to be precise enough because i would need exact areas of it and where it's missing )\n\nhere Boxes seem inefficient because the scaffolding is in the whole image sometimes as you see here, and segmentation seems to expensive to manually create. Do you have any ideas at all, any suggestions please?\n\nfor now I plan to manully annotate some segmentations, then train a preliminary model, use it to segment the rest, manually correct its segmentations etc .. ( even this seems complicated does anyone know if correcting segmentations using roboflow is as easy as correcting boxes? )\n\nthanks in advance",
    "author": "Ok_Shoulder_83",
    "timestamp": "2025-09-17T05:07:59",
    "url": "https://reddit.com/r/computervision/comments/1njbbn0/how_to_annotate_big_objects_for_object_detection/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1njb85e",
    "title": "how to annote for yolo",
    "content": "https://preview.redd.it/wpb5c59ysppf1.png?width=2041&amp;format=png&amp;auto=webp&amp;s=1bbdbf5097d49fc12a9eb154d01befea2f957ae3\n\nHello, im trying to calculate measurement of the \"channels\" in the picture. I tride to annote but i couldnt do it properly i guess because i get many wrong outputs.\n\nIn the picture you will see yellow lines  between top and bottom of the waves. I drawed it myself from opencv but i need to do it from yolo.  All 4 lines should be approximately same px so even 1 or 2 correct  line should be fine for me. Does anyone has any idea about how to annote these channels? Can you show me? ",
    "author": "Business-Advance-306",
    "timestamp": "2025-09-17T05:03:15",
    "url": "https://reddit.com/r/computervision/comments/1njb85e/how_to_annote_for_yolo/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nireqa",
    "title": "P PSI: New Stanford paper on world models with zero-shot depth &amp; segmentation",
    "content": "Just saw this new paper from Stanford‚Äôs SNAIL Lab:  \n[https://arxiv.org/abs/2509.09737](https://arxiv.org/abs/2509.09737?utm_source=chatgpt.com)\n\nThey propose Probabilistic Structure Integration (PSI), a world model architecture that doesn‚Äôt just use RGB frames, but also extracts and integrates depth, motion, flow, and segmentation as part of the token stream.\n\n\n\nhttps://preview.redd.it/4llp8hzaxkpf1.png?width=1652&amp;format=png&amp;auto=webp&amp;s=16c7a770e78b5065595958812e9d13ae743b06a5\n\nKey results that seem relevant for CV:\n\n* Zero-shot depth + segmentation ‚Üí without training specifically on those tasks\n* Multiple plausible rollouts (probabilistic predictions vs deterministic)\n* More efficient than diffusion-based world models on long-term forecasting tasks\n* Continuous training loop that incorporates causal inference\n\nFeels like an interesting step toward ‚Äústructured token‚Äù models for video/scene understanding. Curious to hear thoughts from this community - is this a promising direction for CV, or still mostly academic at this stage?",
    "author": "Appropriate-Web2517",
    "timestamp": "2025-09-16T12:36:05",
    "url": "https://reddit.com/r/computervision/comments/1nireqa/p_psi_new_stanford_paper_on_world_models_with/",
    "score": 19,
    "num_comments": 2,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nj8v6j",
    "title": "Doubts about KerasCV",
    "content": "Is it possible to prune or int8 quantize models trained through keras\\_cv library? as far as i know it has poor compatibility with tensorflow model optimization toolkit and has its own custom defined layers. Did anyone try it before?",
    "author": "iz_bleep",
    "timestamp": "2025-09-17T02:56:31",
    "url": "https://reddit.com/r/computervision/comments/1nj8v6j/doubts_about_kerascv/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nida0i",
    "title": "Started revising core cv",
    "content": "using the following lectures to revise core computer vision algorithms and other topics.\n\nfollow me on X: [https://x.com/habibtwt\\_](https://x.com/habibtwt_)",
    "author": "Pure_Long_3504",
    "timestamp": "2025-09-16T03:01:11",
    "url": "https://reddit.com/r/computervision/comments/1nida0i/started_revising_core_cv/",
    "score": 52,
    "num_comments": 3,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nj39ao",
    "title": "[D] How is IEEE TIP viewed in the CV/AI/ML community?",
    "content": "",
    "author": "Secondhanded_PhD",
    "timestamp": "2025-09-16T21:11:37",
    "url": "https://reddit.com/r/computervision/comments/1nj39ao/d_how_is_ieee_tip_viewed_in_the_cvaiml_community/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1niscs2",
    "title": "[P] I build a completely free website to help patients to get secondary opinion on mammogram, loading AI model inside browser and completely local inference without data transfer. Optional LLM-based radiology report generation if needed.",
    "content": "",
    "author": "coolwulf",
    "timestamp": "2025-09-16T13:11:22",
    "url": "https://reddit.com/r/computervision/comments/1niscs2/p_i_build_a_completely_free_website_to_help/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nigjwd",
    "title": "What transformer based model should I use for 2D industrial objects? (Segmentation task)",
    "content": "So, this is a follow up to my questions for my Bachelor Thesis, in which I compare a few models for the segmentation of industrial objects, like screwdrivers. I already labeled all my data with segmentation masks(SAM2 and YOLOv11) and in parallel also built a strong YOLOv11 Model as CNN centric model. I will also take in YOOv12 as a hybrid between CNN an Transformer and I will maybe see how good DINOv3 is as a newer model(not necessary, just a nice to have).\n\nNow the question is which model I should add as a Transformer based model, I thought about DETR but I often see that it is mostly for detection, not for segmentation. What are some state of the art models right now for Transformer based models? \n\nThe model must also be loaded onto a NVIDIA Jetson Orin and work well with the OAK-D Camera, because the model will be working on a robotic arm.\n\nThankful for every help I get, If you need any more information, let me know. I will try to answer it. There could also be a few informations on my previous post, maybe that can help-",
    "author": "FragrantPassenger891",
    "timestamp": "2025-09-16T05:49:34",
    "url": "https://reddit.com/r/computervision/comments/1nigjwd/what_transformer_based_model_should_i_use_for_2d/",
    "score": 8,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nim8t7",
    "title": "SGS-1: AI foundation model for creating 3D CAD geometry from image/text",
    "content": "",
    "author": "New_Frosting_39",
    "timestamp": "2025-09-16T09:27:24",
    "url": "https://reddit.com/r/computervision/comments/1nim8t7/sgs1_ai_foundation_model_for_creating_3d_cad/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ni86ih",
    "title": "RF-DETR to pick the perfect avocado",
    "content": "I‚Äôm working on a personal project to help people pick the right avocados.\n\nA little backstory: I grew up on an avocado ranch, and every time I go to the store, it makes me a bit sad to see people squeezing avocados just to guess if they‚Äôre ready to eat.\n\nSo I decided to build a simple app: you take a picture of the avocado you‚Äôre thinking of buying, and it tells you whether it‚Äôs ripe, almost ripe, or overripe.\n\nI‚Äôm using Roboflow‚Äôs RF-DETR model, fine-tuned with some data I already have. Then I‚Äôll take it a step further and supervised fine-tune the model with images of avocados at different ripeness stages, using my knowledge from growing up around them.\n\nWould you use something like this? I think it could be super helpful for making the *perfect* guacamole!",
    "author": "Accomplished_Zone_47",
    "timestamp": "2025-09-15T21:43:17",
    "url": "https://reddit.com/r/computervision/comments/1ni86ih/rfdetr_to_pick_the_perfect_avocado/",
    "score": 8,
    "num_comments": 13,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nikazm",
    "title": "COCO Polygon Orientation Convention: CCW=External, CW=Holes? Need clarification for DETR training",
    "content": "Hey r/computervision!\n\nThis might be the silliest of the silliest question but I am getting nuts. I have seen in a couple of repos and coco datasets that objectw polygons are segmented as clockwise (see https://github.com/cocodataset/cocoapi/issues/153). This is mostly a non-issue, particularly with simple objects. The matter become more complex when dealing with occluded objects or objects with holes. Unfortunately, the dataset I am dealing with has both (sad), see a previous post that I opened here: https://www.reddit.com/r/computervision/comments/1meqpd2/instance\\_segmentation\\_nightmare\\_2700x2700\\_images/.\n\nNow, I managed to manually annotate images in a way that each object is an integer on the image. This way, the image encoded discontinued objects by just having the same number. The issue comes when conversting the dataset to COCO for training (I am aiming to use DETR or similar). Here, when I use libraries such as shapely/scykit-image I get that positive boundaries are counter-clockwise and holes are clockwise. I just want to know if I need to revert those guys for training and to visualise with any standard library. I have enclosed a dummy image with few polygons and the orientations that I get in order to illustrate my point.\n\n  \nAgain, this might be super silly, but given the fact that I am new here, I just want to clarify and get the thing correct from the beginning.\n\n# Obj ID  Expected                        Skimage Class                   Shapely Class                   Orientation Pattern\n\n2       two\\_disconnected\\_circles        two\\_circles             two\\_circles             \\[ccw, ccw\\] / \\[ccw, ccw\\]   \n5       two\\_circles\\_one\\_with\\_hole       1\\_ext\\_2\\_holes           1\\_ext\\_2\\_holes           \\[ccw, ccw, cw\\] / \\[ccw, ccw, cw\\]   \n6       circle\\_with\\_hole        circle\\_with\\_hole        circle\\_with\\_hole        \\[ccw, cw\\] / \\[ccw, cw\\]\n\nhttps://preview.redd.it/oh9vqtxnmjpf1.png?width=128&amp;format=png&amp;auto=webp&amp;s=00cb02b59b2ba580001bac9f9b77118b2faceb53\n\n",
    "author": "Unable_Huckleberry75",
    "timestamp": "2025-09-16T08:15:25",
    "url": "https://reddit.com/r/computervision/comments/1nikazm/coco_polygon_orientation_convention_ccwexternal/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nik527",
    "title": "How to use BoT-SORT tracking model with my own detection model ?",
    "content": "I am developing an object tracking application. I am using RT-DETR from Hugging Face, and I would like to add object tracking functionality to it. The problem is that I am encountering various errors when attempting to clone and build the GitHub repository. This is the link to the GitHub repo I am using: [https://github.com/NirAharon/BoT-SORT?tab=readme-ov-file](https://github.com/NirAharon/BoT-SORT?tab=readme-ov-file) \n\nThe dependencies required to build it seem very old. I created a Python virtual environment for it using Python 3.8 on Ubuntu 24.04. However I am still getting many errors like when I am running \"python3 [setup.py](http://setup.py) develop\", I am getting these kinds of errors \n\nhttps://preview.redd.it/dtecs9qgljpf1.png?width=3832&amp;format=png&amp;auto=webp&amp;s=b141b223ce6645ae4243fc40e10aeef600f92203\n\nI don't know what I am doing is wrong, I am using the exact dependencies they recommended. the only difference I see on their github repo that they were using ubuntu 20 but I am using Ubuntu 24. is there any idea on how to use BoT-SORT with my detection model ?",
    "author": "abdosalm",
    "timestamp": "2025-09-16T08:09:21",
    "url": "https://reddit.com/r/computervision/comments/1nik527/how_to_use_botsort_tracking_model_with_my_own/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nij60b",
    "title": "Serious CV challange",
    "content": "Hello, dear friends. Can u please provide any advice or suggestions on the following topic. I am currently making a model that will generate ionogramm from it's metadata. Basiclly meta to image task. I have pairs of meta + ionogramm and want to create a generative model so it can generate ionogramms based on different metadata. The goal is to correct empirical mathematical models.\n\nThere are 2 problems: architecture and loss function.\n\nThe first idea i came up with was unet-like model. Encoder replaced with couple of MLPs. And basic decoder.  \nWith loss function it's a lot more complicated. MSE/MAE and Chairboneir ain't good. Because data containing pixels is about 1-2%. SSIM as well. Need something that enforces 1 to 1 match with detail to particles i guess.\n\nIonogramm example: [https://imgur.com/a/dstI40c](https://imgur.com/a/dstI40c)",
    "author": "polina_snickers",
    "timestamp": "2025-09-16T07:33:03",
    "url": "https://reddit.com/r/computervision/comments/1nij60b/serious_cv_challange/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ni1yus",
    "title": "Tech demo video for my visual design &amp; mockup platform",
    "content": "This is part of a side project I‚Äôm building called Canvi.\n\nOn just your phone, you can capture real objects and move them around in your environment for mockups, visualizing designs, landscaping, interior design, art, or just having fun.\n\nI'm early in my project but having a ton of fun.\n\nWhat kinds of things you would want to use it for IRL?",
    "author": "w0nx",
    "timestamp": "2025-09-15T16:42:42",
    "url": "https://reddit.com/r/computervision/comments/1ni1yus/tech_demo_video_for_my_visual_design_mockup/",
    "score": 13,
    "num_comments": 7,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ni9f5h",
    "title": "Returning to CV. Last time, lacking a lot of depth (went too wide). Need advice",
    "content": "Last time i worked on computer vision, i touched too many subjects (object detection + tracking, Re-ID, segmentation, pose detection, face spoofing detection, etc) due to my position mostly developing quick prototypes for PoC. Now that I have time, I want to get back to CV before making further career decisions. \n\nI have basic / quite shallow understanding of:\n\n\\- CNNs and Object Detectors (I have followed CS231n and read a lot of papers of object detection models back in the day)\n\n\\- Using Pytorch / TF to implement custom models, basic training techniques\n\n\\- Image Processing and classical CV algos (I have taken a computer vision class in college but i forgot nearly everything at this point)\n\n\\- Transformers and how they work\n\nRight now Im interested in the following:\n\n\\- CV for robotics \n\n\\- Building on top of foundational models (DINOv2, SAM2) etc to create custom solutions with limited dataset, mostly for video analysis\n\n\\-  Brushing up my understanding of Image Processing techniques and Classical CV algo (and their \"modern\" DL-based counterparts)\n\n\\- Also a bit of geospatial analysis\n\n  \nI have done my research using gemini deep research / qwen deep research to create a rough mapping of what i need to learn. I also have read up (manually) on survey / review papers that i can find on the topics above. But I do want to seek advice directly from professionals in the field.\n\n  \nIn the year 2025, for someone returning to computer vision whose last time is before the days of pre-vision transformers, what advice can you give? Forgive me if I'm a bit unclear, I'm quite lost myself actually looking at the sheer amount of catching up i will need to do\n\n  \nThanks in Advance!",
    "author": "[deleted]",
    "timestamp": "2025-09-15T22:54:36",
    "url": "https://reddit.com/r/computervision/comments/1ni9f5h/returning_to_cv_last_time_lacking_a_lot_of_depth/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nhl5tq",
    "title": "Real time computer vision on mobile",
    "content": "Hello there, I wrote a small post on building real time computer vision apps. I would have gained a lot of time by finding info before I got on that field, so I decided to write a bit about it.\n\nI'd love to get feedback, or to find people working in the same field!\n\n",
    "author": "Far-Personality4791",
    "timestamp": "2025-09-15T05:59:14",
    "url": "https://reddit.com/r/computervision/comments/1nhl5tq/real_time_computer_vision_on_mobile/",
    "score": 51,
    "num_comments": 11,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1niaqpe",
    "title": "Feedback needed ‚Äì what am I missing?",
    "content": "",
    "author": "ZucchiniOrdinary2733",
    "timestamp": "2025-09-16T00:16:47",
    "url": "https://reddit.com/r/computervision/comments/1niaqpe/feedback_needed_what_am_i_missing/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nhrdhd",
    "title": "NVIDIA AI Open-Sources ViPE (Video Pose Engine): A Powerful and Versatile 3D Video Annotation Tool for Spatial AI",
    "content": "",
    "author": "ai-lover",
    "timestamp": "2025-09-15T09:56:46",
    "url": "https://reddit.com/r/computervision/comments/1nhrdhd/nvidia_ai_opensources_vipe_video_pose_engine_a/",
    "score": 7,
    "num_comments": 3,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ni1s83",
    "title": "Do you use a business specific framework?",
    "content": "I‚Äôm struggling with formulating this question, but the concept I‚Äôm looking to discuss is whether it makes sense to closely couple CV processes with the business‚Äôs systems, or to keep them more independent. \n\nI‚Äôm in manufacturing and one thing I use CV for is product inspection, where the goal is to flag products that are likely to be rejected by the customer. In a closely coupled system I would train a model on a set of ‚Äúcustomer order IDs‚Äù (the goal being to infer which orders get returned) and the framework would automatically gather the images from our database and feed them into PyTorch or whatever. OTOH in a loosely coupled system I would train the model directly on the images. \n\nIn the later scenario I can easily switch between model training frameworks (for example timm includes a nice script for training classification models), but in the former I have to think less about the peculiarities of our business data. \n\nAny thoughts on this? How do you personally operate? \n\n",
    "author": "InternationalMany6",
    "timestamp": "2025-09-15T16:34:41",
    "url": "https://reddit.com/r/computervision/comments/1ni1s83/do_you_use_a_business_specific_framework/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ni3hz9",
    "title": "Are these the same image?",
    "content": "Spoiler **Alert: Yes - see how broken AI and Hashing can be in:**   [**Weaponized False Positives: How Poisoned Datasets Could Erase Researchers Overnigh**](https://medium.com/@russoatlarge_93541/weaponized-false-positives-how-poisoned-datasets-could-erase-researchers-overnight-188810395602)**t**\n\nhttps://preview.redd.it/sv9xk8uybfpf1.jpg?width=1848&amp;format=pjpg&amp;auto=webp&amp;s=e736e04449ebb0500ab6de22753aff2ed8202c0f\n\n",
    "author": "markatlarge",
    "timestamp": "2025-09-15T17:52:17",
    "url": "https://reddit.com/r/computervision/comments/1ni3hz9/are_these_the_same_image/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nia6fr",
    "title": "I‚Äôm in my first AI/ML job‚Ä¶ but here‚Äôs the twist: no mentor, no team. Seniors, guide me like your younger brother üôè",
    "content": "When I imagined my first AI/ML job, I thought it would be like the movies‚Äîsurrounded by brilliant teammates, mentors guiding me, late-night brainstorming sessions, the works.\n\nThe reality? I do have work to do, but outside of that, I‚Äôm on my own. No team. No mentor. No one telling me if I‚Äôm running in the right direction or just spinning in circles.\n\nThat‚Äôs the scary part: I could spend months learning things that don‚Äôt even matter in the real world. And the one thing I don‚Äôt want to waste right now is time.\n\nSo here I am, asking for help. I don‚Äôt want generic ‚Äúkeep learning‚Äù advice. I want the kind of raw, unfiltered truth you‚Äôd tell your younger brother if he came to you and said:\n\n&gt; ‚ÄúBro, I want to be so good at this that in a few years, companies come chasing me. I want to be irreplaceable, not because of ego, but because I‚Äôve made myself truly valuable. What should I really do?‚Äù\n\n\n\nIf you were me right now, with some free time outside work, what exactly would you:\n\nLearn deeply?\n\nIgnore as hype?\n\nBuild to stand out?\n\nFocus on for the next 2‚Äì3 years?\n\n\nI‚Äôll treat your words like gold. Please don‚Äôt hold back‚Äîtalk to me like family. üôè\n",
    "author": "Forex_Trader2001",
    "timestamp": "2025-09-15T23:40:57",
    "url": "https://reddit.com/r/computervision/comments/1nia6fr/im_in_my_first_aiml_job_but_heres_the_twist_no/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ni9aku",
    "title": "Help identify license plate involved in hit &amp; run.",
    "content": "I was involved in a hit and run yesterday morning, and have been trying to decode the only blurry photo I was able to get.\n\nIt was a California license plate, so either #XXX### or ###XXX# (#= number, X = letter). Been inputting my guesses into O'Reilly's license plate search, but so far no matches for a Chevrolet. I've tried:\n\n* 99 \\_ BSS2 - #0-9\n* 99\\_ RSS2 - #0-9\n* 9A\\_B552 - All letters in alphabet\n* and lots of initial guesses that I didn't track..\n\nHoping some of you can mess with the contrast or something and get less of a blur.\n\nThanks in advance!!",
    "author": "vzlan",
    "timestamp": "2025-09-15T22:47:00",
    "url": "https://reddit.com/r/computervision/comments/1ni9aku/help_identify_license_plate_involved_in_hit_run/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.39,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nhv7z7",
    "title": "Latest trends in Anomaly Detection in Video Processing",
    "content": "Hello, \n\nI am working on anomaly detection in video processing specifically real-time violence and theft detection and I wanted to know what are the latest trends there and what is the latest research I should look into?",
    "author": "emocakeleft",
    "timestamp": "2025-09-15T12:16:52",
    "url": "https://reddit.com/r/computervision/comments/1nhv7z7/latest_trends_in_anomaly_detection_in_video/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nh8mgn",
    "title": "How to prepare for System Design CV interviews",
    "content": "I have some upcoming interviews for perception roles at robotics companies as a new-grad (currently have a BASc) and was wondering what I can do to prepare for rounds that might ask questions pertaining to system design.\n\nI never studied any form of systems design and don't know where to start to be most efficient with my time before the interview. Like is there a distinction between systems design for regular SWE vs. perception roles (and for robotics CV roles if that distinction between them needs to be made)? If so, should I just study the perception variant (to save time) or is it that important to study regular SWE systems design content.\n\nAre there any free online resources that covers these topics that I can study as a complete noob to this? (I am tight on budget at the moment)",
    "author": "jawyau",
    "timestamp": "2025-09-14T18:19:57",
    "url": "https://reddit.com/r/computervision/comments/1nh8mgn/how_to_prepare_for_system_design_cv_interviews/",
    "score": 21,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nhaw8l",
    "title": "Ideas for an F1 project ?",
    "content": "Hi everyone,\n\nI‚Äôm looking to do a project that combines F1 with deep learning and computer vision. I‚Äôm still a student, so I‚Äôm not expecting to reinvent the wheel, but I‚Äôd love to hear what kind of problems or applications you think would make interesting projects.  \nWould love to hear your thoughts ! Thanks in advance !",
    "author": "Zestyclose-Glass459",
    "timestamp": "2025-09-14T20:10:14",
    "url": "https://reddit.com/r/computervision/comments/1nhaw8l/ideas_for_an_f1_project/",
    "score": 7,
    "num_comments": 7,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nhxzdg",
    "title": "I am working on a dataset converter",
    "content": "Hello everyone, it's been a while since I last participate here, but this time I want to share a project I'm working on.\n\nIt's a dataset format converter to prepare them for artificial intelligence model training. Currently, I only have conversion from LabelMe to YoloV8/V11 formats, which are the ones I've always worked with. Here's the link:\nhttps://datasetconverter.toasternerd.dev/\n\nMy goal in sharing this with you is that I need to test it with real people. On the page, there's a ‚Äúfree trial‚Äù that allows a LabelMe format dataset of up to 5MB, and then further down there are different ‚Äúpackages‚Äù that you can pay for via PayPal to upload larger datasets.\n\nTo test the PayPal flow, I set up a test account. If you want to try it out, when you are prompted to log in at checkout, just enter this username and password:\nusername:\nsb-43y47uz46185811@personal.example.com\npassword:\nU&gt;6OZ0sr\n\nThe idea is for you to try it out and give me feedback, let me know what formats you would like to be able to convert, etc. Anything you can think of to help improve the service. Any criticism is welcome. Best regards!",
    "author": "aiduc",
    "timestamp": "2025-09-15T13:58:50",
    "url": "https://reddit.com/r/computervision/comments/1nhxzdg/i_am_working_on_a_dataset_converter/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.17,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nhfewz",
    "title": "Coogle Coral usb problem",
    "content": "My windows 11 computer recognize the coral when i attach it to a usb port and it stays connected untill i restart the computer. Then it's gone. The coral usb itself is still lighting. I can then no longer see it in the device manager. If i then attach it to another usb port it shows up again and stays connected untill a new restart. I have tried to reinstall windows, it doesn't help. I have tried all usb-ports and the same happens. My computer is a Gigabyte, GB-BRi7-10710. I want to use the coral together with Blue Iris which is running CodeProject AI. The Coral works well there untill i restart the computer. I have tried to get help from ChatGPT and Google Gemini, spent two whole days trying to figure this out with no luck.\n\nCan anyone help?",
    "author": "Significant-Kale5864",
    "timestamp": "2025-09-15T00:30:21",
    "url": "https://reddit.com/r/computervision/comments/1nhfewz/coogle_coral_usb_problem/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nhg3cr",
    "title": "Looking for feedback: best name for ‚Äúdataset definition‚Äù concept in ML training",
    "content": "Throwaway account since this is for my actual job and my colleagues will also want to see your replies.¬†\n\n**TL;DR:**¬†We‚Äôre adding a new feature to our model training service: the ability to define subsets or combinations of datasets (instead of always training on the full dataset). We need help choosing a name for this concept ‚Äî see shortlist below and let us know what you think.\n\n‚Äî‚Äî\n\nI‚Äôm part of a team building a training service for computer vision models. At the moment, when you launch a training job on our platform, you can only pick one entire dataset to train on. That works fine in simple cases, but it‚Äôs limiting if you want more control ‚Äî for example, combining multiple datasets, filtering classes, or defining your own splits.\n\nWe‚Äôre introducing a new concept to fix this: a way to¬†*describe*¬†the dataset you actually want to train on, instead of always being stuck with a full dataset.\n\n**High-level idea**\n\nUsers should be able to:\n\n* Select subsets of data (specific classes, percentages, etc.)\n* Merge multiple datasets into one\n* Define train/val/test splits\n* Save these instructions and reuse them across trainings\n\nSo instead of always training on the ‚Äúraw‚Äù dataset, you‚Äôd train on your¬†**defined**¬†dataset, and you could reuse or share that definition later.\n\n**Technical description**\n\nUnder the hood, this is a new Python module that works alongside our existing Dataset module. Our current Dataset module executes operations immediately (filter, merge, split, etc.). This new module, however, is¬†**lazy**: it just¬†*registers*¬†the operations. When you call .build(), the operations are executed and a Dataset object is returned. The module can also export its operations into a human-readable JSON file, which can later be reloaded into Python. That way, a dataset definition can be shared, stored, and executed consistently across environments.\n\nNow we‚Äôre debating what to actually call this concept, and we'd appreciate your input. Here‚Äôs the shortlist we‚Äôve been considering:\n\n* Data Definitions\n* Data Specs\n* Data Specifications\n* Data Selections\n* Dataset Pipeline\n* Dataset Graph\n* Lazy Dataset\n* Dataset Query\n* Dataset Builder\n* Dataset Recipe\n* Dataset Config\n* Dataset Assembly\n\nWhat do you think works best here? Which names make the most sense to you as an ML/computer vision developer? And are there any names we should rule out right away because they‚Äôre misleading?\n\nPlease vote, comment, or suggest alternatives.",
    "author": "Little-Intention-465",
    "timestamp": "2025-09-15T01:16:39",
    "url": "https://reddit.com/r/computervision/comments/1nhg3cr/looking_for_feedback_best_name_for_dataset/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nhcgfd",
    "title": "Compare and list down silmilarities and diffrence between cam model image and its real image",
    "content": "The data contains the following:1.\n\n Images¬†of a physical part : &lt;&gt;\\_Real.jpeg2.\n\n Image of the digital CAD model: &lt;&gt;\\_CAD.png3. \n\nA mask generated¬†from the cad model (where part name is given in the json file and the pixel value¬†provided for the same part): &lt;&gt;\\_Mask.png4. \n\nThe json containing list of parts: &lt;&gt;\\_PartNamesToPixelMap.json\n\nProblem Statement : The goal is to devise a working sample to know if all the parts in the CAD image are available in the¬† real image. Identify if a part listed in the json is present or absent in the real image.1.\n\n Display/highlight the parts present¬†in Real and CAD image2\n\n Display/Highlight the parts absent in Real Image\n\nProblem Statement 2:¬† Device a high level architecture¬†in case we also want to know if the parts present are at the correct location or correct dimensions compared to the CAD image.¬†",
    "author": "Every-Computer170",
    "timestamp": "2025-09-14T21:31:53",
    "url": "https://reddit.com/r/computervision/comments/1nhcgfd/compare_and_list_down_silmilarities_and_diffrence/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nhfq5k",
    "title": "What's state of the art line crossing model",
    "content": "What's state of the art for counting number of people entering a place given a high volume and crowded area",
    "author": "Own-Cycle5851",
    "timestamp": "2025-09-15T00:51:16",
    "url": "https://reddit.com/r/computervision/comments/1nhfq5k/whats_state_of_the_art_line_crossing_model/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nglcqa",
    "title": "Computer Vision Obscured Numbers",
    "content": "Hi All,\n\nI\\`m working on a project to determine numbers from SVHN dataset while including other country unique IDs too. Classification model was done prior to number detection but I am unable to correctly abstract out the numbers for this instance 04-52. \n\nI\\`vr tried PaddleOCR and Yolov4 but it is not able to detect or fill the missing parts of the numbers.\n\nWould require some help from the community for some advise on what approaches are there for vision detection apart from LLM models like chatGPT for processing.\n\nThanks.",
    "author": "lofan92",
    "timestamp": "2025-09-14T00:57:46",
    "url": "https://reddit.com/r/computervision/comments/1nglcqa/computer_vision_obscured_numbers/",
    "score": 16,
    "num_comments": 19,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ngryuz",
    "title": "Suggestions for visual slam.",
    "content": "Hello,\nI want to do a project which involves visual-slam. I don't know where to start. \nThe project utilises visual slam for localisation and mapping for a rough and uneven terrain.\n\nThe robot I am going to use is nao v6.\nIt has two cameras. \n",
    "author": "kopc238",
    "timestamp": "2025-09-14T06:56:39",
    "url": "https://reddit.com/r/computervision/comments/1ngryuz/suggestions_for_visual_slam/",
    "score": 5,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nglxs4",
    "title": "How to evaluate Hyperparamter/Code Changes in RF-DETR",
    "content": "Hey, I'm currently working on a object detection project where I need to detect sometimes large, sometimes small rectangular features in the near and distance.\n\n\nI previously used ultralytics with varying success, then I switched to RF-DETR because of the licence and suggested improvements.\n\nHowever I'm seeing that it has a problem with smaller Objects and overall I noticed it's designed to work with smaller resolutions (as you can find in some of the resizing code)\n\nI started editing some of the code and configs.\n\nSo I'm wondering how I should evaluate if my changes improved anything?\n\nI tried having the same dataset and split, and training each time to exactly 10 epochs, then evaluating the metrics.\nBut the results feel fairly random.\n\n",
    "author": "stehen-geblieben",
    "timestamp": "2025-09-14T01:33:21",
    "url": "https://reddit.com/r/computervision/comments/1nglxs4/how_to_evaluate_hyperparamtercode_changes_in/",
    "score": 7,
    "num_comments": 5,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nh929b",
    "title": "Using YOLO11n for stock patterns",
    "content": "Hey everyone I thought this is a fun little project in which I put together an app that lets me stream my monitor in real time and run yolo11n on a trained model for stock patterns. I‚Äôm able to load up different models that are trained so if I have a dataset that‚Äôs been annotated with a specific pattern it‚Äôs possible to load up to this app.  ",
    "author": "fikaslo",
    "timestamp": "2025-09-14T18:40:55",
    "url": "https://reddit.com/r/computervision/comments/1nh929b/using_yolo11n_for_stock_patterns/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ngllt0",
    "title": "MMDetection Beginner Struggles",
    "content": "Hi everyone, I‚Äôm new to computer vision and am doing research at my university that is using computer vision. We‚Äôre trying to recreate a paper where the paper used MMDetection to classify materials (objects) in the image using coco.json and roboflow for the image processing.\n\nHowever, I find using MMDetection difficult and have read this from others as well. Still new to computer vision so I was wondering 1. Which object classification models are more user friendly and 2. What environment to use. Thanks!",
    "author": "MindlessPhilosophy68",
    "timestamp": "2025-09-14T01:12:54",
    "url": "https://reddit.com/r/computervision/comments/1ngllt0/mmdetection_beginner_struggles/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nfy542",
    "title": "Unified API to SOTA vision models",
    "content": "I organized my past works to handle many SOTA vision models with ONNX, and released as the open source repository.\nYou can use the simple and unified API for any models. Just create the model and pass an image, and you can get results.\nI hope it helps someone who wants to handle several models in the simple way.",
    "author": "earlier_adopter",
    "timestamp": "2025-09-13T06:56:32",
    "url": "https://reddit.com/r/computervision/comments/1nfy542/unified_api_to_sota_vision_models/",
    "score": 7,
    "num_comments": 2,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nfdnyb",
    "title": "Lightweight open-source background removal model (runs locally, no upload needed)",
    "content": "Hi all,\n\nI‚Äôve been working on [withoutbg](https://github.com/withoutbg/withoutbg), an open-source tool for background removal. It‚Äôs a lightweight matting model that runs locally and does not require uploading images to a server.\n\n**Key points:**\n\n* Python package (also usable through an API)\n* Lightweight model, works well on a variety of objects and fairly complex scenes\n* MIT licensed, free to use and extend\n\n**Technical details:**\n\n* Uses **Depth-Anything v2 small** as an upstream model, followed by a matting model and a refiner model sequentially\n* Developed with **PyTorch**, converted into **ONNX** for deployment\n* Training dataset sample: [withoutbg100 image matting dataset](https://withoutbg.com/resources/withoutbg100-image-matting-dataset) (purchased the alpha matte)\n* Dataset creation methodology: [how I built alpha matting data](https://withoutbg.com/resources/creating-alpha-matting-dataset) (some part of it)\n\nI‚Äôd really appreciate feedback from this community, model design trade-offs, and ideas for improvements. Contributions are welcome.\n\nNext steps: Dockerized REST API, serverless (AWS Lambda + S3), and a GIMP plugin.",
    "author": "Naive_Artist5196",
    "timestamp": "2025-09-12T13:18:16",
    "url": "https://reddit.com/r/computervision/comments/1nfdnyb/lightweight_opensource_background_removal_model/",
    "score": 149,
    "num_comments": 27,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nfslyv",
    "title": "Advice on Advanced Computer Vision Learning",
    "content": "Hi everyone,\n\nI want to grow my skills in computer vision and would love some advice. I know the basics and also have some projects built, but now I want to go deeper into advanced areas. I am especially interested in real time computer vision, 3D vision like stereo, SLAM and point clouds, AR and VR, robotics, visual odometry, sensor fusion, and newer models like vision transformers. I also want to learn how to deploy and optimize models for production and real time use. If you know any good resources such as courses, books, research papers or GitHub projects for these topics please share them.\n\nI also want to look for a remote junior or entry level computer vision job that I can do from Pakistan. If you know any job boards, communities or companies that hire remotely it would be great to hear about them. Tips on building a portfolio or open source projects that can help me stand out would also be very helpful.\n\nThanks in advance for any guidance.",
    "author": "Sannad98",
    "timestamp": "2025-09-13T01:52:42",
    "url": "https://reddit.com/r/computervision/comments/1nfslyv/advice_on_advanced_computer_vision_learning/",
    "score": 11,
    "num_comments": 6,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nfxhw8",
    "title": "Real-time joystick control of Temad on Raspberry Pi 5 with an OpenCV preview ‚Äî latency &amp; stability notes",
    "content": "I‚Äôve been tinkering with a small side build: a Raspberry Pi 5 driving Temad with a USB joystick, plus a lightweight OpenCV preview so I can see what the gimbal ‚Äúsees‚Äù while I move it.\n\nWhat I ended up doing (no buzzwords, just what worked):\n\nKept joystick input separate from capture/display; added a small dead-zone + smoothing to avoid jitter.\n\nOpenCV preview on the Pi with a simple frame cap so CPU doesn‚Äôt spike and the UI stays responsive.\n\nBasic on-screen stats (FPS/drops) to sanity-check latency.\n\nThings that bit me:\nJoystick device IDs changing across adapters.\n\nBuffering differences (v4l2 vs. other backends).\n\nPreview gets laggy fast without throttling.\n\nShort demo for context (not selling anything):\nhttps://www.youtube.com/watch?v=2Y9RFeHrDUA\n\nIf you‚Äôre curious, I‚Äôm happy to share versions/configs. Always keen to learn how others keep Pi-side previews snappy.",
    "author": "Big-Mulberry4600",
    "timestamp": "2025-09-13T06:27:32",
    "url": "https://reddit.com/r/computervision/comments/1nfxhw8/realtime_joystick_control_of_temad_on_raspberry/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1ng31ca",
    "title": "Single object detection",
    "content": "Hello everyone. I need to build an object detection model for an object that I designed myself. The object detection will mostly be from videos that only have my object in it. However, I worry that the deep learning model becomes overfit to detecting everything as my object since it is the only object in the dataset. Is it something to worry and do I need to use another method for this? Thank you for the answers in advance.",
    "author": "Bl4ck8ird",
    "timestamp": "2025-09-13T10:14:34",
    "url": "https://reddit.com/r/computervision/comments/1ng31ca/single_object_detection/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nf6xn7",
    "title": "Building being built üèóÔ∏è (video created with computer vision)",
    "content": "Blog post here: [https://zl-labs.tech/post/2024-12-06-cv-building-timelapse/](https://zl-labs.tech/post/2024-12-06-cv-building-timelapse/)",
    "author": "lukerm_zl",
    "timestamp": "2025-09-12T08:54:45",
    "url": "https://reddit.com/r/computervision/comments/1nf6xn7/building_being_built_video_created_with_computer/",
    "score": 82,
    "num_comments": 16,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nevt9p",
    "title": "The world‚Äôs first screenless laptop is here, Spacetop G1 turns AR glasses into a 100-inch workspace.Cool innovation or just unnecessary hype?",
    "content": "",
    "author": "Minimum_Minimum4577",
    "timestamp": "2025-09-11T23:25:10",
    "url": "https://reddit.com/r/computervision/comments/1nevt9p/the_worlds_first_screenless_laptop_is_here/",
    "score": 60,
    "num_comments": 33,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nfeolx",
    "title": "Weaponized False Positives: How Poisoned Datasets Could Erase Researchers Overnight",
    "content": "",
    "author": "markatlarge",
    "timestamp": "2025-09-12T13:58:43",
    "url": "https://reddit.com/r/computervision/comments/1nfeolx/weaponized_false_positives_how_poisoned_datasets/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nfpsg4",
    "title": "where to get ideas for fyp bachelors level for ai (nlp or cv)?",
    "content": "i gotta give proposal for my fyp please help",
    "author": "nouman6093",
    "timestamp": "2025-09-12T22:58:08",
    "url": "https://reddit.com/r/computervision/comments/1nfpsg4/where_to_get_ideas_for_fyp_bachelors_level_for_ai/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nf0l79",
    "title": "Final Project Computer Engineering Student",
    "content": "Looking for suggestion on project proposal for my final year as a computer engineering student.",
    "author": "Designer_Guava_4067",
    "timestamp": "2025-09-12T04:25:00",
    "url": "https://reddit.com/r/computervision/comments/1nf0l79/final_project_computer_engineering_student/",
    "score": 8,
    "num_comments": 7,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nf4u14",
    "title": "Archery training app with AI form evaluation (7-factor, 16-point schema) + cloud-based score tracking",
    "content": "Hello everyone,\n\nI‚Äôve developed an archery app that combines performance analysis with score tracking. It uses an AI module to evaluate shooting form across **7 dimensions**, with a **16-point scoring schema**:\n\n* *StanceScore*: 0‚Äì3\n* *AlignmentScore*: 0‚Äì3\n* *DrawScore*: 0‚Äì3\n* *AnchorScore*: 0‚Äì3\n* *AimScore*: 0‚Äì2\n* *ReleaseScore*: 0‚Äì2\n* *FollowThroughScore*: 0‚Äì2\n\nAfter each session, the AI generates a feedback report highlighting strong and weak areas, with personalized improvement tips. Users can also interact with a **chat-based ‚Äúcoach‚Äù** for technique advice or equipment questions.\n\nOn the tracking side, the app offers features comparable to MyTargets, but adds:\n\n* **Cloud sync** across devices\n* **Cross-platform portability** (Android ‚Üî iOS)\n* Persistent performance history for long-term analysis\n\nI‚Äôm curious about two things:\n\n1. From a user perspective, what additional features would make this more valuable?\n2. From a technical/ML perspective, how would you approach refining the scoring model to capture nuances of form?\n\nNot sure if i can link the app, but the name is ArcherSense, its on IOs and Android.\n\nhttps://preview.redd.it/k5guhghavqof1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=255380b0479d4afbcf774ccb563c12ddc0b6bd81\n\n",
    "author": "link983d",
    "timestamp": "2025-09-12T07:32:19",
    "url": "https://reddit.com/r/computervision/comments/1nf4u14/archery_training_app_with_ai_form_evaluation/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "computervision",
    "post_id": "1nezkku",
    "title": "How to discard unwanted images(items occlusions with hand) from a large chuck of images collected from top in ecommerce warehouse packing process?",
    "content": "I am an engineer part of an enterprise into ecommerce. We are capturing images during packing process.\n\nThe goal is to build SKU segmentation on cluttered items in a bin/cart.\n\n  \nFor this we have an annotation pipeline but we cant push all images into the annotation pipeline and this is where we are exploring approaches to build a preprocessing layer where we can discard majority of the images where items gets occluded by hands, or if there is raw material kept on the side also coming in photo like tapes etc.\n\nNot possible to share the real picture so i am sharing a sample. Just think that there are warehouse carts as many of you might have seen if you already solved this problem or into ecommerce warehousing. \n\nOne way i am thinking is using multimodal APIs like Gemini or GPT5 etc with the prompt whether this contain hand or not? \n\nHas anyone tackled a similar problem in warehouse or manufacturing settings? \n\nWhat scalable approaches( say model driven, heuristics etc) would you recommend for filtering out such noisy frames before annotation?\n\nhttps://preview.redd.it/jofg1pdympof1.png?width=310&amp;format=png&amp;auto=webp&amp;s=ace7428f1b73e786581ab4d2255857b226fd0a3c\n\n",
    "author": "Worth-Card9034",
    "timestamp": "2025-09-12T03:27:52",
    "url": "https://reddit.com/r/computervision/comments/1nezkku/how_to_discard_unwanted_imagesitems_occlusions/",
    "score": 4,
    "num_comments": 6,
    "upvote_ratio": 0.83,
    "is_original_content": false
  }
]