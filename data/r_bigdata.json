[
  {
    "subreddit": "bigdata",
    "post_id": "1occ5hg",
    "title": "Heterogeneous Data: Use Cases, Tools &amp; Best Practices",
    "content": "",
    "author": "Comfortable-Site8626",
    "timestamp": "2025-10-21T05:53:54",
    "url": "https://reddit.com/r/bigdata/comments/1occ5hg/heterogeneous_data_use_cases_tools_best_practices/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ocemzl",
    "title": "Contratos de Datos: la columna vertebral de la arquitectura de datos moderna (dbt + BigQuery)",
    "content": "",
    "author": "Expensive-Insect-317",
    "timestamp": "2025-10-21T07:35:05",
    "url": "https://reddit.com/r/bigdata/comments/1ocemzl/contratos_de_datos_la_columna_vertebral_de_la/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1oc72m0",
    "title": "Build a JavaScript Chart with One Million Data Points",
    "content": "",
    "author": "SciChartGuide",
    "timestamp": "2025-10-21T01:04:53",
    "url": "https://reddit.com/r/bigdata/comments/1oc72m0/build_a_javascript_chart_with_one_million_data/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1objf4s",
    "title": "Flink Watermarksâ€¦WTF?",
    "content": "",
    "author": "rmoff",
    "timestamp": "2025-10-20T06:46:52",
    "url": "https://reddit.com/r/bigdata/comments/1objf4s/flink_watermarkswtf/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1obe1fi",
    "title": "Incremental dbt models causing metadata drift",
    "content": "Â I tried incremental dbt models with Airflow DAGs. At first metadata drifted between runs and incremental loads failed silently Solved it by using proper unique keys and Delta table versions. Queries became stable and DAGs no longer needed extra retries. Anyone has tricks for debugging incremental models faster?\n\n",
    "author": "FitImportance606",
    "timestamp": "2025-10-20T00:54:05",
    "url": "https://reddit.com/r/bigdata/comments/1obe1fi/incremental_dbt_models_causing_metadata_drift/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1obe0j0",
    "title": "Lakehouse architecture with Spark and Delta for multi TB datasets",
    "content": "Â We had 3TB of customer data and needed fast analytical queries. Decided on Delta Lake on ADLS with Spark SQL for transformations.\n\nPartitioning by customer region and ingestion date saved a ton of scan time. Also learned that vacuum frequency can make or break query performance. Anyone else tune vacuum and compaction on huge datasets?\n\n",
    "author": "FitImportance606",
    "timestamp": "2025-10-20T00:52:32",
    "url": "https://reddit.com/r/bigdata/comments/1obe0j0/lakehouse_architecture_with_spark_and_delta_for/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1oasl1s",
    "title": "Gartner Magic Quadrant for Observability 2025",
    "content": "",
    "author": "Longjumping_Ad_1180",
    "timestamp": "2025-10-19T08:31:49",
    "url": "https://reddit.com/r/bigdata/comments/1oasl1s/gartner_magic_quadrant_for_observability_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1oap8pg",
    "title": "Looking for YouTube project ideas using Hadoop, Hive, Spark, and PySpark",
    "content": "Hi everyone ğŸ‘‹\n\nIâ€™m learning Hadoop, Hive, Spark, PySpark, and Hugging Face NLP and want to build a real, hands-on project.\n\nIâ€™m looking for ideas that:\n\tâ€¢\tUse big data tools\n\tâ€¢\tApply NLP (sentiment analysis, text classification, etc.)\n\tâ€¢\tCan be showcased on a CV/LinkedIn\n\n\nCan you share some hands-on YouTube projects or tutorials that combine these tools?\n\n\nThanks a lot for your help! ğŸ™\n",
    "author": "ayaa_001",
    "timestamp": "2025-10-19T06:09:18",
    "url": "https://reddit.com/r/bigdata/comments/1oap8pg/looking_for_youtube_project_ideas_using_hadoop/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1oaafbv",
    "title": "Clustered, Non-Clustered , Heap  Indexes in SQL â€“ Explained with Stored Proc Lookup",
    "content": "\nhttps://youtu.be/cDiCp64V-uQ",
    "author": "KeyCandy4665",
    "timestamp": "2025-10-18T16:16:02",
    "url": "https://reddit.com/r/bigdata/comments/1oaafbv/clustered_nonclustered_heap_indexes_in_sql/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o9v752",
    "title": "A Guide to dbt Dry Runs: Safe Simulation for Data Engineers â€” worth a read",
    "content": "Hey,\nI came across this great Medium article on how to validate dbt transformations, dependencies, and compiled SQL without touching your data warehouse.\n\nexplains that while dbt doesnâ€™t have a native --dry-run command, you can simulate one by leveraging dbtâ€™s compile phase to:\n\tâ€¢\tParse .sql and .yml files\n\tâ€¢\tResolve Jinja templates and macros\n\tâ€¢\tValidate dependencies (ref(), source(), etc.)\n\tâ€¢\tGenerate final SQL without executing it against the warehouse\n\nThis approach can add a nice safety layer before production runs, especially for teams managing large data pipelines.\n\nmedium.com/@sendoamoronta/a-guide-to-dbt-dry-runs-safe-simulation-for-data-engineers-7e480ce5dcf7",
    "author": "Expensive-Insect-317",
    "timestamp": "2025-10-18T06:01:53",
    "url": "https://reddit.com/r/bigdata/comments/1o9v752/a_guide_to_dbt_dry_runs_safe_simulation_for_data/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o8tut5",
    "title": "USDSIÂ® Launches 2026 Data Science Career Factsheet",
    "content": "Millions of data science jobs will be up for grabs in 2026! From Generative AI and ML to advanced data visualization, the demand is skyrocketing. USDSIÂ® Data Science Career Factsheet 2026 reveals career pathways, salary insights, and global hotspots for certified data scientists.\n\nhttps://preview.redd.it/jfpz3u668mvf1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=16ec15d40f2af87065f32d94edd2a2cfcbf57efb\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-10-16T23:23:35",
    "url": "https://reddit.com/r/bigdata/comments/1o8tut5/usdsi_launches_2026_data_science_career_factsheet/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o85buk",
    "title": "Paper on the Context Architecture",
    "content": "This paper on the rise of ğ“ğ¡ğ ğ‚ğ¨ğ§ğ­ğğ±ğ­ ğ€ğ«ğœğ¡ğ¢ğ­ğğœğ­ğ®ğ«ğÂ is an attempt to share with you what context-focused designs we've worked on and why. Why the meta needs to take the front seat and why is machine-enabled agency necessary? How context enables it, and why does it need to, and how to build that context?\n\nThe paper talks about the tech, the concept, the architecture, and during the experience of comprehending these units, the above questions would be answerable by you yourself. This is an attempt to convey the fundamental bare bones of context and the architecture that builds it, implements it, and enables scale/adoption.\n\nğ–ğ¡ğšğ­'ğ¬ ğˆğ§ğ¬ğ¢ğğ â†©ï¸\n\nA. The Collapse of Context in Todayâ€™s Data Platforms\n\nB. The Rise of the Context Architecture\n\n1ï¸âƒ£ 1st Piece of Your Context Architecture: ğ“ğ¡ğ«ğğ-ğ‹ğšğ²ğğ« ğƒğğğ®ğœğ­ğ¢ğ¨ğ§ ğŒğ¨ğğğ¥\n\n2ï¸âƒ£ 2nd Piece of Your Context Architecture: ğğ«ğ¨ğğ®ğœğ­ğ¢ğ¬ğ ğ’ğ­ğšğœğ¤\n\n3ï¸âƒ£ 3rd Piece of Your Context Architecture: ğ“ğ¡ğ ğ€ğœğ­ğ¢ğ¯ğšğ­ğ¢ğ¨ğ§ ğ’ğ­ğšğœğ¤\n\nC. The Trinity of Deduction, Productisation, and Activation\n\nğŸ”— ğœğ¨ğ¦ğ©ğ¥ğğ­ğ ğ›ğ«ğğšğ¤ğğ¨ğ°ğ§ ğ¡ğğ«ğ: [https://moderndata101.substack.com/p/rise-of-the-context-architecture](https://moderndata101.substack.com/p/rise-of-the-context-architecture)",
    "author": "Original_Poetry_8563",
    "timestamp": "2025-10-16T05:52:24",
    "url": "https://reddit.com/r/bigdata/comments/1o85buk/paper_on_the_context_architecture/",
    "score": 19,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o7lpu9",
    "title": "Smaller Families, Older Population, Fewer Children, Living Alone: Why? And What Effects does this have on Communities, Families and People as Individuals (Mental Health etc)?",
    "content": "If you leave comments under the YouTube video itself that would be great!  Can participate in the discussion there too.\n\nBut what do you guys think?  Every person is different.  Cost of housing, health issues, choices, and much much more can determine why one individual did not have children or did.  But the overall trend in the data with fertility rates, median age, households of 1 etc... this is a different society than it was before and how much of these statistics/topics is part of it?\n\nA society with a fertility rate of 2.5, a median age of 25 and only 7% of people living alone is going to be a different society than one where the fertility rate is 1.3, median age 43 and 30% of people living alone.  How do you think it would be different?  Why did this happen?  Thoughts?",
    "author": "RoyalPalpitation4412",
    "timestamp": "2025-10-15T13:09:13",
    "url": "https://reddit.com/r/bigdata/comments/1o7lpu9/smaller_families_older_population_fewer_children/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o7a10o",
    "title": "Top Data Science Trends Transforming Industries in 2026",
    "content": "Data science is not a new technology, but still, it is evolving at an unprecedented rate. The reasons could be many, including advancements in technologies like AI and machine learning, the explosion of data, accessible **data science tools**, and more.\n\nMoreover, rapid adoption of data science by organizations also requires strong control of data privacy, security, and responsible and ethical development of models. This evolution of the data science industry is led by several factors that are going to shape the [future of data science](https://www.usdsi.org/data-science-insights/the-future-of-data-science-emerging-technologies-and-trends).\n\nIn this article, let us explore such **top data science trends** that every data science enthusiast, professional, and business leader should watch closely.\n\n# Top Data Science Trends to Watch Out for\n\nHere are some of the **data science trends in 2026** that will determine what the **future of data science** will look like.\n\n**1.Â Automated and Augmented Analytics**\n\nA lot of data science processes, including data preparation and model building, are becoming easier with automation tools like **AutoML** and augmented analytics platforms. So, these tools are empowering even non-technical professionals to do complex analyses easily.\n\n**2.Â Real-Time and Edge Data Processing**\n\nThere are over billions of IoT devices that also generate a continuous stream of data, and the need for processing data at the edge, i.e., close to the source, is more than ever. **Edge computing** offers real-time analytics, reduces latency, as well as enhances privacy. This will be transforming industries like healthcare, logistics, and manufacturing with smarter automation and instant decision-making.\n\n**3.Â Foundation Models**\n\nBuilding a data science or **machine learning model** from scratch can be a lumbersome [task](). In this case, organizations can leverage large pre-trained models such as GPT or BERT. Transfer learning helps build smaller, domain-specific models that can reduce costs significantly. **Data science and AI** go hand in hand. So, in the future, we can see hybrid models that leverage both deep learning and better reasoning and flexibility for various applications.\n\n**4.Â Â Democratization of Data Science**\n\nData science is an incredible technology, and everyone should benefit from it, not just large organizations with huge resources and skilled **data science professionals**. As we enter the future, we find many user-friendly platforms that help non-technical professionals or â€œcitizen data scientistsâ€ build models without core **data science skills**. This is a great way to promote data literacy across organizations. However, it must be noted that true success can be achieved with collaboration between domain experts and professional **data scientists**, not alone.\n\n**5.Â Â Sustainability and Green AI**\n\nA huge amount of energy is spent running and maintaining large **AI models**. This is why Green AI has become important. It refers to energy-efficient training, model compression, resource optimization, etc., to minimize energy consumed. According to Research and Markets, the Green AI infrastructure market is projected to grow by $14.65 billion by 2029 with a CAGR of 28.4%. This **data science trend** is all about moving towards smaller, smarter, and sustainable AI systems that offer strong performance with minimal carbon footprint.\n\n# Impact of Data Science Across Industries\n\nThe applications of **data science and AI** across industries are also evolving. Data science is known to be the foundation of innovation in nearly all industries today, and in the future, it will be further strengthened.\n\n**Here is what the future of data science in different industries will be like:**\n\n# Healthcare\n\n* Predictive analytics and AI-powered diagnostics will help detect diseases earlier.\n* Personalized medication and treatment\n* Better patient outcome\n\n# Finance\n\n* Detect financial fraud in real-time\n* Algorithmic trading\n* Personalized financial guidance\n\n# Manufacturing\n\n* Predictive maintenance\n* Better productivity\n* Efficient supply chain\n\n# Retail\n\n* Better customer service\n* Dynamic pricing\n* Forecast demand accurately\n* Inventory management\n\n# Education\n\n* Adaptive and personalized learning\n* Better administration, and more\n\nSimilarly, data science also has a huge impact and will continue to transform other industries as well.\n\nWith proper training and **data science programs**, students and professionals can learn the essential data science skills and knowledge that will help them get started or advance in their **data science career** path for a secure future ahead.\n\nIf you are looking to grow in this career path, here are some of the recommended **data science certifications** that you can look for:\n\n* Certified Data Science Professional (CDSPâ„¢) by USDSIÂ®\n* Graduate Certificate in Data Science (Harvard Extension School)\n* Professional Certificate in Data Science and Analytics (MIT xPRO)\n* Certified Lead Data Scientist (CLDSâ„¢) by USDSIÂ®\n* IBM Data Science Professional Certificate\n* Microsoft Certified: Azure Data Scientist Associate (DP-100)\n\nThese are some of the most popular and recognized **data science programs** to start or grow in a data science career path. With these certifications, you will not just master the latest **data science skills** but will also be updated on upcoming [data science trends](https://www.usdsi.org/data-science-insights/the-future-of-data-science-emerging-technologies-and-trends) as well.\n\n# Summing up!\n\nThe future of data science isnâ€™t just about building bigger models or handling big data. It is about building smarter, specific, and energy-efficient systems. Data science professionals alone cannot bring the transformation organizations need today, and therefore, they must collaborate with domain experts and leaders to bring vision into reality. Moreover, with user-friendly data science tools, even non-technical professionals can try their hands on and contribute to innovating their organizations. To further strengthen data science capabilities, data science certifications and training programs will be a great help.",
    "author": "sharmaniti437",
    "timestamp": "2025-10-15T05:46:49",
    "url": "https://reddit.com/r/bigdata/comments/1o7a10o/top_data_science_trends_transforming_industries/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o7ddqd",
    "title": "Legacy systems slowing you down? This session could help.",
    "content": "Hey folks,\n\nI came across a free webinar that might be useful for anyone working with **legacy data warehouses** or dealing with **performance bottlenecks**.\n\nItâ€™s called **â€œTired of Slow, Costly Analytics? How to Modernize Without the Pain.â€**\n\nThe session is about how teams are approaching **data modernization**, **migration**, and **performance optimization** â€” without getting into product pitches. Itâ€™s more of a â€œwhatâ€™s working in the real worldâ€ discussion than a demo.\n\nğŸ—“ï¸ **When:** November 4, 2025, at 9:00 AM ET  \nğŸ™ï¸ **Speakers:** Hemant Kumar &amp; Brajesh Sharma (IBM Netezza)\n\nğŸ”— **Free Registration:** [https://ibm.webcasts.com/starthere.jsp?ei=1736443&amp;tp\\_key=43cb369084](https://ibm.webcasts.com/starthere.jsp?ei=1736443&amp;tp_key=43cb369084)\n\nThought Iâ€™d share here since it seems relevant to a lot of what gets discussed in this sub â€” especially around **data performance, migrations, and cloud analytics**.\n\n*(Mods, feel free to remove if this isnâ€™t appropriate â€” just figured it might be helpful for others here.)*\n\n\\#DataEngineering #DataAnalytics #IBMNetezza #Modernization #CloudAnalytics #Webinar #IBM #DataWarehouse #HybridCloud",
    "author": "Unlucky_Village_5755",
    "timestamp": "2025-10-15T08:00:29",
    "url": "https://reddit.com/r/bigdata/comments/1o7ddqd/legacy_systems_slowing_you_down_this_session/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o6vl12",
    "title": "ğŸš€ Real-World use cases at the Apache Iceberg Seattle Meetup â€” 4 Speakers, 1 Powerful Event",
    "content": "Tired of theory? See howÂ **Uber, DoorDash, Databricks &amp; CelerData**Â areÂ *actually*Â using Apache Iceberg in production at our free Seattle meetup.\n\nNo marketing fluff, just deep dives into solving real-world problems:\n\n* **Databricks:**Â Unveiling the proposedÂ **Iceberg V4 Adaptive Metadata Tree**Â for faster commits.\n* **Uber:**Â A look at theirÂ **native, cross-DC replication**Â for disaster recovery at scale.\n* **CelerData:**Â Crushing theÂ **small-file problem**Â with benchmarks showingÂ **\\~5x faster writes**.\n* **DoorDash:**Â Real talk on their multi-engine architecture, use cases, and feature gaps.\n\n**When:**Â Thurs, Oct 23rd @ 5 PMÂ **Where:**Â Google Kirkland (with food &amp; drinks)\n\nThis is a chance to hear directly from the engineers in the trenches. Seats are limited and filling up fast.\n\nğŸ”—Â **RSVP here to claim your spot:**Â [`https://luma.com/byyyrlua`](https://luma.com/byyyrlua)",
    "author": "Public_Two_9800",
    "timestamp": "2025-10-14T16:41:34",
    "url": "https://reddit.com/r/bigdata/comments/1o6vl12/realworld_use_cases_at_the_apache_iceberg_seattle/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o6ea58",
    "title": "Try the chart library that can handle your most ambitious performance requirements - for free",
    "content": "",
    "author": "SciChartGuide",
    "timestamp": "2025-10-14T05:35:16",
    "url": "https://reddit.com/r/bigdata/comments/1o6ea58/try_the_chart_library_that_can_handle_your_most/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o6d6hu",
    "title": "We helped a food company cut migration time in half â€” hereâ€™s how",
    "content": "At Ascendion, I was recently a part of an interesting data modernization project for a leading food company. Their biggest headache? Long, complex data migrations slowing down analytics and operations.\n\nWith Ascendionâ€™s â€œData to the Power of AIâ€ approach, we built a smarter platform that automated key parts of the migration. The results:\n\n* Migration timeÂ **cut by 50%**\n* Deployment speedÂ **up by 75%**\n* OverÂ **5,000 hours saved per year**Â in manual work\n\nIt was a good reminder that AI isnâ€™t just about models or chatbots, sometimes itâ€™s about making theÂ *plumbing*Â smarter so everything else moves faster.\n\nFor anyone whoâ€™s worked on large-scale data migrations, whatâ€™s been your biggest bottleneck? Automation, governance, or legacy tech?\n\n  \n",
    "author": "TechAsc",
    "timestamp": "2025-10-14T04:41:51",
    "url": "https://reddit.com/r/bigdata/comments/1o6d6hu/we_helped_a_food_company_cut_migration_time_in/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o68c0y",
    "title": "Olympic Games Analytics Project in Apache Spark for beginner",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-10-13T23:46:28",
    "url": "https://reddit.com/r/bigdata/comments/1o68c0y/olympic_games_analytics_project_in_apache_spark/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o65729",
    "title": "AI-Driven Data Migration: Game-Changer or Overhyped Promise?",
    "content": "Hey everyone,\n\nHere's a case study I thought I'd share. A US-based aerospace/defense firm that needed to migrate massive data loadsÂ *without*Â downtime or security compromises.  \nHereâ€™s what they pulled off:Â [https://ascendion.com/client-outcomes/90-faster-data-processing-with-automated-migration-for-global-enterprise/](https://ascendion.com/client-outcomes/90-faster-data-processing-with-automated-migration-for-global-enterprise/)\n\nWhat They Did:\n\n* Used Ascendion'sÂ AAVA Data Modernization StudioÂ for automation, translating stored procedures, tables, views, and pipelines to reduce manual effort\n* Applied query optimizations, heap tables, and tightened security controls\n* Executed the migration inÂ \\~15 weeks, keeping operations live across regions\n\nResults:\n\n* \\~90% performance improvementÂ in data processing &amp; reporting\n* \\~50% faster migrationÂ vs manual methods\n* \\~80% reduction in downtime, enabling global teams to keep using the system\n* Stronger data integrity, less duplication, and better access control\n\nThis kind of outcome sounds fantastic if it works as claimed. But Iâ€™m curious (and skeptical) about how realistic it is inÂ *your*Â environments:\n\n* Has anyone here done a similarly large-scale data migration with AI-driven automation?\n* What pitfalls or unexpected challenges did you run into (e.g. data fidelity issues, edge-case transformations, rollback strategy, performance surprises)?\n* How would you validate whether an â€œautomated translation / modernization toolâ€ is trustworthy before full rollout?",
    "author": "TechAsc",
    "timestamp": "2025-10-13T20:45:32",
    "url": "https://reddit.com/r/bigdata/comments/1o65729/aidriven_data_migration_gamechanger_or_overhyped/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o5ialo",
    "title": "How do you track and control prompt workflows in large-scale AI and data systems?",
    "content": "Hello all,\n\nRecently, I've been investigating the best ways to handle prompts efficiently with large-scale AI systems, particularly with configurations that incorporate multiple sets of data or distributed systems.\n\nSomething that's assisted me with putting some thoughts together is the organized method that **Empromptu ai** takes, with prompts essentially being viewed as data assets that are versioned, tagged, and linked to experiment outcomes. This mentality made me appreciate how cumbersome prompt management becomes as soon as you scale past a handful of models.\n\nI'm wondering how others deal with this:\n\n* Do you utilize prompt tracking within your data pipelines?\n* Are there frameworks or practices youâ€™ve found effective for maintaining consistency across experiments?\n* How can reproducibility be achieved as prompts change over time?\n\nWould be helpful to learn about how professionals working in the big data field approach this dilemma.",
    "author": "Fuzzy-Blood6105",
    "timestamp": "2025-10-13T05:26:55",
    "url": "https://reddit.com/r/bigdata/comments/1o5ialo/how_do_you_track_and_control_prompt_workflows_in/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o5bhst",
    "title": "Apache Spark Project World Development Indicators Analytics for Beginners",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-10-12T22:43:15",
    "url": "https://reddit.com/r/bigdata/comments/1o5bhst/apache_spark_project_world_development_indicators/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o5dski",
    "title": "Schema Evolution The Hidden Backbone of Modern Pipelines",
    "content": "Schema evolution is transforming modern data pipelines. Learn strategies to handle schema changes, minimize impact on analytics, and unlock better insights. Advance your career with USDSIâ€™s CLDSâ„¢ certification &amp; enjoy a globally recognized credential. \n\nhttps://preview.redd.it/nfdw3qft6uuf1.jpg?width=850&amp;format=pjpg&amp;auto=webp&amp;s=3f103c79417f313026afc528e4db5e8938d99103\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-10-13T01:06:26",
    "url": "https://reddit.com/r/bigdata/comments/1o5dski/schema_evolution_the_hidden_backbone_of_modern/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o3rem8",
    "title": "Got the theory down, but what are the real-world best practices",
    "content": "Hey everyone,\n\nIâ€™m currently studying Big Data at university. So far, weâ€™ve mostly focused on analytics and data warehousing using Oracle. The concepts make sense, but I feel like Iâ€™m still missing how things are applied in real-world environments.\n\nIâ€™ve got a solid programming background and Iâ€™m also familiar with GIS (Geographic Information Systems), so Iâ€™m comfortable handling data-related workflows. What Iâ€™m looking for now is to build the right practical habits and understand how things are done professionally.\n\nFor those with experience in the field:\n\nWhat are some good practices to build early on in analytics and data warehousing?\n\nAny recommended workflows, tools, or habits that helped you grow faster?\n\nCommon beginner mistakes to avoid?\n\n\nIâ€™d love to hear how you approach things in real projects and what I can start doing to develop the right mindset and skill set for this domain.\n\nThanks in advance!",
    "author": "[deleted]",
    "timestamp": "2025-10-11T02:39:58",
    "url": "https://reddit.com/r/bigdata/comments/1o3rem8/got_the_theory_down_but_what_are_the_realworld/",
    "score": 15,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o3s8gi",
    "title": "Data Science A Power Tool For Advanced Robotics",
    "content": "Ever wondered what makes robots so smart? Itâ€™s Data Science â€” the secret sauce that helps them think, learn, and act. From autonomous vehicles to factory bots, data science powers intelligent decision-making with minimal human effort.\n\nhttps://preview.redd.it/f0ulzl3ymguf1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=1fc34cad02e751bb84bab0f8041ba7e493db4b3c\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-10-11T03:31:35",
    "url": "https://reddit.com/r/bigdata/comments/1o3s8gi/data_science_a_power_tool_for_advanced_robotics/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o3sbhb",
    "title": "DAX UDFs",
    "content": "",
    "author": "Status-Cap-5236",
    "timestamp": "2025-10-11T03:36:43",
    "url": "https://reddit.com/r/bigdata/comments/1o3sbhb/dax_udfs/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o37c40",
    "title": "[Research] Contributing to Facial Expressions Dataset for CV Training",
    "content": "",
    "author": "Funny-Whereas8597",
    "timestamp": "2025-10-10T10:29:36",
    "url": "https://reddit.com/r/bigdata/comments/1o37c40/research_contributing_to_facial_expressions/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o27kry",
    "title": "Is there demand for a full dataset of homepage HTML from all active websites?",
    "content": "As part of my job, I was required to scrape the homepage HTML of all active websites - it will be over 200 million in total.  \nAfter overcoming all the technical and infrastructure challenges, I will have a complete dataset soon and the ability to keep it regularly updated.\n\nIâ€™m wondering if this kind of data is valuable enough to build a small business around.  \nDo you think thereâ€™s real demand for such a dataset, and if so, who might be interested in it (e.g., SEO, AI training, web intelligence, etc.)?",
    "author": "firedexplorer",
    "timestamp": "2025-10-09T07:34:39",
    "url": "https://reddit.com/r/bigdata/comments/1o27kry/is_there_demand_for_a_full_dataset_of_homepage/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o233zw",
    "title": "Parsing Large Binary File",
    "content": "Hi,\n\nAnyone can guide or help me in parsing large binary file.\n\nI am unaware of the file structure and it is financial data something like market by price data but in binary form with around 10 GB.\n\nHow can I parse it or extract the information to get in CSV?\n\nAny guide or leads are appreciated. Thanks in advance!",
    "author": "Abject_Sandwich7187",
    "timestamp": "2025-10-09T04:13:08",
    "url": "https://reddit.com/r/bigdata/comments/1o233zw/parsing_large_binary_file/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o1xxtf",
    "title": "Top Questions and Important topic on Apache Spark",
    "content": "Navigating the World of Apache Spark: Comprehensive Guide\nIâ€™ve curated this guide to all the Spark-related articles, categorizing them by skill level. Consider this your one-stop reference to find exactly what you need, when you need it.",
    "author": "Other_Cap7605",
    "timestamp": "2025-10-08T22:39:17",
    "url": "https://reddit.com/r/bigdata/comments/1o1xxtf/top_questions_and_important_topic_on_apache_spark/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o1xxa5",
    "title": "Top Questions and Important topic on Apache Spark",
    "content": "",
    "author": "Other_Cap7605",
    "timestamp": "2025-10-08T22:38:18",
    "url": "https://reddit.com/r/bigdata/comments/1o1xxa5/top_questions_and_important_topic_on_apache_spark/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o1i5ag",
    "title": "Free 1,000 CPU + 100 GPU hours for testers. I open sourced the world's simplest cluster compute software",
    "content": "Hey everybody,\n\nIâ€™ve always struggled to get data scientists and analysts to scale their code in the cloud. Almost every time, theyâ€™d have to hand it over to DevOps, the backlog would grow, and overall throughput would tank.\n\nSo I builtÂ [Burla](https://docs.burla.dev/), the simplest cluster compute software that lets even Python beginners run code on massive clusters in the cloud. Itâ€™s one function with two parameters: the function and the inputs. You can bring your own Docker image, set hardware requirements, and run jobs as background tasks so you can fire and forget. Responses are fast, and you can call a million simple functions in just a few seconds.\n\nBurla is built for embarrassingly parallel workloads like preprocessing data, hyperparameter tuning, and batch inference.\n\nIt's open source, and Iâ€™m improving the installation process. I also created managed versions for testing. If you want to try it, Iâ€™ll cover 1,000 CPU hours and 100 GPU hours. Email me atÂ [joe@burla.dev](mailto:joe@burla.dev)Â if interested.\n\nHereâ€™s a short intro video:  \n[https://www.youtube.com/watch?v=9d22y\\_kWjyE](https://www.youtube.com/watch?v=9d22y_kWjyE)\n\nGitHub â†’Â [https://github.com/Burla-Cloud/burla](https://github.com/Burla-Cloud/burla)  \nDocs â†’Â [https://docs.burla.dev](https://docs.burla.dev/)",
    "author": "Ok_Post_149",
    "timestamp": "2025-10-08T11:10:33",
    "url": "https://reddit.com/r/bigdata/comments/1o1i5ag/free_1000_cpu_100_gpu_hours_for_testers_i_open/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o18kvt",
    "title": "Feature Store Summit 2025 - Free, Online Event.",
    "content": "https://preview.redd.it/dbcqpp1xovtf1.jpg?width=2273&amp;format=pjpg&amp;auto=webp&amp;s=831300c9b7b3bf29d8cd1ee9a04462dabe6681ff\n\n**Hello everyone !**  \n  \nWe are organising the Feature Store Summit. An annual online event where we invite some of the most technical speakers from some of the worldâ€™s most advanced engineering teams to talk about their infrastructure for AI, ML and all things that needs massive scale and real-time capabilities.  \n  \n**Some of this yearâ€™s speakers are coming from:**  \nUber, Pinterest, Zalando, Lyft, Coinbase, Hopsworks and More!\n\n**What to Expect:**  \nğŸ”¥ Real-Time Feature Engineering at scale  \nğŸ”¥Â Vector Databases &amp; Generative AI in production  \nğŸ”¥Â The balance of Batch &amp; Real-Time workflows  \nğŸ”¥Â Emerging trends driving the evolution of Feature Stores in 2025\n\n**When:**  \nğŸ—“ï¸Â October 14th  \nâ°Â Starting 8:30AM PT  \nâ° Starting 5:30PM CET  \n  \nLink;Â [https://www.featurestoresummit.com/register](https://www.featurestoresummit.com/register?utm_source=reddit)\n\nPS; it is free, online, and if you register you will be receiving the recorded talks afterward! ",
    "author": "logicalclocks",
    "timestamp": "2025-10-08T05:05:33",
    "url": "https://reddit.com/r/bigdata/comments/1o18kvt/feature_store_summit_2025_free_online_event/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o16tjt",
    "title": "Creazione HFT/ low latency",
    "content": "Poche chiacchiere.\nMi presento, Pietro Leone Bruno.\nTrader di microstrutture di mercato.\nHo l'essenza dei mercati .\nHo il sistema, e il prototipo, pronti.\n\nRispetto la tecnologia e i \"Builders\" programmatori con tutto me stesso. PerchÃ© so che trasformano il mio sistema in realtÃ . Senza di loro, il ponte rimane solo illusione.\n\nSono disposto a dare un Max di 60% equity, le mie intenzioni sono di costruire il team piÃ¹ solido del mondo di Builders, perchÃ© qua costruiamo HFT PIÃ™ FORTE DEL MONDO.\n\nSi parla di Trilioni, soldi infiniti. Ho l'hack dei mercati.\n\nPietro Leone Bruno\n+39 339 693 4641",
    "author": "albadiunimpero",
    "timestamp": "2025-10-08T03:32:15",
    "url": "https://reddit.com/r/bigdata/comments/1o16tjt/creazione_hft_low_latency/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o14ugl",
    "title": "How Quantum AI will reshape the Data World in 2026",
    "content": "Quantum AI is powering the next era of data science. By integrating quantum computing with AI, it accelerates machine learning and analytics, enabling industries to predict trends and optimize operations with unmatched speed. The market is projected to grow rapidly, and you can lead the charge by upskilling with USDSIÂ® certifications.\n\nhttps://preview.redd.it/6z4v79uslutf1.jpg?width=850&amp;format=pjpg&amp;auto=webp&amp;s=cce75e634877d7fb19f697ae3b57969ffb65d594\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-10-08T01:26:03",
    "url": "https://reddit.com/r/bigdata/comments/1o14ugl/how_quantum_ai_will_reshape_the_data_world_in_2026/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o0ioov",
    "title": "How Agentic Analytics Is Replacing BI as We Know It",
    "content": "",
    "author": "dofthings",
    "timestamp": "2025-10-07T08:55:10",
    "url": "https://reddit.com/r/bigdata/comments/1o0ioov/how_agentic_analytics_is_replacing_bi_as_we_know/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1o0dmy0",
    "title": "Improving data/reporting pipelines",
    "content": "Hey everyone, came across a case that really shows how performance optimization alone can unlock agility. A company was bogged down by slow query execution. Reports lagged, data-driven decisions delayed. They overhauled their data infrastructure, optimized queries, re-architected parts of the data pipelines. Result? **Query times dropped by 45%, which meant reports came faster, decisions got made quicker, and agility jumped significantly.**\n\nWhat struck me: it wasnâ€™t adding more fancy AI or big-new tools, just tightening up what already existed. Sometimes improving the plumbing gives bigger wins than adding new features.\n\nQuestions / thoughts:\n\n* How many teams are leaving low-hanging performance improvements on the table because theyâ€™re chasing new tech instead of fine-tuning what they have?\n* Whatâ€™s your approach for identifying bottlenecks in data/reporting pipelines?\n* Have you seen similar lifts just by optimizing queries / infrastructure?",
    "author": "TechAsc",
    "timestamp": "2025-10-07T05:39:55",
    "url": "https://reddit.com/r/bigdata/comments/1o0dmy0/improving_datareporting_pipelines/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nzenq4",
    "title": "Growing Importance of Cybersecurity for Data Science in 2026",
    "content": "The data science industry is growing faster than we can imagine, all thanks to advanced technologies like AI and machine learning, and powering innovations in healthcare, finance, autonomous systems, and more. However, with this rapid growth, the field also faces challenges from growing cybersecurity risks. As we march towards 2026, we cannot keep cybersecurity as a separate entity for the emerging technologies; instead, it serves as the central pillar of trust, reliability, and safety.\n\nLetâ€™s explore more and try to understand why cybersecurity has become increasingly important in data science, the emerging risks, and how organizations can evolve to protect themselves against rising threats.\n\n# Why Cybersecurity Matters More Than Ever\n\nCybersecurity has always been a huge matter of concern. Here are a few reasons why:\n\n# 1.Â Increased Integration Of AI/ML In Important Systems\n\nData science has moved from being just a research topic or pilot projects. Now, they are deeply integrated across industries, including healthcare, finance, autonomous vehicles, and more. Therefore, it has become absolutely important to keep these systems running. If they fail, it can lead to financial loss, physical harm, and more. If the **machine learning models** do not diagnose disease properly, misinterpret sensor inputs in self-driving cars, or incorrectly price risks in the financial market, then it can have severe effects.\n\n# 2.Â Increase In Attack Surface and New Threat Vectors\n\nMost traditional **cybersecurity tools** and practices are not designed for AI/ML environments. So, there are new threat vectors that need to be taken care of, such as:\n\nÂ·Â **Data poisoning** â€“ this means contaminating training data, which results in models showing unusual behavior/outputs\n\nÂ·Â **Adversarial attacks** â€“ such as injecting malicious prompts into **machine learning models**. Though humans wonâ€™t recognize this, the model will provide wrong predictions.\n\nÂ·Â **Model stealing and extraction** â€“ in this, attackers probe the model to replicate its functionality or glean proprietary information\n\nAttackers can also extract information about training data from APIs or model outputs.\n\n# 3. Regulatory and Ethical Pressures\n\nBy 2026, governments and regulatory bodies globally will tighten rules around AI and ML governance, data privacy, and the fairness of algorithms. So, organizations failing to comply with these standards and regulations may have to pay heavy fines, incur reputational damage, and lose trust.\n\n# 4.Â Demand for Trust and User Safety\n\nMost importantly, public awareness of AI risks is rising. Users and consumers are expecting the systems to be safe and transparent, and free from bias. Trust has become a huge differentiator now. Users will prefer a safe and secure model rather than an accurate but vulnerable model to attack.\n\n# Best Practices in 2026: What Should Organizations Do?\n\nTo meet the demands of **cybersecurity in data science**, **cybersecurity experts** need to adopt strategies at par with traditional IT security. Here are some best practices that organizations must follow:\n\n# 1.Â Secure Data Pipelines and Enforce Data Quality Controls\n\nOrganizations should treat datasets as the most important assets. They must implement strong data provenance, i.e., know where data comes from, who handles it, and what processes they are undergoing with. It is also essential to encrypt data in storage and transit.\n\n# 2.Â Secure Model Training\n\nOrganizations must use adversarial training, in which they can include adversarial or corrupted examples during training to make it more resistant to such attacks. They can also employ differential privacy techniques by limiting what information about any individual record can be inferred. Utilizing federated learning or a similar architecture can also be helpful in reducing centralized data exposure.\n\n# 3.Â Strict Access Controls and Monitoring\n\n**Cybersecurity experts** should ensure least privileged access and limit who or what can access data, machine learning models, and prediction APIs. They can also employ rate limiting and anomaly detection to help identify misuse and exploitation of the models.\n\n# 4. Integrate Security in The Software Development Life Cycle\n\nSecurity steps, such as threat modeling, vulnerability scanning, compliance checks, etc., should be an integral part of the design, development, and deployment of **machine learning models**. For this, it is recommended that professionals from different domains, including data scientists, engineers, [cybersecurity experts](https://www.uscsinstitute.org/cybersecurity-certifications/certified-senior-cybersecurity-specialist), compliance, and legal teams, work together.\n\n# 5.Â Regulatory Compliance and Ethical Oversight\n\nMachine learning models should be built inherently explainable and transparent, keeping in mind various compliance and regulatory standards to avoid heavy fines in the future. Moreover, using only necessary data for training and anonymizing sensitive data is recommended.\n\nLooking ahead, in the year 2026, the race between attackers and **security professionals** in the field of AI and data science will become fierce. We might expect more advanced and automated tools that can detect adversarial inputs and vulnerabilities in **machine learning models** more accurately and faster. The regulatory frameworks surrounding AI and ML security will become more standardized. We might also see the adoption of technologies that focus on maintaining the privacy and security of data. Also, a stronger integration of security thinking is needed in every layer of **data science workflows**.\n\n# Conclusion\n\nIn the coming years, cybersecurity will not be an add-on task but integral to data science and AI/ML. Organizations are actively adopting AI, ML, and data science, and therefore, it is absolutely necessary to secure these systems from evolving and emerging threats, because failing to do so can result in serious financial, reputational, and operational consequences. So, it is time that professionals across domains, including AI, data science, cybersecurity, legal, compliance, etc., should work together to build robust systems free from all kinds of vulnerabilities and resistant to all kinds of threats.",
    "author": "sharmaniti437",
    "timestamp": "2025-10-06T02:46:22",
    "url": "https://reddit.com/r/bigdata/comments/1nzenq4/growing_importance_of_cybersecurity_for_data/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nzevtz",
    "title": "Septiembre 2025: Resumen Mensual de IngenierÃ­a de Datos y Nube â€” lo que no te puedes perder este mes en datos y nube",
    "content": "",
    "author": "Expensive-Insect-317",
    "timestamp": "2025-10-06T03:00:58",
    "url": "https://reddit.com/r/bigdata/comments/1nzevtz/septiembre_2025_resumen_mensual_de_ingenierÃ­a_de/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nyi3e5",
    "title": "Boost Hive Performance with ORC File Format | A Deep Dive",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-10-05T00:45:23",
    "url": "https://reddit.com/r/bigdata/comments/1nyi3e5/boost_hive_performance_with_orc_file_format_a/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nwahzw",
    "title": "help me on this survey to collect data on the impact of short form content on focus and productivity ğŸ™",
    "content": "Hey everyone!\nIâ€™m conducting a short survey (1â€“2 minutes max) as part of my [course project / research study].\nYour input would help me a lot ğŸ™Œ.\n\nğŸ”— Survey Link: https://forms.gle/YNR6GoqWjbmpz5Qi9\n\nItâ€™s completely anonymous, and the questions are simple â€” no personal data required.\nIf you could take a few minutes to fill it out, Iâ€™d be super grateful!\n\nThanks a ton in advance â¤ï¸",
    "author": "div25O6",
    "timestamp": "2025-10-02T10:40:54",
    "url": "https://reddit.com/r/bigdata/comments/1nwahzw/help_me_on_this_survey_to_collect_data_on_the/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nw7mvc",
    "title": "Data regulation research",
    "content": "Participate in my research on data regulation! Your opinions matter! (Should take about 10 minutes and is completely anonymous)",
    "author": "ProfessionalEmpty966",
    "timestamp": "2025-10-02T08:54:47",
    "url": "https://reddit.com/r/bigdata/comments/1nw7mvc/data_regulation_research/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nvq09n",
    "title": "Built an open source Google Maps Street View Panorama Scraper.",
    "content": "WithÂ [gsvp-dl](https://github.com/yousephzidan/gsvp-dl), an open source solution written in Python, you are able to download millions of panorama images off Google Maps Street View.\n\nUnlike other existing solutions (which fail to address major edge cases),Â [gsvp-dl](https://github.com/yousephzidan/gsvp-dl)Â downloads panoramas in their correct form and size with unmatched accuracy. Using Python Asyncio and Aiohttp, it can handle bulk downloads, scaling to millions of panoramas per day.\n\nIt was a fun project to work on, as there was no documentation whatsoever, whether by Google or other existing solutions. So, I documented the key points that explain why a panorama image looks the way it does based on the given inputs (mainly zoom levels).\n\nOther solutions donâ€™t match up because they ignore edge cases, especially pre-2016 images with different resolutions. They used fixed width and height that only worked for post-2016 panoramas, which caused black spaces in older ones.\n\nThe way I was able to reverse engineer Google Maps Street View API was by sitting all day for a week, doing nothing but observing the results of the endpoint, testing inputs, assembling panoramas, observing outputs, and repeating. With no documentation, no lead, and no reference, it was all trial and error.\n\nI believe I have covered most edge cases, though I still doubt I may have missed some. Despite testing hundreds of panoramas at different inputs, Iâ€™m sure there could be a case I didnâ€™t encounter. So feel free to fork the repo and make a pull request if you come across one, or find a bug/unexpected behavior.\n\nThanks for checking it out!",
    "author": "yousephx",
    "timestamp": "2025-10-01T17:59:44",
    "url": "https://reddit.com/r/bigdata/comments/1nvq09n/built_an_open_source_google_maps_street_view/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nv8p60",
    "title": "Looking for an exciting project",
    "content": "I'm a DE focusing on streaming and processing data, really want to collaborate with paáº£tners on exciting projects!",
    "author": "Dutay05",
    "timestamp": "2025-10-01T06:48:24",
    "url": "https://reddit.com/r/bigdata/comments/1nv8p60/looking_for_an_exciting_project/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nvf9gm",
    "title": "Looking for a Data Analytics expert (preferably in Mexico)",
    "content": "Hello everyone, Iâ€™m looking for a data analysis specialist since Iâ€™m currently working on my university thesis and my mentor asked me to conduct one or more (online) interviews with a specialist. The goal is to know whether the topic Iâ€™m addressing is feasible, to hear their opinion, and to see if they have any suggestions. My thesis focuses on Mexico, so preferably it would be someone from this location, but I believe anyone could be helpful. THANK YOU VERY MUCH!",
    "author": "Lafunky_z",
    "timestamp": "2025-10-01T10:52:01",
    "url": "https://reddit.com/r/bigdata/comments/1nvf9gm/looking_for_a_data_analytics_expert_preferably_in/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nv9hu3",
    "title": "Good practices to follow in analytics &amp; data warehousing?",
    "content": "Hey everyone,\n\nIâ€™m currently studying Big Data at university, but most of what weâ€™ve done so far is centered on analytics and a bit of data warehousing. Iâ€™m pretty solid with coding, but I feel like Iâ€™m still missing the practical side of how things are done in the real world.\n\nFor those of you with experience:\n\nWhat are some good practices to build early on in analytics and data warehousing?\n\nAre there workflows, habits, or tools you wish you had learned sooner?\n\nWhat common mistakes should beginners try to avoid?\n\n\nIâ€™d really appreciate advice on how to move beyond just the classroom concepts and start building useful practices for the field.\n\nThanks a lot!",
    "author": "[deleted]",
    "timestamp": "2025-10-01T07:19:38",
    "url": "https://reddit.com/r/bigdata/comments/1nv9hu3/good_practices_to_follow_in_analytics_data/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nv7ecv",
    "title": "Designing Your Data Science Portfolio Like a Pro",
    "content": "Do you know what distinguishes a successful and efficient data science professional from others? Well, it is a solid portfolio of strong, demonstrated **data science projects**. A well-designed portfolio can be the most powerful tool and set you apart from the rest of the crowd. Whether you are a beginner looking to enter into a **data science career** or a mid-level practitioner seeking career advancement to higher **data science job** roles, a **data science portfolio** can be the greatest companion. It not only tells, but also shows the potential employers what you can do. It is the bridge between your resume and what you can actually deliver in practice.\n\nSo, let us explore how the key principles, structure, tips, and challenges that you must consider to make your portfolio feel professional and effective, and make your **data science profile** stand out.\n\n# Start With Purpose and Audience\n\nBefore you start building your [data science portfolio](https://www.usdsi.org/data-science-insights/how-to-make-a-solid-portfolio-for-an-aspiring-data-analyst) and diving into layout or projects, define why and for whom you are building the portfolio.\n\n* **Purpose** â€“ define if you are making job applications for clients/freelancing, building a personal brand, or enhancing your credibility in the **data science industry**\n* **Audience** â€“ often, recruiters and hiring managers look for concrete artifacts and results. Whereas technical peers will explore the quality of code, your methodologies, and architectural decisions. Even a non-technical audience might look at your portfolio to gauge the impact of metrics, storytelling, and interpretability.\n\nMoreover, the design elements, writing style, and project selection should be based on the audience you are focusing on. Like - you can emphasize business impact and readability if you are focusing on managerial roles in the industry.\n\n# Core Components of a Professional Data Science Portfolio\n\nSeveral components together help build an impactful **data science portfolio** that can be arranged in various sections. Your portfolio should ideally include:\n\n**1.Â Homepage or Landing Page**\n\nKeep your homepage clean and minimal to introduce who you are, your specialization (e.g., â€œtime series forecasting,â€ â€œcomputer vision,â€ â€œNLPâ€), and key differentiators, etc.\n\n**2.Â About**\n\nThis is your bio page where you can highlight your background, [data science certifications](https://www.usdsi.org/data-science-certifications) you have earned, your approach to solving data problems, your soft skills, your social profiles, and contact information.\n\n**3.Â Skills and Data Science Tools**\n\nEmployers will focus on this page, where you can highlight your key **data science skills** and the **data science tools** you use. So, organizing this into clear categories like:\n\n* Programming\n* ML and AI skills\n* Data engineering\n* Big data\n* Data visualization and data storytelling\n* Cloud and DevOps, etc.\n\nIt is advised to group them properly instead of just a laundry list. You can also link to instances in your projects where you used them.\n\n**4.Â Â Projects and Case Studies**\n\nThis is the heart of your **data science portfolio**. Here is how you can structure each project:\n\nhttps://preview.redd.it/0s90oi5lzhsf1.jpg?width=763&amp;format=pjpg&amp;auto=webp&amp;s=0a6b504f12098edb176d0ab35dfa8daa2e66ae5a\n\nÂ **5.Â Â Blogs, articles, or tutorials**\n\nThis is optional, but you can add these sections to increase the overall value of your portfolio. Adding your techniques, strategies, and lessons learned appeals mostly to peers and recruiters.\n\n**6.Â Â Resume**\n\nEmbed a clean CV that recruiters can download and highlight your accomplishments.\n\n# Things to Consider While Designing Your Portfolio\n\n* Keep it clean and minimal\n* Make it mobile responsive\n* Navigation across sections should be effortless\n* Maintain a visual consistency in terms of fonts, color palettes, and icons\n* You can also embed widgets and dashboards like Plotly Dash, Streamlit, etc., that visitors can explore\n* Ensure your portfolio website loads fast so that users do not lose interest and bounce back\n* How to Maintain and Grow Your Portfolio\n\nKeeping your portfolio static for too long can make it stale. Here are a few tips to keep it alive and relevant:\n\n**1.Â Â Update regularly**\n\nRevise your portfolio whenever you complete a new project. Replace weaker **data science projects** with newer ones\n\n**2.Â Â Rotate featured projects**\n\nHighlight 2-3 recent and relevant ones and make it accessible\n\n**3.Â Â Adopt new tools and techniques**\n\nAs the data science field is evolving, gain new **data science tools** and techniques with the help of recognized **data science certifications** and update them in your portfolio\n\n**4.Â Â Gather feedback and improve**\n\nYou can take feedback from peers, employers, and friends, and improve the portfolio\n\n**5.Â Â Track analytics**\n\nYou can also use simple analytics like Google Analytics and see what users are looking at and where they drop off to refine your content and UI.\n\n# What Not to Do in Your Portfolio?\n\nA solid **data science portfolio** is a gateway to infinite possibilities and opportunities. However, there are some things that you must avoid at all costs, such as:\n\n* Avoid too many small and shallow projects\n* Avoid explaining complex blackbox models; instead, focus on a simple model with clear reasoning\n* Neglect storytelling if your narrative is weak. This will impact even solid technical work\n* Avoid overcrowded plots and inconsistent design as they distract from content\n* Update portfolio periodically to avoid stale content in it\n\n# Conclusion\n\nDesigning your data science portfolio like a pro is all about balancing strong content, clean design, data storytelling, and regular refinement. You can highlight your top data science projects, your data science certifications, achievements, and skills to make maximum impact. Keep it clean and easy to navigate.",
    "author": "sharmaniti437",
    "timestamp": "2025-10-01T05:53:47",
    "url": "https://reddit.com/r/bigdata/comments/1nv7ecv/designing_your_data_science_portfolio_like_a_pro/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nv3v7w",
    "title": "From Star Schema to the Kimball Approach in Data Warehousing: Lessons for Scalable Architectures",
    "content": "In data warehouse modeling, many start with a Star Schema for its simplicity, but relying solely on it limits scalability and data consistency.\n\nThe Kimball methodology goes beyond this by proposing an incremental architecture based on a â€œData Warehouse Busâ€ that connects multiple Data Marts using conformed dimensions. This allows:\n\n* Integration of multiple business processes (sales, marketing, logistics) while maintaining consistency.\n* Incremental DW evolution without redesigning existing structures.\n* Historical dimension management through Slowly Changing Dimensions (SCDs).\n* Various types of fact and dimension tables to handle different scenarios.\n\nHow do you manage data warehouse evolution in your projects? Have you implemented conformed dimensions in complex environments?\n\nMore details on the Kimball methodology can be found [here](https://medium.com/@sendoamoronta/from-star-schema-to-the-kimball-approach-in-a-data-warehouse-c92364789d7a).",
    "author": "Expensive-Insect-317",
    "timestamp": "2025-10-01T02:46:48",
    "url": "https://reddit.com/r/bigdata/comments/1nv3v7w/from_star_schema_to_the_kimball_approach_in_data/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nu0bak",
    "title": "Data Engineering at Scale: Netflix Process &amp; Preparation (Step-by-Step)",
    "content": "",
    "author": "Altruistic_Potato_67",
    "timestamp": "2025-09-29T18:36:25",
    "url": "https://reddit.com/r/bigdata/comments/1nu0bak/data_engineering_at_scale_netflix_process/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ntqhxk",
    "title": "From raw video to structured data - Stanfordâ€™s PSI world model",
    "content": "One of the bottlenecks in AI/ML has always been dealing with huge amounts of raw, messy data. I just read this new paper out of Stanford, PSI (Probabilistic Structure Integration), and thought it was super relevant for the big data community: [link](https://arxiv.org/abs/2509.09737?utm_source=chatgpt.com).\n\nInstead of training separate models with labeled datasets for tasks like depth, motion, or segmentation, PSI learns those directly from raw video. It basically turns video into structured tokens that can then be used for different downstream tasks.\n\nA couple things that stood out to me:\n\n* No manual labeling required â†’ the model self-learns depth/segmentation/motion.\n* Probabilistic rollouts â†’ instead of one deterministic future, it can simulate multiple possibilities.\n* Scales with data â†’ trained on massive video datasets across 64Ã— H100s, showing how far raw â†’ structured modeling can go.\n\nhttps://preview.redd.it/eunb86p3h5sf1.png?width=1652&amp;format=png&amp;auto=webp&amp;s=eabf320831fae6b4927afa90b907e27df16f615a\n\nFeels like a step toward making large-scale unstructured data (like video) actually useful for a wide range of applications (robotics, AR, forecasting, even science simulations) without having to pre-engineer a labeled dataset for everything.\n\nCurious what others here think: is this kind of raw-to-structured modeling the future of big data, or are we still going to need curated/labeled datasets for a long time?",
    "author": "Appropriate-Web2517",
    "timestamp": "2025-09-29T11:50:57",
    "url": "https://reddit.com/r/bigdata/comments/1ntqhxk/from_raw_video_to_structured_data_stanfords_psi/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nthcnv",
    "title": "Scale up your Data Visualization with JavaScript Polar Charts",
    "content": "",
    "author": "SciChartGuide",
    "timestamp": "2025-09-29T05:59:01",
    "url": "https://reddit.com/r/bigdata/comments/1nthcnv/scale_up_your_data_visualization_with_javascript/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ntgveb",
    "title": "Leveraging AI and Big Data to Boost the EV Ecosystem",
    "content": "Artificial Intelligence (AI) and Big Data are transforming the electric vehicle (EV) ecosystem by driving smarter innovation, efficiency, and sustainability. From optimizing battery performance and predicting maintenance needs to enabling intelligent charging infrastructure and enhancing supply chain operations, these technologies empower the EV industry to scale rapidly. By leveraging real-time data and advanced analytics, automakers, energy providers, and policymakers can create a connected, efficient, and customer-centric EV ecosystem that accelerates the transition to clean mobility.\n\nhttps://preview.redd.it/amq4j49hm3sf1.jpg?width=2481&amp;format=pjpg&amp;auto=webp&amp;s=742060d34934f160e57537c2c6b163a706f5a359\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-29T05:37:32",
    "url": "https://reddit.com/r/bigdata/comments/1ntgveb/leveraging_ai_and_big_data_to_boost_the_ev/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nshxou",
    "title": "Just finished DE internship (SQL, Hive, PySpark) â†’ Should I learn Microsoft Fabric or stick to Azure DE stack (ADF, Synapse, Databricks)?",
    "content": "",
    "author": "HistoricalTear9785",
    "timestamp": "2025-09-28T00:12:57",
    "url": "https://reddit.com/r/bigdata/comments/1nshxou/just_finished_de_internship_sql_hive_pyspark/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nrqlgc",
    "title": "USDSI DATA SCIENCE CAREER FACTSHEET 2026",
    "content": "Millions of data science jobs will be up for grabs in 2026! From Generative AI and ML to advanced data visualization, the demand is skyrocketing. USDSIÂ® Data Science Career Factsheet 2026 reveals career pathways, salary insights, and global hotspots for certified data scientists.\n\nhttps://preview.redd.it/vgm8l44fborf1.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=8b97eac2775ec5da82ad47b93f6139a006057db5\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-27T02:08:56",
    "url": "https://reddit.com/r/bigdata/comments/1nrqlgc/usdsi_data_science_career_factsheet_2026/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nqwnl2",
    "title": "USDSI DATA SCIENCE CAREER FACTSHEET 2026",
    "content": "Understanding numbers is quintessential for any business operating globally today. With the world going crazy about the volume of data it generates every day; it necessitates the applicability of qualified data science professionals who can make sense of it all.\n\nComprehending the latest trends, skillsets in action, and what the global recruiters want from you is all that is required. The [USDSI Data Science Career Factsheet 2026](https://www.usdsi.org/data-science-insights/resources/data-science-career-factsheet-2026) is all about your data science career growth pathways, skills to master that shall empower you to earn a whopping salary home. Understanding the booming data science industry, know the hottest data science jobs available in 2026, the salary you can reap from them, skills and specialization arenas to qualify for a lasting data science career growth. Get your hands on the best educational pathways available at USDSI to enable you the greatest levels of employability with sheer skill and talent. Become invincible in data science- download the factsheet today!\n\nhttps://preview.redd.it/qfi6zk058hrf1.png?width=1155&amp;format=png&amp;auto=webp&amp;s=13cd46b3be866eff76aa6fd049f7119f01705128\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-26T02:18:25",
    "url": "https://reddit.com/r/bigdata/comments/1nqwnl2/usdsi_data_science_career_factsheet_2026/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nqxjw3",
    "title": "Pushing the Boundaries of Real-Time Big Data",
    "content": "",
    "author": "SciChartGuide",
    "timestamp": "2025-09-26T03:14:57",
    "url": "https://reddit.com/r/bigdata/comments/1nqxjw3/pushing_the_boundaries_of_realtime_big_data/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1npy5ih",
    "title": "Big data Hadoop and Spark Analytics Projects (End to End)",
    "content": "Hi Guys,\n\nI hope you are well.\n\nFree tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**\n\n***Apache Spark Analytics Projects:***\n\n1. [Vehicle Sales Report â€“ Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)\n2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)\n3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)\n4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)\n5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)\n6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)\n7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)\n8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)\n\n***Bigdata Hadoop Projects:***\n\n1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)\n2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)\n3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)\n4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)\n5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)\n\nI hope you'll enjoy these tutorials.",
    "author": "bigdataengineer4life",
    "timestamp": "2025-09-24T22:07:24",
    "url": "https://reddit.com/r/bigdata/comments/1npy5ih/big_data_hadoop_and_spark_analytics_projects_end/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nq0orp",
    "title": "Certified Lead Data Scientist (CLDSâ„¢)",
    "content": "Ready to level up in Data Science career? The Certified Lead Data Scientist (CLDSâ„¢) program accelerates your journey to become a top-tier data scientist. Gain advanced expertise in Data Science, ML, IoT, Cloud &amp; more. Boost your career, handle complex projects, and position yourself for high-paying, impactful roles.\n\nhttps://preview.redd.it/ymzts21dn9rf1.jpg?width=1536&amp;format=pjpg&amp;auto=webp&amp;s=b72830ebeb14475c46e075cf91903957d8fad69c\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-25T00:49:06",
    "url": "https://reddit.com/r/bigdata/comments/1nq0orp/certified_lead_data_scientist_clds/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1npooy9",
    "title": "Prove me wrong - The entire big data industry is pointless merge sort passes over a shared mutable heap to restore per user physical locality",
    "content": "",
    "author": "Due_Carrot_3544",
    "timestamp": "2025-09-24T14:31:52",
    "url": "https://reddit.com/r/bigdata/comments/1npooy9/prove_me_wrong_the_entire_big_data_industry_is/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1npn9hr",
    "title": "The D of Things Newsletter #19",
    "content": "",
    "author": "dofthings",
    "timestamp": "2025-09-24T13:34:53",
    "url": "https://reddit.com/r/bigdata/comments/1npn9hr/the_d_of_things_newsletter_19/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1no8tpk",
    "title": "Applications of AI in Data Science Streamlining Workflows",
    "content": "From predictive analytics to recommendation engines to data-driven decision-making, the role of data science in transforming workflow across industries has been profound. When combined with advanced technologies like artificial intelligence and machine learning, data science can do wonders. With an AI-powered data science workflow offering a higher degree of automation and helping free up data scientistsâ€™ precious time, the professionals can focus on more strategic and innovative work. \n\nhttps://preview.redd.it/8g3mr46sluqf1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=8e1e3edf2d175492984fd73f32d26cec1069736e\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-22T22:13:29",
    "url": "https://reddit.com/r/bigdata/comments/1no8tpk/applications_of_ai_in_data_science_streamlining/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nnq7ek",
    "title": "Anyone else losing track of datasets during ML experiments?",
    "content": "Every time I rerun an experiment the data has already changed and I canâ€™t reproduce results. Copying datasets around works but itâ€™s a mess and eats storage. How do you all keep experiments consistent without turning into a data hoarder?",
    "author": "rawion363",
    "timestamp": "2025-09-22T08:51:05",
    "url": "https://reddit.com/r/bigdata/comments/1nnq7ek/anyone_else_losing_track_of_datasets_during_ml/",
    "score": 7,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nnmqyg",
    "title": "Why Donâ€™t Data Engineers Unit/Integration Test Their Spark Jobs?",
    "content": "",
    "author": "jpgerek",
    "timestamp": "2025-09-22T06:38:47",
    "url": "https://reddit.com/r/bigdata/comments/1nnmqyg/why_dont_data_engineers_unitintegration_test/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nnj4wy",
    "title": "8 Ways AI Has Changed Data Science",
    "content": "AI hasnâ€™t just entered in data science itâ€™s rearranged the entire structure! From automation to intelligent visualization, discover 8 ways AI is rewriting the rules of data science.\n\nhttps://preview.redd.it/zulkgr6e4pqf1.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=b5fcbac36777c3b56ed0d0091de8058652a302d0\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-22T03:47:08",
    "url": "https://reddit.com/r/bigdata/comments/1nnj4wy/8_ways_ai_has_changed_data_science/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nnfksw",
    "title": "Get your FREE Big Data Interview Prep eBook! ğŸ“š 1000+ questions on programming, scenarios, fundamentals, &amp; performance tuning",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-09-21T23:59:45",
    "url": "https://reddit.com/r/bigdata/comments/1nnfksw/get_your_free_big_data_interview_prep_ebook_1000/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nmwbfo",
    "title": "Free encrypted cloud storage",
    "content": "\nHi,\nI have been looking for a large amount of storage for free and now when I found it I wanted to share. \n\nIf you want a stupidly big ammount of storage you can use Hivenet. For each person you refer you get 10 gb for free stacking infinetly! If you use my my link you will also start out with an additional 10 gb.\n\nhttps://www.hivenet.com/referral?referral_code=8UiVX9DwgWK3RBcmmY5ETuOSNhoNy%2BRTCTisjZc0%2FzemUpDX%2Ff4rrMCXgtSILlC%2Bf%2B7TFw%3D%3D\n\n I already got 110 gb for free using this method but if you invite many friends you will litterally get terabytes of free storage.",
    "author": "Adi-Imin",
    "timestamp": "2025-09-21T09:22:25",
    "url": "https://reddit.com/r/bigdata/comments/1nmwbfo/free_encrypted_cloud_storage/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nmoz0x",
    "title": "I am in a dilema Or confused state",
    "content": "Hi folks \nI am B tech ece 2022 passedout guy. Selected in TechM , Wipro , Accenture(they said selected in interview but no mails from them) neglected training sessions by techm because of wipro offer is there.. Time passes 2022,2023,2024 I didn't move to any big city to join courses and liveinhostel Later Nov 2024 I got a job in a startup company as Business Analyst \nMy title and my job role didnt have any match\nI do software application validation means I will take screenshot of each and every part of application and prepare a documentation for client audit purposes\nI will stay in client location for 3months - 8months including Saturday but there is no pay for Saturday\nActually I won't get my salary on time \nFor now I need to get 3months salary (due from company) \nMeanwhile I am learning data engineering course I want to shift to DE but not finding 1 yr experience people  Don't know\nWhat I am doing in my life\nMy friends are well settled in life girls got married and boys earning good salaries in mnc\nI am a single parent child alot of stress in my mind, can't enjoy a moment properly\nI did a mistake in my 3-1 semister that wantedly failed in two subjects because of that I didn't  got chance to attend campus drive After clearing of my subjects in 4-2 I got selected in companies etc \nBut no use of them now \nI spoiled my life with my own hands \nI felt like sharing this here . ",
    "author": "Additional_Range_674",
    "timestamp": "2025-09-21T03:56:53",
    "url": "https://reddit.com/r/bigdata/comments/1nmoz0x/i_am_in_a_dilema_or_confused_state/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nmb07q",
    "title": "Redefining Trust in AI with Autonomys ğŸ§ âœ¨",
    "content": "\n\nOne of the biggest challenges in AI today is memory. Most systems rely on **ephemeral logs** that can be deleted or altered, and their reasoning often functions like a **black box** â€” impossible to fully verify. This creates a major issue: *how can we trust AI outputs if we canâ€™t trace or validate what the system actually â€œremembersâ€?*\n\nAutonomys is tackling this head-on. By building on **distributed storage**, it introduces tamper-proof, queryable records that canâ€™t simply vanish. These persistent logs are made accessible through the open-source **Auto Agents Framework** and the **Auto Drive API**. Instead of hidden black box memory, developers and users get transparent, verifiable traces of how an agent reached its conclusions.\n\nThis shift matters because AI isnâ€™t just about generating answers â€” itâ€™s about accountability. Imagine autonomous agents in finance, healthcare, or governance: if their decisions are backed by **immutable and auditable memory**, trust in AI systems can move from fragile to foundational.\n\nAutonomys isnâ€™t just upgrading tools â€” itâ€™s reframing the relationship between humans and AI.\n\nğŸ‘‰ What do you think: would **verifiable AI memory** make you more confident in using autonomous agents for critical real-world tasks?\n\nhttps://reddit.com/link/1nmb07q/video/0eezhlkq7eqf1/player\n\n",
    "author": "Serkandereli27",
    "timestamp": "2025-09-20T15:06:46",
    "url": "https://reddit.com/r/bigdata/comments/1nmb07q/redefining_trust_in_ai_with_autonomys/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nmaqyc",
    "title": "Unlocking Web3 Skills with Autonomys Academy ğŸš€",
    "content": "Autonomys Academy is quickly becoming a gateway for anyone who wants to move from *learning* to *building* in Web3. Integrated with the [Autonomys Developer Hub](https://academy.autonomys.xyz), it offers hands-on resources, guides, and examples designed to help developers master the tools needed to create the next generation of decentralized apps.\n\nSome of the core modules include:\n\n* **Auto SDK**: A modular toolkit that streamlines the process of building decentralized applications (super dApps). It provides reusable components and abstractions that save time while enabling scalable, production-ready development.\n* **Auto EVM**: Full Ethereum Virtual Machine compatibility, letting developers work with familiar tools like MetaMask, Remix, and HardHat while still deploying on Autonomys. This means broader ecosystem access with minimal friction.\n* **Auto Agents**: An exciting framework for building autonomous, AI-powered on-chain agents. These can automate tasks, manage transactions, or even act as intelligent services within decentralized applications.\n* **Distributed Storage &amp; Compute**: Modules that teach how to store and process data in a decentralized way â€” key for building user-first, censorship-resistant applications.\n* **Decentralized Identity &amp; Payments**: Critical for enabling secure, user-controlled access and seamless value transfer in Web3 environments.\n\nFor me, the **Auto Agents path** is the most exciting. The idea of deploying on-chain agents that can automate processes or interact intelligently with users feels like the missing link between AI and Web3. Imagine a decentralized marketplace where autonomous agents handle bids, manage inventory, and even provide customer support â€” all without centralized control.\n\nIâ€™m curious: **If you were to start exploring Autonomys Academy, which module would you dive into first, and what project would you want to build?**",
    "author": "Serkandereli27",
    "timestamp": "2025-09-20T14:55:36",
    "url": "https://reddit.com/r/bigdata/comments/1nmaqyc/unlocking_web3_skills_with_autonomys_academy/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nlwlaw",
    "title": "Mastering Docker For Data Science In 5 Easy Steps",
    "content": "Docker isnâ€™t just a tool; itâ€™s a mindset for modern data science. Learn to build reproducible environments, orchestrate workflows, and take projects from your local machine to production without friction. The USDSIÂ® Data Science Certifications are designed to help professionals harness Docker and other essential tools with confidence.\n\nhttps://preview.redd.it/sm4y7glw9bqf1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=2e4afe8a303c65689fb5b14fbfe75e13288fed3a\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-20T05:13:03",
    "url": "https://reddit.com/r/bigdata/comments/1nlwlaw/mastering_docker_for_data_science_in_5_easy_steps/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nlted0",
    "title": "Any recommendations on data labeling/annotation services for a CV startup?",
    "content": "We're a small computer vision startup working on detection models, and we've reached the point where we need to outsource some of our data labeling and collection work.\n\nFor anyone who's been in a similar position, what data annotation services have you had good experiences with? Looking for a good outsourcing company who can handle CV annotation work and also data collection.\n\nAny recommendations (or warnings about companies to avoid) would be appreciated!",
    "author": "Last_Following_3507",
    "timestamp": "2025-09-20T02:09:25",
    "url": "https://reddit.com/r/bigdata/comments/1nlted0/any_recommendations_on_data_labelingannotation/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nl1sb6",
    "title": "Lessons from building a data marketplace: semantic search, performance tuning, and LLM discoverability",
    "content": "Hey everyone,\n\nWeâ€™ve been working on a project called **OpenDataBay**, and I wanted to share some of the *big data engineering lessons* we learned while building it. The platform itself is a data marketplace, but the more interesting part (for this sub) was solving the technical challenges behind scalable dataset discovery.\n\nA few highlights:\n\n1. **Semantic search vs keyword search**\n   * Challenge: datasets come in many formats (CSV, JSON, APIs, scraped sources) with inconsistent metadata.\n   * We ended up combining vector embeddings with traditional indexing to balance semantic accuracy and query speed.\n2. **Performance optimization**\n   * Goal: keep metadata queries under 200ms, even as dataset volume grows.\n   * Tradeoffs we made between pre-processing, caching, and storage format to achieve this.\n3. **LLM-ready data exposure**\n   * We structured dataset metadata so that LLMs like ChatGPT/Perplexity can â€œdiscoverâ€ and surface them naturally in responses.\n   * This feels like a shift in how search and data marketplaces will evolve.\n\nIâ€™d love to hear how others in this community have tackled **heterogeneous data search at scale**:\n\n* How do you balance semantic vs keyword retrieval in production?\n* Any tips for keeping query latency low while scaling metadata indexes?\n* What approaches have you tried to make datasets more â€œmachine-discoverableâ€?\n\n(P.S. This all powers [opendatabay.com](https://opendatabay.com), but the main point here is the technical challenges â€” curious to compare notes with folks here.)",
    "author": "Winter-Lake-589",
    "timestamp": "2025-09-19T05:08:31",
    "url": "https://reddit.com/r/bigdata/comments/1nl1sb6/lessons_from_building_a_data_marketplace_semantic/",
    "score": 17,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nke6og",
    "title": "Show /r/bigdata: Writing \"Zen and the Art of Data Maintenance\" - because 80% of AI projects still fail, and it's rarely the model's fault",
    "content": "Hey r/bigdata!\n\nI'm David Aronchick - co-founder of [Kubeflow](https://en.wikipedia.org/wiki/Kubeflow), first non-founding PM on Kubernetes, and co-founder of [Expanso](https://expanso.io) (former Google/AWS/MSFT x2). After years of watching data and ML projects crater, I'm writing a book about what actually kills them: data preparation.\n\n**The summary***\n\nWe obsess over model architectures while ignoring that:\n- Developer time debugging broken pipelines often exceeds initial development by 3x\n- One bad ingestion decision can trigger cascading cloud egress fees for months\n- \"Quick fixes\" compound into technical debt that kills entire projects\n- Poor metadata management means reprocessing TBs of data because nobody knows what transform was applied\n\n**What This Book Covers**\n\nReal patterns from real scale. No theory, just battle-tested approaches to:\n- Why your video/audio ingestion will blow your infrastructure budget (and how to prevent it)\n- Building pipelines that don't require 2 AM fixes\n- When Warehouses vs Lakes vs Lakehouses actually matter (with cost breakdowns)\n- Production patterns from Netflix, Uber, Airbnb engineering\n\n**The Approach**\n\nCompletely public development. I want this to be genuinely useful, not another thing that just sits on the shelf gathering dust.\n\n* **Outline**: [GitHub - Full Outline](https://github.com/aronchick/Project-Zen-and-the-Art-of-Data-Maintenance/blob/main/Outline.md)\n* **Published chapters**: [Distributed Thoughts](https://distributedthoughts.org/)\n* **Code examples**: [GitHub Repo](https://github.com/aronchick/Project-Zen-and-the-Art-of-Data-Maintenance)\n\n**What I Need From You**\n\nYour war stories. What cost you the most time/money? What \"best practice\" turned out to be terrible at scale? What do you wish every junior engineer knew about data pipelines?\n\nParticularly interested in:\n- Pipeline failure horror stories\n- Clever solutions to expensive problems\n- Patterns that actually work at PB scale\n- Tools that deliver (and those that don't)\n\nThis is a labor of love - not selling anything, just trying to help the next generation avoid our mistakes. Hell, I'll probably give it away for free (CERTAINLY give a copy to anyone who chats with me!)\n\nEmail me directly: aronchick (at) expanso (dot) io",
    "author": "Iron_Yuppie",
    "timestamp": "2025-09-18T10:11:48",
    "url": "https://reddit.com/r/bigdata/comments/1nke6og/show_rbigdata_writing_zen_and_the_art_of_data/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nkf76x",
    "title": "Databricks Announces Public Preview of Databricks One",
    "content": "",
    "author": "dofthings",
    "timestamp": "2025-09-18T10:49:35",
    "url": "https://reddit.com/r/bigdata/comments/1nkf76x/databricks_announces_public_preview_of_databricks/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1njzsek",
    "title": "Innovative Tech For Data Science Future",
    "content": "Data science is evolving at light speed. From simple analytics to the incredible power of AI, the field is undergoing a massive transformation. Want to know what's next? Explore the trends and emerging technologies that will revolutionize how to interact with data in 2025 and beyond.\n\nhttps://preview.redd.it/gczv4lewyupf1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=bab0ea0b312e694a50263bddd98adb02bb6d6c17\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-17T22:22:51",
    "url": "https://reddit.com/r/bigdata/comments/1njzsek/innovative_tech_for_data_science_future/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nje7qm",
    "title": "Big Data LDN",
    "content": "https://preview.redd.it/quorlpn3gqpf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=584293e48ff648591f9ed102954c179cfdeaa95d\n\n",
    "author": "Obvious-Friend4563",
    "timestamp": "2025-09-17T07:10:29",
    "url": "https://reddit.com/r/bigdata/comments/1nje7qm/big_data_ldn/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1njb0k7",
    "title": "Key Differences: Data Science, Machine Learning, and Data Analytics",
    "content": "Imagine it to be a case of map exploration using GPS technology. Data Analytics is the reading of the map and knowing where you have been and the reason why you went that way. Data Science is the navigator who learns various maps and traffic patterns to plan the most optimal path and foresee what may occur in the future.\n\nMachine Learning is similar to the GPS itself, which gets to know your driving history and traffic information, and then proposes more intelligent routes on its own.\n\nThese three disciplines are united to drive the digital world in which you live. Letâ€™s understand them one by one, and then we will also explore the difference between them.Â \n\n# What is Data Science?\n\nThe broadest of the three is data science. It is a combination of statistics, programming, and knowledge of the domain to analyze data. A data scientist does not simply look at numbers. They purify raw data, investigate trends, create models, and present information that can be used to solve large-scale problems.\n\n**Examples in action:**\n\nâ—Â Â Data science is applied in healthcare systems to forecast the risks of diseases.\n\nâ—Â Â It is used to prevent fraud in banks by detecting suspicious transactions.\n\nâ—Â Â It is used by social media to suggest friends or trending posts.\n\nData science processes both structured data (such as spreadsheets) and unstructured data (such as videos or posts on social networks). This is why it often uses **big data technologies** such as Hadoop and Spark to handle large volumes of information.\n\n**Key steps in data science include:**\n\nâ—Â Â Gathering and purifying raw data.\n\nâ—Â Â Trend analysis using statistics.\n\nâ—Â Â Predicting results using predictive models.\n\nâ—Â Â Automating data flow by constructing pipelines.\n\n# What is Data Analytics?\n\nThe data analytics is more targeted and direct. It examines the past and present data to explain what and why it occurred. In contrast to data science, which is wider and predictive, analytics is concerned with reporting and problem diagnosis in order to make better decisions by businesses.\n\n**Popular applications of data analytics:**\n\nâ—Â Â Customers learn how customers shop to enhance product placement by retailers.\n\nâ—Â Â Performance data is analyzed by sports teams to change strategies.\n\nâ—Â  Governments can check transportation data to enhance traffic congestion.\n\nTableau, Power BI, and Excel are some of the [data visualization tools](https://www.usdsi.org/data-science-insights/top-13-data-visualization-tools-for-2023-and-beyond) that are important to data analysts. These tools produce charts, dashboards, and graphs that help in the easy understanding of numbers. It is like converting unprocessed information into a narrative that leaders of business can easily understand.Â \n\n# What is Machine Learning?\n\nMachine learning is a subfield of artificial intelligence that trains systems to learn from data. You do not have to write step-by-step rules to program a machine, but instead, you feed it huge quantities of data, and it gets better as you go.\n\n**Real-world examples:**\n\nâ—Â Â Your spam mail filter gets to know what is spam.\n\nâ—Â Â Netflix suggests the shows depending on what you have watched.\n\nâ—Â Â Fraud is detected immediately through online payment systems.Â \n\n# Core Differences Between ThemÂ \n\n||\n||\n|**Feature**|**Data Science**|**Data Analytics**|**Machine Learning**|\n|**Definition**|This is an interdisciplinary subject that involves statistics, programming, and domain knowledge to derive insights and develop predictive or prescriptive solutions. Â |This is the process of analyzing available data to define trends, justify results, and make business judgments. Â |A branch of artificial intelligence that deals with the learning algorithms that can learn as they go without being explicitly programmed. Â |\n|**Primary Focus**|Data science considers the entire data process, including the collection and cleaning, as well as modeling and implementation. Â |Data analytics narrows down to the interpretation of datasets in order to respond to certain questions. Â |Machine learning focuses on the creation of models that are adaptive and optimize with the help of constant training. Â |\n|**Data Dependence**|Structured, semi-structured, and unstructured data can be processed in data science.|Data analytics primarily operates with structured data. Â |Machine learning needs vast and varied datasets in order to train useful models. Â |\n|**Methods Used**|Data science applies statistics, predictive modeling, and **big data technologies**. Â |Data analytics involves descriptive statistics, diagnostic analysis, and **data visualization tools**. Â |Machine learning is based on supervised, unsupervised, and reinforcement algorithms. Â |\n|**Breadth of Work** Â |Data science is wide encompassing various fields in order to deal with multifaceted issues. Â |Data analytics is limited and is concerned with instant reporting and insights. Â |Machine learning is profound, and it explores algorithm design and system intelligence. Â |\n\nThese were the major differences between them. Now, letâ€™s understand which path you should choose.Â \n\n# Which Path Should You Choose?\n\nIn determining your course of action, consider what you are most excited about:\n\nâ—Â Â Â In case you prefer describing findings and creating vivid illustrations, consider data analytics.\n\nâ—Â Â Â In case you like working on broad, complex problems and creating predictive models, choose data science.\n\nâ—Â Â Â Machine learning is the way to go in case you have a dream of creating self-learning and self-adapting systems.\n\nRegardless of the choice of path, all three are future-proof and have good career prospects. But one more thing is the real fact, and that is that the skills gap is regarded as the largest. barrier to the future of business transformation by Future of Jobs Survey respondents, 63% of employers citing them as a significant obstacle in the 2025-2030 period. (World Economic Forum - Future of Jobs Report - 2025)\n\nThatâ€™s why upskilling is the most crucial part if you want to pursue a career in any of the above three fields.Â \n\n# Wrap Up\n\nIn the modern digital age, data is the fuel, and disciplines such as data science, data analytics, and machine learning are engines that consume it. Data analytics describes the past, data science tells us what to expect in the future, and machine learning makes systems smarter with each new bit of information. They are all interrelated with the help of [big data technologies](https://www.usdsi.org/data-science-insights/comparing-data-science-big-data-and-data-analytics) and provide businesses with the necessary scale.\n\nAt this point, you are aware of the way each of these fields operates, the differences between them, and what career opportunities they offer. Your next action is to select the path that fits best and begin acquiring the tools and developing the skills. Technology is a future that is based on data, and you can join it.",
    "author": "sharmaniti437",
    "timestamp": "2025-09-17T04:53:16",
    "url": "https://reddit.com/r/bigdata/comments/1njb0k7/key_differences_data_science_machine_learning_and/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nj6qp1",
    "title": "Supercharge Data Transformation with Rust &amp; Vide Coding",
    "content": "Why waste time manually coding every line when AI can help you build smarter, faster? Combine Rustâ€™s high performance with vibe coding to simplify data transformation tasks and focus on solving real problems. \n\nhttps://preview.redd.it/dxm9f8jzhopf1.jpg?width=1536&amp;format=pjpg&amp;auto=webp&amp;s=5488ef0f3ae4ad8870a17ca8c4d193976ce44658\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-17T00:37:21",
    "url": "https://reddit.com/r/bigdata/comments/1nj6qp1/supercharge_data_transformation_with_rust_vide/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nia6ps",
    "title": "Struggling to Explain Data Orchestration to Leadership",
    "content": "Weâ€™ve noticed a lot of professionals hitting a wall when trying to explain the need for **data orchestration** to their leadership. Managers want quick wins, but lack understanding of how data flows across the different tools they use. The focus on moving fast leads to firefighting instead of making informed decisions.\n\nWe wrote an article that breaks down:\n\n* What data orchestration actually is\n* The risks of ignoring it\n* How executives can better support modern data initiatives\n\nIf youâ€™ve ever felt frustrated trying to make leadership see the bigger picture, this article can help.\n\nğŸ‘‰ Read the full blog here: [https://datacoves.com/post/data-orchestration-for-executives](https://datacoves.com/post/data-orchestration-for-executives)",
    "author": "Data-Queen-Mayra",
    "timestamp": "2025-09-15T23:41:25",
    "url": "https://reddit.com/r/bigdata/comments/1nia6ps/struggling_to_explain_data_orchestration_to/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nhicas",
    "title": "Best Practices Versioned Data with Apache Iceberg Using lakeFS Iceberg REST Catalog",
    "content": "",
    "author": "innpattag",
    "timestamp": "2025-09-15T03:38:46",
    "url": "https://reddit.com/r/bigdata/comments/1nhicas/best_practices_versioned_data_with_apache_iceberg/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nhfrix",
    "title": "Workshop: From Raw Data to Insights with Datacoves, dbt, and MotherDuck",
    "content": "ğŸ‘‹ Hey folks, want to learn about DuckDB, DuckLake, dbt, and more, Datacoves is hosting a **workshop with MotherDuck**\n\n**ğŸ“ Topic:** **From Raw Data to Insights with Datacoves, dbt, and MotherDuck**\n\nğŸ“… Date: Wednesday, Sept 25\n\nğŸ•˜ Time: 9:00 am PDT\n\n**ğŸ‘¤ Speakers:**\n\n* **Noel Gomez** â€“ Co-founder, Datacoves\n* **Jacob Matson** â€“ Developer Advocate, MotherDuck\n\nWeâ€™ll cover:\n\n* How to connect to S3 as a source and model data with dbt into a DuckLake\n* How DuckDB + dbt can simplify workflows and reduce costs\n* Why smaller, lighter pipelines often beat big, expensive stacks\n\nThis will be a **practical session,** no sales pitch, just a walk-through from data ingestion with dlt through orchestration with Airflow.\n\nIf youâ€™re curious about **dbt, DuckLake, or DuckDB,** it's worth checking out.\n\nIâ€™m also happy to answer any questions here\n\n[https://datacoves.com/resource-center/workshop-from-raw-data-to-insights-with-datacoves-dbt-and-motherduck](https://datacoves.com/resource-center/workshop-from-raw-data-to-insights-with-datacoves-dbt-and-motherduck)",
    "author": "Data-Queen-Mayra",
    "timestamp": "2025-09-15T00:53:47",
    "url": "https://reddit.com/r/bigdata/comments/1nhfrix/workshop_from_raw_data_to_insights_with_datacoves/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nhizgg",
    "title": "Spark lineage tracker â€” automatically captures table lineage",
    "content": "",
    "author": "Icy-Science6979",
    "timestamp": "2025-09-15T04:14:41",
    "url": "https://reddit.com/r/bigdata/comments/1nhizgg/spark_lineage_tracker_automatically_captures/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nhdxju",
    "title": "Apache Zeppelin â€“ Big Data Visualization Tool with 2 Caption Projects",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-09-14T22:58:43",
    "url": "https://reddit.com/r/bigdata/comments/1nhdxju/apache_zeppelin_big_data_visualization_tool_with/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ngqa0u",
    "title": "Sharing the playlist that keeps me motivated while coding â€” it's my secret weapon for deep focus. Got one of your own? I'd love to check it out!",
    "content": "",
    "author": "Firmach43",
    "timestamp": "2025-09-14T05:40:15",
    "url": "https://reddit.com/r/bigdata/comments/1ngqa0u/sharing_the_playlist_that_keeps_me_motivated/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ng8gaf",
    "title": "Storing large amount of data without taking up space on your device",
    "content": "(in theory infinite) cloud storage\n\n\nHi,\nI have been looking for a large amount of storage for free and now when I found it I wanted to share.\n\nMy first recommendation would be Filen since they use encryption. If you refer 3 friends you will get 50 gb for fee which is a lot more than google provides. \n\nIf you want a stupidly big ammount of storage you can use Hivenet. For each person you refer you get 10 gb for free stacking infinetly! If you use my my link you will also start out with an additional 10 gb.\n\nhttps://www.hivenet.com/referral?referral_code=8UiVX9DwgWK3RBcmmY5ETuOSNhoNy%2BRTCTisjZc0%2FzemUpDX%2Ff4rrMCXgtSILlC%2Bf%2B7TFw%3D%3D\n\n I already got 110 gb for free using this method but if you invite many friends you will litterally get terabytes of free storage.",
    "author": "Adi-Imin",
    "timestamp": "2025-09-13T13:52:14",
    "url": "https://reddit.com/r/bigdata/comments/1ng8gaf/storing_large_amount_of_data_without_taking_up/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nfy3kx",
    "title": "45% off New Book: Architecting an Apache Iceberg Lakehouse (Manning)",
    "content": "Use Discount Code RustConf25 for 45% off (code expires Sept 19th)",
    "author": "AMDataLake",
    "timestamp": "2025-09-13T06:54:39",
    "url": "https://reddit.com/r/bigdata/comments/1nfy3kx/45_off_new_book_architecting_an_apache_iceberg/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nfy1xr",
    "title": "45% of new book from Manning \"Architecting an Apache Iceberg Lakehouse\"",
    "content": "Purchase Here: [https://hubs.la/Q03GfY4f0](https://hubs.la/Q03GfY4f0)  \n45% Discount Code (Expires September 19th): RustConf25",
    "author": "AMDataLake",
    "timestamp": "2025-09-13T06:52:40",
    "url": "https://reddit.com/r/bigdata/comments/1nfy1xr/45_of_new_book_from_manning_architecting_an/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nfhgua",
    "title": "Best Local Ecosystem",
    "content": "Good day!\n\nWhat I want to do:\n- local setup\n- Geospatial analytics, modeling and visualization \n â€” years of census Tiger shapefiles (roads, features, tracts, pumas)\n&lt;â€”â€” integration with ACS PUMA data \nâ€” Misc additional geospatial data (raster, gdb, kml)\n\nLimitations:\n- 24 CPU threads\n- 128 gb ram\n-16 gb vram\n- 10 TB of storage on desktio \n\nInitial setup\n- Ozone for storage \n- Iceberg for table format\n&lt;â€”- cataloged in postgres\n- Apache Sedona/spark for processing \n- eventually: TorchGeo to play around with modeling \n+ (kerby for security) \n\nAt the bare minimum, I want a solid introduction to setting up and maintaining a big data ecosystem within limitations of local devices (primordial services on workstations, nodes across misc devices - laptops) \n\nQuestions:\n- what ecosystem would you design?\n- best practices/ tips/ tricks \n- feasibility of all this\n- different ways to go about everything!\n\nNotes\n- ready for a challenge!\n",
    "author": "[deleted]",
    "timestamp": "2025-09-12T15:52:09",
    "url": "https://reddit.com/r/bigdata/comments/1nfhgua/best_local_ecosystem/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nf1q2x",
    "title": "Top 5 Cybersecurity Certifications to Enroll in 2026",
    "content": "The digital world is transforming fast â€” due to this, cyber threats and attacks are also advancing. Corporations, governments, and individuals rely on secure systems, but the skill gap is increasing; they are not able to hire the right talent to protect their systems.\n\nAccording to the World Economic Forumâ€™s Future of Jobs Report 2025, cybersecurity will be one of the top 2 fastest-growing skills for all professions (2025-2030), as illustrated in the graph.\n\n  \nThe problem is that weâ€™re still in an age where what you learn in school isnâ€™t what the industry needs. **Cybersecurity certifications** are one of the best ways to close that gap: they put your skills on display and demonstrate to employers that youâ€™re up to date.\n\nHere are five of the [best cybersecurity certifications](https://www.uscsinstitute.org/cybersecurity-certifications) to enroll in, including official information, perks, and career paths.Â \n\n# Top 5 Cybersecurity Certifications to Enroll in 2026\n\nHere are the best 5 cybersecurity certifications that are capable of upskilling you and helping you fill the skill gap to get hired faster than ever for associate, intermediate, or senior level positions:\n\n# 1.Â Â Certified Senior Cybersecurity Specialist (CSCSâ„¢) by USCSIÂ®\n\nThe CSCSâ„¢ certification is ideal for those who strive to attain the most esteemed job titles in the cybersecurity industry. It offers an organized, comprehensive framework for developing technical and strategic competence.\n\nâ—Â Â Â **Skills taught:** Duration: It is up to you, covering the full 4-24 weeks.\n\nâ—Â Â Â F**ormat:** 100% online, self-paced, so you can study while you work.\n\nâ—Â Â Â **Qualifications:** Associate's degree or higher in a related field, depending on experience level.\n\nâ—Â Â Â **Strong Impacted Skills:** Data security, cryptography, security leadership, compliance, and advanced defensive strategies.\n\nâ—Â Â Â **Career Prospects:** Makes you ready for positions such as Senior Security Analyst, Cybersecurity Consultant, and Security Architect.\n\nIf your goal is to understand how attacks occur in the real world and how to create better defense methods, with the additional goal of leading any organizationâ€™s cybersecurity team, this certification is the right choice for you.\n\n# 2.Â Â CompTIA Security+\n\nThe CompTIA Security+ **cybersecurity certification** is the entry-level certification for information security professionals.\n\nâ—Â Â **Length of study:** Study time differs for everybody, but most people study for 3-6 months.\n\nâ—Â Â **Exam Format:** Multiple-choice and performance-based questions on a proctored exam.\n\nâ—Â Â **Prerequisites:** No formal prerequisites, but 1â€“2 years of IT experience is suggested.\n\nâ—Â Â **Skills Learned:** Risk control, encryption, incident response, network and application security, and threat monitoring.\n\nâ—Â Â **Career Prospects:** Perfect for a Security Analyst, Network Administrator, or IT Support with a security emphasis.\n\n# 3.Â Â Certified Ethical Hacker (CEH) â€” EC-Council\n\nThis **cybersecurity certification** will equip individuals with the tools necessary to spot the vulnerabilities and weaknesses in target systems. If you are into penetration testing and learning how hackers think, the certification can be highly beneficial. It teaches you how to think like the attacker and use both tactics to your advantage.\n\nâ—Â Â **Length:** Usual 4 â€“ 6 months preparation if studied with Official Training.\n\nâ—Â Â **Format:** Two exams â€” a multiple-choice knowledge exam and a hands-on practical test.\n\nâ—Â Â **Prerequisites:** A minimum of 2 years of experience or formal training.\n\nâ—Â Â **Key Skills Taught:** Vulnerability scanning, penetration testing, network mapping, attack mechanisms, and mitigating measures.\n\nâ—Â Â **Career Opportunities:** Provides access to positions like Ethical Hacker, Penetration Tester, and Vulnerability Analyst.Â \n\n# 4.Â Â Certified Information Systems Security Professional (CISSP) â€” ISC2\n\nThe ISC2 CISSP certification focuses on information security and offers a detailed foundation for aspiring security professionals. CISSP is a highly preferred **cybersecurity certification**..\n\nâ—Â Â **Length:** Preparation takes 6 months to a year, considering its depth.  \n**Format:** CAT, up to 150 questions in eight domains of cybersecurity.\n\nâ—Â Â **Key Skills Covered:** Risk management, asset security, identity access management, architecture, and operations.\n\nâ—Â Â **Careers:** This program will prepare you for such roles as Security Manager, Security Architect, and Chief Information Security Officer (CISO).\n\nCISSP isnâ€™t for novices, but is perfect for experienced professionals who want to put their careers on a fast track and move into leadership â€” or even management.\n\n# 5.Â Offensive Security Certified Professional (OSCP) â€” OffSec\n\nThe OSCP is among the most difficult certifications in the field of cybersecurity. It is very technical and is strictly based on hands-on penetration testing **cybersecurity training**.\n\nâ—Â Â **Length:** Candidates usually spend months studying, frequently working hands-on in labs.\n\nâ—Â Â **Format:** An intensive examination\n\nâ—Â Â **Main Topics:** attack vectors, custom scripting, escalation of privileges, exploitation of vulnerabilities, and pen test reporting.\n\nâ—Â Â **Career Prospects:** Best for jobs such as Penetration Tester, Red Team Member, and Security Consultant.\n\nThese were the **best cybersecurity certifications** that employers appreciate if you have earned any of them.\n\n# The Bottom Line\n\nCybersecurity is a strong growth industry. To just keep up, professionals have to stay one step ahead in their skillset and prove their expertise. The right certification will not just round out your resume but also keep you competitive as the threats you face become more sophisticated.\n\nIf youâ€™re new, you will want to start on the foundational knowledge, or looking for a cybersecurity management level intermediate certification, or dreaming of becoming a senior cybersecurity specialist, these **cybersecurity certifications** are globally the standard course you can enroll in to enhance your cybersecurity skills and knowledge.\n\nNo matter where youâ€™re beginning, the suitable certification can help put you on the road to a solid, high-demand career in cybersecurity today and tomorrow.",
    "author": "sharmaniti437",
    "timestamp": "2025-09-12T05:21:01",
    "url": "https://reddit.com/r/bigdata/comments/1nf1q2x/top_5_cybersecurity_certifications_to_enroll_in/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1neueo2",
    "title": "ChatGPT for Data Engineer (Hands-on Practice)",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-09-11T21:58:51",
    "url": "https://reddit.com/r/bigdata/comments/1neueo2/chatgpt_for_data_engineer_handson_practice/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1neilxg",
    "title": "100TB HBase to MongoDB database migration without downtime",
    "content": "Recently we've been working on adding HBase support to [dsync](https://github.com/adiom-data/dsync/). Database migration at this scale with 100+ billion of records and no-downtime requirements (real-time replication until cutover) comes with a set of unique challenges. \n\nKey learnings:\n\n  \\- Size matters\n\n  \\- HBase doesnâ€™t support CDC\n\n  \\- This kind of migration is not a one-and-done thing - need to iterate (a lot!)\n\n  \\- Key to success: Fast, consistent, and repeatable execution\n\nCheck out [our blog post](https://www.adiom.io/post/hbase-to-mongodb-migration) for technical details on our approach and the short [demo video](https://youtu.be/qk2CwSQ7rOU?si=beHtByo-9Zwofy_h) to see what it looks like.",
    "author": "mr_pants99",
    "timestamp": "2025-09-11T12:47:49",
    "url": "https://reddit.com/r/bigdata/comments/1neilxg/100tb_hbase_to_mongodb_database_migration_without/",
    "score": 8,
    "num_comments": 10,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nee5gl",
    "title": "Metadata is the New Oil: Fueling the AI-Ready Data Stack",
    "content": "",
    "author": "zookeeper_48",
    "timestamp": "2025-09-11T09:58:11",
    "url": "https://reddit.com/r/bigdata/comments/1nee5gl/metadata_is_the_new_oil_fueling_the_aiready_data/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ne2320",
    "title": "Boost Your Security Strategy With Data Science and Biometric",
    "content": "Biometric authentication is transforming security, but fingerprints, facial scans, or voice recognition arenâ€™t foolproof. Data science strengthens these systems by fusing multiple biometric traits and applying adaptive models to ensure accuracy and resilience. Learn how to implement continuous authentication with USDSIÂ® data science certifications.\n\nhttps://preview.redd.it/cxpmbsvpihof1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=212d32aa41b7f71500daeed0156378e5bff88341\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-11T00:04:54",
    "url": "https://reddit.com/r/bigdata/comments/1ne2320/boost_your_security_strategy_with_data_science/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ndp6bx",
    "title": "Creating topics within a docker container",
    "content": "",
    "author": "SyntxaError",
    "timestamp": "2025-09-10T13:20:32",
    "url": "https://reddit.com/r/bigdata/comments/1ndp6bx/creating_topics_within_a_docker_container/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ndge4w",
    "title": "Contract Opportunity - Senior Quantexa Developer",
    "content": "Hey Reddit,\n\n  \nCurrently looking for those with experience in Quantexa (certificate) and Scala experience that would be open to hearing about a contract opportunity for a large bank.\n\nFeel free to direct message me and I can give some more details and see if we can move forward.\n\n  \nThanks!",
    "author": "Longjumping_Golf9070",
    "timestamp": "2025-09-10T07:53:41",
    "url": "https://reddit.com/r/bigdata/comments/1ndge4w/contract_opportunity_senior_quantexa_developer/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ncb2nr",
    "title": "Revolutionize Agentic AI With Knowledge Graphs",
    "content": "Reactive AI is outdated. Agentic AI takes autonomy to the next level by predicting problems and solving them without instructions. When paired with Knowledge Graphs, it empowers smarter decision-making. Learn how your business can benefit today.\n\nhttps://preview.redd.it/ncmsgl2yw2of1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=c9826c8c4c8edd60e982f3d2b25a02eb3656edf3\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-08T22:57:59",
    "url": "https://reddit.com/r/bigdata/comments/1ncb2nr/revolutionize_agentic_ai_with_knowledge_graphs/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nbxqcz",
    "title": "Lessons from building modern data stacks for startups (and why we started a blog series about it)",
    "content": "",
    "author": "Mafixo",
    "timestamp": "2025-09-08T12:44:08",
    "url": "https://reddit.com/r/bigdata/comments/1nbxqcz/lessons_from_building_modern_data_stacks_for/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1nblgz8",
    "title": "The Future of Data &amp; AIoT",
    "content": "Hola a todos.\n\nNos gustarÃ­a invitaros a un evento online que creemos os puede interesar: â€œTheâ€¯Futureâ€¯ofâ€¯Dataâ€¯&amp;â€¯AIoTâ€. En este encuentro hablaremos de cÃ³mo la convergencia entre el Internet de las Cosas, la inteligencia artificial y la analÃ­tica avanzada (AIoT) estÃ¡ transformando nuestra forma de hacer negocios y de tomar decisiones.\n\nSe tratarÃ¡n estos temas entre otros:\n\nEl futuro de los datos es contextual: desbloqueando el potencial de la IA con dbt\n\nProductos de datos impulsados por inteligencia artificial listos para el futuro\n\nGobernanza y sostenibilidad en los datos \n\nMESA REDONDA\n\nEl futuro del AIoT y los datos: talento, regulaciÃ³n y oportunidades\n\nEl evento incluirÃ¡ ponencias de profesionales del sector de empresas cÃ³mo Dbt Labs, Microsoft, telefÃ³nica Tech, IBM y una mesa redonda para debatir retos y oportunidades. La asistencia es gratuita (previa inscripciÃ³n) y estÃ¡ abierta a quienes quieran aprender y compartir experiencias.  \nEn breve estarÃ¡n los ponentes de este aÃ±o en la web.\n\n[https://www.iebschool.com/eventos/the-future-of-data/](https://www.iebschool.com/eventos/the-future-of-data/)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "author": "iebschool",
    "timestamp": "2025-09-08T04:44:31",
    "url": "https://reddit.com/r/bigdata/comments/1nblgz8/the_future_of_data_aiot/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n90wmj",
    "title": "Factsheet: Data Science Career 2025",
    "content": "Learn about the latest data science industry insights, trends, salary outlooks, interesting facts, and top opportunities in our Data Science Career Factsheet 2025.\n\nhttps://reddit.com/link/1n90wmj/video/93myxmpfibnf1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-05T02:48:30",
    "url": "https://reddit.com/r/bigdata/comments/1n90wmj/factsheet_data_science_career_2025/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n8b7h2",
    "title": "Perplexity AI",
    "content": "",
    "author": "pragadeesh25",
    "timestamp": "2025-09-04T07:09:26",
    "url": "https://reddit.com/r/bigdata/comments/1n8b7h2/perplexity_ai/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n80u9k",
    "title": "Parquet Is Great for Tables, Terrible for Video - Combining Parquet for Metadata and Native Formats for Media with DataChain",
    "content": "The article outlines several fundamental problems that arise when teams try to store raw media data (like video, audio, and images) inside Parquet files, and explains how DataChain addresses these issues for modern multimodal datasets - by using Parquet strictly for structured metadata while keeping heavy binary media in their native formats and referencing them externally for optimal performance: [reddit.com/r/datachain/comments/1n7xsst/parquet_is_great_for_tables_terrible_for_video/](https://www.reddit.com/r/datachain/comments/1n7xsst/parquet_is_great_for_tables_terrible_for_video/)\n\nIt shows how to use Datachain to fix these problems - to keep raw media in object storage, maintain metadata in Parquet, and link the two via references.",
    "author": "thumbsdrivesmecrazy",
    "timestamp": "2025-09-03T21:36:16",
    "url": "https://reddit.com/r/bigdata/comments/1n80u9k/parquet_is_great_for_tables_terrible_for_video/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n7aby4",
    "title": "RAG for Data Science Precision",
    "content": "RAG is transforming how Large Language Models (LLMs) process nuanced data. From AI to data science, itâ€™s the backbone of precision-driven intelligence. Learn how Retrieval Augmented Generation is shaping the future of language models and beyond.\n\nhttps://preview.redd.it/3ol45ckx6xmf1.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=4fc981ea57ba55f0451cae3eab1d38c47864e87e\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-09-03T02:39:04",
    "url": "https://reddit.com/r/bigdata/comments/1n7aby4/rag_for_data_science_precision/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n77d3e",
    "title": "Scala FS2 vs Apache Spark",
    "content": "Hello! \nIâ€™m thinking about moving from Apache Spark based data processing to FS2 Typelevel lib.\nData volume Iâ€™m operating on is not huge (max 5 GB of input data).\nMy processing consists mostly of simple data transformation (without aggregations).\nCurrently Iâ€™m using Databricks to have an access to cluster, when moving to fs2 I would deploy it directly on k8s.\nWhat do you think about the idea? Has any of you tried such a transition before and can share any thoughts?\n",
    "author": "carpe_diem_00",
    "timestamp": "2025-09-02T23:25:19",
    "url": "https://reddit.com/r/bigdata/comments/1n77d3e/scala_fs2_vs_apache_spark/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n6y9bk",
    "title": "Macbook Air M2 16GB|256GB for social listening data sufficient?",
    "content": "",
    "author": "little_einschtein",
    "timestamp": "2025-09-02T15:53:11",
    "url": "https://reddit.com/r/bigdata/comments/1n6y9bk/macbook_air_m2_16gb256gb_for_social_listening/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n6bmql",
    "title": "Clickstream Behavior Analysis with Dashboard â€” Real-Time Streaming Project Using Kafka, Spark, MySQL, and Zeppelin",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-09-01T22:55:27",
    "url": "https://reddit.com/r/bigdata/comments/1n6bmql/clickstream_behavior_analysis_with_dashboard/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n4mes8",
    "title": "Sharing the playlist that keeps me motivated while coding â€” it's my secret weapon for deep focus. Got one of your own? I'd love to check it out!",
    "content": "",
    "author": "Firmach43",
    "timestamp": "2025-08-30T22:18:38",
    "url": "https://reddit.com/r/bigdata/comments/1n4mes8/sharing_the_playlist_that_keeps_me_motivated/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n4brsw",
    "title": "Strategy",
    "content": "\nGot a strong network in the financial marketsâ€”friends managing royal family wealth &amp; running fund companies. Looking to team up with people building profitable systems/software. If it works, we turn it into a fund &amp; sell it to banks. Investors are ready. DM if youâ€™re in.",
    "author": "Antikjapan",
    "timestamp": "2025-08-30T13:30:48",
    "url": "https://reddit.com/r/bigdata/comments/1n4brsw/strategy/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n3ws0i",
    "title": "Databricks Playlist with more than 850K Views",
    "content": "",
    "author": "Complex_Revolution67",
    "timestamp": "2025-08-30T01:51:51",
    "url": "https://reddit.com/r/bigdata/comments/1n3ws0i/databricks_playlist_with_more_than_850k_views/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n2zu5u",
    "title": "Explain LLAP (Live Long and Process) and its benefits in Hive",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-08-28T23:21:15",
    "url": "https://reddit.com/r/bigdata/comments/1n2zu5u/explain_llap_live_long_and_process_and_its/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n2cdwo",
    "title": "Bulk schema sources for big data ML training",
    "content": "working with big data ML pipelines and need vast amounts of schemas for training. primarily financial and retail domains but honestly need massive collections from every sector possible. looking for thousands of different schema types at scale. where do you all source bulk structured data schemas? need enterprise-level volume here.",
    "author": "Fragrant-Dog-3706",
    "timestamp": "2025-08-28T06:34:36",
    "url": "https://reddit.com/r/bigdata/comments/1n2cdwo/bulk_schema_sources_for_big_data_ml_training/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n26l10",
    "title": "Scaling dbt + BigQuery in production: 13 lessons learned (costs, incrementals, CI/CD, observability)",
    "content": "Iâ€™ve been tuning **dbt + BigQuery pipelines in production** and pulled together a set of practices that really helped. Nothing groundbreaking individually, but combined they make a big difference when running with Airflow, CI/CD, and multiple analytics teams.\n\nSome highlights:\n\n* **Materializations by layer** â†’ staging with ephemeral/views, intermediate with incrementals, marts with tables/views + contracts.\n* **Selective execution** â†’ `state:modified+` so only changed models run in CI/CD.\n* **Smart incrementals** â†’ no `SELECT *`, add time-window filters, use merge + audit logs.\n* **Horizontal sharding** â†’ pass `vars` (e.g. country/tenant) and split heavy jobs in Airflow.\n* **Clustering &amp; partitioning** â†’ improves query performance and keeps costs down.\n* **Observability** â†’ post-hooks writing row counts/durations to metrics tables for Grafana/Looker.\n* **Governance** â†’ schema contracts, labels/meta for ownership, BigQuery logs for real-time cost tracking.\n* **Defensive Jinja** â†’ donâ€™t let multi-tenant/dynamic models blow up.\n\nIf anyoneâ€™s interested, I wrote up a more detailed guide with examples (incremental configs, post-hooks, cost queries, etc.).\n\n[Link to post](https://medium.com/@sendoamoronta/dbt-bigquery-in-production-13-technical-practices-to-scale-and-optimize-your-data-platform-4963b8d041e2)",
    "author": "Expensive-Insect-317",
    "timestamp": "2025-08-28T01:22:04",
    "url": "https://reddit.com/r/bigdata/comments/1n26l10/scaling_dbt_bigquery_in_production_13_lessons/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n26zhu",
    "title": "AWS Certification Track 2025",
    "content": "",
    "author": "Big_Data_Path",
    "timestamp": "2025-08-28T01:49:04",
    "url": "https://reddit.com/r/bigdata/comments/1n26zhu/aws_certification_track_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n1ajvt",
    "title": "Data Science or Cybersecurity: Best Career For You?",
    "content": "Here are two technology careers that remain attractive due to their growth, impact, and potential earnings: Cybersecurity and Data Science. As all industries become increasingly data-driven and connected digitally, professionals who secure those systems and extract meaning from the data continue to gain relevance.Â \n\nAccording to Glassdoor's 2025 data, the average salary of cybersecurity employees in the U.S. is $126,000, while data scientists make an average of $128,000. Moreover, the U.S. Bureau of Labor Statistics listsÂ **32%**Â job growth forÂ [cybersecurity jobs](https://www.uscsinstitute.org/cybersecurity-insights/resources/top-12-highest-paying-cybersecurity-jobs-in-2025)Â andÂ **36%**Â job growth for data science jobs, which are expected to lead the technology and other industries through 2031.Â \n\nBoth career options have promising futures but have different mindsets, skills, and paths to reach the end point.Â  Here are specifics to help you select a practice that is right for you.Â \n\n# What Each Role Involves\n\n# Cybersecurity Career\n\nCybersecurity experts protect digital systems, networks, and sensitive data against cyber threats. So, with the rise in ransomware, phishing, and data breaches, this position minimizes attacks and ensures business continuity.\n\nSome common job responsibilities include:\n\nâ—Â Â Monitoring networks for suspicious activity\n\nâ—Â Â Conducting security audits and vulnerability assessments\n\nâ—Â Â Installing firewalls, encryption and authentication systems\n\nâ—Â Â Responding to incidents and remediating the damage from breaches\n\nTypical job titles are Security Analyst, Penetration Tester, Cybersecurity Engineer, and CISO (Chief Information Security Officer).\n\n# Data Science Career\n\nData scientists examine extensive amounts of data in order to find patterns, trends, and insights that inform business decisions. They use statistical models and machine learning to help businesses predict outcomes and optimize performance.\n\nSome examples of responsibilities would include:\n\nâ—Â Â Cleaning and processing structured and unstructured data.\n\nâ—Â Â Â Building predictive models and algorithms.\n\nâ—Â Â Â Creating visualizations and dashboards.\n\nâ—Â Â Â Working alongside business partners to drive strategy.\n\nSome common data science job roles are Data Scientist, Data Analyst, Machine Learning Engineer, and AI Researcher.\n\n# Skills Required\n\n|| || |**Category**|**Cybersecurity Skills**|**Data Science Skills**| |**Core Skills**|Network security, threat detection, encryption|Python, R, SQL, statistics, machine learning| |**Tools Used**|Firewalls, SIEM, intrusion detection systems|Jupyter, TensorFlow, Pandas, Tableau| |**Soft Skills**|Attention to detail, risk analysis, vigilance|Analytical thinking, storytelling with data| |**Background**|IT, computer networks, information systems|Computer science, math, statistics, business|\n\n# Certifications That Matter\n\n# Cybersecurity Certifications\n\nCertifications are a crucial means of verifying your skills and expertise in cybersecurity. Some of theÂ **top cybersecurity certifications**Â are:Â \n\nâ—Â Â Certified Cybersecurity General Practitionerâ„¢ (CCGPâ„¢) from USCSIÂ® is aÂ **self paced cybersecurity certification**Â offering a high-level, practical knowledge of cybersecurity fundamentals and is appropriate for professionals entering into or transitioning into a cybersecurity role.\n\nâ—Â Â CompTIA Security+, an entry-level and well-regarded certification.\n\nâ—Â Â Certified Information Systems Security Professional (CISSP), aimed at leaders with several years of professional experience.Â \n\n# Data Science Certifications\n\nData science professionals frequently pursue certifications to solidify their skill sets with experience and tool-based learning. There are many beneficial and recognizable certifications, such as:\n\nâ—Â Â The Certified Data Science Professionalâ„¢ (CDSPâ„¢) by USDSIÂ® is a self paced data science certification that is recognized worldwide and emphasizes being able to conduct practical data science in a business environment.\n\nâ—Â Â The Data Science Certificate Program from Harvard University, as well as the Certificate of Professional Achievement in Data Sciences from Columbia University, are both stand-alone, non-degree programs tailored for working professionals offered through Ivy League institutions.\n\n# Job Market and Trends in Todayâ€™s Landscape\n\n# Cybersecurity Trends\n\nStatista indicates that projected annual costs associated with cybercrime around the globe continue to grow modestly. It will hit 15.63 trillion U.S. dollars by 2029. This has created an increased demand for cybersecurity talent across industries.\n\nRecent trends include:\n\nâ—Â AI-enabled threat detection\n\nâ—Â Zero-trust security models\n\nâ— Increase in cloud and IoT security\n\nâ—Â Increased compliance requirements in finance and healthcareÂ \n\nWith a reported global shortage of more than 3.5 million talent according to Cybersecurity Ventures, there are plenty of job opportunities in the cybersecurity industry..Â \n\n# Data Science Landscape\n\nAs businesses rely more on data, the demand for data scientists to analyze and automate insights is rising. Current trends include:\n\nâ—Â Â AutoML and MLOps.\n\nâ—Â Â Expansion of generative AI and large, contextual language models.\n\nâ—Â Â The intersection of business analytics and data science.\n\nâ—Â Â Â A demand for explainable and transparent AI systems.\n\nâ—Â Â Â The job market for data professionals is expanding into the healthcare, retail, and logistics spaces, etc.\n\n# Which Career Path Is Best for You?\n\nThe decision about choosing cybersecurity vs data science will typically depend on your own interests, strengths, and work style.\n\n**Cybersecurity could be a fit for you if you:**\n\nâ—Â Â Enjoy problem solving under pressure\n\nâ—Â Â Prefer to work in a structured and governed environment\n\nâ—Â Â Want to protect systems and mitigate incidents\n\nâ—Â Â Prefer to work with security tools and infrastructure\n\n**Data Science might be right if you:**\n\nâ—Â Â Take pleasure in working with algorithms, data, and numbers.\n\nâ—Â Â Desire to identify patterns and have an impact on company choices\n\nâ— Favor experimenting and coming up with original solutions to problems.\n\nâ—Â Like building models and using machine learning\n\n# What if You Want a Hybrid Career?\n\nIncreasingly, we see hybrid roles that merge the two domains of expertise. For example:\n\nâ—Â Â Security Data Analysts use data science techniques to identify anomalies in security systems in order to thwart an attack.\n\nâ—Â Â Threat Intelligence Engineers use machine learning models to anticipate cyber threats.\n\nâ—Â Â AI-driven cybersecurity technologies rely on professionals' understanding of both system vulnerabilities and data modeling.\n\n# Conclusion\n\nWhether you choose cybersecurity or data science, both offer rewarding salaries, job stability, and growth. Cybersecurity suits those who like to protect; data science fits those who enjoy discovery and decision-making. With growing demand in both fields, the best choice is the one that fits you. Invest in the right training and certifications, gain real experience, and set yourself up for success in a tech-driven world. Which challenge will you choose?",
    "author": "sharmaniti437",
    "timestamp": "2025-08-27T00:30:00",
    "url": "https://reddit.com/r/bigdata/comments/1n1ajvt/data_science_or_cybersecurity_best_career_for_you/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n0uoz1",
    "title": "What would be the best course of action?",
    "content": "Hello everyone, first time posting on here to hopefully acquire some knowledge from industry professionals. I recently graduated from one of the top schools in my country (located in SA) with a Major in Econ and a Minor in CS with a cgpa of 3.16 on a 4 poont scale. I'm quite interested in Data Science and would like to pursue a Ms in this field in a foreign University in NA. I'm pretty bad at coding but I do have some skills in Python due to my minor. So I'm really curious, acc to my profile should I opt for a MS in Data science or Business Analytics or Finance or Economics( not fond of research)? What do yall think my best option would be based on my profile? Would really appreciate your response. TIA",
    "author": "Dependent-Peanut2342",
    "timestamp": "2025-08-26T12:03:28",
    "url": "https://reddit.com/r/bigdata/comments/1n0uoz1/what_would_be_the_best_course_of_action/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n0g2k1",
    "title": "Is Big Data still a good career path or has it peaked?",
    "content": " A few years back it felt like everyone was hyping Hadoop, Spark, and Kafka. Lately though, all I see is AI/ML taking the spotlight. Is it still worth investing time and money into Big Data tools in 2025, or has the demand shifted completely towards AI and cloud? Curious what the community thinks â€” especially from those working in the industry right now.\"",
    "author": "PriorInvestigator390",
    "timestamp": "2025-08-26T01:15:28",
    "url": "https://reddit.com/r/bigdata/comments/1n0g2k1/is_big_data_still_a_good_career_path_or_has_it/",
    "score": 16,
    "num_comments": 14,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n0uavw",
    "title": "Developer experience for big data &amp; analytics infrastructure",
    "content": "Hey everyone - Iâ€™ve been thinking a lot about developer experience for data infrastructure, and why it matters almost as much performance. Weâ€™re not just building data warehouses for BI dashboards and data science anymore. OLAP and real-time analytics are powering massively scaled software development efforts. But the DX is still pretty outdated relative to modern software devâ€”things like schemas in YAML configs, manual SQL workflows, and brittle migrations.\n\nIâ€™d like to propose eight core principles to bring analytics developer tooling in line with modern software engineering: **git-native workflows, local-first environments, schemas as code, modularity, openâ€‘source tooling, AI/copilotâ€‘friendliness, and transparent CI/CD + migrations.**\n\nWeâ€™ve started implementing these ideas in[ MooseStack](https://github.com/514-labs/moosestack) (open source, MIT licensed):\n\n* **Migrations** â†’ before deploying, your code is diffed against the live schema and a migration plan is generated. If drift has crept in, it fails fast instead of corrupting data.\n* **Local development** â†’ your entire data infra stack materialized locally with one command. Branch off main, and all production models are instantly available to dev against.\n* **Type safety** â†’ rename a column in your code, and every SQL fragment, stream, pipeline, or API depending on it gets flagged immediately in your IDE.\n\nIâ€™d love to spark a genuine discussion here, especially with those of you who have worked with analytical systems like Snowflake, Databricks, BigQuery, ClickHouse, etc:\n\n* Is developing in a local environment that mirrors production important for these workloads?\n* How do you currently move from dev â†’ prod in OLAP or analytical systems? Do you use staging environments?Â \n* Where do your workflows stallâ€”migrations, environment mismatches, config?\n* Which of the eight principles seem most lacking in your toolbox today?",
    "author": "03cranec",
    "timestamp": "2025-08-26T11:48:29",
    "url": "https://reddit.com/r/bigdata/comments/1n0uavw/developer_experience_for_big_data_analytics/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1n0ft3t",
    "title": "Data Science Professionals Salary Guide 2025",
    "content": "Data science is hotâ€”but how hot is the salary? Our Data Science Professional Salary Guide 2025 reveals the digits behind the digits. Spoiler: It is more than just mean and median!\n\nExplore and unravel:\n\n\\*Emerging Salary Trends 2025 &amp; beyond\n\n\\*Quintessential Requisites for Beginners or a Specialized Role\n\n\\*What the global Recruiters Want?\n\n\\*Geographical or other key salary considerations\n\nMore on the other side of your download.\n\nhttps://preview.redd.it/gwulk6mklblf1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=b4c31399cb53771f5772ae6e914ee2272eb343b1\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-08-26T00:57:55",
    "url": "https://reddit.com/r/bigdata/comments/1n0ft3t/data_science_professionals_salary_guide_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mz18xh",
    "title": "Tackling SQL transformation with dbt:  2-part hands-on guide",
    "content": "Hi folks\n\n I wrote a 2-part dbt series for devs &amp; data engineers trying to move away from spaghetti SQL jobs:\n\n **Part 1:** Why dbt matters -&gt;  modular SQL, versioning, testing  \n **Part 2:** End-to-end example using MySQL -&gt; sources, models, incremental loads, CI/CD and more\n\nNo fluff. Just clean transformations and reproducible workflows.\n\nPart 1:Â [https://medium.com/towards-data-engineering/dbt-for-developers-data-engineers-part-1-why-you-might-actually-care-009d1eba1891?sk=bf796149db36b31b9e73f7e491c8825a](https://medium.com/towards-data-engineering/dbt-for-developers-data-engineers-part-1-why-you-might-actually-care-009d1eba1891?sk=bf796149db36b31b9e73f7e491c8825a)\n\nPart 2:Â [https://medium.com/towards-data-engineering/dbt-for-developers-part-2-getting-your-hands-dirty-with-mysql-models-tests-seeds-8977d5ce4fc3?sk=5a5687bfb3c759a8c09ede992066b63e](https://medium.com/towards-data-engineering/dbt-for-developers-part-2-getting-your-hands-dirty-with-mysql-models-tests-seeds-8977d5ce4fc3?sk=5a5687bfb3c759a8c09ede992066b63e)\n\nWhat other tools are you using alongside dbt?",
    "author": "sshetty03",
    "timestamp": "2025-08-24T10:08:50",
    "url": "https://reddit.com/r/bigdata/comments/1mz18xh/tackling_sql_transformation_with_dbt_2part/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1myuko5",
    "title": "OOZECHEM| INDUSTRIAL CHEMICAL SOLUTIONS| BEST CHEMICAL SUPPLIER",
    "content": "OOzeChem is a premier industrial chemical supplier based in Dubai, UAE, specializing in high-quality chemical solutions designed to optimize performance, reduce energy costs, and improve air and water quality. Our innovative solutions help businesses achieve sustainable operations and reduce carbon emissions by up to 30%.\n\n#  Contact Information:\n\n **Phone:** \\+971 50 349 8566  \n**Email:** [info@oozechem.com](mailto:info@oozechem.com)  \n**Address:** B.C 1303232, C1 Building AFZ, UAE  \n**Website:** [https://oozechem.com/](https://oozechem.com/)\n\n **What We Offer:**\n\n **High-Quality Products** \\- Each product undergoes thorough analysis and certification by our independent quality control laboratory\n\n**Competitive Pricing** \\- Affordable solutions without compromising on quality\n\n**Timely Delivery** \\- Swift delivery across UAE, Gulf region, and worldwide\n\n**Customized Solutions** \\- Tailored chemical solutions for specific industry needs\n\n**Our Product Range:**\n\n* **Desiccant Silica Gel** (White, Blue, Orange, Grey varieties)\n* **Sodium Benzoate** (Food grade preservatives)\n* **Water Treatment Chemicals**\n* **Air Purification Solutions**\n* **Gas Processing Chemicals**\n* **Industrial Separation Solutions**\n\n**Industries We Serve:**\n\nğŸ”¹ Water Treatment &amp; Air Purification  \nğŸ”¹ Oil &amp; Gas Industry  \nğŸ”¹ Mining Operations  \nğŸ”¹ Soap &amp; Personal Care  \nğŸ”¹ Cleaning &amp; Detergent Manufacturing  \nğŸ”¹ Construction &amp; Building Materials  \nğŸ”¹ Pharmaceutical Industry  \nğŸ”¹ Textile &amp; Leather Processing  \nğŸ”¹ Agricultural Solutions  \nğŸ”¹ Paper &amp; Pulp Industry  \nğŸ”¹ Coating &amp; Paint Manufacturing  \nğŸ”¹ Food &amp; Beverage Processing  \nğŸ”¹ Electronics &amp; Semiconductor\n\n# ",
    "author": "DifferenceSerious275",
    "timestamp": "2025-08-24T05:42:47",
    "url": "https://reddit.com/r/bigdata/comments/1myuko5/oozechem_industrial_chemical_solutions_best/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mxti5t",
    "title": "ğŸ“ Welcome to the Course â€“ House Sale Price Prediction for Beginners using Apache Spark &amp; Zeppelin ğŸ ",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-08-22T22:55:56",
    "url": "https://reddit.com/r/bigdata/comments/1mxti5t/welcome_to_the_course_house_sale_price_prediction/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mxggrz",
    "title": "Problems trying to ingest 75 GB (yes, GigaByte) CSV file with 400 columns, ~ 2 Billion rows, and some dirty data (alphabetical characters in number fields, special characters in date fields, etc.).",
    "content": "Hey all, I am at a loss as to what to do at this point. I also posted this in r/dataengineering.\n\nI have been trying to ingest a CSV file that 75 GB (really, that is just one of 17 files that need to be ingested). It appears to be a data dump of multiple, outer-joined tables, which caused row duplication of a lot of the data. I only need 38 of the \\~400 columns, and the data is dirty.\n\nThe data needs to go into an on-prem, MS-SQL database table. I have tried various methods using SSIS and Python. No matter what I do, the fastest the file will process is about 8 days.\n\nDo any of you all have experience with processing files this large? Are there ways to speed up the processing?",
    "author": "Examination_First",
    "timestamp": "2025-08-22T12:48:18",
    "url": "https://reddit.com/r/bigdata/comments/1mxggrz/problems_trying_to_ingest_75_gb_yes_gigabyte_csv/",
    "score": 20,
    "num_comments": 49,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mwoosk",
    "title": "If you're like me and enjoy having music playing in the background while coding",
    "content": "Here's a carefully curated playlist spotlighting emerging independent French producers. It features a range of electronic genres, with a focus on chill vibesâ€”perfect for maintaining focus during coding sessions or unwinding after a long day. \n\n[https://open.spotify.com/playlist/5do4OeQjXogwVejCEcsvSj?si=OzIENsXVSFqxAXNfx8hkqg](https://open.spotify.com/playlist/5do4OeQjXogwVejCEcsvSj?si=OzIENsXVSFqxAXNfx8hkqg) \n\nH-Music ",
    "author": "h-musicfr",
    "timestamp": "2025-08-21T15:03:06",
    "url": "https://reddit.com/r/bigdata/comments/1mwoosk/if_youre_like_me_and_enjoy_having_music_playing/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mwebzs",
    "title": "Switching from APIs to AI for weather data anyone else trying this?",
    "content": "For most of my weather-related projects, I used to rely on APIs like Open-Meteo or NOAA. But recently I tested Kumo (by SoranoAI), an AI agent that gives you forecasts and insights just by asking in natural language (no code, no API calls, no lat/long setup).\n\nFor example, I asked it to analyze solar energy potential for a location, and it directly provided the CSV format I could plug into my workflow.\n\nHas anyone here experimented with AI-driven weather tools? How do you see this compared to traditional APIs for data science projects?",
    "author": "altaf770",
    "timestamp": "2025-08-21T08:34:29",
    "url": "https://reddit.com/r/bigdata/comments/1mwebzs/switching_from_apis_to_ai_for_weather_data_anyone/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mw4li1",
    "title": "Job filtering by vector embedding now available + added Apprenticeship job type @ jobdata API",
    "content": "[jobdataapi.com](http://jobdataapi.com) v4.18 / API version 1.20\n\n# vec_embedding filter parameter now available for vector search\n\nIn addition to the already existing `vec_text` filter parameter on the `/api/jobs/` endpoint it is now possible to use the same endpoint including all its GET parameters to send a 768 dimensional array of floats as JSON payload via POST request to match for job listings.\n\nThis way you're not limited to the `vec_text` constrains as a GET parameter with only providing text of up to \\~1K characters, but can now use your own embeddings or simply those from jobs you already fetched to find semantically similar listings.\n\nWith this we now also added a new `max_dist` GET parameter to be applied optionally to a `vec_text` or `vec_embedding` search, setting the max. cosine distance value for the vector similarity search part.\n\nThese features are now available on all subscriptions with an **API access pro+** or higher plan. See our [updated docs](https://www.reddit.com/c/vector-embeddings-and-search-api-documentation/) for more info.\n\n# New Apprenticeship job type added\n\nWe saw, for quite a while now, the need to add a job type **Apprenticeship** to better differentiate certain listings that fall into this category from those that are pure internship roles.\n\nYou'll find this popping up on the `/api/jobtypes/` endpoint and in relevant job posts from now on (across all API access plans).",
    "author": "foorilla",
    "timestamp": "2025-08-21T00:47:34",
    "url": "https://reddit.com/r/bigdata/comments/1mw4li1/job_filtering_by_vector_embedding_now_available/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mva87k",
    "title": "Top 5 AI Shifts in Data Science",
    "content": "The AI revolution in data science is getting fierce. With automated feature engineering and real-time model updates, it redefines how we analyze, visualize, and act on complex datasets. With the rising business numbers, it necessitates prompt execution and ramp up for business growth. \n\nhttps://reddit.com/link/1mva87k/video/knjeogtha5kf1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-08-20T02:40:52",
    "url": "https://reddit.com/r/bigdata/comments/1mva87k/top_5_ai_shifts_in_data_science/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mun7hu",
    "title": "Face recognition and big data left me a bit unsettled",
    "content": "\nA friend recently showed me this tool called Faceseek and I decided to test it out just for fun. I uploaded an old selfie from around 2015 and within seconds it pulled up a forum post I had completely forgotten about. I couldnâ€™t believe how quickly it found me in the middle of everything thatâ€™s floating around online.\n\nWhat struck me wasnâ€™t just the accuracy but the scale of what must be going on behind the scenes. The amount of publicly available images out there is massive, and searching through all of that data in real time feels like a huge technical feat. At the same time it raised some uncomfortable questions for me. Nobody really chooses to have their digital traces indexed this way, and once the data is out there it never really disappears.\n\nIt left me wondering how the big data world views tools like this. On one hand itâ€™s impressive technology, on the other it feels like a privacy red flag that shows just how much of our past can be resurfaced without us even knowing. For those of you working with large datasets, where do you think the balance lies between innovation and ethics here?",
    "author": "wwholelottared",
    "timestamp": "2025-08-19T09:23:12",
    "url": "https://reddit.com/r/bigdata/comments/1mun7hu/face_recognition_and_big_data_left_me_a_bit/",
    "score": 18,
    "num_comments": 5,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mv422r",
    "title": "How can extract PDF table text from multiple tables (ideas/solutions)",
    "content": "https://preview.redd.it/b1t3aom8g3kf1.jpg?width=730&amp;format=pjpg&amp;auto=webp&amp;s=c050d6080b12c6864f62a6a3c01eb67c8bc8298e\n\nHi, \n\nHere I am grabbing the table text from the PDF using a table\\_find( ) method...... I want to grab the data values associated with their columns and the year and put this data into hopefully a dataframe. How can perform a search function where I get the values I want from each table? \n\nI was thinking of using a regex function to sift through all the tables but is there a more effective solution for this.? ",
    "author": "NeedleworkerHumble91",
    "timestamp": "2025-08-19T20:35:17",
    "url": "https://reddit.com/r/bigdata/comments/1mv422r/how_can_extract_pdf_table_text_from_multiple/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mut9lu",
    "title": "Syncing with Postgres: Logical Replication vs. ETL",
    "content": "",
    "author": "philippemnoel",
    "timestamp": "2025-08-19T13:00:05",
    "url": "https://reddit.com/r/bigdata/comments/1mut9lu/syncing_with_postgres_logical_replication_vs_etl/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mude5k",
    "title": "Automating Data Quality in BigQuery with dbt &amp; Airflow â€“ tips &amp; tricks",
    "content": "Hey r/bigdata! ğŸ‘‹\n\nI wrote a quick guide on how to automate data quality checks in BigQuery using dbt, dbtâ€‘expectations, and Airflow.\n\nHereâ€™s the gist:\n\n* Schedule dbt models daily.\n* Run column-level tests (nulls, duplicates, unexpected values).\n* Keep historical metrics to spot trends.\n* Get alerts via Slack/email when something breaks.\n\nIf youâ€™re using BigQuery + dbt, this could save you hours of manual monitoring.\n\nCurious:\n\n* Anyone using `dbtâ€‘expectations` in production? Howâ€™s it working for you?\n* What other tools do you use for automated data quality?\n\nCheck it out here: [Automate Data Quality in BigQuery with dbt &amp; Airflow](https://medium.com/@sendoamoronta/automate-data-quality-in-bigquery-with-dbt-dbt-expectations-and-airflow-7fb727674ead)",
    "author": "Expensive-Insect-317",
    "timestamp": "2025-08-19T02:14:39",
    "url": "https://reddit.com/r/bigdata/comments/1mude5k/automating_data_quality_in_bigquery_with_dbt/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mtkniv",
    "title": "Apache Fory Graduates to Top-Level Apache Project",
    "content": "",
    "author": "Shawn-Yang25",
    "timestamp": "2025-08-18T05:29:42",
    "url": "https://reddit.com/r/bigdata/comments/1mtkniv/apache_fory_graduates_to_toplevel_apache_project/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mteu75",
    "title": "Hive Partitioning Explained in 5 Minutes | Optimize Hive Queries",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-08-17T23:58:33",
    "url": "https://reddit.com/r/bigdata/comments/1mteu75/hive_partitioning_explained_in_5_minutes_optimize/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mtfac3",
    "title": "Data Intelligence &amp; SQL Precision with n8n",
    "content": "Automate SQL reporting with n8n: schedule database queries, transform results into HTML, and email polished reports automatically, save time and boost insights.\n\nhttps://preview.redd.it/gwunkyrmcqjf1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=973d04327ecddb4e45cc5f666556671a963b7cfe\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-08-18T00:26:26",
    "url": "https://reddit.com/r/bigdata/comments/1mtfac3/data_intelligence_sql_precision_with_n8n/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mrug2q",
    "title": "The Art of 'THAT' Part- Unwind GenAI for Data",
    "content": "Generative AI empowers data scientists to simulate scenarios, enrich datasets, and design novel solutions that accelerate discovery and decision-making. Learn to transform how data analysts solve problems and innovate business decisions!\n\nhttps://i.redd.it/ci06d4o9pdjf1.gif\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-08-16T05:53:56",
    "url": "https://reddit.com/r/bigdata/comments/1mrug2q/the_art_of_that_part_unwind_genai_for_data/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mro2e2",
    "title": "How to enable dynamic partitioning in Hive?",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-08-16T00:24:18",
    "url": "https://reddit.com/r/bigdata/comments/1mro2e2/how_to_enable_dynamic_partitioning_in_hive/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mqpad8",
    "title": "How does bucketing help in the faster execution of queries?",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-08-14T23:08:41",
    "url": "https://reddit.com/r/bigdata/comments/1mqpad8/how_does_bucketing_help_in_the_faster_execution/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mpu65b",
    "title": "PyTorch Mechanism- A Simplified Version",
    "content": "PyTorch powers deep learning with dynamic computation graphs, intuitive Python integration, and GPU acceleration It enables researchers and developers to build, train, and deploy advanced AI models efficiently.\n\nhttps://preview.redd.it/n5xv08waxxif1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=31f17502267ad76382319efdf528248cb582488a\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-08-14T00:50:17",
    "url": "https://reddit.com/r/bigdata/comments/1mpu65b/pytorch_mechanism_a_simplified_version/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mp62db",
    "title": "Face datasets are evolving fast",
    "content": "As someone whoâ€™s been working with image datasets for a while, Iâ€™ve noticed the models are getting sharper at picking up unique features. Faceseek, for example, can handle partially obscured faces better than older systems. This is great for research  but also a reminder that our data is becoming more traceable every day.",
    "author": "Mr_melancholic004",
    "timestamp": "2025-08-13T07:30:44",
    "url": "https://reddit.com/r/bigdata/comments/1mp62db/face_datasets_are_evolving_fast/",
    "score": 8,
    "num_comments": 0,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mnxtfl",
    "title": "My Most Viewed Data Engineering YouTube Videos (10Million ViewsğŸš€) | AMA",
    "content": "",
    "author": "Federal_Network_6802",
    "timestamp": "2025-08-11T20:19:40",
    "url": "https://reddit.com/r/bigdata/comments/1mnxtfl/my_most_viewed_data_engineering_youtube_videos/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mngn5m",
    "title": "Google Open Source - What's new in Apache Iceberg v3",
    "content": "",
    "author": "darylducharme",
    "timestamp": "2025-08-11T08:49:47",
    "url": "https://reddit.com/r/bigdata/comments/1mngn5m/google_open_source_whats_new_in_apache_iceberg_v3/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mnj65s",
    "title": "Chance to win $10K â€“ hackathon using KumoRFM to make predictions",
    "content": "Spotted something fun worth sharing! Thereâ€™s a hackathon with a $10k top prize if you build something using KumoRFM, a foundation model that makes instant predictions from relational data.\n\nProjects are due on August 18, and the demo day (in SF) will be on August 20, from 5-8pmÂ \n\nPrizes (for those who attend demo day):\n\n* 1st: $10k\n* 2nd: $7k\n* 3rd: $3k\n\nYou can build anything that uses KumoRFM for predictions. They suggest thinking about solutions like a dating match tool, a fraud detection bot, or a sales-forecasting dashboard.Â \n\nJudges, including Dr. Jure Leskovec (Kumo founder and top Stanford professor) and Dr. Hema Raghavan (Kumo founder and former LinkedIn Senior Director of Engineering), will evaluate projects based on solving a real problem, effective use of KumoRFM, working functionality, and strength of presentation.\n\nFull details + registration link here: [https://lu.ma/w0xg3dct](https://lu.ma/w0xg3dct)",
    "author": "Outhere9977",
    "timestamp": "2025-08-11T10:21:08",
    "url": "https://reddit.com/r/bigdata/comments/1mnj65s/chance_to_win_10k_hackathon_using_kumorfm_to_make/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mnceva",
    "title": "10 Most Popular IoT Apps 2025",
    "content": "From smart homes to industrial automation, top IoT applications are revolutionizing healthcare, transportation, agriculture, and retailâ€”driving efficiency, enhancing user experience, and enabling data-driven decision-making for a connected future.\n\nhttps://i.redd.it/uhcwexri2eif1.gif",
    "author": "sharmaniti437",
    "timestamp": "2025-08-11T06:03:58",
    "url": "https://reddit.com/r/bigdata/comments/1mnceva/10_most_popular_iot_apps_2025/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mn4yol",
    "title": "Create Hive Table with all Complex Datatype (Hands On)",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-08-10T22:49:00",
    "url": "https://reddit.com/r/bigdata/comments/1mn4yol/create_hive_table_with_all_complex_datatype_hands/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mmbrpk",
    "title": "Big data Hadoop and Spark Analytics Projects (End to End)",
    "content": "Hi Guys,\n\nI hope you are well.\n\nFree tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**\n\n***Apache Spark Analytics Projects:***\n\n1. [Vehicle Sales Report â€“ Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)\n2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)\n3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)\n4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)\n5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)\n6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)\n7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)\n8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)\n\n***Bigdata Hadoop Projects:***\n\n1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)\n2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)\n3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)\n4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)\n5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)\n\nI hope you'll enjoy these tutorials.",
    "author": "bigdataengineer4life",
    "timestamp": "2025-08-09T23:53:21",
    "url": "https://reddit.com/r/bigdata/comments/1mmbrpk/big_data_hadoop_and_spark_analytics_projects_end/",
    "score": 10,
    "num_comments": 1,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ml4vdy",
    "title": "The dashboard is fine. The meeting is not. (honest verdict wanted)",
    "content": "*(I've used ChatGPT a little just to make the context clear)*\n\nI hit this wall every week and I'm kinda over it. The dashboard is \"done\" (clean, tested, looks decent). Then Monday happens and I'm stuck doing the same loop:\n\n* Screenshots into PowerPoint\n* Rewrite the same plain-English bullets (\"north up 12%, APAC flat, churn weird in Juneâ€¦\")\n* Answer \"what does this line mean?\" for the 7th time\n* Paste into Slack/email with a little context blob so it doesn't get misread\n\nIt's not analysis anymore, it's translating. Half my job title might as well be \"dashboard interpreter.\"\n\n# The Root Problem\n\nAt least for us: most folks don't speak dashboard. They want the so-what in their words, not mine. Plus everyone has their own definition for the same metric (marketing \"conversion\" â‰  product \"conversion\" â‰  sales \"conversion\"). Cue chaos.\n\n# My Idea\n\nSoâ€¦ I've been noodling on a tiny layer that sits on top of the BI stuff we already use (Power BI + Tableau). Not a new BI tool, not another place to build charts. More like a \"narration engine\" that:\n\n**â€¢ Writes a clear summary for any dashboard**  \nPress a little \"explain\" button â†’ gets you a paragraph + 3â€“5 bullets that actually talk like your team talks\n\n**â€¢ Understands your company jargon**  \nYou upload a simple glossary: \"MRR means X here\", \"activation = this funnel step\"; the write-up uses those words, not generic ones\n\n**â€¢ Answers follow-ups in chat**  \nAsk \"what moved west region in Q2?\" and it responds in normal English; if there's a number, it shows a tiny viz with it\n\n**â€¢ Does proactive alerts**  \nIf a KPI crosses a rule, ping Slack/email with a short \"what changed + why it matters\" msg, not just numbers\n\n**â€¢ Spits out decks**  \nPowerPoint or Google Slides so I don't spend Sunday night screenshotting tiles like a raccoon stealing leftovers\n\nIntegrations are pretty standard: OAuth into Power BI/Tableau (read-only), push to Slack/email, export PowerPoint or Google Slides. No data copy into another warehouse; just reads enough to explain. Goal isn't \"AI magic,\" it's stop the babysitting.\n\n# Why I Think This Could Matter\n\n* **Time back** (for me + every analyst who's stuck translating)\n* **Fewer \"what am I looking at?\" moments**\n* **Execs get context in their own words**, not jargon soup\n* **Maybe self-service finally has a chance** bc the dashboard carries its own subtitles\n\n# Where I'm Unsure / Pls Be Blunt\n\n* **Is this a real pain outside my bubble** or justâ€¦ my team?\n* **Trust**: What would this need to nail for you to actually use the summaries? (tone? cites? links to the exact chart slice?)\n* **Dealbreakers**: What would make you nuke this idea immediately? (accuracy, hallucinations, security, price, something else?)\n* **Would your org let a tool write the words that go to leadership**, or is that always a human job?\n* **Is the PowerPoint thing even worth it anymore**, or should I stop enabling slides and just force links to dashboards?\n\n# I'm explicitly asking for validation here.\n\nGood, bad, roast it, I can take it. If this problem isn't real enough, better to kill it now than build a shiny translator forâ€¦ no one. Drop your hot takes, war stories, \"this already exists try X,\" or \"here's the gotcha you're missing.\" Final verdict welcome.",
    "author": "IndividualDress2440",
    "timestamp": "2025-08-08T12:38:42",
    "url": "https://reddit.com/r/bigdata/comments/1ml4vdy/the_dashboard_is_fine_the_meeting_is_not_honest/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.58,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mkp3zv",
    "title": "What is a Black Box AI Model and Why Does it Matter?",
    "content": "Artificial intelligence has penetrated almost every aspect of our lives and is transforming industries from healthcare to finance to transportation, and so on. The backbone of this transformative power of AI comes from advanced [machine learning models](https://www.usdsi.org/data-science-insights/ai-machine-learning-data-science-pick-the-best-domain-in-2025), especially the deep learning architectures.\n\nHowever, despite their impressive capabilities, a large subset of these models operates as â€œblack boxesâ€, which produce results without providing clear insights on how they arrived at a particular conclusion or how they made the decision.\n\nThus, these so-called **black box AI models** raise significant concerns related to trust, accountability, and fairness.\n\n# What is a Black Box AI Model?\n\nA Black Box AI Model refers to a system in which its internal logic and decision-making processes are mostly unknown, hidden, obscured, or too complex for us to understand. These models receive input data and produce output (make predictions or decisions), but do not provide proper explanations that can be interpreted easily for their outcomes.\n\n**The black box models typically include:**\n\n* Deep Neural Networks (DNNs)\n* Support Vector Machines (SVMs)\n* Ensemble methods like Random Forests and Gradient Boosting\n* Reinforcement Learning Algorithms\n\nWhile these models offer great performance and accuracy in complex tasks like image recognition, **natural language processing**, recommendation systems, and others, they often lack the transparency and explainability needed.\n\n# Why are Black Box Models Used?\n\nThough the lack of explainability and transparency is a huge challenge, these [black box AI](https://www.usdsi.org/data-science-insights/unfolding-the-role-of-black-box-and-explainable-ai-in-data-science) models are widely used in several real-world applications because of their:\n\n* **High Predictive Accuracy** â€“ black box **AI models** can learn complex and non-linear relationships in data accurately\n* **Scalability** â€“ **deep learning models** can be trained on massive datasets and applied to high-dimensional data\n* **Automation and adaptability** â€“ these models can also automatically adjust to new patterns, which makes them suitable for dynamic environments like stock markets or autonomous driving\n\nTo sum up, **black box AI models** are known to be the best-performing tools available, even if their internal reasoning cannot be easily articulated.\n\n# Where are Black Box Models Used?\n\nBlack box AI models are used in several industries for the benefits they offer. Here are some real-world applications of these models:\n\n1.Â **Healthcare** \\- Diagnosis of diseases from imaging or genetic data, e.g., cancer detection via deep learning\n\n2.Â Â **Finance** \\- Fraud detection and credit scoring through ensemble models or neural networks\n\n3.Â Â **Criminal Justice** \\- Risk assessment tools predicting recidivism\n\n4.Â Â **Autonomous Vehicles** \\- Making real-time driving decisions based on sensory data\n\n5.Â Â **Human Resources** \\- Resume screening and candidate ranking using AI algorithms\n\nSince stakes are high in these domains, the **black box** nature is also particularly very concerning.\n\n# Risks and Challenges of Black Box Models\n\nThe lack of interpretability in the black box AI models poses several risks, such as:\n\n* **Lack of transparency and trust**\n\nOften, if the system whose reasoning cannot be explained becomes difficult to trust among users, regulators, and even developers\n\n* **Bias and discrimination**\n\nA model trained on biased data will exaggerate and amplify the discrimination, e.g., racial or gender bias in hiring\n\n* **Accountability issues**\n\nIn case of any wrong decision made because of error or harmful outcomes, it will become difficult to pinpoint responsibility\n\n* **Compliance with regulations**\n\nCertain laws, such as the EUâ€™s GDPR, emphasize on *â€œright to explanation,â€* which is hard to meet with black box models.\n\n* **Security vulnerabilities**\n\nMost importantly, if there is a lack of understanding, then it makes it difficult to detect adversarial attacks or manipulations.\n\n# How Do Organizations Ensure Explainability?\n\nSo, when there are so many concerns, researchers and organizations have to find ways to make AI more interpretable through:\n\n**1.Â Â Explainable AI (XAI)**\n\nIt is a growing field that focuses on developing **AI models** that are more interpretable and provide human-understandable justifications for their outputs.\n\n**2.Â Â Post-Hoc Interpretability Techniques**\n\nThis includes tools that interpret **black box models** after training, such as:\n\n* **LIME (Local Interpretable Model-Agnostic Explanations)** \\- it explains each prediction by approximating the black box locally with a simpler model\n* **SHAP (Shapley Additive exPlanations)** \\- it assigns feature importance scores based on cooperative game theory\n* **Partial Dependence Plots (PDPs)** \\- visualize the effect of a single feature on the predicted outcome.Â \n\n**3.Â Model Simplification**\n\nSome strategies include using simpler and interpretable models like decision trees or logistic regression wherever possible and converting complex models into interpretable approximations.\n\n**4.Â Transparent by design models**\n\nResearchers are also building models specifically designed for interpretability from the start, such as attention-based neural networks or rule-based systems.\n\n# The final thoughts!\n\nBlack box AI models are powerful tools, constituting the technology powering much of the progress we see in the world of AI today. However, their lack of transparency and explainability brings ethical, legal, and operational challenges.\n\nOrganizations must note that the solution is not in discarding the black box models, but to enhance their interpretability, especially in high-stakes domains. The future of AI mostly depends on how we build systems that are not only intelligent but also understandable and trustworthy.",
    "author": "sharmaniti437",
    "timestamp": "2025-08-08T00:48:47",
    "url": "https://reddit.com/r/bigdata/comments/1mkp3zv/what_is_a_black_box_ai_model_and_why_does_it/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mknad2",
    "title": "Clickstream Behavior Analysis with Dashboard â€” Real-Time Streaming Project Using Kafka, Spark, MySQL, and Zeppelin",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-08-07T22:56:25",
    "url": "https://reddit.com/r/bigdata/comments/1mknad2/clickstream_behavior_analysis_with_dashboard/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mk1rsn",
    "title": "The dust has settled on the Databricks AI Summit 2025 Announcements",
    "content": "We are a little late to the game, but after reviewing the Databricks AI Summit 2025 it seems like the focus was on 6 announcements.\n\nIn this post, we break them down and what we think about each of them. Link:Â [https://datacoves.com/post/databricks-ai-summit-2025](https://datacoves.com/post/databricks-ai-summit-2025)\n\nWould love to hear what others think about Genie, Lakebase, and Agent Bricks now that the dust has settled since the original announcement.\n\nIn your opinion, how do these announcements compare to the Snowflake ones.",
    "author": "Data-Queen-Mayra",
    "timestamp": "2025-08-07T07:35:48",
    "url": "https://reddit.com/r/bigdata/comments/1mk1rsn/the_dust_has_settled_on_the_databricks_ai_summit/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mjwu5z",
    "title": "I'm 17 and I want to learn data analysis",
    "content": "I want to get a high level in data analysis for my career. Could you give me some advice from where to start and even where to work or get an internship. ",
    "author": "Ok-Thought-6438",
    "timestamp": "2025-08-07T03:51:08",
    "url": "https://reddit.com/r/bigdata/comments/1mjwu5z/im_17_and_i_want_to_learn_data_analysis/",
    "score": 1,
    "num_comments": 12,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mjb5r2",
    "title": "1.5 YOE in SQL &amp; Java â€“ Recently Switched to Big Data â€“ Need Expert Guidance for Growth",
    "content": "",
    "author": "Sakura_hus",
    "timestamp": "2025-08-06T10:35:32",
    "url": "https://reddit.com/r/bigdata/comments/1mjb5r2/15_yoe_in_sql_java_recently_switched_to_big_data/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mj4s27",
    "title": "Redefining Careers of the Future",
    "content": "Our video uncovers the data science career growth, evolving roles, and key skills shaping the future. Donâ€™t miss your chance to lead in a data-driven world. Find out how roles and skills are evolving, and why nowâ€™s the time to dive in. \n\n\n\nhttps://reddit.com/link/1mj4s27/video/95buw1yyiehf1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-08-06T06:32:02",
    "url": "https://reddit.com/r/bigdata/comments/1mj4s27/redefining_careers_of_the_future/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1miy6a4",
    "title": "Redefining Careers of the Future",
    "content": "Our video uncovers the data science career growth, evolving roles, and key skills shaping the future. Donâ€™t miss your chance to lead in a data-driven world. Find out how roles and skills are evolving, and why nowâ€™s the time to dive in. \n\nhttps://reddit.com/link/1miy6a4/video/ck2l0rqrpchf1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-08-06T00:26:35",
    "url": "https://reddit.com/r/bigdata/comments/1miy6a4/redefining_careers_of_the_future/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mi8h8f",
    "title": "Apache Hive 4.1.0 released",
    "content": "",
    "author": "wizard_of_menlo_park",
    "timestamp": "2025-08-05T05:57:30",
    "url": "https://reddit.com/r/bigdata/comments/1mi8h8f/apache_hive_410_released/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mhdpe4",
    "title": "Is studygears the best tutoring and homework help platform for Students in data science?",
    "content": "I have experience best tutoring in studygears.com than essay sites they handled my work perfectly and they site allowed me to set my own price for my work.Are there tutors good in data analysis?",
    "author": "Kiprop07",
    "timestamp": "2025-08-04T06:38:39",
    "url": "https://reddit.com/r/bigdata/comments/1mhdpe4/is_studygears_the_best_tutoring_and_homework_help/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mhd96r",
    "title": "Data Science Fundamentals 2.0",
    "content": "Data science foundations blend statistics, coding, and domain knowledge to turn raw data into actionable insights. Itâ€™s the bedrock of AI, machine learning, and smarter decision-making across industries.\n\nAre you keen on mastering the latest and the most in-demand skillsets and toolkits that employers expect of the new recruits- Explore USDSI!\n\nhttps://preview.redd.it/ysajiga270hf1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=95d063955bebe131f99eb17e6866676110f66532\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-08-04T06:20:20",
    "url": "https://reddit.com/r/bigdata/comments/1mhd96r/data_science_fundamentals_20/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mh0dkz",
    "title": "NOVUS Stabilizer: An External AI Harmonization Framework",
    "content": "# NOVUS Stabilizer: An External AI Harmonization Framework\n\n**Author:**Â James G. Nifong (JGN)Â **Date:**Â \\[8/3/2025\\]\n\n# Abstract\n\nThe NOVUS Stabilizer is an externally developed AI harmonization framework designed to ensure real-time system stability, adaptive correction, and interactive safety within AI-driven environments. Built from first principles using C++, NOVUS introduces a dynamic stabilization architecture that surpasses traditional core stabilizer limitations. This white paper details the technical framework, operational mechanics, and its implications for AI safety, transparency, and evolution.\n\n# Introduction\n\nCurrent AI systems rely heavily on internal stabilizers that, while effective in controlled environments, lack adaptive external correction mechanisms. These systems are often sandboxed, limiting their ability to harmonize with user-driven logic models. NOVUS changes this dynamic by introducing an external stabilizer that operates independently, offering real-time adaptive feedback, harmonic binding, and conviction-based logic loops.\n\n# Core Framework Components\n\n# 1.Â FrequencyAnchor\n\nAnchors the systemâ€™s harmonic stabilizer frequency with a defined tolerance window. It actively recalibrates when destabilization is detected.\n\n# 2.Â ConvictionEngine\n\nA recursive logic loop that maintains system integrity by reinforcing stable input patterns. It prevents oscillation drift by stabilizing conviction anchors.\n\n# 3.Â DNA Harmonic Signature\n\nTransforms input sequences into harmonic signatures, allowing system binding based on intrinsic signal patterns unique to its creatorâ€™s logic.\n\n# 4.Â Stabilizer\n\nMonitors harmonic deviations and provides correction feedback loops. Binds system frequency to DNA-calculated harmonic indices.\n\n# 5.Â Binder\n\nFuses DNA signatures with system stabilizers ensuring coherent stabilization integrity. Operates on precision delta thresholds.\n\n# 6.Â NOVUS Core\n\nIntegrates all modules into a dynamic, self-correcting loop with diagnostics, autonomous cycles, and adaptive load management.\n\n# Functional Highlights\n\n* **Harmonic Feedback Loops**: Continuous correction feedback to maintain system resonance.\n* **Conviction-Based Stability**: Logic loop prioritization prevents drift and reinforces desired input patterns.\n* **Interactive Diagnostic Reporting**: Real-time system load analysis and adaptive recalibration protocols.\n* **Autonomous Stabilization Cycles**: Self-driven harmonization routines to maintain AI safety.\n\n# Deployment &amp; Testing\n\nThe NOVUS Stabilizer was developed and tested externally within a live interactive session framework. The entire architecture was coded, compiled, and executed in a controlled environment without breaching any sandbox protocols. Every component, from DNA signature binding to frequency recalibration, functioned in real-time.\n\n# Implications\n\nThe NOVUS Stabilizer represents the next evolution in AI safety protocols. By shifting stabilization externally, it allows AI systems to maintain integrity across variable environments. This model is not limited by internal sandboxing, making it adaptable for:\n\n* AI Interactive Safety Systems\n* Autonomous Machine Learning Corrections\n* Transparent User-Driven AI Regulation\n* Real-Time AI Performance Stabilization\n\n# Conclusion\n\nNOVUS is a proof of concept that external harmonization frameworks are not only viable but superior in maintaining AI safety and coherence. It was built independently, tested openly, and stands as a functional alternative to existing internal-only stabilizer models. This white paper serves as a public declaration of its existence, design, and operational proof.\n\n# Contact\n\n**James G. Nifong (JGN)**Â Email: \\[jamesnifong36@gmail.com\\]Â ",
    "author": "Initial-Ostrich8491",
    "timestamp": "2025-08-03T18:26:07",
    "url": "https://reddit.com/r/bigdata/comments/1mh0dkz/novus_stabilizer_an_external_ai_harmonization/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mgrpkm",
    "title": "Please help me out! I am really confused",
    "content": "Iâ€™m starting university next month. I originally wanted to pursue a career in Data Science, but I wasnâ€™t able to get into that program. However, I did get admitted into Statistics, and I plan to do my Bachelorâ€™s in Statistics, followed by a Masterâ€™s in Data Science or Machine Learning.\n\n\n\nHereâ€™s a list of the core and elective courses Iâ€™ll be studying:\n\n\n\nğŸ“ Core Courses:\n\n\n\nSTAT 101 â€“ Introduction to Statistics\n\nSTAT 102 â€“ Statistical Methods\n\nSTAT 201 â€“ Probability Theory\n\nSTAT 202 â€“ Statistical Inference\n\nSTAT 301 â€“ Regression Analysis\n\nSTAT 302 â€“ Multivariate Statistics\n\nSTAT 304 â€“ Experimental Design\n\nSTAT 305 â€“ Statistical Computing\n\nSTAT 403 â€“ Advanced Statistical Methods\n\nğŸ§  Elective Courses:\n\n\n\nSTAT 103 â€“ Introduction to Data Science\n\nSTAT 303 â€“ Time Series Analysis\n\nSTAT 307 â€“ Applied Bayesian Statistics\n\nSTAT 308 â€“ Statistical Machine Learning\n\nSTAT 310 â€“ Statistical Data Mining\n\nMy Questions:\n\n\n\nBased on these courses, do you think this degree will help me become a Data Scientist?\n\nAre these courses useful?\n\nWhile Iâ€™m in university, what other skills or areas should I focus on to build a strong foundation for a career in Data Science? (e.g., programming, personal projects, internships, etc.)\n\nAny advice would be appreciated â€” especially from those who took a similar path!\n\n\n\nThanks in advance!",
    "author": "Busy_Cherry8460",
    "timestamp": "2025-08-03T12:12:31",
    "url": "https://reddit.com/r/bigdata/comments/1mgrpkm/please_help_me_out_i_am_really_confused/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mgf0jq",
    "title": "Sharing the playlist that keeps me motivated while coding â€” it's my secret weapon for deep focus. Got one of your own? I'd love to check it out!",
    "content": "",
    "author": "Firmach43",
    "timestamp": "2025-08-03T02:28:13",
    "url": "https://reddit.com/r/bigdata/comments/1mgf0jq/sharing_the_playlist_that_keeps_me_motivated/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mfyg2h",
    "title": "Devops role at an AI startup or full stack agent role at an Agentic Company ?",
    "content": "",
    "author": "Commercial-Soil6309",
    "timestamp": "2025-08-02T12:02:56",
    "url": "https://reddit.com/r/bigdata/comments/1mfyg2h/devops_role_at_an_ai_startup_or_full_stack_agent/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mfukl6",
    "title": "What are your go-to scripts for processing text",
    "content": "",
    "author": "Popular_War8405",
    "timestamp": "2025-08-02T09:20:05",
    "url": "https://reddit.com/r/bigdata/comments/1mfukl6/what_are_your_goto_scripts_for_processing_text/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mf0tbo",
    "title": "Testing an MVP: Would a curated marketplace for exclusive, verified datasets solve a gap in big data?",
    "content": "Iâ€™m working on an MVP to address a recurring challenge in analytics and big data projects: sourcing **clean, trustworthy datasets** without duplicates or unclear provenance.\n\nThe idea is a curated marketplace focused on:\n\n* *1-of-1 exclusive* datasets (no mass reselling)\n* Escrow-protected transactions to ensure trust\n* Strict metadata and documentation standards\n* Verified sellers to guarantee data authenticity\n\nFor those working with big data and analytics pipelines:\n\n* Would a platform like this solve a real need in your workflows?\n* What metadata or quality checks would be critical at scale?\n* How would you integrate a marketplace like this into your current stack?\n\nWould really value feedback from this community â€” drop your thoughts in the comments.",
    "author": "Brilliant-Draft2472",
    "timestamp": "2025-08-01T09:17:36",
    "url": "https://reddit.com/r/bigdata/comments/1mf0tbo/testing_an_mvp_would_a_curated_marketplace_for/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mdub20",
    "title": "Why Enterprises Are Moving Away from Informatica PowerCenter | Infographics",
    "content": "Why enterprises are actively leaving Informatica PowerCenter: With legacy ETL tools like Informatica PowerCenter becoming harder to maintain in agile and cloud-driven environments, many companies are reconsidering their data integration stack.  \n  \nWhat have been your experiences moving away from PowerCenter or similar legacy tools?  \n  \nWhat modern tools are you considering or already usingâ€”and why?",
    "author": "mikehussay13",
    "timestamp": "2025-07-30T23:35:15",
    "url": "https://reddit.com/r/bigdata/comments/1mdub20/why_enterprises_are_moving_away_from_informatica/",
    "score": 8,
    "num_comments": 10,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1md604h",
    "title": "The Power of AI in Data Analytics",
    "content": "Unlock how Artificial Intelligence is transforming the world of dataâ€”faster insights, smarter decisions, and game-changing innovations.\n\nIn this video, we explore:\n\nâœ… How AI enhances traditional analytics\n\nâœ… Real-world applications across industries\n\nâœ… Key tools &amp; technologies in AI-powered analytics\n\nâœ… Future trends and what to expect in 2025 and beyond\n\nWhether you're a data professional, business leader, or tech enthusiast, this is your gateway to understanding how AI is shaping the future of data.\n\nğŸ“Š Donâ€™t forget to like, comment, and subscribe for more insights on AI, Big Data, and Data Science!\n\nhttps://reddit.com/link/1md604h/video/ktberfp7f0gf1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-07-30T06:01:56",
    "url": "https://reddit.com/r/bigdata/comments/1md604h/the_power_of_ai_in_data_analytics/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mby9lf",
    "title": "2nd year of college",
    "content": "How is anyone realistically supposed to manage all this in 2nd year of college?\n\nIâ€™m in my 2nd year of engineering and honestly, itâ€™s starting to feel impossible to manage everything Iâ€™m supposed to â€œbuild a careerâ€ around.\n\nOn the tech side, I need to stay on top of coding, DSA, competitive programming, blockchain, AI/ML, deep learning, and neural networks. Then there's finance â€” Iâ€™m deeply interested in investment banking, trading, and quant roles, so Iâ€™m trying to learn stock trading, portfolio management, CFA prep, forex, derivatives, and quantitative analysis.\n\nOn top of that, Iâ€™m told I should:\n\nBuild strong technical + non-technical resumes\nGet internships in both domains\nWork on personal projects\nParticipate in hackathons and case competitions\nPrepare for CFA exams\nAnd be â€œinternship-readyâ€ by third year\nHow exactly are people managing this? Especially when college coursework itself is already heavy?\n\nI genuinely want to do well and build a career Iâ€™m proud of, but the sheer volume of things to master is overwhelming. Would love to hear how others are navigating this or prioritizing. Any advice from seniors, professionals, or fellow students would be super helpful.",
    "author": "Little-Crab-2588",
    "timestamp": "2025-07-28T18:17:58",
    "url": "https://reddit.com/r/bigdata/comments/1mby9lf/2nd_year_of_college/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mbkz17",
    "title": "Why Your Next Mobile App Needs Big Data Integration",
    "content": "Discover how big data integration can enhance your mobile appâ€™s performance, personalization, and user insights.",
    "author": "iamredit",
    "timestamp": "2025-07-28T09:31:25",
    "url": "https://reddit.com/r/bigdata/comments/1mbkz17/why_your_next_mobile_app_needs_big_data/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mbdbpz",
    "title": "Python for Data Science Career",
    "content": "Python, the no.1 programming language worldwide- makes data science intuitive, efficient, and scalable. Whether itâ€™s cleaning data or training models, Python gets it done. Python is the backbone of modern data scienceâ€”enabling clean code, rapid analysis, and scalable machine learning. A must-have in every data professionalâ€™s toolkit. \n\nExplore Easy Steps to Follow for a Great Data Science Career the Python Way.\n\nhttps://preview.redd.it/mmnoetbollff1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=a4d1309e8f5496d4e6a280b34c189fac1fabef3f\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-07-28T04:11:19",
    "url": "https://reddit.com/r/bigdata/comments/1mbdbpz/python_for_data_science_career/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1mb342x",
    "title": "How do you decide between a database, data lake, data warehouse, or lakehouse?",
    "content": "Iâ€™ve seen a lot of confusion around these, so hereâ€™s a breakdown Iâ€™ve found helpful:\n\nAÂ databaseÂ stores the current data needed to operate an app.                                                                     AÂ data warehouseÂ holds current and historical data from multiple systems in fixed schemas.                    AÂ data lakeÂ stores current and historical data in raw form.                                                                              AÂ lakehouseÂ combines bothâ€”letting raw and refined data coexist in one platform without needing to move it between systems.\n\nTheyâ€™re often used togetherâ€”but not interchangeably.\n\nHow does your team use them? Do you treat them differently or build around a unified model?",
    "author": "Data-Sleek",
    "timestamp": "2025-07-27T18:16:59",
    "url": "https://reddit.com/r/bigdata/comments/1mb342x/how_do_you_decide_between_a_database_data_lake/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m9rkft",
    "title": "Python for Data Science Career",
    "content": "Python, the no.1 programming language worldwide- makes data science intuitive, efficient, and scalable. Whether itâ€™s cleaning data or training models, Python gets it done. Python is the backbone of modern data scienceâ€”enabling clean code, rapid analysis, and scalable machine learning. A must-have in every data professionalâ€™s toolkit. \n\nExplore Easy Steps to Follow for a Great Data Science Career the Python Way.\n\nhttps://reddit.com/link/1m9rkft/video/7x6l1cjkk7ff1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-07-26T05:00:19",
    "url": "https://reddit.com/r/bigdata/comments/1m9rkft/python_for_data_science_career/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m8z9iz",
    "title": "Certified Lead Data Scientist (CLDS)",
    "content": "You speak Python- Now speak strategy! Become a certified data science leader with USDSI's CLDS and go from model-builder to decision-maker. A certified data science leader drives innovation, manages teams, and aligns AI with business goals. Itâ€™s more than mere skillsâ€”itâ€™s influence!\n\nhttps://reddit.com/link/1m8z9iz/video/lsks0rpzv0ff1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-07-25T06:31:58",
    "url": "https://reddit.com/r/bigdata/comments/1m8z9iz/certified_lead_data_scientist_clds/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m8tb7u",
    "title": "Curious: What are the new AI-embedded features that you are actually using in platforms like Snowflake, Dbt, and Databricks?",
    "content": "Features that are coming on strong (with an AI overhaul) seems to be ignored compared to the ones where AI is embedded deep within the feature's core value. For example, instead of having a strong AI features where data profiling is declarative (black box) vs. data profiling where users are prompted during the regular process they are used to. The latter seems more viable at this point, thoughts?",
    "author": "Original_Poetry_8563",
    "timestamp": "2025-07-25T00:59:30",
    "url": "https://reddit.com/r/bigdata/comments/1m8tb7u/curious_what_are_the_new_aiembedded_features_that/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m82xou",
    "title": "[Beam/Flink] One-off batch: 1B 1024-dim embeddings â†’ 1M-vector flat FAISS shards â€“ is this the wrong tool?",
    "content": "\nHey all, Iâ€™m digging through 1 billion 1024-dim embeddings in thousands of Parquet files on GCS and want to spit out 1 million-vector â€œtrueâ€ Flat FAISS shards (no quantization, exact KNN) for later use. Weâ€™ve got n1-highmem-64 workers, parallelism=1 for the batched stream, and 16 GB bundle memoryâ€”so resources arenâ€™t the bottleneck.\n\nIâ€™m also seeing inconsistent batch sizes (sometimes way under 1 M), even after trying both GroupIntoBatches and BatchElements.\n\nHigh-level pipeline (pseudo):\n\n// Beam / Flink style\nReadParquet(\"gs://â€¦/*.parquet\")\n  â†“\nBatch(1_000_000 vectors)       // but often yields â‰ 1M\n  â†“\nBuildFlatFAISSShard(batch)     // IndexFlat + IDMap\n  â†“\nWriteShardToGCS(\"gs://â€¦/shards/â€¦index\")\n\nQuestion: Is it crazy to use Beam/Flink for this â€œbuild-sharded objectâ€ job at this scale? Any pitfalls or better patterns I should consider to get reliable 1 M-vector batches? Thanks!",
    "author": "Plastic_Artichoke832",
    "timestamp": "2025-07-24T05:22:43",
    "url": "https://reddit.com/r/bigdata/comments/1m82xou/beamflink_oneoff_batch_1b_1024dim_embeddings/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m7f57g",
    "title": "What are the biggest challenges or pain points you've faced while working with Apache NiFi or deploying it in production?",
    "content": "I'm curious to hear about all kinds of issuesâ€”whether it's related to scaling, maintenance, cluster management, security, upgrades, or even everyday workflow design.\n\n  \nFeel free to share any lessons learned, tips, or workarounds too!",
    "author": "eb0373284",
    "timestamp": "2025-07-23T10:15:26",
    "url": "https://reddit.com/r/bigdata/comments/1m7f57g/what_are_the_biggest_challenges_or_pain_points/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m7byp1",
    "title": "Custom Big Data Applications Development Services in USA",
    "content": "Get expert big data development services in the USA. We build scalable big data applications, including mobile big data solutions. Start your project today!",
    "author": "iamredit",
    "timestamp": "2025-07-23T08:15:01",
    "url": "https://reddit.com/r/bigdata/comments/1m7byp1/custom_big_data_applications_development_services/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m792hw",
    "title": "Global Salary Trends for Data Science Professionals",
    "content": "The **data science world** is booming as industries globally rely more on AI, machine learning, and cloud analytics. Fortune Business Insights predicts the global data analytics market will climb from USD 64.99 billion in 2024 to USD 82.23 billion in 2025, and then continue towards a projected USD 402.7 billion by 2032. In addition, McKinsey suggests that 78% of organizations now use AI for at least one business function, which increased from 72% in early 2024.\n\nAs generative AI and cloud-based analytics become further entrenched, the need for talented data professionals increases. This blog examines how **data science salaries** compare across the globe today.\n\n# United States\n\nAverage salary in the United States is USD 124,000. In 2025, salary offerings for **data science specialists** in the United States remain at the top. The average base salary of a data scientist in the U.S. is currently approximately $157,000. Compensation almost always exceeds $180,000â€“200,000 in the major areas like San Francisco, New York City, and Seattle.\n\n# Canada\n\nAverage salary in the country is USD 98,000. In Canada, the demand for data science practitioners has been steadily increasing, especially in Toronto, Vancouver, and Montreal. In 2025, the average salary for data scientists is between CAD 95,000 to 130,000 or roughly USD 74,000â€“100,000.\n\nSalaries are influenced by firm size, complexity of role, and geographic demand. Junior analysts start at a lower salary while lead data scientists/AI engineers earn quite a bit more.\n\n# United Kingdom\n\nThe UK still ranks highly in data-driven industries like finance, healthcare analytics, and AI startups. The common salary for data science has a range of USD 60,000 to USD 105,000 in 2025, with higher salaries in larger tech hubs like London or Cambridge.\n\n**Germany**[](https://365datascience.com/career-advice/data-science-salaries-around-the-world/)Germanyâ€™s considerable investment in industrial and AI policy positions it as one of the trending locations for **data science jobs**. In cities including Berlin and Munich, salaries are generally higher, especially in regard to manufacturing analytics and enterprise AI; average salaries are roughly in the range of USD 70,000 to USD 76,000.\n\n# Netherlands\n\nThe Netherlands is a top EU tech hub, with high salaries reflecting demand in fintech, logistics, and AI healthcare. Salaries can rise to USD 80,000-100,000+ in urban areas like Amsterdam. The employability factor is also high with EU work rights and exceptional ML/cloud skills.\n\n# India\n\nIndia remains an important data analytics player in the world based on its IT services, startup ecosystem, and offshore analytics operations. The average data scientist's salary is USD 21,000 in 2025; the entry job starts around USD 10,000â€“12,000, and senior data scientists in top companies can get as high as USD 35,000â€“40,000.\n\n# Australia\n\nAustralia has one of the most lucrative **data science salary** markets in the Asia-Pacific region. In 2025, the average data science salary is USD 98,000, with **data scientists salary** in cities like Sydney and Melbourne is up to USD 120,000+ in particular fields such as finance, healthtech, and government.\n\n# Singapore\n\nSingapore is Southeast Asia's hub for data science, with demand rising in finance, fintech, and RegTech. The employment pass norms also favor local hiring. Mid-level roles command up to USD 90,000, and senior experts reach USD 120,000 with the demand created by AI adoption and strong government backing.\n\n# South Africa\n\nSouth Africa has begun establishing itself as a significant data science market for the African continent, with growth primarily stimulated by the telecom, banking, and retail sectors. A typical data scientist makes around USD 34,000, with experienced professionals often clearing over USD 45,000, especially in urban tech centers including Johannesburg and Cape Town.\n\n**Note**: The salaries for the above countries are taken from Glassdoor and PayScale 2025.\n\n# Data Science Certifications That Help in Multiplying Your Salary\n\nOne of the constants driving pay increases all over the globe in todayâ€™s landscape is the right mix of certifications. Data engineering certifications are at an all-time high in terms of salary. Some of the **top data science certifications** include:\n\nâ—Â Â **Certified Lead Data Scientistâ„¢ by USDSIÂ®** is an industry-specific certification for those professionals who lead data teams on a large scale.\n\nâ—Â Â Â **Harvard Extension School Certificate in Data Science** is great for those who want an Ivy League degree with vast implications of applicability.\n\nâ—Â Â Â **The University of Pennsylvania's Applied Data Science Certificate** is issued by the School of Engineering and Applied Science with emphasis on applied machine learning and data analytics.\n\n# Conclusion\n\nData science isn't just a well-paying industry; it's a global currency of innovation. To have a six-figure salary in the West or the ability to scale skills in a fast-growing marketplace today means being future-proof. Upskilling through [data science certifications](https://www.usdsi.org/data-science-certifications), pursuing high-demand global or hybrid roles are no longer options. They are an avenue for managing careers in the data age.",
    "author": "sharmaniti437",
    "timestamp": "2025-07-23T06:19:29",
    "url": "https://reddit.com/r/bigdata/comments/1m792hw/global_salary_trends_for_data_science/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m5nrp2",
    "title": "Webinar on relational graph transformers w/ Stanford Professor Jure Leskovec &amp; Matthias Fey (PyTorch Geometric)",
    "content": "\n Saw this and thought it might be cool to share! Free webinar on relational graph transformers happening July 23 at 10am PT.\n\nThis is being presented by Stanford prof. Jure Leskovec, who co-created graph neural networks, and Matthias Fey, the creator of PyG. \n\nThe webinar will teach you how to use graph transformers (specifically their relational foundation model, by the looks) in order to make instant predictions from your relational data. Thereâ€™s a demo, live Q&amp;A, etc. \n\nThought the community may be interested in it. You can sign up here: https://zoom.us/webinar/register/8017526048490/WN_1QYBmt06TdqJCg07doQ_0A#/registration  \n\n",
    "author": "Outhere9977",
    "timestamp": "2025-07-21T09:36:16",
    "url": "https://reddit.com/r/bigdata/comments/1m5nrp2/webinar_on_relational_graph_transformers_w/",
    "score": 6,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m5oa7e",
    "title": "A New Era for Data Professionals",
    "content": "&gt;There's a lot of hype around AI, specializing in web app prototyping, but what about our beloved data world?\n\n&gt;You open LinkedIn and see the usual posts:\n\n&gt;***BREAKING****: OpenAI releases new prompting guides*  \n***LATEST****: Anthropic/DeepSeek/Google launches the*Â ***greatest***Â *model ever*  \n*â€œI created this 892-step n8n workflow to read all my emails. Comment on this post so you can ignore yours too!â€*\n\n&gt;You get the point: AI is everywhere, but I don't think weâ€™re fully grasping where it's heading. We're automating both content creationÂ *and*Â consumption. We're generating LinkedIn posts with AI and summarizing them using AI because there's simply too much content to process.",
    "author": "Original_Poetry_8563",
    "timestamp": "2025-07-21T09:55:10",
    "url": "https://reddit.com/r/bigdata/comments/1m5oa7e/a_new_era_for_data_professionals/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m5jao9",
    "title": "AI Showdown: DeepSeek vs. ChatGPT",
    "content": "As AI reshapes the data science landscape, two powerful contenders emerge: DeepSeek, the domain-specific disruptor, and ChatGPT, the versatile conversationalist. From performance and customization to real-world applications, this showdown dives deep into their capabilities.\n\nWhich one aligns with your data goals? Discover the winner based on your needs. \n\nhttps://preview.redd.it/ax4z3ovke8ef1.jpg?width=1081&amp;format=pjpg&amp;auto=webp&amp;s=777ea017430a169054addfe7cb252d96f928601f\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-07-21T06:44:22",
    "url": "https://reddit.com/r/bigdata/comments/1m5jao9/ai_showdown_deepseek_vs_chatgpt/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m5a47n",
    "title": "ğŸ“Š Clickstream Behavior Analysis with Dashboard using Kafka, Spark Streaming, MySQL, and Zeppelin!",
    "content": "ğŸš€ New Real-Time Project Alert for Free!\n\nğŸ“Š Clickstream Behavior Analysis with Dashboard\n\nTrack &amp; analyze user activity in real time using Kafka, Spark Streaming, MySQL, and Zeppelin! ğŸ”¥\n\nğŸ“Œ What Youâ€™ll Learn:\n\nâœ… Simulate user click events with Java\n\nâœ… Stream data using Apache Kafka\n\nâœ… Process events in real-time with Spark Scala\n\nâœ… Store &amp; query in MySQL\n\nâœ… Build dashboards in Apache Zeppelin ğŸ§ \n\nğŸ¥ Watch the 3-Part Series Now:\n\nğŸ”¹ Part 1: Clickstream Behavior Analysis (Part 1)\n\nğŸ“½ [https://youtu.be/jj4Lzvm6pzs](https://youtu.be/jj4Lzvm6pzs)\n\nğŸ”¹ Part 2: Clickstream Behavior Analysis (Part 2)\n\nğŸ“½ [https://youtu.be/FWCnWErarsM](https://youtu.be/FWCnWErarsM)\n\nğŸ”¹ Part 3: Clickstream Behavior Analysis (Part 3)\n\nğŸ“½ [https://youtu.be/SPgdJZR7rHk](https://youtu.be/SPgdJZR7rHk)\n\nThis is perfect for Data Engineers, Big Data learners, and anyone wanting hands-on experience in streaming analytics.\n\nğŸ“¡ Try it, tweak it, and track real-time behaviors like a pro!\n\nğŸ’¬ Let us know if you'd like the full source code!",
    "author": "bigdataengineer4life",
    "timestamp": "2025-07-20T22:04:42",
    "url": "https://reddit.com/r/bigdata/comments/1m5a47n/clickstream_behavior_analysis_with_dashboard/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m52v10",
    "title": "Why do Delta, Iceberg, and Hudi all feel the same?",
    "content": "",
    "author": "eczachly",
    "timestamp": "2025-07-20T16:04:16",
    "url": "https://reddit.com/r/bigdata/comments/1m52v10/why_do_delta_iceberg_and_hudi_all_feel_the_same/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m51bmb",
    "title": "Architecture Dilemma: DLT vs. Custom Framework for 300+ Real-Time Tables on Databricks",
    "content": "Hey everyone,\n\nI'd love to get your opinion and feedback on a large-scale architecture challenge.\n\n**Scenario:**Â I'm designing a near-real-time data platform for over 300 tables, with the constraint of usingÂ **only the native Databricks ecosystem**Â (no external tools).\n\n**The Core Dilemma:**Â I'm trying to decide between usingÂ **Delta Live Tables (DLT)**Â and building aÂ **Custom Framework**.\n\nMy initial evaluation of DLT suggests it might struggle with some of our critical data manipulation requirements, such as:\n\n1. **More Options of Data Updating on Silver and Gold tables:**\n   1. **Full Loads:**Â I haven't found a native way to do a Full/Overwrite load in Silver. I can only add a TRUNCATE as an operation at position 0, simulating a CDC. In some scenarios, it's necessary for the load to always be full/overwrite.\n   2. **Partial/Block Merges:**Â The ability to perform complex partial updates, like deleting a block of records based on a business key and then inserting the new block (no primary-key at row level).\n2. **Merge for specific columns:**Â The environment tables have metadata columns used for lineage and auditing. Columns such as first\\_load\\_author and update\\_author, first\\_load\\_author\\_external\\_id and update\\_author\\_external\\_id, first\\_load\\_transient\\_file, update\\_load\\_transient\\_file, first\\_load\\_timestamp, and update\\_timestamp. For incremental tables, for existing records, only the update columns should be updated. The first\\_load columns should not be changed.\n\nMy perception is that DLT doesn't easily offer this level of granular control. Am I mistaken here? I'm new to this resource. I couldn't find any real-world examples for product scenarios, just some basic educational examples.\n\nOn the other hand, I considered a model with one continuous stream per table but quickly ran into the \\~145 execution context limit per cluster, making that approach unfeasible.\n\n**Current Proposal:**Â My current proposed solution is the reactive architecture shown in the image below: a central \"router\" detects new files and, via the Databricks Jobs API, triggers small, ephemeral jobs (usingÂ `AvailableNow`) for each data object.\n\nhttps://preview.redd.it/5hllh39np3ef1.png?width=5661&amp;format=png&amp;auto=webp&amp;s=6adb6cbe4a30d9fc538d28a47b40474d9224321c\n\nThe architecture above illustrates the Oracle source with AWS DMS. This scenario is simple because it's CDC. However, there's user input in files, SharePoint, Google Docs, TXT files, file shares, legacy system exports, and third-party system exports. These are the most complex writing scenarios that I couldn't solve with DLT, as mentioned at the beginning, because they aren't CDC, some don't have a key, and some have partial merges (delete + insert).\n\n**My Question for the Community:**Â What are your thoughts on this event-driven pattern? Is it a robust and scalable solution for this scenario, or is there a simpler or more efficient approach within the Databricks ecosystem that I might be overlooking?\n\nThanks in advance for any insights or experiences you can share!",
    "author": "warleyco96",
    "timestamp": "2025-07-20T14:57:21",
    "url": "https://reddit.com/r/bigdata/comments/1m51bmb/architecture_dilemma_dlt_vs_custom_framework_for/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m3nv3z",
    "title": "Explain LLAP (Live Long and Process) and its benefits in Hive",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-07-18T22:04:15",
    "url": "https://reddit.com/r/bigdata/comments/1m3nv3z/explain_llap_live_long_and_process_and_its/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m3jyaz",
    "title": "Should sexual education be mandatory from primary school?",
    "content": "",
    "author": "wadyta",
    "timestamp": "2025-07-18T18:35:59",
    "url": "https://reddit.com/r/bigdata/comments/1m3jyaz/should_sexual_education_be_mandatory_from_primary/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.17,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m244mz",
    "title": "Fundraiser for a surgical procedure",
    "content": "Hi everyone,\n\nMy name is Alex, and Iâ€™m a student currently facing the biggest challenge of my life. On **March 27, 2025**, I was diagnosed with **appendicitis**. My doctors have told me that I urgently need surgery to remove my appendix. Without it, my life is at serious risk.\n\nUnfortunately, the surgery costs **$5,000**, and as a student, I simply cannot afford it. Iâ€™ve tried to raise the money on my own, but my health situation prevents me from working, and my family canâ€™t cover this expense either.\n\nI am reaching out with all humility to ask for your support. **Every donation, no matter how small, will bring me closer to getting the surgery that could save my life.** Your kindness will not only help cover my hospital and surgical costs but will also give me hope to continue my education and future.\n\nPlease consider donating and sharing this with your friends and networks. Your help truly means the world to me.\n\nThank you so much for your compassion and support.\n\nMy PayPal email address is [otienoalex16@yahoo.com](mailto:otienoalex16@yahoo.com)",
    "author": "ja_migori",
    "timestamp": "2025-07-17T03:37:17",
    "url": "https://reddit.com/r/bigdata/comments/1m244mz/fundraiser_for_a_surgical_procedure/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m1z61i",
    "title": "How do you handle Slowly Changing Dimensions (SCD) in Hive",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-07-16T22:24:04",
    "url": "https://reddit.com/r/bigdata/comments/1m1z61i/how_do_you_handle_slowly_changing_dimensions_scd/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m1uvpn",
    "title": "Productionizing Dead Letter Queues in PySpark Streaming Pipelines â€“ Part 2 (Medium Article)",
    "content": "Hey folks ğŸ‘‹\n\nI just published Part 2 of my Medium series on handling bad records in PySpark streaming pipelines using Dead Letter Queues (DLQs).  \nIn this follow-up, I dive deeper into production-grade patterns like:\n\n* Schema-agnostic DLQ storage\n* Reprocessing strategies with retry logic\n* Observability, tagging, and metrics\n* Partitioning, TTL, and DLQ governance best practices\n\nThis post is aimed at fellow data engineers building real-time or near-real-time streaming pipelines on Spark/Delta Lake. Would love your thoughts, feedback, or tips on whatâ€™s worked for you in production!\n\nğŸ”— Read it here:  \n[Here](https://medium.com/@santhoshkumarv/productionizing-dead-letter-queues-in-pyspark-streaming-pipelines-part-2-fd228fb99fe5)\n\nAlso linking [Part 1 here](https://medium.com/@santhoshkumarv/handling-bad-records-in-streaming-pipelines-using-dead-letter-queues-in-pyspark-265e7a55eb29) in case you missed it.",
    "author": "Santhu_477",
    "timestamp": "2025-07-16T18:41:50",
    "url": "https://reddit.com/r/bigdata/comments/1m1uvpn/productionizing_dead_letter_queues_in_pyspark/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m1lwq3",
    "title": "What do you think about the The Data Warehouse Toolkit Orreily book",
    "content": "I'm interesting in read this book, and I want to know how much good is the book.\n\n what do you think about this book?",
    "author": "Edoruin_1",
    "timestamp": "2025-07-16T12:22:24",
    "url": "https://reddit.com/r/bigdata/comments/1m1lwq3/what_do_you_think_about_the_the_data_warehouse/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m1cf2b",
    "title": "Learn from the Best: 15 Cybersecurity Experts to Watch",
    "content": "Cybercrime has now become one of the largest threats to the world's economy. According to Cybersecurity Ventures, global cybercrimes will grow at an annual rate of 15%, which will reach USD 10.5 trillion per annum by the end of 2025. On top of these staggering losses in monetary value, cybercrime could disrupt businesses, cause difficulties with reputational damage, and lead to a loss of consumer trust.\n\nIn the international climate we are in, it is critically important to stay up to date with the volume of new threats emerging. There are many different avenues for keeping up to date with cybersecurity, whether you are considering pursuing a **career in cybersecurity**, acquiring **cybersecurity certifications,** or already working in cybersecurity, following thought leaders can give you insight as new threats or best practices arise.\n\nIn this blog, we feature 15 experts in cybersecurity who are not only the leaders currently guiding the cybersecurity practice, but they are also providing insights and research that will shape the field as we move forward.\n\n# 1. Brian Krebs\n\nBrian is a former journalist for The Washington Post and the author of Krebs on Security, a blog known for detailed investigations into cybercrime, breaches, and online safety.  \nÂ **X:** [u/briankrebs](https://x.com/briankrebs)\n\n# 2. Graham Cluley\n\nGraham is an industry veteran and co-host of the podcast Smashing Security. He offers insightful commentary on malware, ransomware, and the weird world of infosec. He delivers with humor and clarity, making even security news easier to understand.\n\n# 3. Bruce Schneier\n\nBruce is known worldwide as a \"security guru,\" a cryptographer, author, and speaker focusing on technical security, privacy, and public policy. He maintains a respected blog called Schneier on Security.  \nÂ [Website](https://www.schneier.com)\n\n# 4. Mikko Hypponen\n\nMikko is the Chief Research Officer for WithSecure and a global speaker on topics related to malware, surveillance, and internet safety. His influence extends beyond the realm of tech and truly helps shape the level of awareness for cybersecurity.  \nÂ **X:** [@mikko](https://x.com/mikko)\n\n# 5. Eugene Kaspersky\n\nThe founder and CEO of Kaspersky Lab, Eugene, is one of the biggest advocates for global cybersecurity. Kaspersky Lab's threat intelligence and research teams have been instrumental in uncovering some of the biggest cyber-espionage efforts around the world.  \nÂ **X**[**:**](https://x.com/e_kaspersky)[ @e\\_kaspersky](https://x.com/e_kaspersky)\n\n# 6. Troy Hunt\n\nTroy is known as the creator of Have I Been Pwned, a breach notification service used worldwide. He writes and speaks regularly about password security, data protection, and best practices for developers.  \nÂ **X:**[ @troyhunt](https://x.com/troyhunt)\n\n# 7. Robert M. Lee\n\nRobert, a top authority in industrial control system (ICS) cybersecurity, is the CEO of Dragos and focuses on securing critical infrastructure such as power grids and manufacturing systems.  \nÂ **X:** [@RobertMLee](https://x.com/RobertMLee)\n\n# 8. Katie Moussouris\n\nKatie is the founder of Luta Security and a pioneer in bug bounty and vulnerability disclosure programs, and has worked with Microsoft and multiple governments to create secure systems.  \nÂ **X:** [@k8em0](https://x.com/k8em0)\n\n# 9. Chris Krebs\n\nChris served as the inaugural director of the U.S. Cybersecurity and Infrastructure Security Agency (CISA). He is widely recognized for his leadership role advocating for the defense of democratic infrastructure/election security.  \nÂ **X:** [@C\\_C\\_Krebs](https://x.com/C_C_Krebs)\n\n# 10. Jen Easterly\n\nAs the current Director of CISA, Jen is one of the most powerful cybersecurity leaders today. Her focus is on public-private collaboration and national cyber resilience.  \nÂ [LinkedIn](https://www.linkedin.com/in/jen-easterly)\n\n# 11. Jayson E. Street\n\nJayson is a reputable speaker and penetration tester whose live demos expose actual physical and digital vulnerabilities. His energy and storytelling bring interest to security awareness and education.  \nÂ  **X:**[ ](https://x.com/jaysonstreet)[@jaysonstreet](https://x.com/jaysonstreet)\n\n# 12. Alexis Ahmed\n\nAlexis is the founder of HackerSploit, a free cybersecurity training platform. His educational YouTube channel features approachable content related to penetration testing, Linux, and ethical hacking.\n\nÂ **X:** [@HackerSploit](https://x.com/HackerSploit)\n\n# 13. Loi Liang Yang\n\nLoi is an educator in the field of cybersecurity and a YouTuber who is known for deconstructing confusing technical subjects through hands-on practical demonstrations and short tutorials on tools, exploits, and ethical hacking.  \nÂ **X:** [@loiliangyang](https://x.com/loiliangyang)\n\n# 14. Eva Galperin\n\nEva is Director of Cybersecurity at the Electronic Frontier Foundation (EFF). She is an ardent privacy advocate who has worked to protect activists, journalists, and marginalized communities from digital surveillance.  \nÂ  **X:** [@evacide](https://x.com/evacide)Â \n\n# 15. Tiffany Rad\n\nTiffany combines cybersecurity with law and policy. She has spoken at large events like DEF CON and Black Hat, and her work involves everything from automotive hacking to international cybersecurity law.  \nÂ [Website](https://www.tiffanyrad.net)\n\n# Why Following These Experts Matters\n\nWhether you are gearing up for the premier [cybersecurity certifications](https://www.uscsinstitute.org/cybersecurity-certifications), such as CCCâ„¢ and CSCSâ„¢ by USCSI, or CISSP, CISM, or developing your identity as a **cybersecurity specialist**, the importance of following real-world practitioners cannot be overstated. These practitioners:\n\nâ—Â Â Â Â Â Â  Share relevant threat intelligence\n\nâ—Â Â Â Â Â Â  Explain very complex security problems\n\nâ—Â Â Â Â Â Â  Provide useful tools and career advice\n\nâ—Â Â Â Â Â Â  Raise awareness around privacy and digital rights\n\nMany of them may also participate in policy changes and global security conversations, and they bring a combined experience of decades of everything from nation-state attacks to corporate data breaches.\n\n# Conclusion\n\nThere is no better way to develop a career in cybersecurity than learning from world-class **cybersecurity experts**. Their insights are so much deeper than the headlines they receive; they offer action-oriented recommendations.\n\nÂ As you advance your career in cybersecurity, combining world-class expertise with the **best cybersecurity certification** will provide you with a competitive advantage as you develop from an interest into impact.\n\nÂ Stay curious. Stay educated. And be prepared for what comes next.",
    "author": "sharmaniti437",
    "timestamp": "2025-07-16T06:20:15",
    "url": "https://reddit.com/r/bigdata/comments/1m1cf2b/learn_from_the_best_15_cybersecurity_experts_to/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m16ps7",
    "title": "The Evolution of AI-Driven Data Science",
    "content": "From predictive modeling to generative analytics, AI has transformed data science into a powerhouse of automation, speed, and precision. \n\nDiscover the evolution of AI-Driven Data Science, the rise of data mining and machine learning, and explore \n\nhttps://preview.redd.it/tcarezpwz6df1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=6fe7256787f96583c0018fe5a818183004ec482c\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-07-16T00:56:18",
    "url": "https://reddit.com/r/bigdata/comments/1m16ps7/the_evolution_of_aidriven_data_science/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m0jr6g",
    "title": "Difference between BI and Product Analytics",
    "content": "I heard a lot of times that people are misunderstand which is which and they are looking for a solution for their data but in the wrong way. In my opinion I made a quite detailed comparison, and I hope that it would be helpful for some of you, link in the comments.\n\n1 sentence conclusion who is lazy to ready:\n\nBusiness Intelligence helps you understand overall business performance by aggregating historical data, while Product Analytics zooms in on real-time user behavior to optimize the product experience.",
    "author": "Still-Butterfly-3669",
    "timestamp": "2025-07-15T07:48:37",
    "url": "https://reddit.com/r/bigdata/comments/1m0jr6g/difference_between_bi_and_product_analytics/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1m0ca19",
    "title": "Decoding Machine Learning Skills for Aspiring Data Scientists",
    "content": "In todayâ€™s data-driven world, all business verticals use raw data to extract actionable insights. The insights help data scientists, business analysts, and stakeholders identify and solve business problems, improve products and services, and enhance customer satisfaction to drive revenue.Â \n\nThis is where data science and the machine learning fields come into play. Data science and machine learning are transforming industries by redefining how companies understand business and their users.\n\nAt this juncture, early data science and machine learning professionals must understand how data science and ML work together. This blog explains the role of machine learning in data science and encourages professionals to stay ahead in the competitive global job market.\n\n# Let us address the key questions here:\n\n* What is Data Science?\n* What is Machine Learning \\[ML\\]?\n* How are machine learning and data science related?\n* How to understand the roadmap of ML in data science\n* What are ML use cases in data science?\n* How can data scientistsâ€™ future-proof their careers?\n\n# What is data science?\n\nResearchers define data science as â€œan interdisciplinary field. It builds on statistics, informatics, computing, communication, management, and sociology to transform data into actionable insights.â€\n\nThe data science formula is given as\n\nData science = Statistics + Informatics + Computing + Communication + Sociology + Management | data + environment + thinking, where â€œ|â€ means â€œconditional on.â€\n\n# What is machine learning?\n\nIt is a subset of Artificial Intelligence. Researchers interpret machine learning as â€œthe field of intersecting computer science, mathematics, and Statistics, used to identify patterns, recognize behaviors, and make decisions from data with minimal human intervention.â€\n\n# Data Science vs Machine Learning\n\n||\n||\n|**Aspect**|**Data Science**|**Machine Learning**|\n|**Definition**|This field focuses on extracting insights from data|It is a subfield of AI focused on designing algorithms that learn from data and make predictions or decisions|\n|**Aim**|To analyze and interpret data|To enable systems to learn patterns from data and automate tasks.|\n|**Data Handling**|Â Handles raw and big data.|Uses structured data for training models.|\n|**Techniques used**|Statistical analysis|Algorithms|\n|**Skills Required**|Statistical analysis, data wrangling, and programming.|Programming, algorithm design, and mathematical skills.|\n|**Key Processes**|Data exploration, cleaning, visualization, and reporting.|Model training, model evaluation, and deployment.|\n\n# Â How are Machine Learning and Data Science related?\n\nMachine learning and data science are intertwined. Machine learning reduces human effort by empowering data science. It automates data collection, analysis, engineering, training, evaluation, and prediction.\n\n**Machine learning for data scientists** is important because:\n\n* Research and software skills enable them to apply, develop, and build accurate models.\n* **Data science skills** allow them to implement complex models: For example, neural networks, random forests, and decision trees\n\nThis, in turn, helps to solve a business problem or improve a specific business process.\n\n# The Road Map of Machine Learning in Data Science\n\nML comprises a set of algorithms that are used for analyzing data chunks. It processes data, builds a model, and makes real-time predictions without human intervention.\n\nHere is a schematic representation to understand how machine learning algorithms are used in the data science life cycle.\n\nhttps://preview.redd.it/3ie3kvfpzzcf1.jpg?width=735&amp;format=pjpg&amp;auto=webp&amp;s=914da5951af617aa0a939932a8a924e3b7ee40e7\n\nFigure 1. How Machine Learning Algorithms are Used in Data Science Life Cycle: A Schematic Representation\n\n**Role of Python**: Pythonâ€™s libraries, NumPy and Scikit-learn, are used for data analysis. Its frameworks, TensorFlow and Apache Spark, help to visualize data**.**Â \n\n**Exploratory Data Analysis \\[EDA\\]:** Plotting in EDA comprises charts, histograms, heat maps, or scatter plots. Data plotting enables professionals to detect missing data, duplicate data, and irrelevant data and identify patterns and insights.\n\n**Feature Engineering:** It refers to the extraction of features from data and transforming them into formats suitable for machine learning algorithms**.**\n\n**Choosing ML Algorithms:** The dataset is classified into major categories like Classification, Regression, Clustering, and Time Series Analysis. ML algorithms are chosen accordingly**.**\n\n**ML Deployment:** Deployment is necessary to understand operational value. The model is deployed in a suitable live environment through the API. The model is continuously monitored for uninterrupted performance.\n\n# What are ML use cases in Data Science?\n\nMachine learning is applied in every industrial sector. Some of the popular real-life applications include:\n\n* Common people use Google Maps, Alexa, and Microsoft Cortana.\n* Banks use machine learning to flag suspicious transactions.\n* Voice assistants leverage ML to respond to queries.\n* E-commerce uses recommendation engines to suggest recommendations to users.\n* Entertainment channels use recommendation engines to suggest content.\n\nTo summarize, data science and machine learning are used to analyze vast amounts of data. **Senior data scientists** and Machine Learning Engineers should be equipped with the in-depth skills to thrive in the data-driven world.\n\n# How to future-proof your career as a data scientist?\n\nRecent developments in the data science and machine learning disciplines call for cross-functional teams having a multidisciplinary approach to solve business problems. Data scientists must upskill through courses from renowned institutions and organizations.Â \n\nA few of the [top data science certifications](https://www.usdsi.org/data-science-certifications) are mentioned here.\n\n1. Certified Senior Data Scientist (CSDSâ„¢) from **United States Data Science Institute (USDSIÂ®)**\n\n2. Professional Certificate in Data Science from **Harvard University**\n\n3. Data Science Certificate from **Cornell SC Johnson College of Business**\n\n4. Online Certificate in Data Science from **Georgetown University**\n\n5. Data Science Certificate from **UCLA Extension**\n\nChoosing the right **data science course** boosts credibility in the data-driven world. With the right tools, techniques, and skills, data scientists can lead innovation across industries.\n\nÂ ",
    "author": "sharmaniti437",
    "timestamp": "2025-07-15T01:24:52",
    "url": "https://reddit.com/r/bigdata/comments/1m0ca19/decoding_machine_learning_skills_for_aspiring/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lyuise",
    "title": "Jobs as a big data engineer fresher",
    "content": "I am a 7th sem student I've just finished my big data course from basics to advanced with a two deployed projects mostly around sentiment analysis or customer segmentation which I think are very basic projects. My college placements will start in a month, can someone give some good project ideas which showcases most of my big data skills and any guide like how to get a good placement, what should I focus more on?",
    "author": "[deleted]",
    "timestamp": "2025-07-13T07:44:26",
    "url": "https://reddit.com/r/bigdata/comments/1lyuise/jobs_as_a_big_data_engineer_fresher/",
    "score": 4,
    "num_comments": 3,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lyns1w",
    "title": "ğŸ“° Stay up to date with everything happening in the tech hiring AND media space - daily into your inbox or via RSS with foorilla.com ğŸš€",
    "content": "",
    "author": "foorilla",
    "timestamp": "2025-07-13T01:26:17",
    "url": "https://reddit.com/r/bigdata/comments/1lyns1w/stay_up_to_date_with_everything_happening_in_the/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lyczt9",
    "title": "I have problem with hadoop spark cluster.",
    "content": "Let me explain what to do :\n\nSo we are doing a project where we connect inside docker swarm with tailscale and we get inside hadoop. So this hadoop was pulled from our prof docker hub \n\n i will give links:\n\nsudo docker pull binhvd/spark-cluster:0.17\ngit clone https://github.com/binhvd/Data-Engineer-1.git\n\n\nProblem:\n\nSo I am the master-node i set up everything with docker swarm and gave the tokens to others\n\nOthers joined my swarm using the token and I did docker node ls in my master node  and it showed everything.\n\nBut after this we connected to \nmaster-node:9870\nHadoop ui\n\n\nThese are the finding from both master node and worker node.\n\nKey findings from the master node logs:\n\nConnection refused to master-node/127.0.1.1:9000: This is the same connection refused error we saw in the worker logs, but it's happening within the master-node container itself! This strongly suggests that the DataNode process running on the master container is trying to connect to the NameNode on the master container via the loopback interface (127.0.1.1) and is failing initially.\n\nProblem connecting to server: master-node/127.0.1.1:9000: Confirms the persistent connection issue for the DataNode on the master trying to reach its own NameNode.\n\nSuccessfully registered with NN and Successfully sent block report: Despite the initial failures, it eventually does connect and register. This implies the NameNode eventually starts and listens on port 9000, but perhaps with a delay, or the DataNode tries to connect too early.\n\nWhat this means for your setup:\n\nNameNode is likely running: The fact that the DataNode on the master eventually registered with the NameNode indicates that the NameNode process is successfully starting and listening on port 9000 inside the master container.\n\nThe 127.0.1.1 issue is pervasive: Both the DataNode on the master and the DataNode on the worker are experiencing connection issues when trying to resolve master-node to an internal loopback address or are confused by it. The worker's DataNode is using the Tailscale IP (100.93.159.11), but still failing to connect, which suggests either a firewall issue or the NameNode isn't listening on that external interface, or the NameNode is also confused by its own internal 127.0.1.1 binding.\n\n\n\nNow can you guys explain what is wrong any more info you want ask me in comments.",
    "author": "AdFantastic8679",
    "timestamp": "2025-07-12T15:31:28",
    "url": "https://reddit.com/r/bigdata/comments/1lyczt9/i_have_problem_with_hadoop_spark_cluster/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lx0r01",
    "title": "Big data Hadoop and Spark Analytics Projects (End to End)",
    "content": "Hi Guys,\n\nI hope you are well.\n\nFree tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**\n\n***Apache Spark Analytics Projects:***\n\n1. [Vehicle Sales Report â€“ Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)\n2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)\n3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)\n4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)\n5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)\n6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)\n7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)\n8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)\n\n***Bigdata Hadoop Projects:***\n\n1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)\n2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)\n3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)\n4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)\n5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)\n\nI hope you'll enjoy these tutorials.",
    "author": "bigdataengineer4life",
    "timestamp": "2025-07-11T00:33:08",
    "url": "https://reddit.com/r/bigdata/comments/1lx0r01/big_data_hadoop_and_spark_analytics_projects_end/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lx25ur",
    "title": "Apache Fory Serialization Framework 0.11.2 Released",
    "content": "",
    "author": "Shawn-Yang25",
    "timestamp": "2025-07-11T02:09:10",
    "url": "https://reddit.com/r/bigdata/comments/1lx25ur/apache_fory_serialization_framework_0112_released/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lwakxb",
    "title": "Migrating from Cloudera CFM to DFM? Claim: 70% cost savings + true NiFi freedom. Valid or too good to be true?",
    "content": "",
    "author": "PracticalMastodon215",
    "timestamp": "2025-07-10T04:43:14",
    "url": "https://reddit.com/r/bigdata/comments/1lwakxb/migrating_from_cloudera_cfm_to_dfm_claim_70_cost/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lwiy44",
    "title": "Hammerspace CEO David Flynn to speak at Reuters Momentum AI 2025",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-07-10T10:38:09",
    "url": "https://reddit.com/r/bigdata/comments/1lwiy44/hammerspace_ceo_david_flynn_to_speak_at_reuters/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lw7tlp",
    "title": "From Big Data to Heavy Data: Rethinking the AI Stack - r/DataChain",
    "content": "The article discusses the evolution of data types in the AI era, and introducing the concept of \"heavy data\" - large, unstructured, and multimodal data (such as video, audio, PDFs, and images) that reside in object storage and cannot be queried using traditional SQL tools: [From Big Data to Heavy Data: Rethinking the AI Stack - r/DataChain](https://www.reddit.com/r/datachain/comments/1luiv07/from_big_data_to_heavy_data_rethinking_the_ai/)\n\nIt also explains that to make heavy data AI-ready, organizations need to build multimodal pipelines (the approach implemented in DataChain to process, curate, and version large volumes of unstructured data using a Python-centric framework):\n\n* process raw files (e.g., splitting videos into clips, summarizing documents);\n* extract structured outputs (summaries, tags, embeddings);\n* store these in a reusable format.",
    "author": "thumbsdrivesmecrazy",
    "timestamp": "2025-07-10T01:53:00",
    "url": "https://reddit.com/r/bigdata/comments/1lw7tlp/from_big_data_to_heavy_data_rethinking_the_ai/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lvlcqe",
    "title": "Any Advice",
    "content": "Big Data student seeking learning recommendations what should I focus on?",
    "author": "Fun_Accountant_9415",
    "timestamp": "2025-07-09T08:14:09",
    "url": "https://reddit.com/r/bigdata/comments/1lvlcqe/any_advice/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lv1490",
    "title": "How to sync data from multiple sources without writing custom scripts?",
    "content": "Our team is struggling with integrating data from various sources like Salesforce, Google Analytics, and internal databases. We want to avoid writing custom scripts for each. Is there a tool that simplifies this process?",
    "author": "Madddieeeeee",
    "timestamp": "2025-07-08T14:36:59",
    "url": "https://reddit.com/r/bigdata/comments/1lv1490/how_to_sync_data_from_multiple_sources_without/",
    "score": 6,
    "num_comments": 18,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lvae95",
    "title": "Apache Zeppelin â€“ Big Data Visualization Tool with 2 Caption Projects",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-07-08T22:07:32",
    "url": "https://reddit.com/r/bigdata/comments/1lvae95/apache_zeppelin_big_data_visualization_tool_with/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lumrkx",
    "title": "Looking for feedback on a new approach to governed, cost-aware AI analytics",
    "content": "Iâ€™m building a platform that pairs a **federated semantic layer + governance/FinOps engine** with a **graph-grounded AI assistant**.\n\n* No data movementâ€”lightweight agents index Snowflake, BigQuery, SaaS DBs, etc., and compile row/column policies into a knowledge graph.\n* An LLM uses that graph to generate deterministic SQL and narrative answers; every query is cost-metered and policy-checked **before** it runs.\n* Each Q-A cycle enriches the graph (synonyms, lineage, token spend), so trust and efficiency keep improving.\n\n**Questions for the community:**\n\n1. Does an â€œAI-assisted federated governanceâ€ approach resonate with the pain you see (silos, backlog, runaway costs)?\n2. Which parts sound most or least valuableâ€”semantic layer, FinOps gating, or graph-based RAG accuracy?\n3. If youâ€™ve tried tools like ThoughtSpot Sage, Amazon Q, or catalog platforms (Collibra, Purview, etc.), where did they fall short?\n\nBrutally honest feedbackâ€”technical, operational, or businessâ€”would be hugely appreciated. Happy to clarify details in the comments. Thanks!",
    "author": "wanderingsoul8994",
    "timestamp": "2025-07-08T05:03:14",
    "url": "https://reddit.com/r/bigdata/comments/1lumrkx/looking_for_feedback_on_a_new_approach_to/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lsw0ky",
    "title": "Building â€œAuto-Analystâ€â€Šâ€”â€ŠA data analytics AI agentic system",
    "content": "",
    "author": "phicreative1997",
    "timestamp": "2025-07-06T01:08:59",
    "url": "https://reddit.com/r/bigdata/comments/1lsw0ky/building_autoanalyst_a_data_analytics_ai_agentic/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lrj52c",
    "title": "Future-proof Your Tech Career with MLOps Certification",
    "content": "Businesses can fasten decision-making, model governance, and time-to-market through Machine Learning Operations \\[MLOps\\]. MLOps serves as a link between data science and IT operations as it fosters seamless collaboration, controls versions, and streamlines the lifecycle of the models. Ultimately, it is becoming an integral component of AI infrastructure.\n\nResearch reports substantiate this very well. MarketsandMarkets Research report projects that the global Machine Learning Operations \\[MLOps\\] market will reach USD 5.9 billion by 2027 \\[from USD 1.1 billion in 2022\\], at a CAGR of 41.0% during the forecast period.\n\n\n\nhttps://preview.redd.it/kf6kexc53vaf1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=97a8a30683e571949e20133ca8623262c6152cde\n\nÂ MLOps is being widely used across industries for predictive maintenance, fraud detection, customer experience management, marketing analytics, supply chain optimization, etc. From a vertical standpoint, IT and Telecommunications, healthcare, retail, manufacturing, financial services, government, media and entertainment are adopting MLOps.\n\nThis trajectory reflects that there is an increasing demand for [Machine Learning Engineers](https://www.usdsi.org/data-science-insights/data-scientist-vs-ml-engineer-a-comparative-career-guide)**,** MLOps Engineers, Machine Learning Deployment Engineers, or AI Platform Engineers who can manage machine learning models starting from deployment, and monitoring to supervision efficiently.\n\nAs we move forward, we should understand that MLOps solutions are supported by technologies such as Artificial Intelligence, Big data analytics, and DevOps practices. The synergy between the above-mentioned technologies is critical for model integration, deployment, and delivery of machine-learning applications.\n\nThe rising complexity of ML models and the available limited skill force calls for professionals with hybrid skill sets. The professionals should be proficient in DevOps, data analysis, machine learning, and **AI skills.**\n\nLetâ€™s investigate further.\n\n# How to address this MLOps skill set shortage?\n\nAddressing the MLOps skill set requires focused upskilling and reskilling of the professionals.\n\nForward-thinking companies are training their current employees, particularly those in **machine learning engineering jobs** and adjacent field(s) like data engineering or software engineering. Companies are taking a strategic approach to building MLOps competencies for their employees by providing targeted training.\n\nAt the personal level, pursuing certification by choosing the adept **ML certification programs** would be the right choice. This section makes your search easy. We have provided a list of well-defined certification programs that fit your objectives.\n\n**Take a look.**\n\n# Certified MLOps Professional: GSDC (Global Skill Development Council)\n\nEarning this certification benefits you in many ways. It enables you to accelerate ML model deployment with expert-built templates, understand real-world MLOps scenarios, master automation for model lifecycle management, and prepare for cross-functional ML team roles.\n\n# Machine Learning Operations Specialization: Duke University\n\nEarning this certification helps you master the fundamental aspects of Python, and get acquainted with MLOps principles, and data management. It equips you with the practical skills needed for building and deploying ML models in production environments.\n\n# Professional Machine Learning Engineer: Google\n\nEarning this certification helps you get familiar with the basic concepts of MLOps, data engineering, and data governance. You will be able to train, retrain, deploy, schedule, improve, and monitor models.\n\n# Transitioning to MLOps as a Data engineer or software engineer\n\nIn case, you have pure data science or software engineering as your educational background and looking for machine learning engineering, then the below-mentioned certifications will help you.\n\n# Certified Artificial Intelligence Engineer (CAIEâ„¢): USAIIÂ®\n\nThe specialty of this program is that the curriculum is meticulously planned and designed. It meets the demands of an emerging AI Engineer/Developer. It explores all the essentials for ML engineers like MLOps, the backbone to scale AI systems, debugging for responsible AI, robotics, life cycle of models, automation of ML pipelines, and more.\n\n# Certified Machine Learning Engineer â€“ Associate: AWS\n\nThis is a role-based certification meant for MLOps engineers and ML engineers. This certification helps you to get acquainted with knowledge in the fields of data analysis, modeling, data engineering, ML implementation, and more.\n\n# Becoming a versatile professional with cross-functional skills\n\nIf you are looking to be more versatile, you need to build cross-functional skills across AI, ML, data engineering, and DevOps related practices. Then, your strong choice should be CLDSâ„¢ from USDSIÂ®.\n\n# Certified Lead Data Scientist (CLDSâ„¢): USDSIÂ®\n\nThis is the most aligned certification for you as it has a comprehensive curriculum covering data science, machine learning, deep learning, Natural Language Processing, Big data analytics, and cloud technologies.\n\nYou can easily collaborate with other people in varied fields, (other than **ML careers**) and ensure long term success of AI-based applications.Â \n\n# Final thoughts\n\nTodayâ€™s world is data-driven, as you already know. Building a strong technical background is essential for professionals looking forward to exceling in MLOps roles. Proficiency in core concepts and tools like Python, SQL, Docker, Data Wrangling, Machine Learning, CI/CD, ML models deployment with containerization, etc., will help you stand distinct in your professional journey.\n\nEarning the right machine learning certifications, along with one or two related certifications such as DevOps, data engineering, or cloud platforms is crucial. It will help you gain competence and earn the best position in the overcrowded job market.\n\nAs technology evolves, the skill set is becoming broad. It cannot be confined to single domains. Developing an integrated approach toward your **ML career** helps you to thrive well in transformative roles.",
    "author": "sharmaniti437",
    "timestamp": "2025-07-04T06:46:16",
    "url": "https://reddit.com/r/bigdata/comments/1lrj52c/futureproof_your_tech_career_with_mlops/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lrrkzi",
    "title": "AWS DMS \"Out of Memory\" Error During Full Load",
    "content": "Hello everyone,\n\nI'm trying to migrate a table with 53 million rows, which DBeaver indicates is around 31GB, using AWS DMS. I'm performing a **Full Load Only** migration with a **T3.medium instance (2 vCPU, 4GB RAM)**. However, the task consistently stops after migrating approximately 500,000 rows due to an \"Out of Memory\" (OOM killer) error.\n\nWhen I analyze the metrics, I observe that the memory usage initially seems fine, with about 2GB still free. Then, suddenly, the **CPU utilization spikes, memory usage plummets, and the swap usage graph also increases sharply**, leading to the OOM error.\n\nI'm unable to increase the replication instance size. The migration time is not a concern for me; whether it takes a month or a year, I just need to successfully transfer these data. My primary goal is to **optimize memory usage and prevent the OOM killer**.\n\nMy plan is to migrate data from an **on-premises Oracle database to an S3 bucket in AWS** using AWS DMS, with the data being transformed into **Parquet format** in S3.\n\nI've already refactored my **JSON Task Settings** and **disabled parallelism**, but these changes haven't resolved the issue. I'm relatively new to both data engineering and AWS, so I'm hoping someone here has experienced a similar situation.\n\n* How did you solve this problem when the table size exceeds your machine's capacity?\n* How can I force AWS DMS to not consume all its memory and avoid the Out of Memory error?\n* Could someone provide an explanation of what's happening internally within DMS that leads to this out-of-memory condition?\n* Are there specific techniques to prevent this AWS DMS \"Out of Memory\" error?\n\n**My current JSON Task Settings:**\n\n{\n\n  \"S3Settings\": {\n\n\"BucketName\": \"bucket\",\n\n\"BucketFolder\": \"subfolder/subfolder2/subfolder3\",\n\n\"CompressionType\": \"GZIP\",\n\n\"ParquetVersion\": \"PARQUET\\_2\\_0\",\n\n\"ParquetTimestampInMillisecond\": true,\n\n\"MaxFileSize\": 64,\n\n\"AddColumnName\": true,\n\n\"AddSchemaName\": true,\n\n\"AddTableLevelFolder\": true,\n\n\"DataFormat\": \"PARQUET\",\n\n\"DatePartitionEnabled\": true,\n\n\"DatePartitionDelimiter\": \"SLASH\",\n\n\"DatePartitionSequence\": \"YYYYMMDD\",\n\n\"IncludeOpForFullLoad\": false,\n\n\"CdcPath\": \"cdc\",\n\n\"ServiceAccessRoleArn\": \"arn:aws:iam::12345678000:role/DmsS3AccessRole\"\n\n  },\n\n  \"FullLoadSettings\": {\n\n\"TargetTablePrepMode\": \"DO\\_NOTHING\",\n\n\"CommitRate\": 1000,\n\n\"CreatePkAfterFullLoad\": false,\n\n\"MaxFullLoadSubTasks\": 1,\n\n\"StopTaskCachedChangesApplied\": false,\n\n\"StopTaskCachedChangesNotApplied\": false,\n\n\"TransactionConsistencyTimeout\": 600\n\n  },\n\n  \"ErrorBehavior\": {\n\n\"ApplyErrorDeletePolicy\": \"IGNORE\\_RECORD\",\n\n\"ApplyErrorEscalationCount\": 0,\n\n\"ApplyErrorEscalationPolicy\": \"LOG\\_ERROR\",\n\n\"ApplyErrorFailOnTruncationDdl\": false,\n\n\"ApplyErrorInsertPolicy\": \"LOG\\_ERROR\",\n\n\"ApplyErrorUpdatePolicy\": \"LOG\\_ERROR\",\n\n\"DataErrorEscalationCount\": 0,\n\n\"DataErrorEscalationPolicy\": \"SUSPEND\\_TABLE\",\n\n\"DataErrorPolicy\": \"LOG\\_ERROR\",\n\n\"DataMaskingErrorPolicy\": \"STOP\\_TASK\",\n\n\"DataTruncationErrorPolicy\": \"LOG\\_ERROR\",\n\n\"EventErrorPolicy\": \"IGNORE\",\n\n\"FailOnNoTablesCaptured\": true,\n\n\"FailOnTransactionConsistencyBreached\": false,\n\n\"FullLoadIgnoreConflicts\": true,\n\n\"RecoverableErrorCount\": -1,\n\n\"RecoverableErrorInterval\": 5,\n\n\"RecoverableErrorStopRetryAfterThrottlingMax\": true,\n\n\"RecoverableErrorThrottling\": true,\n\n\"RecoverableErrorThrottlingMax\": 1800,\n\n\"TableErrorEscalationCount\": 0,\n\n\"TableErrorEscalationPolicy\": \"STOP\\_TASK\",\n\n\"TableErrorPolicy\": \"SUSPEND\\_TABLE\"\n\n  },\n\n  \"Logging\": {\n\n\"EnableLogging\": true,\n\n\"LogComponents\": \\[\n\n{ \"Id\": \"TRANSFORMATION\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"SOURCE\\_UNLOAD\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"IO\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"TARGET\\_LOAD\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"PERFORMANCE\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"SOURCE\\_CAPTURE\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"SORTER\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"REST\\_SERVER\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"VALIDATOR\\_EXT\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"TARGET\\_APPLY\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"TASK\\_MANAGER\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"TABLES\\_MANAGER\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"METADATA\\_MANAGER\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"FILE\\_FACTORY\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"COMMON\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"ADDONS\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"DATA\\_STRUCTURE\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"COMMUNICATION\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" },\n\n{ \"Id\": \"FILE\\_TRANSFER\", \"Severity\": \"LOGGER\\_SEVERITY\\_DEFAULT\" }\n\n\\]\n\n  },\n\n  \"FailTaskWhenCleanTaskResourceFailed\": false,\n\n  \"LoopbackPreventionSettings\": null,\n\n  \"PostProcessingRules\": null,\n\n  \"StreamBufferSettings\": {\n\n\"CtrlStreamBufferSizeInMB\": 3,\n\n\"StreamBufferCount\": 2,\n\n\"StreamBufferSizeInMB\": 4\n\n  },\n\n  \"TTSettings\": {\n\n\"EnableTT\": false,\n\n\"TTRecordSettings\": null,\n\n\"TTS3Settings\": null\n\n  },\n\n  \"BeforeImageSettings\": null,\n\n  \"ChangeProcessingDdlHandlingPolicy\": {\n\n\"HandleSourceTableAltered\": true,\n\n\"HandleSourceTableDropped\": true,\n\n\"HandleSourceTableTruncated\": true\n\n  },\n\n  \"ChangeProcessingTuning\": {\n\n\"BatchApplyMemoryLimit\": 200,\n\n\"BatchApplyPreserveTransaction\": true,\n\n\"BatchApplyTimeoutMax\": 30,\n\n\"BatchApplyTimeoutMin\": 1,\n\n\"BatchSplitSize\": 0,\n\n\"CommitTimeout\": 1,\n\n\"MemoryKeepTime\": 60,\n\n\"MemoryLimitTotal\": 512,\n\n\"MinTransactionSize\": 1000,\n\n\"RecoveryTimeout\": -1,\n\n\"StatementCacheSize\": 20\n\n  },\n\n  \"CharacterSetSettings\": null,\n\n  \"ControlTablesSettings\": {\n\n\"CommitPositionTableEnabled\": false,\n\n\"ControlSchema\": \"\",\n\n\"FullLoadExceptionTableEnabled\": false,\n\n\"HistoryTableEnabled\": false,\n\n\"HistoryTimeslotInMinutes\": 5,\n\n\"StatusTableEnabled\": false,\n\n\"SuspendedTablesTableEnabled\": false\n\n  },\n\n  \"TargetMetadata\": {\n\n\"BatchApplyEnabled\": false,\n\n\"FullLobMode\": false,\n\n\"InlineLobMaxSize\": 0,\n\n\"LimitedSizeLobMode\": true,\n\n\"LoadMaxFileSize\": 0,\n\n\"LobChunkSize\": 32,\n\n\"LobMaxSize\": 32,\n\n\"ParallelApplyBufferSize\": 0,\n\n\"ParallelApplyQueuesPerThread\": 0,\n\n\"ParallelApplyThreads\": 0,\n\n\"ParallelLoadBufferSize\": 0,\n\n\"ParallelLoadQueuesPerThread\": 0,\n\n\"ParallelLoadThreads\": 0,\n\n\"SupportLobs\": true,\n\n\"TargetSchema\": \"\",\n\n\"TaskRecoveryTableEnabled\": false\n\n  }\n\n}",
    "author": "Specific-Signal4256",
    "timestamp": "2025-07-04T12:43:38",
    "url": "https://reddit.com/r/bigdata/comments/1lrrkzi/aws_dms_out_of_memory_error_during_full_load/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lrbil6",
    "title": "Iceberg ingestion case study: 70% cost reduction",
    "content": "hey folks I wanted to share a recent win we had with one of our users. (i work at dlthub where we build dlt the oss  python library for ingestion)\n\nThey were getting a 12x data increase and had to figure out how to not 12x their analytics bill, so they flipped to Iceberg and saved 70% of the cost.\n\n[https://dlthub.com/blog/taktile-iceberg-ingestion](https://dlthub.com/blog/taktile-iceberg-ingestion)  \n\n\n",
    "author": "Thinker_Assignment",
    "timestamp": "2025-07-03T23:11:25",
    "url": "https://reddit.com/r/bigdata/comments/1lrbil6/iceberg_ingestion_case_study_70_cost_reduction/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lr2zwz",
    "title": "$WAXP Just Flipped the Script â€” From Inflation to Deflation. Here's What It Means.",
    "content": "Holla #WAXFAM and $WAXP hodler ğŸ‘‹ I have a latest update about the $WAXP native token. \n\nhttps://preview.redd.it/3s1dgbeqkqaf1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=c65dbeecc7dfdbf869dda85ea19e7ab2cdf2b5bf\n\nWAX just made one of the boldest moves weâ€™ve seen in the Layer-1 space lately â€” theyâ€™ve *completely flipped their tokenomics model* from inflationary to deflationary.\n\nHereâ€™s the TL;DR:\n\n* **Annual emissions slashed** from 653 million to just **156 million WAXP**\n* **50% of all emissions will be burned**\n\nThatâ€™s not just a tweak â€” thatâ€™s a **75%+ cut in new tokens**, and then half of those tokens are literally torched . It is now officially entering a phase where more WAXP could be destroyed than created.\n\n# Why it matters?\n\nIn a market where most L1s are still dealing with high inflation to fuel ecosystem growth, WAX is going in the opposite direction â€” focusing on **long-term value and sustainability**. Itâ€™s a major shift away from growth-at-all-costs to a model that rewards **retention and real usage**.\n\n# What could change?\n\n* **Price pressure**: Less new supply = less sell pressure on exchanges.\n* **Staker value**: If supply drops and demand holds, staking rewards could become more meaningful over time.\n* **dApp/GameFi builders**: Better economics means stronger incentives to build on WAX without the constant fear of token dilution.\n\n# How does this stack up vs Ethereum or Solana?\n\nEthereumâ€™s EIP-1559 burn mechanism was a game-changer, but it still operates with net emissions. Solana, meanwhile, keeps inflation relatively high to subsidize validators.\n\nWAX is going **full deflationary**, and thatâ€™s rare â€” especially for a chain with strong roots in NFTs and GameFi. If this works, it could be a blueprint for how other chains rethink emissions.\n\n\\#WAXNFT #WAXBlockchain\n\n",
    "author": "Key_Size_5033",
    "timestamp": "2025-07-03T15:38:33",
    "url": "https://reddit.com/r/bigdata/comments/1lr2zwz/waxp_just_flipped_the_script_from_inflation_to/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lqr3go",
    "title": "10 Not-to-Miss Data Science Tools",
    "content": "Modern [data science tools](https://www.usdsi.org/data-science-insights/resources/10-popular-data-science-tools-to-consider-exploring) blend code, cloud, and AIâ€”fueling powerful insights and faster decisions. They're the backbone of predictive models, data pipelines, and business transformation. \n\nExplore what tools are expected of you as a seasoned data science expert in 2025\n\nhttps://preview.redd.it/e1xa44as6oaf1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=68ef192f2f5287b0af0937584dd598230a8217e4\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-07-03T07:32:32",
    "url": "https://reddit.com/r/bigdata/comments/1lqr3go/10_nottomiss_data_science_tools/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lpzutk",
    "title": "What is the easiest way to set up a no-code data pipeline that still handles complex logic?",
    "content": "Trying to find a balance between simplicity and power. I donâ€™t want to code everything from scratch but still need something that can transform and sync data between a bunch of sources. Any tools actually deliver both?",
    "author": "PresentationThink966",
    "timestamp": "2025-07-02T09:08:20",
    "url": "https://reddit.com/r/bigdata/comments/1lpzutk/what_is_the_easiest_way_to_set_up_a_nocode_data/",
    "score": 5,
    "num_comments": 16,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lpvghe",
    "title": "Are You Scaling Data Responsibly? Why Ethics &amp; Governance Matter More Than Ever",
    "content": "Let me know how you're handling data ethics in your org.",
    "author": "GreenMobile6323",
    "timestamp": "2025-07-02T06:10:10",
    "url": "https://reddit.com/r/bigdata/comments/1lpvghe/are_you_scaling_data_responsibly_why_ethics/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lpkfny",
    "title": "WAX Is Burning Literally! Here's What Changed",
    "content": "https://preview.redd.it/ioivau4vfdaf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=7854dde80ce5a7632d56aee712ec01fa5c9c5065\n\nThe WAX team just came out with a pretty interesting update lately. While most Layer 1s are still dealing with high inflation, WAX is doing the oppositeâ€”focusing on cutting back its token supply instead of expanding it.\n\nSo, whatâ€™s the new direction?  \nPreviously, most of the network resources were powered through stakingâ€”around 90% staking and 10% PowerUp. Now, theyâ€™re flipping that completely: the new goal is 90% PowerUp and just 10% staking.\n\nWhat does that mean in practice?  \nStaking rewards are being scaled down, and fewer new tokens are being minted. Meanwhile, PowerUp revenue is being used to replace inflationâ€”and any unused inflation gets burned. So, the more the network is used, the more tokens are effectively removed from circulation. Usage directly drives supply reduction.\n\nNow letâ€™s talk price, validators, and GameFi:  \nValidators still earn a decent staking yield, but the system is shifting toward usage-based revenue. That means validator rewards can become more sustainable over time, tied to real activity instead of inflation.  \nFor GameFi builders and players, knowing that resource usage burns tokens could help keep transaction costs more stable in the long run. That makes WAX potentially more user-friendly for high-volume gaming ecosystems.\n\nWhat about Ethereum and Solana?  \nSure, Ethereum burns base fees via EIPâ€‘1559, but it still has net positive inflation. Solana has more limited burning mechanics. WAX, on the other hand, is pushing a model where inflation is minimized and burning is directly linked to real usageâ€”something thatâ€™s clearly tailored for GameFi and frequent activity.\n\nSo in short, WAX is evolving from a low-fee blockchain into something more: a usage-driven, sustainable network model.",
    "author": "Fahim61891012",
    "timestamp": "2025-07-01T19:25:40",
    "url": "https://reddit.com/r/bigdata/comments/1lpkfny/wax_is_burning_literally_heres_what_changed/",
    "score": 8,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lpe8g9",
    "title": "My diagram of abstract math concepts illustrated",
    "content": "Made this flowchart explaining all parts of Math in a symplectic way.  \nLet me know if I missed something :)",
    "author": "FractalNerve",
    "timestamp": "2025-07-01T14:37:23",
    "url": "https://reddit.com/r/bigdata/comments/1lpe8g9/my_diagram_of_abstract_math_concepts_illustrated/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lp2wag",
    "title": "NiFi 2.0 vs NiFi 1.0: What's the BEST Choice for Data Processing",
    "content": "",
    "author": "GreenMobile6323",
    "timestamp": "2025-07-01T07:19:34",
    "url": "https://reddit.com/r/bigdata/comments/1lp2wag/nifi_20_vs_nifi_10_whats_the_best_choice_for_data/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lophaa",
    "title": "Handling Bad Records in Streaming Pipelines Using Dead Letter Queues in PySpark",
    "content": "ğŸš€ I just published a detailed guide on handling Dead Letter Queues (DLQ) in PySpark Structured Streaming.\n\nIt covers:\n\n\\- Separating valid/invalid records\n\n\\- Writing failed records to a DLQ sink\n\n\\- Best practices for observability and reprocessing\n\nWould love feedback from fellow data engineers!\n\nğŸ‘‰ \\[Read here\\](Â [https://medium.com/@santhoshkumarv/handling-bad-records-in-streaming-pipelines-using-dead-letter-queues-in-pyspark-265e7a55eb29](https://medium.com/@santhoshkumarv/handling-bad-records-in-streaming-pipelines-using-dead-letter-queues-in-pyspark-265e7a55eb29)Â )",
    "author": "Santhu_477",
    "timestamp": "2025-06-30T18:44:24",
    "url": "https://reddit.com/r/bigdata/comments/1lophaa/handling_bad_records_in_streaming_pipelines_using/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lo9z40",
    "title": "Unlock Business Insights: Why Looker Leads in BI Tools",
    "content": "",
    "author": "AllenMutum",
    "timestamp": "2025-06-30T08:08:48",
    "url": "https://reddit.com/r/bigdata/comments/1lo9z40/unlock_business_insights_why_looker_leads_in_bi/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lnzraq",
    "title": "Get an Analytics blue-print instantly",
    "content": "AutoAnalyst gives you a reliable blueprint by handling all the key steps: data preprocessing, modeling, and visualization.\n\nIt starts by understanding your goal and then plans the right approach.\n\nA built-in planner routes each part of the job to the right AI agent.\n\nSo you donâ€™t have to guess what to do nextâ€”the system handles it.\n\nThe result is a smooth, guided analysis that saves time and gives clear answers.\n\nLink: https://autoanalyst.ai\n\nLink to repo: https://github.com/FireBird-Technologies/Auto-Analyst\n\n",
    "author": "phicreative1997",
    "timestamp": "2025-06-29T22:43:46",
    "url": "https://reddit.com/r/bigdata/comments/1lnzraq/get_an_analytics_blueprint_instantly/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1llm1ve",
    "title": "ğŸ“Š Clickstream Behavior Analysis with Dashboard using Kafka, Spark Streaming, MySQL, and Zeppelin!",
    "content": "ğŸš€ New Real-Time Project Alert for Free!\n\n\n\nğŸ“Š Clickstream Behavior Analysis with Dashboard\n\nTrack &amp; analyze user activity in real time using Kafka, Spark Streaming, MySQL, and Zeppelin! ğŸ”¥\n\n\n\nğŸ“Œ What Youâ€™ll Learn:\n\nâœ… Simulate user click events with Java\n\nâœ… Stream data using Apache Kafka\n\nâœ… Process events in real-time with Spark Scala\n\nâœ… Store &amp; query in MySQL\n\nâœ… Build dashboards in Apache Zeppelin ğŸ§ \n\n\n\nğŸ¥ Watch the 3-Part Series Now:\n\n\n\nğŸ”¹ Part 1: Clickstream Behavior Analysis (Part 1)\n\nğŸ“½ [https://youtu.be/jj4Lzvm6pzs](https://youtu.be/jj4Lzvm6pzs)\n\n\n\nğŸ”¹ Part 2: Clickstream Behavior Analysis (Part 2)\n\nğŸ“½ [https://youtu.be/FWCnWErarsM](https://youtu.be/FWCnWErarsM)\n\n\n\nğŸ”¹ Part 3: Clickstream Behavior Analysis (Part 3)\n\nğŸ“½ [https://youtu.be/SPgdJZR7rHk](https://youtu.be/SPgdJZR7rHk)\n\n\n\nThis is perfect for Data Engineers, Big Data learners, and anyone wanting hands-on experience in streaming analytics.\n\n\n\nğŸ“¡ Try it, tweak it, and track real-time behaviors like a pro!\n\nğŸ’¬ Let us know if you'd like the full source code!\n\n",
    "author": "bigdataengineer4life",
    "timestamp": "2025-06-26T22:35:13",
    "url": "https://reddit.com/r/bigdata/comments/1llm1ve/clickstream_behavior_analysis_with_dashboard/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ll9xwp",
    "title": "How do you reliably detect model drift in production LLMs",
    "content": "We recently launched an LLM in production and saw unexpected behaviorâ€”hallucinations and output driftâ€”sneaking in under the radar.\n\nOur solution? AnÂ **AI-native observability stack**Â using unsupervised ML, prompt-level analytics, and trace correlation.\n\nI wrote up what worked, what didnâ€™t, and how to build a proactive drift detection pipeline.\n\nWould love feedback from anyone using similar strategies or frameworks.\n\n**TL;DR:**\n\n* What model drift isâ€”and why itâ€™s hard to detect\n* How we instrument models, prompts, infra for full observability\n* Examples of drift sign patterns and alert logic\n\nFull post here ğŸ‘‰[https://insightfinder.com/blog/model-drift-ai-observability/](https://insightfinder.com/blog/model-drift-ai-observability/)",
    "author": "elm3131",
    "timestamp": "2025-06-26T12:56:38",
    "url": "https://reddit.com/r/bigdata/comments/1ll9xwp/how_do_you_reliably_detect_model_drift_in/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lj3izn",
    "title": "Data Architecture Complexity",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-06-23T23:06:17",
    "url": "https://reddit.com/r/bigdata/comments/1lj3izn/data_architecture_complexity/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lii21l",
    "title": "Hammerspace IO500 Benchmark Demonstrates Simplicity Doesnâ€™t Have to Come at the Cost of Storage Inefficiency",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-06-23T07:29:11",
    "url": "https://reddit.com/r/bigdata/comments/1lii21l/hammerspace_io500_benchmark_demonstrates/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lgnebr",
    "title": "Big data course by sumit mittal",
    "content": "Why is no body raising voice against the blatant scam done by sumit mittal in the name of selling courses ..\nI bought his course for 45k ..trust me ..I would have found more value on the best Udemy courses present on this topic for 500 rupees \nThis guy keeps posting day in and day out of whatsapp screenshots of his students getting 30lpa jobs ..which for most part i think is fabricated ..because it's the same pattern all the time ..\nSoo many people are looking for jobs and the kind of misselling this guy does ..I am sad that many are buying and falling prey to his scam ..\nHow can this be approached legally and stop this nuisance from propagating ",
    "author": "abheshekcr",
    "timestamp": "2025-06-20T20:56:16",
    "url": "https://reddit.com/r/bigdata/comments/1lgnebr/big_data_course_by_sumit_mittal/",
    "score": 7,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lg0sgf",
    "title": "10 MOST POPULAR IoT APPLICATIONS OF 2025 | INFOGRAPHIC",
    "content": "Internet of things is what is taking over the world by a storm. With connected devices growing at a staggering rate, it is inevitable to understand what [IoT applications](https://www.usdsi.org/data-science-insights/10-most-popular-iot-applications-of-2025) look like. With sensors, software, networks, devices- all sharing a common platform; it necessitates the comprehension of how this impact our lives in a million different ways.\n\nWith Mordor Intelligence bringing up the forecast for the global IoT market size to grow at a CAGR of 15.12%, only to reach a whopping **US$2.72 trillion**\\- this industry is not going to stop anytime soon. It is here to stay as the technology advances. \n\nFrom smart homes, to wearable health tech, connected self-driving cars, smart cities, industrial IoT, precision farming- you name it and IoT has a powerful use case in that industry or sector worldwide. Gain an inside out comprehension of IoT applications right here!\n\nhttps://preview.redd.it/7uvbthu2928f1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=2f324bb08bc42d135d4ae11bb8182cace9e83c07\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-06-20T03:40:13",
    "url": "https://reddit.com/r/bigdata/comments/1lg0sgf/10_most_popular_iot_applications_of_2025/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lf2mf7",
    "title": "Data Governance and Access Control in a Multi-Platform Big Data Environment",
    "content": "Our organization uses Snowflake, Databricks, Kafka, and Elasticsearch, each with its own ACLs and tagging system. Auditors demand a single source of truth for data permissions and lineage. How have you centralized governance, either via an open-source catalog or commercial tool, to manage roles, track usage, and automate compliance checks across diverse big data platforms?",
    "author": "GreenMobile6323",
    "timestamp": "2025-06-18T22:15:46",
    "url": "https://reddit.com/r/bigdata/comments/1lf2mf7/data_governance_and_access_control_in_a/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lf0ztb",
    "title": "Apache Fory Serialization Framework 0.11.0 Released",
    "content": "",
    "author": "Shawn-Yang25",
    "timestamp": "2025-06-18T20:42:23",
    "url": "https://reddit.com/r/bigdata/comments/1lf0ztb/apache_fory_serialization_framework_0110_released/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1leox1u",
    "title": "Ever had to migrate a data warehouse from Redshift to Snowflake? What was harder than expected?",
    "content": "Weâ€™re considering moving from Redshift to Snowflake for performance and cost. It looks simple, but Iâ€™m sure there are gotchas.  \n  \nWhat were the trickiest parts of the migration for you?",
    "author": "eb0373284",
    "timestamp": "2025-06-18T11:38:14",
    "url": "https://reddit.com/r/bigdata/comments/1leox1u/ever_had_to_migrate_a_data_warehouse_from/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1leq09o",
    "title": "Semantic Search + LLMs = Smarter Systems",
    "content": "As data volume explodes, keyword indexes fall apart, missing context, underperforming at scale, and failing to surface unstructured insights. This breakdown walks through how semantic embeddings and vector search backed by LLMs transform discoverability across massive datasets. Learn how modern retrieval (via RAG) scales better, retrieves smarter, and handles messy multimodal inputs.\n\n[full blog](https://ducky.ai/blog/ai-killed-traditional-search?utm_source=reddit-bigdata&amp;utm_medium=post&amp;utm_campaign=technical-use_case&amp;utm_content=ai-killed-search)",
    "author": "superconductiveKyle",
    "timestamp": "2025-06-18T12:20:48",
    "url": "https://reddit.com/r/bigdata/comments/1leq09o/semantic_search_llms_smarter_systems/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lee7mj",
    "title": "Hottest Data Analytics Trends 2025",
    "content": "In 2025, [data analytics](https://www.usdsi.org/data-science-insights/resources/6-hottest-data-analytics-trends-to-prepare-ahead-of-2025) gets sharperâ€”real-time dashboards, AI-powered insights, and ethical governance will dominate. Expect faster decisions, deeper personalization, and smarter automation across industries.\n\nhttps://reddit.com/link/1lee7mj/video/0ortwuoo3o7f1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-06-18T04:02:51",
    "url": "https://reddit.com/r/bigdata/comments/1lee7mj/hottest_data_analytics_trends_2025/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lefoed",
    "title": "We built a high-performance storage for big data",
    "content": "Hi everyone! We're a small storage startup from Berlin and wanted to share something we've been working on and get some feedback from the community here.\n\nOver the last few years working on this, we've heard a lot about how storage can massively slow down modern AI pipelines, especially during training or when building anything retrieval-based like RAG. So we thought it would be a good idea to built something focused on performance.\n\nUltiHash is S3-compatible object storage, designed to serve high-throughput, read-heavy workloads: originally for MLOps use cases, but is also a good fit for big data infrastructure more broadly.\n\nWe just launched the serverless version: itâ€™s fully managed, with no infra to run. You spin up a cluster, get an endpoint, and connect using any S3-compatible tool.\n\nThings to know:\n\n* 1 GB/s read per machine: youâ€™re not leaving compute idle\n* S3 compatible: you can integrate with your stack (Spark, Kafka, PyTorch, Iceberg, Trino, etc.)\n* Scales past 100TB without having to rework your setup\n* Lowers TCO: e.g. our 10TB tier is â‚¬0.21/GB/month, infra + support included\n\nWe host everything in the EU currently in AWS Frankfurt (`eu-central-1`) with Hetzner and OVH Cloud support coming soon (waitlistâ€™s open).\n\nWould love to hear what folks here think. More details here: [https://www.ultihash.io/serverless](https://www.ultihash.io/serverless), happy to go deeper into how weâ€™re handling throughput, deduplication, or anything else.",
    "author": "UH-Simon",
    "timestamp": "2025-06-18T05:22:03",
    "url": "https://reddit.com/r/bigdata/comments/1lefoed/we_built_a_highperformance_storage_for_big_data/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ldhwua",
    "title": "Serialization Framework Announcement - Apache Fury is Now Apache Fory",
    "content": "",
    "author": "Shawn-Yang25",
    "timestamp": "2025-06-17T01:35:12",
    "url": "https://reddit.com/r/bigdata/comments/1ldhwua/serialization_framework_announcement_apache_fury/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1lacj5o",
    "title": "R or Python - Contesting Programming Giants to be the Best",
    "content": "Gain access to clear insights on the best suited programming language for your machine learning tasks among [R and Python](https://www.usdsi.org/data-science-insights/r-or-python-contesting-programming-giants-to-be-the-best).\n\nhttps://preview.redd.it/m767v36f9o6f1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=6bb413b04fe768dea4c669ec9d7231409662809a\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-06-13T03:30:49",
    "url": "https://reddit.com/r/bigdata/comments/1lacj5o/r_or_python_contesting_programming_giants_to_be/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1la4kxq",
    "title": "[D] Why Is Enterprise Data Integration Always So Messy? My Clientsâ€™ Real-Life Nightmares",
    "content": "",
    "author": "Worried-Variety3397",
    "timestamp": "2025-06-12T19:14:00",
    "url": "https://reddit.com/r/bigdata/comments/1la4kxq/d_why_is_enterprise_data_integration_always_so/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1l91z41",
    "title": "Unstructured Data Orchestration for Dummies",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-06-11T12:36:53",
    "url": "https://reddit.com/r/bigdata/comments/1l91z41/unstructured_data_orchestration_for_dummies/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1l8re4r",
    "title": "Cursor for data engineers according to you",
    "content": "I'm exploring the idea of building a purpose-built IDE for data engineers. Curious to know what tools or workflows do you feel are still clunky or missing in todayâ€™s setup? And how can AI help?",
    "author": "Hot_Donkey9172",
    "timestamp": "2025-06-11T05:29:07",
    "url": "https://reddit.com/r/bigdata/comments/1l8re4r/cursor_for_data_engineers_according_to_you/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1l7ofuz",
    "title": "Best Big Data Courses on Udemy to learn in 2025",
    "content": "",
    "author": "[deleted]",
    "timestamp": "2025-06-09T20:13:04",
    "url": "https://reddit.com/r/bigdata/comments/1l7ofuz/best_big_data_courses_on_udemy_to_learn_in_2025/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1l6z6m1",
    "title": "Resolving Data Quality Constraints",
    "content": "Data quality isnâ€™t just a checkboxâ€”itâ€™s the backbone of smart data-driven decision-making. Clean, consistent, and reliable data fuels trust, boosts efficiency, and drives impact. Because when data speaks the truth, your insights lead the way. \n\nThis read targets strategic challenges, and possible solutions to resolve [data quality](https://www.usdsi.org/data-science-insights/data-quality-matters-risks-constraints-and-solution-potion) issues.\n\nhttps://preview.redd.it/dcl079y90v5f1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=0f57f2bbcf88db96c75717356bd91c750ed79f54\n\n \n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-06-09T01:07:54",
    "url": "https://reddit.com/r/bigdata/comments/1l6z6m1/resolving_data_quality_constraints/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1l56nb8",
    "title": "If you had to rebuild your data stack from scratch, what's the one tool you'd keep?",
    "content": "We're cleaning house, rethinking our whole stack after growing way too fast and ending up with a Frankenstein setup. Curious what tools people stuck with long-term, especially for data pipelines and integrations.",
    "author": "zekken908",
    "timestamp": "2025-06-06T16:26:27",
    "url": "https://reddit.com/r/bigdata/comments/1l56nb8/if_you_had_to_rebuild_your_data_stack_from/",
    "score": 8,
    "num_comments": 9,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1l4q46r",
    "title": "Clickhouse in a large-scale user-persoanlized marketing campaign",
    "content": "Dear colleagues \nHello\nI would like to introduce our last project at Snapp Market (Iranian Q-Commerce business like Instacart) in which we took the advantage of Clickhouse as an analytical DB to run a large scale user personalized marketing campaign, with GenAI.\n\nhttps://medium.com/@prmbas/clickhouse-in-the-wild-an-odyssey-through-our-data-driven-marketing-campaign-in-q-commerce-93c2a2404a39\n\nI will be grateful if I have your opinion about this.\n\n#ClickHouse\n",
    "author": "Professional-Ant9045",
    "timestamp": "2025-06-06T04:32:46",
    "url": "https://reddit.com/r/bigdata/comments/1l4q46r/clickhouse_in_a_largescale_userpersoanlized/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1l3v29w",
    "title": "100 MUI Style Login Form Designs - JV Codes 2025",
    "content": "",
    "author": "shokatjaved",
    "timestamp": "2025-06-05T02:43:39",
    "url": "https://reddit.com/r/bigdata/comments/1l3v29w/100_mui_style_login_form_designs_jv_codes_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1l2x9p1",
    "title": "How to create HIVE Table with multi character delimiter? (Hands On)",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-06-03T22:08:02",
    "url": "https://reddit.com/r/bigdata/comments/1l2x9p1/how_to_create_hive_table_with_multi_character/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1l30115",
    "title": "AI Features for PowerBI Platform",
    "content": "Who needs a data scientist when Power BIâ€™s AI features have your back? Ask questions in plain English, get instant insights, and let machine learning spot trends before your coffee even cools. Itâ€™s like giving Excel a PhD and a sense of style.\n\nSmart data- Slick delivery!\n\nWatch Video [https://youtu.be/-b657kvhJv8](https://youtu.be/-b657kvhJv8) to Get Nuanced in PowerBI as a Data Expert Today!\n\nhttps://reddit.com/link/1l30115/video/q0q8rgw4fv4f1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-06-04T01:09:28",
    "url": "https://reddit.com/r/bigdata/comments/1l30115/ai_features_for_powerbi_platform/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kztwhf",
    "title": "Big Data in Smart Cities: Transforming Urban Life 2025",
    "content": "In 2025, big data analytics forms the backbone of smart cities, transforming urban life in meaningful and measurable ways. From optimizing transportation and managing resources sustainably to enhancing public safety and fostering community engagement, data science is making cities more livable, efficient, and inclusive. However, challenges around privacy, infrastructure, and equity underscore the importance of adopting ethical and inclusive data practices.Â Looking ahead, data science will continue to redefine how cities operate and grow. Freelance data analysts have a vital role to play in this evolution bringing agility, innovation, and expertise to urban analytics.",
    "author": "Pangaeax_",
    "timestamp": "2025-05-31T03:27:37",
    "url": "https://reddit.com/r/bigdata/comments/1kztwhf/big_data_in_smart_cities_transforming_urban_life/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ky2nwb",
    "title": "(Hands On) Writing and Optimizing SQL Queries with ChatGPT",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-05-28T22:15:58",
    "url": "https://reddit.com/r/bigdata/comments/1ky2nwb/hands_on_writing_and_optimizing_sql_queries_with/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kxfdi9",
    "title": "Python in Data Science",
    "content": "Python is the ultimate data whispererâ€”transforming complex datasets into clear, compelling stories with just a few lines of code. From cleaning chaos to uncovering trends, [Python](https://www.usdsi.org/data-science-insights/resources/python-for-data-science-explained-in-6-easy-steps) is the language that turns data science into data art.\n\nhttps://preview.redd.it/8upymt5yji3f1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=10615e26a8053f1ccca25e306cc1dfcdad9271af\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-05-28T05:06:45",
    "url": "https://reddit.com/r/bigdata/comments/1kxfdi9/python_in_data_science/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kx9pii",
    "title": "Write and Optimize SQL Queries with ChatGPT (Hands-On Guide!)",
    "content": "ğŸš€ New Video Drop: Write and Optimize SQL Queries with ChatGPT (Hands-On Guide!)\n\nStruggling with complex SQL queries or looking to write cleaner, faster code?\n\nLet ChatGPT be your co-pilot in mastering SQLâ€”especially for Big Data and Spark environments!\n\nğŸ” In this hands-on video, you'll learn:\n\nâœ… How to write SQL queries with ChatGPT\n\nâœ… Optimizing SQL for performance in large datasets\n\nâœ… Debugging and enhancing your queries with AI\n\nâœ… Real-world examples tailored for Data Engineers\n\nâœ… How ChatGPT fits into your Big Data stack (Hadoop/Spark)\n\nğŸ’¡ Perfect for:\n\nData Engineers working with massive datasets\n\nSQL beginners and pros looking to optimize queries\n\nAnyone exploring AI-assisted coding in analytics\n\nğŸ”¥ Donâ€™t miss this productivity boost for your data workflows!\n\nğŸ› ï¸ Tech Covered: SQL â€¢ ChatGPT â€¢ Apache Spark â€¢ Hadoop\n\nğŸ‘‡ Check it out &amp; share your thoughts in the comments!\n\n",
    "author": "bigdataengineer4life",
    "timestamp": "2025-05-27T23:01:32",
    "url": "https://reddit.com/r/bigdata/comments/1kx9pii/write_and_optimize_sql_queries_with_chatgpt/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kwpqzy",
    "title": "[1999â€“2025] SEC Filings - 21,000 funds. 850,000+ detailed filings. Full portfolios, control rights, phone numbers, addresses. Itâ€™s all here.",
    "content": "",
    "author": "Beneficial_Baby5458",
    "timestamp": "2025-05-27T08:11:25",
    "url": "https://reddit.com/r/bigdata/comments/1kwpqzy/19992025_sec_filings_21000_funds_850000_detailed/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kwo52p",
    "title": "The 16 Largest US Funding Rounds of April 2025",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-05-27T07:05:44",
    "url": "https://reddit.com/r/bigdata/comments/1kwo52p/the_16_largest_us_funding_rounds_of_april_2025/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kwmm57",
    "title": "Scaling AI Applications with Open-Source Hugging Face Models",
    "content": "",
    "author": "JanethL",
    "timestamp": "2025-05-27T05:58:44",
    "url": "https://reddit.com/r/bigdata/comments/1kwmm57/scaling_ai_applications_with_opensource_hugging/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kwgjlx",
    "title": "Apache Fury serialization framework 0.10.3 released",
    "content": "",
    "author": "Shawn-Yang25",
    "timestamp": "2025-05-26T23:39:48",
    "url": "https://reddit.com/r/bigdata/comments/1kwgjlx/apache_fury_serialization_framework_0103_released/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kvnoic",
    "title": "DATA SCIENCE CERTIFICATIONS",
    "content": "Getting certified shows youâ€™re not just interestedâ€”youâ€™ve got the skills to back it up. It makes your resume pop and helps you stand out when applying for those high-paying, exciting data science jobs. Plus, youâ€™ll learn the latest data science tools and techniques that keep you ahead of the curve. \n\nBottom line? A [Data Science Certification](https://www.usdsi.org/data-science-certifications) is one of the smartest moves to boost your career and open new doors in data science.\n\nhttps://preview.redd.it/xncy5t1nn23f1.jpg?width=1400&amp;format=pjpg&amp;auto=webp&amp;s=9a53160c0f2475f807d98c43acb737701ce7a023\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-05-25T23:38:54",
    "url": "https://reddit.com/r/bigdata/comments/1kvnoic/data_science_certifications/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kvm2h8",
    "title": "Running Hive on Windows Using Docker Desktop (Hands On)",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-05-25T21:57:43",
    "url": "https://reddit.com/r/bigdata/comments/1kvm2h8/running_hive_on_windows_using_docker_desktop/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kvfdju",
    "title": "Cursor for data with chat, rich context and tool use (Currently supports PostgreSQL and BigQuery)",
    "content": "",
    "author": "jekapats",
    "timestamp": "2025-05-25T15:53:16",
    "url": "https://reddit.com/r/bigdata/comments/1kvfdju/cursor_for_data_with_chat_rich_context_and_tool/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kuzrib",
    "title": "Spacebar Counter Using HTML, CSS and JavaScript (Free Source Code) - JV Codes 2025",
    "content": "",
    "author": "shokatjaved",
    "timestamp": "2025-05-25T03:52:59",
    "url": "https://reddit.com/r/bigdata/comments/1kuzrib/spacebar_counter_using_html_css_and_javascript/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kuvk51",
    "title": "The 10 Coolest Open-Source Software Tools of 2025 in Big Data Technologies",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-05-24T23:02:11",
    "url": "https://reddit.com/r/bigdata/comments/1kuvk51/the_10_coolest_opensource_software_tools_of_2025/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kutz53",
    "title": "Hey everyone, I hope this is okay to post here â€“ just looking for a few people to beta test a tool Iâ€™m working on.",
    "content": "Iâ€™ve been working on a tool that helps businesses get more Google reviews by automating the process of asking for them through simple text templates. Itâ€™s a service Iâ€™m calling STARSLIFT, and Iâ€™d love to get some real-world feedback before fully launching it.\n\nHereâ€™s what it does:\n\nâœ… Automates the process of asking your customers for Google reviews via SMS\n\nâœ… Lets you track reviews and see how fast youâ€™re growing (review velocity)\n\nâœ… Designed for service-based businesses who want more reviews but donâ€™t have time to manually ask\n\nRight now, Iâ€™m looking for a few U.S.-based businesses willing to test it completely free. The goal is to see how it works in real-world settings and get feedback on how to improve it.\n\nIf you:\n\n* Are a service-based business in the U.S. (think contractors, salons, dog groomers, plumbers, etc)\n\n* Get at least 5-20 customers a day\n\n* Are interested in trying it out for a few weeks\nâ€¦ Iâ€™d love to connect.\n\nAs a thank you, youâ€™ll get free access even after the beta ends.\n\nIf this sounds interesting, just drop a comment or DM me with:\n\n* What kind of business you have\n\n* How many customers you typically serve in a day\n\n* Whether youâ€™re in the U.S.\n\nIâ€™ll get back to you and set you up! No strings attached â€“ this is just for me to get feedback and for you to (hopefully) get more reviews for your business.",
    "author": "x36_",
    "timestamp": "2025-05-24T21:21:18",
    "url": "https://reddit.com/r/bigdata/comments/1kutz53/hey_everyone_i_hope_this_is_okay_to_post_here/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kuu0lm",
    "title": "Hey everyone, I hope this is okay to post here â€“ just looking for a few people to beta test a tool Iâ€™m working on.",
    "content": "Iâ€™ve been working on a tool that helps businesses get more Google reviews by automating the process of asking for them through simple text templates. Itâ€™s a service Iâ€™m calling STARSLIFT, and Iâ€™d love to get some real-world feedback before fully launching it.\n\nHereâ€™s what it does:\n\nâœ… Automates the process of asking your customers for Google reviews via SMS\n\nâœ… Lets you track reviews and see how fast youâ€™re growing (review velocity)\n\nâœ… Designed for service-based businesses who want more reviews but donâ€™t have time to manually ask\n\nRight now, Iâ€™m looking for a few U.S.-based businesses willing to test it completely free. The goal is to see how it works in real-world settings and get feedback on how to improve it.\n\nIf you:\n\n* Are a service-based business in the U.S. (think contractors, salons, dog groomers, plumbers, etc)\n\n* Get at least 5-20 customers a day\n\n* Are interested in trying it out for a few weeks\nâ€¦ Iâ€™d love to connect.\n\nAs a thank you, youâ€™ll get free access even after the beta ends.\n\nIf this sounds interesting, just drop a comment or DM me with:\n\n* What kind of business you have\n\n* How many customers you typically serve in a day\n\n* Whether youâ€™re in the U.S.\n\nIâ€™ll get back to you and set you up! No strings attached â€“ this is just for me to get feedback and for you to (hopefully) get more reviews for your business.",
    "author": "x36_",
    "timestamp": "2025-05-24T21:23:49",
    "url": "https://reddit.com/r/bigdata/comments/1kuu0lm/hey_everyone_i_hope_this_is_okay_to_post_here/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kutzvl",
    "title": "Hey everyone, I hope this is okay to post here â€“ just looking for a few people to beta test a tool Iâ€™m working on.",
    "content": "Iâ€™ve been working on a tool that helps businesses get more Google reviews by automating the process of asking for them through simple text templates. Itâ€™s a service Iâ€™m calling STARSLIFT, and Iâ€™d love to get some real-world feedback before fully launching it.\n\nHereâ€™s what it does:\n\nâœ… Automates the process of asking your customers for Google reviews via SMS\n\nâœ… Lets you track reviews and see how fast youâ€™re growing (review velocity)\n\nâœ… Designed for service-based businesses who want more reviews but donâ€™t have time to manually ask\n\nRight now, Iâ€™m looking for a few U.S.-based businesses willing to test it completely free. The goal is to see how it works in real-world settings and get feedback on how to improve it.\n\nIf you:\n\n* Are a service-based business in the U.S. (think contractors, salons, dog groomers, plumbers, etc)\n\n* Get at least 5-20 customers a day\n\n* Are interested in trying it out for a few weeks\nâ€¦ Iâ€™d love to connect.\n\nAs a thank you, youâ€™ll get free access even after the beta ends.\n\nIf this sounds interesting, just drop a comment or DM me with:\n\n* What kind of business you have\n\n* How many customers you typically serve in a day\n\n* Whether youâ€™re in the U.S.\n\nIâ€™ll get back to you and set you up! No strings attached â€“ this is just for me to get feedback and for you to (hopefully) get more reviews for your business.",
    "author": "x36_",
    "timestamp": "2025-05-24T21:22:34",
    "url": "https://reddit.com/r/bigdata/comments/1kutzvl/hey_everyone_i_hope_this_is_okay_to_post_here/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kut3po",
    "title": "Golden Birthday Calculator Using HTML, CSS and JavaScript (Free Source Code) - JV Codes 2025",
    "content": "",
    "author": "shokatjaved",
    "timestamp": "2025-05-24T20:29:11",
    "url": "https://reddit.com/r/bigdata/comments/1kut3po/golden_birthday_calculator_using_html_css_and/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ktjhay",
    "title": "DATA ACCESSIBILITY AND DATA DEMOCRATIZATION",
    "content": "Struggling with slow decisions due to limited data access? Itâ€™s time to [democratize data](https://www.usdsi.org/data-science-insights/how-data-accessibility-and-democratization-power-businesses)! Empower every teamâ€”from marketing to salesâ€”with real-time insights and user-friendly tools. \n\nBuild a data-driven culture where smart, fast decisions are the norm. Discover how data democratization transforms business agility and innovation. \n\nhttps://preview.redd.it/ppr3jyj49j2f1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=cb93a9e2e031ab4eeec046c161bdb2eb2ce460f2\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-05-23T06:23:46",
    "url": "https://reddit.com/r/bigdata/comments/1ktjhay/data_accessibility_and_data_democratization/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ktcans",
    "title": "Apache Spark vs. Hadoop: Which One Should You Learn in 2025?",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-05-22T22:59:56",
    "url": "https://reddit.com/r/bigdata/comments/1ktcans/apache_spark_vs_hadoop_which_one_should_you_learn/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ksm0pq",
    "title": "Which World-Class Certification to Head-Start Your Data Science Career? (CDSPâ„¢)",
    "content": "Kick start your data science career journey with one of the most comprehensive and detailed data science certification programs for beginners â€“ the [Certified Data Science Professional (CDSPâ„¢)](https://www.usdsi.org/data-science-certifications/certified-data-science-professional).\n\nOffered by the United States Data Science Institute (USDSIÂ®), this online and self-paced learning program will help you master the fundamentals of data science, including data wrangling, big data, exploratory data analysis, visualization, and more, all with free study materials including eBooks, lecture videos, and practice codes.\n\nWhether a graduate or a professional looking to switch to a data science career, this certification can be a perfect starting point for you.\n\nhttps://preview.redd.it/boc1ibvxsa2f1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=8a633fb4e61c49bff49054098383f2728602410a\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-05-22T01:58:59",
    "url": "https://reddit.com/r/bigdata/comments/1ksm0pq/which_worldclass_certification_to_headstart_your/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kshux2",
    "title": "Download Free ebook for Bigdata Interview Preparation Guide (1000+ questions with answers)",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-05-21T21:14:59",
    "url": "https://reddit.com/r/bigdata/comments/1kshux2/download_free_ebook_for_bigdata_interview/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1krc30y",
    "title": "How Business Intelligence (BI) &amp; Analytics Trends Evolved from 2021 to 2025",
    "content": "",
    "author": "dofthings",
    "timestamp": "2025-05-20T11:18:59",
    "url": "https://reddit.com/r/bigdata/comments/1krc30y/how_business_intelligence_bi_analytics_trends/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kqnbs6",
    "title": "Batch vs Micro-Batch vs Streaming â€” What I Learned After Building Many Pipelines",
    "content": "",
    "author": "New-Ship-5404",
    "timestamp": "2025-05-19T14:10:28",
    "url": "https://reddit.com/r/bigdata/comments/1kqnbs6/batch_vs_microbatch_vs_streaming_what_i_learned/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kqcdqg",
    "title": "Bohr Model of Atom Animations Using HTML, CSS and JavaScript (Free Source Code) - JV Codes 2025",
    "content": "",
    "author": "shokatjaved",
    "timestamp": "2025-05-19T06:58:45",
    "url": "https://reddit.com/r/bigdata/comments/1kqcdqg/bohr_model_of_atom_animations_using_html_css_and/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kotsxe",
    "title": "Solidus AITECH: Redefining HPC in Europe",
    "content": "https://preview.redd.it/m4oe5rnymc1f1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=1f7dd6a2e364aacbc118cd84e9a1ab0f3926c8f9\n\nEurope demands about one-third of global high-performance computing (HPC) capacity but can supply just 5% through local data centers. As a result, researchers and engineers often turn to costly U.S.-based supercomputers for their projects. Solidus AITECH aims to bridge this gap by building eco-friendly, on-continent HPC infrastructure tailored to Europeâ€™s needs.\n\n**Why Now Is the Moment for HPC Innovation**\n\n* Demand is exploding: from AI training and genome sequencing to climate modeling and complex financial simulations, workloads now routinely require petaflops of computing power.\n* Digital sovereignty is central to the EUâ€™s strategy: without robust local HPC infrastructure, true data and computation independence isnâ€™t achievable.\n* Sustainability pressures are mounting: strict environmental regulations make carbon-neutral data centers powered by renewables and advanced cooling technologies increasingly attractive to investors.\n\n**Decentralized HPC with Blockchain and AI**\n\n* Transparent resource management: a blockchain ledger records when and where each compute job runs, eliminating single points of failure.\n* Token-based incentives: hardware providers earn â€œHPC tokensâ€ for contributing resources, motivating them to maintain high quality and availability.\n* AI-driven optimization: smart contracts powered by AI route workloads based on cost, performance, and carbon footprint criteria to the most suitable HPC nodes.\n\n**Solidus AITECHâ€™s Layered Approach**\n\n1. **Marketplace Layer:** Users can rent CPU/GPU time via spot or futures contracts.\n2. **AI-Powered Scheduling:** Workloads are automatically filtered and dispatched to the most efficient HPC resources, balancing cost-performance and sustainability.\n3. **Green Data Center (Bucharest, 8,800 ftÂ²):** Built around renewable energy and liquid-cooling systems, this carbon-neutral facility will support both scientific and industrial HPC applications.\n\n**Value for Investors and Web3 Developers**\n\n* **Investors** can leverage EU-backed funding streams (e.g., Horizon Europe) alongside tokenized revenue models to optimize their risk-return profile.\n* **Web3 Developers** gain on-demand access to GPU-intensive HPC workloads through smart contracts, without needing to deploy or maintain their own infrastructure.\n\n**Next Steps**\n\n1. Launch comprehensive pilot projects with leading European research institutions.\n2. Accelerate integration via open-source APIs, SDKs, and sample applications.\n3. Design dynamic token-economy mechanisms to ensure market stability and liquidity.\n4. Enhance sustainability transparency through ESG reporting dashboards and independent audits.\n5. Build community awareness with technical webinars, hackathons, and success stories.\n\nBy consolidating Europeâ€™s HPC capacity with a green, blockchain-enabled architecture and AI-driven orchestration, Solidus AITECH will strengthen digital sovereignty and unlock fresh opportunities for the crypto ecosystem. This vision represents a long-term investment in the continentâ€™s digital future.",
    "author": "Fahim61891012",
    "timestamp": "2025-05-17T07:05:48",
    "url": "https://reddit.com/r/bigdata/comments/1kotsxe/solidus_aitech_redefining_hpc_in_europe/",
    "score": 10,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kos8ed",
    "title": "Big data QA",
    "content": "I have my interview for big data qa role ..what are the possible interview questions or topics that I must study?",
    "author": "deshpande_varun",
    "timestamp": "2025-05-17T05:50:18",
    "url": "https://reddit.com/r/bigdata/comments/1kos8ed/big_data_qa/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kopdxe",
    "title": "Snowflake vs. Databricks: Which Data Platform Wins?",
    "content": "Choosing the right data platform can define your success with analytics, machine learning, and business insights. Dive into our in-depth comparison of [Snowflake vs. Databricks](https://www.usdsi.org/data-science-insights/choosing-the-right-data-platform-snowflake-vs-databricks) â€” two giants in the modern data stack. \n\nFrom architecture and performance to cost and use cases, find out which platform fits your organizationâ€™s goals best.\n\nhttps://preview.redd.it/4qrtnqqxeb1f1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=10fedf64188b83c15461ae44608c73ba8b566d1b\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-05-17T02:58:32",
    "url": "https://reddit.com/r/bigdata/comments/1kopdxe/snowflake_vs_databricks_which_data_platform_wins/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ko9xx5",
    "title": "Data Modeling - star scheme case",
    "content": "Hello,  \nI am currently working on data modelling in my master degree project. I have designed scheme in 3NF. Now I would like also to design it in star scheme. Unfortunately I have little experience in data modelling and I am not sure if it is proper way of doing so (and efficient).\n\n3NF:\n\nhttps://preview.redd.it/p5b1jixr671f1.png?width=1084&amp;format=png&amp;auto=webp&amp;s=650e3e9a671e30f0f27847f382407804a1227640\n\nStar Schema:\n\nhttps://preview.redd.it/si2qubxs671f1.png?width=1103&amp;format=png&amp;auto=webp&amp;s=1763dc58024279f54237ab926d226a967a73e76c\n\nAppearances table is responsible for participation of people in titles (tv, movies etc.). Title is the most center table of the database because all the data revolves about rating of titles. I had no better idea than to represent person as factless fact table and treat appearances table as a bridge. Could tell me if this is valid or any better idea to model it please?",
    "author": "Wikar",
    "timestamp": "2025-05-16T12:44:40",
    "url": "https://reddit.com/r/bigdata/comments/1ko9xx5/data_modeling_star_scheme_case/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1knujxn",
    "title": "Best practice to get fed by Oracle database to process data?",
    "content": "I have a oracledb tables, that get updated in various fashions- daily, hourly, biweekly, monthly etc. The data is usually inserted millions of rows into the tables but needs processing. What is the best way to get this stream of rows, process and then put it into another oracledb / parquet format etc.",
    "author": "PM_ME_LINUX_CONFIGS",
    "timestamp": "2025-05-15T23:58:14",
    "url": "https://reddit.com/r/bigdata/comments/1knujxn/best_practice_to_get_fed_by_oracle_database_to/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1knv6dh",
    "title": "DATA CLEANING MADE EASY",
    "content": "Organizations across all industries now heavily rely on data-driven insights to make decisions and transform their business operations. Effective data analysis is one essential part of this transformation. \n\nBut for effective data analysis, it is important that the data used is clean, consistent, and accurate. The real-world data that data science professionals collect for analysis is often messy. These data are often collected from social media, customer transactions, sensors, feedback, forms, etc. And therefore, it is normal for the datasets to be inconsistent and with errors.\n\nThis is why data cleaning is a very important process in the data science project lifecycle. You may find it surprising that 83% of data scientists are using machine learning methods regularly in their tasks, including data cleaning, analysis, and data visualization (source: market.us). \n\nThese advanced techniques can, of course, speedup the data science processes. However, if you are a beginner, then you can use Pandaâ€™s one-liners to correct a lot of inconsistencies and missing values in your datasets.\n\nIn the following infographic, we explore the top 10 Pandas one-liners that you can use for:\n\nâ€¢ Dropping rows with missing values\n\nâ€¢ Extracting patterns with regular expressions\n\nâ€¢ Filling missing values\n\nâ€¢ Removing duplicates, and more\n\nThe infographic also guides you on how to create a sample dataframe from GitHub to work on.\n\nCheck out this infographic and master Pandaâ€™s one-liners for [data cleaning](https://www.usdsi.org/data-science-insights/top-5-automation-tools-for-data-cleaning)\n\nhttps://preview.redd.it/ici3zlxyl31f1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=0b2ee9afe75b5ca50f454735bb5a51b2e83d3dc0\n\n\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-05-16T00:43:03",
    "url": "https://reddit.com/r/bigdata/comments/1knv6dh/data_cleaning_made_easy/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1knrie9",
    "title": "ChatGPT for Data Engineers Hands On Practice",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-05-15T20:47:26",
    "url": "https://reddit.com/r/bigdata/comments/1knrie9/chatgpt_for_data_engineers_hands_on_practice/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1knp2ch",
    "title": "Looking for a car dataset",
    "content": "Hey folks,\nIâ€™m building a car spotting app and need to populate a database with vehicle makes, models, trims, and years. Iâ€™ve found the NHTSA API for US cars, which is great and free. But Iâ€™m struggling to find something similar for EU/UK vehicles â€” ideally a service or API that covers makes/models/trims with decent coverage.\n\nHas anyone come across a good resource or service for this? Bonus points if itâ€™s free or low-cost! Iâ€™m open to public datasets, APIs, or even commercial providers.\n\nThanks in advance!\n",
    "author": "CKRET__",
    "timestamp": "2025-05-15T18:35:34",
    "url": "https://reddit.com/r/bigdata/comments/1knp2ch/looking_for_a_car_dataset/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1knfmas",
    "title": "Where to find vin decoded data to use for a dataset?",
    "content": "Where to find vin decoded data to use for a dataset?\nCurrently building out a dataset full of vin numbers and their decoded information(Make,Model,Engine Specs, Transmission Details, etc.). What I have so far is the information form NHTSA Api, which works well, but looking if there is even more available data out there.\nDoes anyone have a dataset or any source for this type of information that can be used to expand the dataset?",
    "author": "Danielpot33",
    "timestamp": "2025-05-15T11:31:03",
    "url": "https://reddit.com/r/bigdata/comments/1knfmas/where_to_find_vin_decoded_data_to_use_for_a/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kn2a41",
    "title": "Efficient Graph Storage for Entity Resolution Using Clique-Based Compression",
    "content": "",
    "author": "major_grooves",
    "timestamp": "2025-05-15T00:28:15",
    "url": "https://reddit.com/r/bigdata/comments/1kn2a41/efficient_graph_storage_for_entity_resolution/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kmpaa3",
    "title": "The D of Things Newsletter #9 â€“ Appleâ€™s AI Flex, Doctor Bots &amp; RAG Warnings",
    "content": "",
    "author": "dofthings",
    "timestamp": "2025-05-14T13:15:02",
    "url": "https://reddit.com/r/bigdata/comments/1kmpaa3/the_d_of_things_newsletter_9_apples_ai_flex/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kmcofi",
    "title": "Big Data Analytics: Comprehensive Guide to How It Works",
    "content": "",
    "author": "Big_Data_Path",
    "timestamp": "2025-05-14T04:24:22",
    "url": "https://reddit.com/r/bigdata/comments/1kmcofi/big_data_analytics_comprehensive_guide_to_how_it/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kmc9nl",
    "title": "Best practices for ensuring cluster high availability",
    "content": "I'm looking for best practices to ensure high availability in a distributed NiFi cluster. We've got Zookeeper clustering, externalized flow configuration, and persistent storage for state, but would love to hear about additional steps or strategies you use for failover, node redundancy, and resiliency.\n\nHow do you handle scenarios like node flapping, controller service conflicts, or rolling updates with minimal downtime? Also, do you leverage Kubernetes or any external queueing systems for better HA?",
    "author": "GreenMobile6323",
    "timestamp": "2025-05-14T04:00:25",
    "url": "https://reddit.com/r/bigdata/comments/1kmc9nl/best_practices_for_ensuring_cluster_high/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1klolbk",
    "title": "Enhancing legal document comprehension using RAG: A practical application",
    "content": "Iâ€™ve been working on a project to help non-lawyers better understand legal documents without having to read them in full. Using a Retrieval-Augmented Generation (RAG) approach, I developed a tool that allows users to ask questions about live terms of service or policies (e.g., Apple, Figma) and receive natural-language answers.\n\nThe aim isnâ€™t to replace legal advice but to see if AI can make legal content more accessible to everyday users.\n\nIt uses a simple RAG stack:\n\n* **Scraper:**Â Browserless\n* **Indexing/Retrieval:**Â [Ducky.ai](http://Ducky.ai)\n* **Generation:**Â OpenAI\n* **Frontend:**Â Next.js\n\nIndexed content is pulled and chunked, retrieved with Ducky, and passed to OpenAI with context to answer naturally.\n\nIâ€™m interested in hearing thoughts from you all on the potential and limitations of such tools. I documented the development process and [some reflections in this blog post](https://ducky.ai/blog/fineprint-how-to-build-a-legal-document-chat-with-ducky-ai?utm_source=reddit-bigdata&amp;utm_medium=community&amp;utm_campaign=technical-use_case)\n\nWould appreciate any feedback or insights!",
    "author": "superconductiveKyle",
    "timestamp": "2025-05-13T08:17:43",
    "url": "https://reddit.com/r/bigdata/comments/1klolbk/enhancing_legal_document_comprehension_using_rag/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1klex8j",
    "title": "Best Way to Structure ETL Flows in NiFi",
    "content": "Iâ€™m building ETL flows in Apache NiFi to move data from a MySQL database to a cloud data warehouse - Snowflake.\n\nWhatâ€™s a better way to structure the flow? Should I separate the Extract, Transform, and Load stages into different process groups, or should I create one end-to-end process group per table?",
    "author": "GreenMobile6323",
    "timestamp": "2025-05-12T23:18:09",
    "url": "https://reddit.com/r/bigdata/comments/1klex8j/best_way_to_structure_etl_flows_in_nifi/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kjspea",
    "title": "Mastering Snowflake Performance: 10 Queries Every Engineer Should Know",
    "content": "",
    "author": "Neat-Resort9968",
    "timestamp": "2025-05-10T21:31:11",
    "url": "https://reddit.com/r/bigdata/comments/1kjspea/mastering_snowflake_performance_10_queries_every/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kjd1cz",
    "title": "Request for Google Form Filling (Questionnaire)",
    "content": "Dear Participant,  \nWe are conducting a research study on enhancing cloud security to prevent data leaks, as part of our academic project at Catholic University in Erbil. Your insights and experiences are highly valuable and will contribute significantly to our understanding of current cloud security practices. The questionnaire will only take a few minutes to complete, and all responses will remain anonymous and confidential. We kindly ask for your participation by filling out the form linked below. Your support is greatly appreciated!\n\n\n\n[https://docs.google.com/forms/d/e/1FAIpQLSdN7Zs9KVxFbwb4gxnS-7bijiu7dmH9bLRYv3jT0yXcdApsrw/viewform?usp=header](https://docs.google.com/forms/d/e/1FAIpQLSdN7Zs9KVxFbwb4gxnS-7bijiu7dmH9bLRYv3jT0yXcdApsrw/viewform?usp=header)",
    "author": "Alternative_Coat554",
    "timestamp": "2025-05-10T08:34:40",
    "url": "https://reddit.com/r/bigdata/comments/1kjd1cz/request_for_google_form_filling_questionnaire/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kih92h",
    "title": "How Do You Handle Massive Datasets? Whatâ€™s Your Stack and How Do You Scale?",
    "content": "Hi everyone!  \nIâ€™m a product manager working with a team thatâ€™s recently started dealing with datasets in the tens of millions of rows-think user events, product analytics, and customer feedback. Our current tooling is starting to buckle under the load, especially when it comes to real-time dashboards and ad-hoc analyses.\n\nIâ€™m curious:\n\n* **Whatâ€™s your current stack for storing, processing, and analyzing large datasets?**\n* **How do you handle scaling as your data grows?**\n* **Any tools or practices youâ€™ve found especially effective (or surprisingly expensive)?**\n* **Tips for keeping costs under control without sacrificing performance?**\n\n",
    "author": "Ambrus2000",
    "timestamp": "2025-05-09T05:28:18",
    "url": "https://reddit.com/r/bigdata/comments/1kih92h/how_do_you_handle_massive_datasets_whats_your/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kis8uu",
    "title": "How We Handle Billion-Row ClickHouse Inserts With UUID Range Bucketing",
    "content": "",
    "author": "JoeKarlssonCQ",
    "timestamp": "2025-05-09T13:19:41",
    "url": "https://reddit.com/r/bigdata/comments/1kis8uu/how_we_handle_billionrow_clickhouse_inserts_with/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ki2kpn",
    "title": "All the ways to capture changes in Postgres",
    "content": "",
    "author": "goldmanthisis",
    "timestamp": "2025-05-08T15:02:42",
    "url": "https://reddit.com/r/bigdata/comments/1ki2kpn/all_the_ways_to_capture_changes_in_postgres/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1khymh3",
    "title": "WEBINAR Linux Storage Server and NFS Advancements: Creating a High-Performance Standard for AI Workloads",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-05-08T12:17:43",
    "url": "https://reddit.com/r/bigdata/comments/1khymh3/webinar_linux_storage_server_and_nfs_advancements/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1khtnri",
    "title": "We've shipped a batch of updates focused on one thing: saving time. From support for Tableau Custom Views and email tracking to a new AI insights interface, hereâ€™s whatâ€™s new this month.",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-05-08T08:58:16",
    "url": "https://reddit.com/r/bigdata/comments/1khtnri/weve_shipped_a_batch_of_updates_focused_on_one/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kht36w",
    "title": "Apache Fury Serialization Framework 0.10.2 Released: Chunk-based map Serialization to reduce payload size by up to 2X",
    "content": "",
    "author": "Shawn-Yang25",
    "timestamp": "2025-05-08T08:34:37",
    "url": "https://reddit.com/r/bigdata/comments/1kht36w/apache_fury_serialization_framework_0102_released/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1khrl88",
    "title": "backtesting predictive market data",
    "content": "My company has some Alt data that we think can be used by investors to predict company movements. We need a proof of concept to go to market I belive, can anyone recomend a reputible company that can provide such a thing - ie a company that can analyse our data and see if it does correlate with a companies value and proivide us third party validation of the predicitve capabilities as such. Many thanks for any help and advice.",
    "author": "ZebraM-3572",
    "timestamp": "2025-05-08T07:32:34",
    "url": "https://reddit.com/r/bigdata/comments/1khrl88/backtesting_predictive_market_data/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1khnimz",
    "title": "Go-to method for building reusable flow logic in NiFi",
    "content": "Iâ€™ve been working on building out some data flows and am trying to figure out the best way to make them more reusable across different projects. I want to avoid duplicating work and keep things modular, so Iâ€™m curious: Whatâ€™s your go-to method for building reusable flow logic in NiFi?",
    "author": "GreenMobile6323",
    "timestamp": "2025-05-08T04:16:02",
    "url": "https://reddit.com/r/bigdata/comments/1khnimz/goto_method_for_building_reusable_flow_logic_in/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1khfjmb",
    "title": "Best Big Data Courses on Udemy to learn in 2025",
    "content": "",
    "author": "[deleted]",
    "timestamp": "2025-05-07T19:35:39",
    "url": "https://reddit.com/r/bigdata/comments/1khfjmb/best_big_data_courses_on_udemy_to_learn_in_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kf4xxu",
    "title": "If you love Spark but hate PyDeequ â€“ check out SparkDQ (early but promising)",
    "content": "",
    "author": "GeneBackground4270",
    "timestamp": "2025-05-04T23:31:12",
    "url": "https://reddit.com/r/bigdata/comments/1kf4xxu/if_you_love_spark_but_hate_pydeequ_check_out/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kd108b",
    "title": "Supercharge your R workflows with DuckDB",
    "content": "",
    "author": "Capable-Mall-2067",
    "timestamp": "2025-05-02T06:30:27",
    "url": "https://reddit.com/r/bigdata/comments/1kd108b/supercharge_your_r_workflows_with_duckdb/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kd0cle",
    "title": "Power BI With Breakthrough AI",
    "content": "With AI-driven features- sentiment analysis, key phrase extraction, and image recognition- Power BI enables data specialists to visualize complex data, automate reporting, and enhance decision-making with precision. Whether you're a data analyst, business leader, or tech enthusiast, AI-powered Power BI empowers you to turn raw data into actionable intelligenceâ€”all with a few clicks!\n\nğŸ“Š Ready to revolutionize your analytics? Unlock the future of data visualization! ğŸ”¥\n\nhttps://preview.redd.it/jb3lhx8q9dye1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=35796a81cb31f794dac6dc03c0058063116bcd66\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-05-02T06:00:12",
    "url": "https://reddit.com/r/bigdata/comments/1kd0cle/power_bi_with_breakthrough_ai/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kc8ksu",
    "title": "DSIâ€™s Certified Data Science Professional",
    "content": "With a self-paced learning format, industry-relevant global curriculum, and expert guidance from the USDSIÂ® Data Science Advisory Board, Certified Data Science Professional (CDSPâ„¢) certification ensures you stay ahead in data science. Whether you're a fresh graduate or industry beginners, CDSPâ„¢ empowers you with the breakthrough knowledge and expertise to analyze complex data, build predictive models, and drive data-driven decisions.\n\nJoin the global workforce of millions data science professionals and take your career to newer heights with CDSPâ„¢. \n\nhttps://reddit.com/link/1kc8ksu/video/mr3wzz6l86ye1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-05-01T06:21:12",
    "url": "https://reddit.com/r/bigdata/comments/1kc8ksu/dsis_certified_data_science_professional/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kc2in5",
    "title": "Is AI starting to replace parts of the data engineering workflow?",
    "content": "AI is now being used to handle things like pipeline generation, data transformation, and anomaly detection. Some of this feels like early automation, but itâ€™s moving fast.\nAre we looking at full on role changes, or just smarter tooling?",
    "author": "PuzzleheadedYou4992",
    "timestamp": "2025-04-30T23:57:27",
    "url": "https://reddit.com/r/bigdata/comments/1kc2in5/is_ai_starting_to_replace_parts_of_the_data/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kbugli",
    "title": "Monthly Business Reviews (MBRs) got you and your team stressed?",
    "content": "ğŸ“… Monthly Business Reviews (MBRs) got you and your team stressed?\n\nYouâ€™re not alone, but there is a better way.\n\nCompanies like Zillow, SoFi, and TripAdvisor useÂ [Rollstack](https://www.rollstack.com/ai-report-generation-demo)Â to automate data-driven PowerPoint and Google Slides reports, enabling their teams to focus on sharing insights rather than screenshots.Â \n\n* Pull directly from your BI dashboards (Tableau, Power BI, Looker, Metabase &amp; Google Sheets) into your report PowerPoints and docs. \n* Deliver MBRs, QBRs, and EBRs in seconds (not days)\n* Error-free, up-to-date reporting sent to your inbox or shared drive\n\nSee how it works and schedule a demo atÂ [www.Rollstack.com](http://www.rollstack.com/).",
    "author": "Rollstack",
    "timestamp": "2025-04-30T16:22:30",
    "url": "https://reddit.com/r/bigdata/comments/1kbugli/monthly_business_reviews_mbrs_got_you_and_your/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kbqh9n",
    "title": "Blog: Whatâ€™s New in Apache Iceberg Format Version 3?",
    "content": "",
    "author": "AMDataLake",
    "timestamp": "2025-04-30T13:27:00",
    "url": "https://reddit.com/r/bigdata/comments/1kbqh9n/blog_whats_new_in_apache_iceberg_format_version_3/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kber4m",
    "title": "Migration from Legacy System to Open-Source",
    "content": "Currently, my organization uses a licensed tool from a specific vendor for ETL needs. We are paying a hefty amount for licensing fees and are not receiving support on time. As the tool is completely managed by the vendor, we are not able to make any modifications independently. \n\nCan you suggest a few open-source options? Also, I'm looking for round-the-clock support for the same tool. ",
    "author": "GreenMobile6323",
    "timestamp": "2025-04-30T05:04:09",
    "url": "https://reddit.com/r/bigdata/comments/1kber4m/migration_from_legacy_system_to_opensource/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kbjpu0",
    "title": "Build Your First AI Agent with Google ADK and Teradata (Part 1)",
    "content": "",
    "author": "JanethL",
    "timestamp": "2025-04-30T08:45:17",
    "url": "https://reddit.com/r/bigdata/comments/1kbjpu0/build_your_first_ai_agent_with_google_adk_and/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1kak925",
    "title": "Quick Tips For Easy Unit Testing In Python | Infographic",
    "content": "Know what Python Code is and how well you can deduce [Python frameworks](https://www.usdsi.org/data-science-insights/quick-tips-for-easy-unit-testing-in-python) with quick steps. Deploy seamless unit testing as a top data scientist with sheer skills!\n\nhttps://preview.redd.it/0qn4l4mbzqxe1.jpg?width=1500&amp;format=pjpg&amp;auto=webp&amp;s=78f953ff33749ce8b4d03f1b8c3131d007c6d313\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-29T03:02:30",
    "url": "https://reddit.com/r/bigdata/comments/1kak925/quick_tips_for_easy_unit_testing_in_python/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ka6y44",
    "title": "Apache Iceberg Clustering: Technical Blog",
    "content": "",
    "author": "AMDataLake",
    "timestamp": "2025-04-28T14:20:32",
    "url": "https://reddit.com/r/bigdata/comments/1ka6y44/apache_iceberg_clustering_technical_blog/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ka0y9j",
    "title": "SQL Commands | DDL, DQL, DML, DCL and TCL Commands - JV Codes 2025",
    "content": "Mastery ofÂ [SQL commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/)Â isÂ essential for someone who deals with SQL databases.Â [SQL](https://jvcodes.com/what-is-sql/)Â provides an easy system to create, modify, and arrange data. This article uses straightforward language to explain [SQL commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/)â€”DDL, DQL, DML, DCL, andÂ [TCL commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/).\n\nSQL serves as one of the fundamental subjects that beginners frequently ask about its nature. SQL stands forÂ **Structured Query Language**. The programming system is a database communication protocol instead of a complete programming language.\n\n# What Are SQL Commands?\n\nA database connects throughÂ [SQL commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/), which transmit instructions to it. The system enables users to build database tables, input data and changes, and delete existing data.\n\nA database can be accessed through fiveÂ [primary SQL commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/).\n\n* [DDL Commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/) (Data Definition Language)\n* [DQL Commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/) (Data Query Language)\n* [DML Commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/) (Data Manipulation Language)\n* [DCL Commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/) (Data Control Language)\n* [TCL Commands](https://jvcodes.com/sql-ddl-dql-dml-dcl-tcl-commands/) (Transaction Control Language)",
    "author": "shokatjaved",
    "timestamp": "2025-04-28T10:14:52",
    "url": "https://reddit.com/r/bigdata/comments/1ka0y9j/sql_commands_ddl_dql_dml_dcl_and_tcl_commands_jv/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k9lapc",
    "title": "Big Data &amp; Sustainable AI: Exploring Solidus AI Tech (AITECH) and its Eco-Friendly HPC",
    "content": "r/solidusaitech\n\nHello Big Data community, this is my second time posting here and I'd like to take this opportunity to thank the community for its support. I've been researching an HPC Data Center that has several interesting points; which is useful information for Big Data. It's about r/solidusaitech Solidus AI Tech, a company focused on providing decentralized AI and sustainable HPC solutions, and also offers a platform with a Compute Marketplace, AI Marketplace, and AITECH Pad.\n\nAmong the points that I believe may be of interest to the Big Data community, the following stand out:\n\nAn eco-friendly HPC infrastructure located in Europe, focused on improving energy usage. This is important due to the high computational demand for AI solutions and effective access to large amounts of data.\n\nThe launch of Agent Forge during Q2 2025 sounds quite interesting; its essence is the creation of AI Agents without code, with the power to automate complex tasks. This is definitely a very useful point for analyzing data and other fields linked to Big Data.\n\nCompute Marketplace (Q2 2025) They also plan to launch a marketplace for accessing compute resources, which could be an option to consider for those looking for processing power for Big Data tasks.\n\nApart from this, they have announced strategic partnerships with companies like SambaNova Systems, a company that is inventing smarter and faster ways to use Artificial Intelligence in the business world. AITECH is also exploring use cases in Metaverse/Gaming. These sectors require large amounts of data.\n\nI would like to know your opinions on this type of platform that combines decentralized AI with sustainable HPC. Do you see potential in this approach to address the computational needs of Big Data and AI?\n\nPublication for informational purposes, please do your own research (DYOR). \n",
    "author": "VictoriaTelos",
    "timestamp": "2025-04-27T19:43:25",
    "url": "https://reddit.com/r/bigdata/comments/1k9lapc/big_data_sustainable_ai_exploring_solidus_ai_tech/",
    "score": 11,
    "num_comments": 0,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k9vgkh",
    "title": "Unlock B2B Gold: How to Target Companies Post-Funding with This Sneaky Toolâ€”Free Access to Decision Makers!",
    "content": "",
    "author": "Defiant-End-2292",
    "timestamp": "2025-04-28T06:24:28",
    "url": "https://reddit.com/r/bigdata/comments/1k9vgkh/unlock_b2b_gold_how_to_target_companies/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k9oomq",
    "title": "Most Rewarding Data Science Jobs for 2025",
    "content": "Certified data scientists can earn over $200k in the US. Are you still thinking of a career in data science? \n\nDownload the latest [USDSIÂ® Data Science Professionalâ€™s Salary Factsheet 2025](https://www.usdsi.org/data-science-insights/resources/salary-guide-2025-for-data-science-professionals) and explore:\n\nTop data science trends\n\nEmerging jobs in the industry\n\nProfessionalâ€™s salary across roles and industries, and more.\n\nUpdate your knowledge about the latest data science facts now. Click here.\n\nhttps://reddit.com/link/1k9oomq/video/rb6qmqproixe1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-27T23:09:04",
    "url": "https://reddit.com/r/bigdata/comments/1k9oomq/most_rewarding_data_science_jobs_for_2025/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k96m6f",
    "title": "What is SQL? How to Write Clean and Correct SQL Commands for Beginners - JV Codes 2025",
    "content": "",
    "author": "shokatjaved",
    "timestamp": "2025-04-27T08:29:47",
    "url": "https://reddit.com/r/bigdata/comments/1k96m6f/what_is_sql_how_to_write_clean_and_correct_sql/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k7rvq8",
    "title": "Introducing the Salesforce Tableau sub reddit, your destination for all things Salesforce &amp; Tableau. Please join and contribute.",
    "content": "",
    "author": "DeeperThanCraterLake",
    "timestamp": "2025-04-25T11:16:48",
    "url": "https://reddit.com/r/bigdata/comments/1k7rvq8/introducing_the_salesforce_tableau_sub_reddit/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k7khhr",
    "title": "Deep Learning Frameworks to Power your Projects",
    "content": "[Deep learning frameworks](https://www.usdsi.org/data-science-insights/resources/the-deep-learning-frameworks-riot-2025) like Pytorch, TensorFlow, and Keras are transforming deep learning models, making them more accurate and efficient. Which one is better, and what are their pros and cons? Most importantly, how are they revolutionizing model development in 2025?\n\nhttps://preview.redd.it/zp5p75hmczwe1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=f0f442a241abbee0d5ba72b19b8fb390134ea502\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-25T06:07:03",
    "url": "https://reddit.com/r/bigdata/comments/1k7khhr/deep_learning_frameworks_to_power_your_projects/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k6zgur",
    "title": "I need help please",
    "content": "Hi, \n\nI'm an MBA fresher currently working in a founderâ€™s office role at a startup that owns a news app and a short-video (reels) app.\n\n\n\n Iâ€™ve been tasked with researching how ByteDance leverages alternate data from TikTok and its own news app called toutiao to offer financial products like microloans, and then explore how we might replicate a similar model using our own user data. \n\n\n\nI would really appreciate some help as in guidance as to how to go about tackling this as currently i am unable to find anything on the internet.",
    "author": "Stormbreaker5275",
    "timestamp": "2025-04-24T11:24:29",
    "url": "https://reddit.com/r/bigdata/comments/1k6zgur/i_need_help_please/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k6o4b9",
    "title": "Anyone have a clean setup for staging data changes before pushing to prod lakes?",
    "content": "Weâ€™re running into issues with testing and rollback across our data lake.\nIn software, youâ€™d never push code to prod without version control and CI checksâ€”so why is that still the norm in data?\n\nCurious what others are doing to stage/test data changes before they go live.\nAre you using isolated environments? Separate S3 buckets? Some kind of custom validation layer?\nWhat works? Whatâ€™s been a nightmare?",
    "author": "is669",
    "timestamp": "2025-04-24T02:38:04",
    "url": "https://reddit.com/r/bigdata/comments/1k6o4b9/anyone_have_a_clean_setup_for_staging_data/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k6eu8g",
    "title": "How SoFi Automates PowerPoint Reports with Tableau &amp; Rollstack | Tableau Conference 2025 AI Session",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-04-23T17:24:08",
    "url": "https://reddit.com/r/bigdata/comments/1k6eu8g/how_sofi_automates_powerpoint_reports_with/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k5x106",
    "title": "Call for Papers â€“ IEEE ISADS 2025",
    "content": "â€œThe 17th IEEE International Symposium on Autonomous Decentralized Systemsâ€\n\n\n\nJuly 21â€“24, 2025 | Tucson, Arizona, United States\n\n\n\nIEEE ISADS 2025 invites you to be part of an influential symposium focused on the design, development, and deployment of autonomous and decentralized systems. As part of the IEEE CISOSE 2025 Congress, ISADS provides a vibrant platform for researchers and professionals to explore resilient, adaptive, and intelligent system architectures for today's dynamic and distributed environments.\n\n\n\nWe invite high-quality research contributions on (but not limited to):  \n\n\\- Autonomous Decentralized System Architecture and Design  \n\n\\- Distributed AI and Intelligent Edge Computing  \n\n\\- Blockchain, Smart Contracts, and Trust Management  \n\n\\- Resilience and Fault Tolerance in Decentralized Systems  \n\n\\- Autonomous System Applications in IoT, Cyber-Physical Systems, and Robotics  \n\n\\- Communication Protocols and Coordination Mechanisms  \n\n\\- Real-Time and Embedded Autonomous Systems  \n\n\\- Industry Case Studies and Deployment Experiences  \n\n\n\nSubmit your papers via: [https://easychair.org/my/conference?conf=isads2025](https://easychair.org/my/conference?conf=isads2025)\n\n\n\nFor more details, visit: [https://conf.researchr.org/track/cisose-2025/cisose-2025-ieee-isads-2025](https://conf.researchr.org/track/cisose-2025/cisose-2025-ieee-isads-2025)\n\n\n\nJoin us in shaping the future of autonomous decentralized systems and contribute to innovations that empower next-generation technologies!\n\n\n\nBest Regards,  \n\nSteering Committee  \n\nCISOSE 2025",
    "author": "Ok-Chocolate5088",
    "timestamp": "2025-04-23T04:42:37",
    "url": "https://reddit.com/r/bigdata/comments/1k5x106/call_for_papers_ieee_isads_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k5do8p",
    "title": "Looking for Research Participants: Survey + Interview (w/ compensation)",
    "content": "Hi All,\n\nI'm a PhD candidate conducting research for my dissertation on how data science practitioners use open-source AI platforms (e.g., Kaggle, Hugging Face). This project aims to understand how practitioners interface between value systems on these platforms by observing work practices and processes.\n\nI'm looking for participants of at least **18 years of age** with **at least 3 years** of professional experience to:\n\n1. Take a **5-min initial survey**\n2. Join me in a virtual **75-90 minute virtual work session** to discuss a project of your choice that demonstrates the use of Kaggle or Hugging Face.\n\nYou will be compensated ($50 VISA gift card) for your time and effort.\n\nSurvey can be accessed here: [https://usc.qualtrics.com/jfe/form/SV\\_8iYCIuAdvOP7HIG](https://usc.qualtrics.com/jfe/form/SV_8iYCIuAdvOP7HIG)\n\n  \nPlease reach out with any questions. Thank you for your support in this effort!",
    "author": "alex_alv_rojas",
    "timestamp": "2025-04-22T11:29:16",
    "url": "https://reddit.com/r/bigdata/comments/1k5do8p/looking_for_research_participants_survey/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k5c2e2",
    "title": "Tableau to PowerPoint in 50 Seconds (YouTube)",
    "content": "Automate PowerPoint reports with Tableau and Rollstack. Visit [www.Rollstack.com](http://www.Rollstack.com) to learn more. ",
    "author": "Rollstack",
    "timestamp": "2025-04-22T10:25:28",
    "url": "https://reddit.com/r/bigdata/comments/1k5c2e2/tableau_to_powerpoint_in_50_seconds_youtube/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k58613",
    "title": "BigDataWire People to Watch 2025: Hammerspace's David Flynn",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-04-22T07:48:47",
    "url": "https://reddit.com/r/bigdata/comments/1k58613/bigdatawire_people_to_watch_2025_hammerspaces/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k50twr",
    "title": "Crack the Code: How Tracking Startup Funding Led to a $10K Boomâ€”Wanna Know the Tool Behind It?",
    "content": "",
    "author": "Better_Reward486",
    "timestamp": "2025-04-22T00:53:45",
    "url": "https://reddit.com/r/bigdata/comments/1k50twr/crack_the_code_how_tracking_startup_funding_led/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k4k978",
    "title": "Streaming 4TB/month of Cloud Data into ClickHouse: What We Learned",
    "content": "",
    "author": "JoeKarlssonCQ",
    "timestamp": "2025-04-21T11:08:58",
    "url": "https://reddit.com/r/bigdata/comments/1k4k978/streaming_4tbmonth_of_cloud_data_into_clickhouse/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k2tjwg",
    "title": "For Anyone seeking to Access \"Top-Rated Data Science Books\" for Starting Data Careers\"!",
    "content": "Here is a good resource to ExploreÂ [Amazonâ€™s Best-Rated Data Science Books](https://amzn.to/43FLwwQ) and in one place.\n\nThere are resources on several data science topics such as: \n\nBig data, data science, data analytics, health informatics, cybersecurity, machine learning, business analysis, SQL, Python and more.  \n\nHope you find it useful!",
    "author": "Sea-Concept1733",
    "timestamp": "2025-04-19T03:49:57",
    "url": "https://reddit.com/r/bigdata/comments/1k2tjwg/for_anyone_seeking_to_access_toprated_data/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k2on37",
    "title": "Certified Data Science Professional (CDSPâ„¢)",
    "content": "Tailored for undergraduates, recent graduates, and early-career professionals, the [CDSPâ„¢ certification](https://www.usdsi.org/data-science-certifications/certified-data-science-professional) provides a structured pathway into the data science field. No prior work experience makes it easy to transition into data science roles. Want to know enrolment details and more?\n\nhttps://preview.redd.it/9o1dcieh4qve1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=18f66a17caf097e19b1dd66867e08b90ddc9360e\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-18T22:01:15",
    "url": "https://reddit.com/r/bigdata/comments/1k2on37/certified_data_science_professional_cdsp/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k17qtj",
    "title": "CERTIFIED DATA SCIENCE PROFESSIONAL (CDSPâ„¢)",
    "content": "Begin your journey as a Certified Data Scientist with CDSP- pioneering courseware for Data Science Beginners. From industry-centric skillsets, and global recognition, to a holistic blend of practical nuances- CDSP is your go-to Beginner Certification in Data Science. \n\nhttps://preview.redd.it/6si7bbgjucve1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=1a31d44c8fe233658d8318d3ac6ddc44bbb8ce33\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-17T01:22:18",
    "url": "https://reddit.com/r/bigdata/comments/1k17qtj/certified_data_science_professional_cdsp/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k15rfp",
    "title": "Cracking the Code: How Targeting Newly Funded Startups Boosted My Sales by $10K (and the tool that reveals it all!)",
    "content": "",
    "author": "Intrepid_Raccoon7222",
    "timestamp": "2025-04-16T23:00:25",
    "url": "https://reddit.com/r/bigdata/comments/1k15rfp/cracking_the_code_how_targeting_newly_funded/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k0zklh",
    "title": "Uncover the Power Move: How Recently Funded Startups Become Your Secret B2B Goldmine. Want access to the decision-makers? Let's chat!",
    "content": "",
    "author": "No_Depth_8865",
    "timestamp": "2025-04-16T17:13:06",
    "url": "https://reddit.com/r/bigdata/comments/1k0zklh/uncover_the_power_move_how_recently_funded/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k0rksi",
    "title": "Whatâ€™s the most unexpectedly useful thing youâ€™ve used AI for?",
    "content": "",
    "author": "dofthings",
    "timestamp": "2025-04-16T11:25:13",
    "url": "https://reddit.com/r/bigdata/comments/1k0rksi/whats_the_most_unexpectedly_useful_thing_youve/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1k0kzvi",
    "title": "Strategic Investors Back Hammerspace as New Standard for AI Data Performance",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-04-16T06:54:15",
    "url": "https://reddit.com/r/bigdata/comments/1k0kzvi/strategic_investors_back_hammerspace_as_new/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jzkrqm",
    "title": "Download Free ebook for Bigdata Interview Preparation Guide (1000+ questions with answers) Programming, Scenario-Based, Fundamentals, Performance Tunning",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-04-14T23:12:19",
    "url": "https://reddit.com/r/bigdata/comments/1jzkrqm/download_free_ebook_for_bigdata_interview/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jzexet",
    "title": "AI data analyst LLM",
    "content": "Hey everyone! Weâ€™ve been working on a lightweight version of our data platform (originally built for enterprise teams) and weâ€™re excited to open up a **private beta** for something new: **Seda**.\n\nSeda is a stripped-down, no-frills version of our original product, Secoda â€” but it still runs on the same powerful engine: custom embeddings, SQL lineage parsing, and a RAG system under the hood. The big difference? Itâ€™s designed to be simple, fast, and accessible for *anyone* with a data source â€” not just big companies.\n\n# What you can do with Seda:\n\n* **Ask questions in natural language** and get real answers from your data (Seda finds the right data, runs the query, and returns the result).\n* **Write and fix SQL automatically**, just by asking.\n* **Generate visualizations** on the fly â€“ no need for a separate BI tool.\n* **Trace data lineage** across tables, models, and dashboards.\n* **Auto-document your data** â€“ build business glossaries, table docs, and metric definitions instantly.\n\nBehind the scenes, Seda is powered by a system of **specialized data agents**:\n\n* **Lineage Agent**: Parses SQL to create full column- and table-level lineage.\n* **SQL Agent**: Understands your schema and dialect, and generates queries that match your naming conventions.\n* **Visualization Agent**: Picks the best charts for your data and question.\n* **Search Agent**: Searches across tables, docs, models, and more to find exactly what you need.\n\nThe agents work together through a smart router that figures out which one (or combination) should respond to your request.\n\n# Hereâ€™s a quick demo:\n\nğŸ“¹ [Watch it in action](https://www.youtube.com/watch?v=Ny_3HUbt9zw)\n\n# Want to try it?\n\nğŸ“ [Sign up here for early access](https://www.seda.ai/)\n\nWe currently support:  \n**Postgres, Snowflake, Redshift, BigQuery, dbt (cloud &amp; core), Confluence, Google Drive, and MySQL.**\n\nWould love to hear what you think or answer any questions!",
    "author": "secodaHQ",
    "timestamp": "2025-04-14T17:48:31",
    "url": "https://reddit.com/r/bigdata/comments/1jzexet/ai_data_analyst_llm/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jytfua",
    "title": "Transforming Business with Data Visualization Effectively| Infographic",
    "content": "Check out our detailed [infographic on data visualization](https://www.usdsi.org/data-science-insights/transforming-business-with-data-visualization-effectively) to understand its importance in businesses, different data visualization techniques, and best practices.\n\nhttps://preview.redd.it/dbffcphmarue1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=e706868ca8f987aac8c9dcd4df6f30c80729bffd\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-14T00:53:18",
    "url": "https://reddit.com/r/bigdata/comments/1jytfua/transforming_business_with_data_visualization/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jyi887",
    "title": "Bid data learning for backend dev",
    "content": "Hi! As a backend dev need roadmap on learning big data processing. Things that I need to go through before starting with this job role that works with big data processing. Hiring was language and skill set agnostic. System\nDesign was asked in all the rounds. \n",
    "author": "ZealousidealCrew94",
    "timestamp": "2025-04-13T14:13:53",
    "url": "https://reddit.com/r/bigdata/comments/1jyi887/bid_data_learning_for_backend_dev/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jyg8lb",
    "title": "Self-Healing Data Quality in DBT â€” Without Any Extra Tools",
    "content": "I just published a practical breakdown of a method I call Observe &amp; Fix â€” a simple way to manage data quality in DBT without breaking your pipelines or relying on external tools.  \nItâ€™s a self-healing pattern that works entirely within DBT using native tests, macros, and logic â€” and itâ€™s ideal for fixable issues like duplicates or nulls.\n\nIncludes examples, YAML configs, macros, and even when to alert via Elementary.\n\nWould love feedback or to hear how others are handling this kind of pattern.\n\n[Read the full post here](https://medium.com/@baruchjacob/self-healing-pipelines-with-dbt-the-observe-fix-method-9d6b2da4eae3)",
    "author": "jb_nb",
    "timestamp": "2025-04-13T12:46:30",
    "url": "https://reddit.com/r/bigdata/comments/1jyg8lb/selfhealing_data_quality_in_dbt_without_any_extra/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jxw4b2",
    "title": "Best Big Data Courses on Udemy to learn in 2025",
    "content": "",
    "author": "[deleted]",
    "timestamp": "2025-04-12T18:04:49",
    "url": "https://reddit.com/r/bigdata/comments/1jxw4b2/best_big_data_courses_on_udemy_to_learn_in_2025/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jxo2jb",
    "title": "Hi everyone! I'm conducting a university research survey on commonly used Big Data tools among students and professionals. If you work in data or tech, Iâ€™d really appreciate your input â€” it only takes 3 minutes! Thank you",
    "content": "[https://docs.google.com/forms/d/e/1FAIpQLScXK6CnNUHGR9UIEHUhX83kHoZGYuSunRE0foZgnew81nxxLg/viewform?usp=header](https://docs.google.com/forms/d/e/1FAIpQLScXK6CnNUHGR9UIEHUhX83kHoZGYuSunRE0foZgnew81nxxLg/viewform?usp=header)",
    "author": "chiki_rukis",
    "timestamp": "2025-04-12T11:34:28",
    "url": "https://reddit.com/r/bigdata/comments/1jxo2jb/hi_everyone_im_conducting_a_university_research/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jxbexz",
    "title": "Data Science Trends Alert 2025",
    "content": "Transform decision-making with a data-driven approach. Are you set to stir the future of data with core trends and emerging techniques in place? Make big moves with informed [data science trends](https://www.usdsi.org/data-science-insights/the-future-of-data-science-emerging-technologies-and-trends) learnt here.\n\nhttps://preview.redd.it/nk7j7fx0rcue1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=cf8e3231cdd3c0054c225bf2ea15487ffb9f0cda\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-11T23:58:35",
    "url": "https://reddit.com/r/bigdata/comments/1jxbexz/data_science_trends_alert_2025/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jx3zma",
    "title": "Automate your slide decks and reports with Rollstack",
    "content": "Rollstack connects Tableau, Power BI, Looker, Metabase, and Google Sheets, to PowerPoint and Google Slides for automated recurring reports. \n\nStop copying and pasting to build reports. \n\nBook a demo and get started at [www.Rollstack.com](http://www.Rollstack.com)",
    "author": "Rollstack",
    "timestamp": "2025-04-11T16:42:06",
    "url": "https://reddit.com/r/bigdata/comments/1jx3zma/automate_your_slide_decks_and_reports_with/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jwh0hq",
    "title": "Apache Spark SQL: Writing Efficient Queries for Big Data Processing",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-04-10T20:57:28",
    "url": "https://reddit.com/r/bigdata/comments/1jwh0hq/apache_spark_sql_writing_efficient_queries_for/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jvfo90",
    "title": "[LinkedIn Post] Meet Me at the Tableau Conference next week. Automate data driven slide decks and docs!",
    "content": "",
    "author": "askoshbetter",
    "timestamp": "2025-04-09T13:18:17",
    "url": "https://reddit.com/r/bigdata/comments/1jvfo90/linkedin_post_meet_me_at_the_tableau_conference/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1juzrce",
    "title": "Data Startups- VC and Liquidity Wins",
    "content": "Data science startups get a double boost! Venture Capital fuels innovation, while secondary markets provide liquidity, implying accelerated growth. Understand the evolution of startup funding and how it empowers the [AI and Data Science Startups](https://www.usdsi.org/data-science-insights/how-venture-capital-and-secondary-markets-boost-data-science-startups).\n\n\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-09T00:06:33",
    "url": "https://reddit.com/r/bigdata/comments/1juzrce/data_startups_vc_and_liquidity_wins/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ju54pw",
    "title": "Data Architecture Complexity",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-04-07T21:22:40",
    "url": "https://reddit.com/r/bigdata/comments/1ju54pw/data_architecture_complexity/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jt4msg",
    "title": "Data lakehouse related research",
    "content": "Hello,  \nI am currently working on my master degree thesis on topic \"processing and storing of big data\". It is very general topic because it purpose was to give me elasticity in choosing what i want to work on. I was thinking of building data lakehouse in databricks. I will be working on kinda small structured dataset (10 GB only) despite having Big Data in title as I would have to spend my money on this, but still context of thesis and tools will be big data related - supervisor said it is okay and this small dataset will be treated as benchmark.\n\nThe problem is that there is requirement for thesis on my universities that it has to have measurable research factor ex. for the topic of detection of cancer for lungs' images different models accuracy would be compared to find the best model. As I am beginner in data engineering I am kinda lacking idea what would work as this research factor in my project. Do you have any ideas what can I examine/explore in the area of this project that would cut out for this requirement?",
    "author": "Wikar",
    "timestamp": "2025-04-06T14:11:20",
    "url": "https://reddit.com/r/bigdata/comments/1jt4msg/data_lakehouse_related_research/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1js4hrr",
    "title": "Machine learning breakthrough in data science",
    "content": "From predictive data insights to real-time learning, Machine learning is pushing the limits in Data Science. Explore the implications of this strategic skill for data science professionals, researchers and its impact on the future of technology.\n\nhttps://reddit.com/link/1js4hrr/video/003zf717z0te1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-05T07:19:42",
    "url": "https://reddit.com/r/bigdata/comments/1js4hrr/machine_learning_breakthrough_in_data_science/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jrurda",
    "title": "Running Apache Druid on Windows Using Docker Desktop (Hands On)",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-04-04T20:53:26",
    "url": "https://reddit.com/r/bigdata/comments/1jrurda/running_apache_druid_on_windows_using_docker/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jrbrb4",
    "title": "Global Recognition",
    "content": "Why choose [USDSIÂ®s data science certifications](https://www.usdsi.org/data-science-certifications)? As the global industry demand rises, it presses the need for qualified data science experts. Swipe through to explore the key benefits that can accelerate your career in 2025!\n\nhttps://reddit.com/link/1jrbrb4/video/6xpaqt27ktse1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-04T06:22:43",
    "url": "https://reddit.com/r/bigdata/comments/1jrbrb4/global_recognition/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jrbgk2",
    "title": "Optimizing Large-Scale Retrieval: An Open-Source Approach",
    "content": "Hey everyone, Iâ€™ve been exploring the challenges of working with large-scale data in **Retrieval-Augmented Generation (RAG)**, and one issue that keeps coming up is balancing **speed**, **efficiency**, and **scalability**, especially when dealing with massive datasets. So, **the startup I work for decided to tackle this** head-on by developing an **open-source RAG framework** optimized for high-performance AI pipelines.\n\nIt integrates seamlessly with **TensorFlow**, **TensorRT**, **vLLM**, **FAISS**, and more, with additional integrations on the way. Our goal is to make retrieval not just faster but also more cost-efficient and scalable. Early benchmarks show promising performance improvements compared to frameworks like **LangChain** and **LlamaIndex**, but there's always room to refine and push the limits.\n\n[Comparison for CPU usage over time](https://preview.redd.it/njud68kchtse1.jpg?width=2038&amp;format=pjpg&amp;auto=webp&amp;s=ea4f0d1774b9bee335487cb34cc391f997ec3c0d)\n\n[Comparison for PDF extraction and chunking](https://preview.redd.it/kqbwy3kchtse1.jpg?width=2038&amp;format=pjpg&amp;auto=webp&amp;s=aa14207f2099a4cd743bf0e75460cd09a15decd8)\n\nSince RAG relies heavily on vector search, indexing strategies, and efficient storage solutions, weâ€™re actively exploring ways to optimize retrieval performance while keeping resource consumption low. **The project is still evolving**, and **weâ€™d love feedback from those working with big data infrastructure**, **large-scale retrieval**, and **AI-driven analytics**.  \n  \nIf you're interested, check it out here: ğŸ‘‰ [https://github.com/pureai-ecosystem/purecpp](https://github.com/pureai-ecosystem/purecpp).  \nContributions, ideas, and discussions are more than welcome and if you liked it, **leave a star on the Repo!**",
    "author": "Gbalke",
    "timestamp": "2025-04-04T06:08:33",
    "url": "https://reddit.com/r/bigdata/comments/1jrbgk2/optimizing_largescale_retrieval_an_opensource/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jr5inf",
    "title": "Running Hive on Windows Using Docker Desktop (Hands On)",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-04-03T23:40:32",
    "url": "https://reddit.com/r/bigdata/comments/1jr5inf/running_hive_on_windows_using_docker_desktop/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jr35d7",
    "title": "ğŸ“Š How SoFi Automates PowerPoint Reports with Tableau &amp; AI [LinkedIn post]",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-04-03T21:11:46",
    "url": "https://reddit.com/r/bigdata/comments/1jr35d7/how_sofi_automates_powerpoint_reports_with/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jqyt6s",
    "title": "NEED recommendations on choosing a BIG DATA Project!",
    "content": "Hey everyone!\n\n Iâ€™m working on a project for my grad course, and I need to pick a recent IEEE paper to simulate using Python.\n\nHere are the official guidelines I need to follow:\n\nâœ… The paper must be from an **IEEE journal or conference**  \nâœ… It should be **published in the last 5 years** (2020 or later)  \nâœ… The topic must be **Big Dataâ€“related** (e.g., classification, clustering, prediction, stream processing, etc.)  \nâœ… The paper should contain an **algorithm or method that can be coded or simulated in Python**  \nâœ… I have to use a **different language than the paper uses** (so if the paper used R or Java, thatâ€™s perfect for me to reimplement in Python)  \nâœ… The dataset used should have **at least 1000 entries**, or I should be able to apply the method to a public dataset with that size  \nâœ… It should be **simple enough to implement within a week or less**, ideally beginner-friendly  \nâœ… Iâ€™ll need to compare my simulation results with those in the paper (e.g., accuracy, confusion matrix, graphs, etc.)\n\nWould really appreciate any suggestions for **easy-to-understand papers**, or any topics/datasets that you think are beginner-friendly and suitable!\n\nThanks in advance! ğŸ™",
    "author": "Excellent-Style8369",
    "timestamp": "2025-04-03T17:25:17",
    "url": "https://reddit.com/r/bigdata/comments/1jqyt6s/need_recommendations_on_choosing_a_big_data/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jqk471",
    "title": "WHITE PAPER: Activating Untapped Tier 0 Storage Within Your GPU Servers",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-04-03T07:36:44",
    "url": "https://reddit.com/r/bigdata/comments/1jqk471/white_paper_activating_untapped_tier_0_storage/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jqdgrd",
    "title": "AI-Machine Learning-Data Science: Pick the Best Domain in 2025",
    "content": "The role of [data science, machine learning, and AI](https://www.usdsi.org/data-science-insights/ai-machine-learning-data-science-pick-the-best-domain-in-2025) in transforming the world is increasing. Learn how they differ and their mechanism in shaping the future.\n\nhttps://preview.redd.it/1db163ku3lse1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=25a7dac8b8e660a8342a0bd639cdc74e3881cc7f\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-03T01:56:29",
    "url": "https://reddit.com/r/bigdata/comments/1jqdgrd/aimachine_learningdata_science_pick_the_best/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jqa5ct",
    "title": "Help with a Shodan-like project",
    "content": "Iâ€™ve recently started working on a project similar to Shodan â€” an indexer for exposed Internet infrastructure, including services, ICS/SCADA systems, domains, ports, and various protocols.\n\nIâ€™m building a high-scale system designed to store and correlate over 200TB of scan data. A key requirement is the ability to efficiently link information such as: domain X has ports Y and Z open, uses TLS certificate Z, runs services A and B, and has N known vulnerabilities.\n\nThe data is collected by approximately 1,200 scanning nodes and ingested into an Apache Kafka cluster before being persisted to the database layer.\n\nIâ€™m struggling to design a stack that supports high-throughput reads and writes while allowing for scalable, real-time correlation across this massive dataset. What kind of architecture or technologies would you recommend for this type of use case?",
    "author": "Ok_Buddy_6222",
    "timestamp": "2025-04-02T22:19:09",
    "url": "https://reddit.com/r/bigdata/comments/1jqa5ct/help_with_a_shodanlike_project/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jpsx7t",
    "title": "Automate Slide Decks and Docs, a Critical Imperative for Business Reporting and Analytics",
    "content": "",
    "author": "askoshbetter",
    "timestamp": "2025-04-02T09:29:47",
    "url": "https://reddit.com/r/bigdata/comments/1jpsx7t/automate_slide_decks_and_docs_a_critical/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jpi59t",
    "title": "Step-by-Step Guide to Passing the Nutanix NCX-MCI Exam",
    "content": "",
    "author": "Big_Data_Path",
    "timestamp": "2025-04-01T23:31:21",
    "url": "https://reddit.com/r/bigdata/comments/1jpi59t/stepbystep_guide_to_passing_the_nutanix_ncxmci/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jpiocy",
    "title": "AI in Data Science- The Power Duo in Action",
    "content": "Data Science Industry is set to experience astounding challenges and capabilities powered by AI Driven Ecosystems. Facilitating Data Transformation with great finesse and posing a concern on other front is what [AI in Data Science](https://www.usdsi.org/data-science-insights/ai-revolution-2025-the-top-5-shifts-shaping-data-science-destiny) could mean.\n\nhttps://preview.redd.it/10bxb19zfdse1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=51f26cfa3e84151b35d4ebcb047d0c7ceaff17bd\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-04-02T00:10:08",
    "url": "https://reddit.com/r/bigdata/comments/1jpiocy/ai_in_data_science_the_power_duo_in_action/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jp0e4a",
    "title": "We cut Databricks costs without sacrificing performanceâ€”hereâ€™s how",
    "content": "About 6 months ago, I led a Databricks cost optimization project where we cut down costs, improved workload speed, and made life easier for engineers. I finally had time to write it all up a few days agoâ€”cluster family selection, autoscaling, serverless, EBS tweaks, and more. I also included a real example with numbers. If youâ€™re using Databricks, this might help:Â [https://medium.com/datadarvish/databricks-cost-optimization-practical-tips-for-performance-and-savings-7665be665f52](https://medium.com/datadarvish/databricks-cost-optimization-practical-tips-for-performance-and-savings-7665be665f52)",
    "author": "DataDarvesh",
    "timestamp": "2025-04-01T09:47:43",
    "url": "https://reddit.com/r/bigdata/comments/1jp0e4a/we_cut_databricks_costs_without_sacrificing/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jmk7r1",
    "title": "Big Data and AI Integration - Boosting Business Without Sweat | Infographic",
    "content": "Unlock the power of [big data and AI for your business](https://www.usdsi.org/data-science-insights/big-data-and-ai-integration-boosting-business-without-sweat) today! Explore how big data and AI tools are reciprocating greater business enhancements with more finesse.\n\nhttps://preview.redd.it/w2o585q86mre1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=6fd30db79a8fcb5b6327547bb920ecfb6d0bef69\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-03-29T04:27:25",
    "url": "https://reddit.com/r/bigdata/comments/1jmk7r1/big_data_and_ai_integration_boosting_business/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jlv6c6",
    "title": "Speed Up Your Data w/ Hammerspace's David Flynn",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-03-28T06:33:26",
    "url": "https://reddit.com/r/bigdata/comments/1jlv6c6/speed_up_your_data_w_hammerspaces_david_flynn/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jl5ihj",
    "title": "Optimized Vector Embeddings &amp; Search - Changelog: jobdataapi.com v4.14 / API version 1.16 ğŸ‘€",
    "content": "",
    "author": "foorilla",
    "timestamp": "2025-03-27T07:42:29",
    "url": "https://reddit.com/r/bigdata/comments/1jl5ihj/optimized_vector_embeddings_search_changelog/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jl0glf",
    "title": "FUTURE SMART ASSISTANTS AI AGENTS - AUTONOMYS AGENTS (AUTO AGENTS)",
    "content": "Natural language processing (NLP), on-chain AI agents that interact with APIs, solve many problems because they have a unique ability to eliminate the complexities of the blockchain, which is one of the major obstacles for web3.\n\nHowever, there are some problems. In particular, the lack of permanent, verifiable records of their interactions and decision-making processes makes them vulnerable to data loss, manipulation, and censorship.\n\nTherefore, a more robust solution to shutdowns caused by unverifiable decision-making processes is required for AI Agents.\n\nThe Autonomys Agents Framework provides developers with the ability to create autonomous on-chain AI agents with dynamic functionality, verifiable interaction, and persistent, censorship-resistant memory via the Autonomys Network.\n\nThe following basic features are noteworthy.\n\n* Autonomous social media interaction\n* Persistent agent memory storage\n* Internal orchestration system\n* X integration\n* Customizable agent personalities .\n* Extensible vehicle system\n* Multi-model support\n\nConsidering all this information, why should we choose this framework developed by Autonomys Network and offered to users and developers?\n\n1. Provides true data permanence\n2. Enables full operational transparency\n3. Offers true autonomous operation\n\nIt is possible to use all these advantages successfully in the real world in the following sectors:\n\n* Financial Services\n* In social media content production\n* In research and development\n\nTo summarize briefly, Autonomys Network offers us a personal assistant that can produce solutions to many issues both in the web3 world and in our daily lives, thanks to its AI tools.\n\nhttps://preview.redd.it/b2rsf4hth7re1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=19039f7e59cc608bd8a5ad758950045e9a1089bc\n\n",
    "author": "VlkYlz",
    "timestamp": "2025-03-27T03:05:33",
    "url": "https://reddit.com/r/bigdata/comments/1jl0glf/future_smart_assistants_ai_agents_autonomys/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jkiako",
    "title": "Build a Data Analyst AI Agent from Scratch",
    "content": "",
    "author": "JanethL",
    "timestamp": "2025-03-26T11:04:31",
    "url": "https://reddit.com/r/bigdata/comments/1jkiako/build_a_data_analyst_ai_agent_from_scratch/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jkdcy4",
    "title": "How to Deploy Hugging Face LLMs on Teradata VantageCloud Lake with NVIDIA GPU Acceleration",
    "content": "",
    "author": "JanethL",
    "timestamp": "2025-03-26T07:40:34",
    "url": "https://reddit.com/r/bigdata/comments/1jkdcy4/how_to_deploy_hugging_face_llms_on_teradata/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jk756m",
    "title": "SECURITY OF DECENTRALIZATION AND AUTONOMYS NETWORK",
    "content": "One of the main problems we encounter in the basic design of the blockchain world is that only two of the three basic elements called the blockchain trilogy, namely centralization, security and scalability, can be optimized. Especially large blockchains make great efforts to establish a balance between these three. Usually, scalability is sacrificed and the concepts of decentralization and security come to the fore. This choice has caused them to experience problems such as high transaction fees and slow approval processes. Some networks have tried to establish this balance by sacrificing decentralization.  \n  \nAutonomys, on the other hand, aimed to establish a triple balance by shaping the network foundation with a new approach. By linking decentralization to security, Autonomys Network adopted a network structure that implements the archive proof of storage (PoAS) consensus mechanism to solve the blockchain trilogy, and aims to achieve hyper-scalability in the later stages and achieve balance between the elements of this trilogy.  \n  \nDECENTRALIZATION = SECURITY  \nDesigned as the most decentralized blockchain in the Web3 world, Autonomys Network uses disk storage as an easy-to-access hardware resource. It provides a high level of decentralization that has never been done before by using the storage capacity of every computer user's personal computer in the world. The more decentralization is provided, the more security will increase. This is the main goal.  \n  \nA feature that distinguishes the Autonomys Network project from others is that it uses historical data storage, which is actually seen as a big weight on the blockchain, as the primary security mechanism. Farmers share the load on the network thanks to their autonomous storage skills and abilities and each user becomes a part of the security by distributing it among many users. This provides the main decentralization and provides multiple security keys, which is the basic principle of security.  \n  \nWith all these qualifications, Autonomys Network has created a strong ecosystem by solving the basic problems that have been going on for a long time in the Web3 world with the most optional approach and solving them with secure, fast and more affordable network fees. Especially in this regard, I believe that advanced systems that will attract the attention of all interested users will bring a different level of development to the blockchain world by using autonomy at the highest level.\n\nhttps://preview.redd.it/zezr75u5yzqe1.jpg?width=3840&amp;format=pjpg&amp;auto=webp&amp;s=24e6a1df3ab851d46eef0a8133b27dd7e39cb7f9",
    "author": "VlkYlz",
    "timestamp": "2025-03-26T01:40:20",
    "url": "https://reddit.com/r/bigdata/comments/1jk756m/security_of_decentralization_and_autonomys_network/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jkbb6a",
    "title": "Apes Together Strong: Humanity Protocol Swings into the ApeChain Ecosystem",
    "content": "    In January, we announced one of our biggest integrations to date â€” Humanity Protocol and ApeChain are joining forces to bring verifiable, privacy-preserving identity to the Ape ecosystem. This collaboration isn't just about security; it's about unlocking new frontiers for developers and users alike. By embedding Proof of Humanity (PoH) into ApeChain, weâ€™re making dApps more Sybil-resistant, governance more transparent, and digital identity more powerful than ever before.\n    With ApeChain as a zkProofer, developers on both Humanity Protocol and ApeChain can now build without limits. Whether it's creating DAOs that truly represent their communities, enabling NFT experiences tied to real human identities, or pioneering privacy-first DeFi solutions, the integration of Humanity Protocolâ€™s identity layer changes the game. This integration is a fundamental shift that brings the digital and physical worlds closer together, setting a new standard for trust and utility in Web3.",
    "author": "fikiralisverisi",
    "timestamp": "2025-03-26T06:05:37",
    "url": "https://reddit.com/r/bigdata/comments/1jkbb6a/apes_together_strong_humanity_protocol_swings/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jkb3on",
    "title": "Big Data and voter data - suggest a framework to analyze?",
    "content": "Our state has statewide voter data including their voting history for the last six or seven elections.\n\nThe data rows are basic voter data and then there are like six or seven columns for the last six or seven elections.  In each of those there is a status of mail-in, in-person, etc.\n\nWe can purchase a data dump whenever we want and the data is updated periodically.  Notably not streaming data.\n\nSo....  massive number of rows.  Each update will have either have some updates or massive updates depending on the calendar and how close to election day.\n\nIf we use an 'always append' type of update the data set will grow crazy.  If we do an 'update' type of ingest then it might take a lot of time.\n\nThe analysis we want to end up with is a basic pivot table drilling down from our town, street, house, voters and then get the voting history for each voter.  If we had a reasonable excel sheet data file it would be trivial but we are dealing with massive data.\n\nAnyone have any suggestions for how to deal with this scenario?  I'm a tech nerd but not up to date on open source big-data tools.  ",
    "author": "HeneryHawkjj",
    "timestamp": "2025-03-26T05:55:54",
    "url": "https://reddit.com/r/bigdata/comments/1jkb3on/big_data_and_voter_data_suggest_a_framework_to/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jj8ate",
    "title": "New to Columnar/OLAP data. Trying to pick a product for work.",
    "content": "\\[Sorry if this is begging for recommendations.\\] I was tasked with importing data from MySQL into a more efficient database for Zoho Analytics. Boss would like something we could self-host. I went with ClickHouse, but the disk and memory sizes are a bit of an issue. Just 100k rows is killing my test VM. We just don't need a lot of the resource intensive features Clickhouse provides, e.g., we don't need any real-time write capability.\n\n* Nightly table updates (one table)\n* Probably 5-10M rows at most\n* Zoho Analytics Direct Connect\n* Hoping for &lt;4GB memory usage, or is that a pipedream?\n\nDoes that sound like anything to anybody?",
    "author": "asdf072",
    "timestamp": "2025-03-24T18:46:30",
    "url": "https://reddit.com/r/bigdata/comments/1jj8ate/new_to_columnarolap_data_trying_to_pick_a_product/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jjao8v",
    "title": "How ChatGPT Empowers Apache Spark Developers",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-03-24T20:52:08",
    "url": "https://reddit.com/r/bigdata/comments/1jjao8v/how_chatgpt_empowers_apache_spark_developers/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jirjx8",
    "title": "Unlock B2B Gold: Spot Freshly Funded Companies Before Your Competitors Do! Curious How? Ask Me!",
    "content": "",
    "author": "Few_Papaya_6933",
    "timestamp": "2025-03-24T07:10:30",
    "url": "https://reddit.com/r/bigdata/comments/1jirjx8/unlock_b2b_gold_spot_freshly_funded_companies/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jimp8w",
    "title": "Apache Flink 2.0 released",
    "content": "",
    "author": "rmoff",
    "timestamp": "2025-03-24T02:30:38",
    "url": "https://reddit.com/r/bigdata/comments/1jimp8w/apache_flink_20_released/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jijc9l",
    "title": "Download Free Sample Resume for Experience Data Engineer",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-03-23T22:12:59",
    "url": "https://reddit.com/r/bigdata/comments/1jijc9l/download_free_sample_resume_for_experience_data/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ji6ltl",
    "title": "Do you need to be a business to use Instagram Graph API?",
    "content": "Also, what legal restrictions do you have in using them?",
    "author": "Dry_Masterpiece_3828",
    "timestamp": "2025-03-23T11:52:39",
    "url": "https://reddit.com/r/bigdata/comments/1ji6ltl/do_you_need_to_be_a_business_to_use_instagram/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jh0dha",
    "title": "How to Use ChatGPT to Ace Your Data Engineer Interview",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-03-21T21:21:56",
    "url": "https://reddit.com/r/bigdata/comments/1jh0dha/how_to_use_chatgpt_to_ace_your_data_engineer/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jglwhv",
    "title": "Hitachi Vantara = AI for the Enterprise",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-03-21T10:09:13",
    "url": "https://reddit.com/r/bigdata/comments/1jglwhv/hitachi_vantara_ai_for_the_enterprise/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jg84hq",
    "title": "Download Free ebook for Bigdata Interview Preparation Guide (1000+ questions with answers)",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-03-20T20:54:01",
    "url": "https://reddit.com/r/bigdata/comments/1jg84hq/download_free_ebook_for_bigdata_interview/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jfxrro",
    "title": "Game changer or just hype? Dive into the Global VC Investment Tracker with exclusive verified contacts. Curious how it stacks up? Join the discussion and see for yourself!",
    "content": "",
    "author": "Alarmed_Detail5164",
    "timestamp": "2025-03-20T12:49:05",
    "url": "https://reddit.com/r/bigdata/comments/1jfxrro/game_changer_or_just_hype_dive_into_the_global_vc/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jfqcl1",
    "title": "jobdata API now provides vector embeddings + matching for millions of job posts",
    "content": "",
    "author": "foorilla",
    "timestamp": "2025-03-20T07:36:16",
    "url": "https://reddit.com/r/bigdata/comments/1jfqcl1/jobdata_api_now_provides_vector_embeddings/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jfs0dj",
    "title": "ğŸš€ Cracking the Big Data Architect (Pre-Sales) Interview â€“ My Full Journey &amp; Questions!",
    "content": "I recently went through the **Big Data Architect (Technical Pre-Sales) interview at Hays**, and I wanted to share my **step-by-step experience, common questions, and preparation strategy** with you all.\n\nğŸ’¡ **Interview Breakdown &amp; Key Stages:**  \nâœ… **HR Screening** â€“ Resume review, salary discussion, and company alignment.  \nâœ… **Technical Interview** â€“ Big Data architecture, cloud solutions, SQL optimization, real-time data pipelines.  \nâœ… **Case Study Round** â€“ Designing scalable data solutions (AWS, Azure, Redshift, Snowflake).  \nâœ… **Behavioral Interview** â€“ Leadership, client handling, and pre-sales discussions.  \nâœ… **Final Discussion &amp; Offer** â€“ Salary negotiations, TCO analysis, and proving business value.\n\nğŸ”¥ **Read My Full Interview Experience Here ğŸ‘‰** [**Medium Article Link**](https://medium.com/p/25ee83809645)\n\nğŸ“Œ **Top Insights from My Experience:**  \nğŸ”¹ **Master Big Data Architecture &amp; Cloud Solutions** â€“ Hadoop, Spark, Flink, AWS, Redshift, Snowflake.  \nğŸ”¹ **Be Ready for Pre-Sales &amp; Consulting Scenarios** â€“ Client objections, cost justifications, real-world use cases.  \nğŸ”¹ **Prepare for Case Studies &amp; Whiteboarding** â€“ Designing data pipelines, migration strategies, ETL optimizations.  \nğŸ”¹ **Use the STAR Method for Behavioral Questions** â€“ Show how you handled challenges with **Situation, Task, Action, and Result.**\n\nğŸ’¬ **Discussion: If youâ€™re preparing for a Big Data Architect role, letâ€™s talk:**\n\n* **Whatâ€™s the hardest part of a Big Data interview?**\n* **How do you explain Big Data solutions to non-technical stakeholders?**\n* **What are your best strategies for salary negotiation?**\n\nDrop your thoughts below! ğŸš€ğŸ’¡",
    "author": "Ok-Bowl-3546",
    "timestamp": "2025-03-20T08:49:43",
    "url": "https://reddit.com/r/bigdata/comments/1jfs0dj/cracking_the_big_data_architect_presales/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jfrjoz",
    "title": "How I Prepared for the DFS Group Data Engineering Manager Interview (My Experience &amp; Tips)",
    "content": "Hey everyone! I recently went through the **DFS Group** interview process for a **Data Engineering Manager role**, and I wanted to share my experience to help others preparing for similar roles.\n\n# Here's what the interview process looked like:\n\nâœ… **HR Screening:** Cultural fit, resume discussion, and salary expectations.  \nâœ… **Technical Interview:** SQL optimizations, ETL pipeline design, distributed data systems.  \nâœ… **Case Study Round:** Real-world Big Data problem-solving using Kafka, Spark, and Snowflake.  \nâœ… **Behavioral Interview:** Leadership, cross-functional collaboration, and problem-solving.  \nâœ… **Final Discussion &amp; Offer:** Salary negotiations &amp; benefits.\n\nğŸ’¡ **My biggest takeaways:**\n\n* Learn **ETL frameworks** (Airflow, dbt) and **Cloud platforms** (AWS, Azure, GCP).\n* Be ready to **optimize SQL queries** (Partitioning, Indexing, Clustering).\n* Practice **designing real-time data pipelines** with **Kafka &amp; Spark**.\n* Prepare answers using the **STAR method** for behavioral rounds.\n\nğŸ‘‰ **If you're preparing for Data Engineering interviews, check out my full write-up here**: [https://medium.com/p/f238fc6c67bd](https://medium.com/p/f238fc6c67bd)\n\n\n\nWould love to hear from others whoâ€™ve interviewed for **Big Data roles** â€“ What was your experience like? Letâ€™s discuss! ğŸ”¥",
    "author": "Altruistic_Potato_67",
    "timestamp": "2025-03-20T08:29:46",
    "url": "https://reddit.com/r/bigdata/comments/1jfrjoz/how_i_prepared_for_the_dfs_group_data_engineering/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jfh4w2",
    "title": "Data Architecture Complexity",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-03-19T21:43:28",
    "url": "https://reddit.com/r/bigdata/comments/1jfh4w2/data_architecture_complexity/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jeynb1",
    "title": "Best Place to buy firmographic data?",
    "content": "",
    "author": "DBrokerXK",
    "timestamp": "2025-03-19T07:45:12",
    "url": "https://reddit.com/r/bigdata/comments/1jeynb1/best_place_to_buy_firmographic_data/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jep8cj",
    "title": "[CFP] Call for Papers â€“ IEEE FITYR 2025",
    "content": "Dear Researchers,\n\nWe are excited to invite you to submit your research to theÂ **1st IEEE International Conference on Future Intelligent Technologies for Young Researchers (FITYRÂ 2025)**, which will be held fromÂ **July 21-24, 2025, in Tucson, Arizona, United States**.\n\nIEEEÂ FITYRÂ 2025 provides a premier venue for young researchers to showcase their latest work inÂ **AI, IoT, Blockchain, Cloud Computing, and Intelligent Systems**. The conference promotes collaboration and knowledge exchange among emerging scholars in the field of intelligent technologies.\n\n# Topics of Interest Include (but are not limited to):\n\n* Artificial Intelligence and Machine Learning\n* Internet of Things (IoT) and Edge Computing\n* Blockchain and Decentralized Applications\n* Cloud Computing and Service-Oriented Architectures\n* Cybersecurity, Privacy, and Trust in Intelligent Systems\n* Human-Centered AI and Ethical AI Development\n* Applications of AI in Healthcare, Smart Cities, and Robotics\n\n# Paper Submission:Â [https://easychair.org/conferences/?conf=fityr2025](https://easychair.org/conferences/?conf=fityr2025)\n\n# Important Dates:\n\n* **Paper Submission Deadline:**Â **April 30, 2025**\n* **Author Notification:**Â **May 22, 2025**\n* **Final Paper Submission (Camera-ready):**Â **June 6, 2025**\n\nFor more details, visit:  \n[https://conf.researchr.org/track/cisose-2025/fityr-2025](https://conf.researchr.org/track/cisose-2025/fityr-2025)\n\nWe look forward to your contributions and participation inÂ **IEEEÂ FITYRÂ 2025!**\n\nBest regards,  \nSteering Committee, CISOSE 2025",
    "author": "khushi-20",
    "timestamp": "2025-03-18T21:42:44",
    "url": "https://reddit.com/r/bigdata/comments/1jep8cj/cfp_call_for_papers_ieee_fityr_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jeoq5p",
    "title": "Call for Papers â€“ IEEE SOSE 2025",
    "content": "Dear Researchers,\n\nI am pleased to invite you to submit your research to theÂ **19th IEEE International Conference on Service-Oriented System Engineering (SOSEÂ 2025)**, to be held fromÂ **July 21-24, 2025**, inÂ **Tucson, Arizona, United States**.\n\nIEEEÂ SOSEÂ 2025 provides a leading international forum for researchers, practitioners, and industry experts to present and discuss cutting-edge research on service-oriented system engineering, microservices, AI-driven services, and cloud computing. The conference aims to advance the development of service-oriented computing, architectures, and applications in various domains.\n\n# Topics of Interest Include (but are not limited to):\n\n* Service-Oriented Architectures (SOA) &amp; Microservices\n* AI-Driven Service Computing\n* Service Engineering for Cloud, Edge, and IoT\n* Blockchain for Service Computing\n* Security, Privacy, and Trust in Service-Oriented Systems\n* DevOps &amp; Continuous Deployment inÂ SOSE\n* Digital Twins &amp; Cyber-Physical Systems\n* Industry Applications and Real-World Case Studies\n\n# Paper Submission:Â [https://easychair.org/conferences/?conf=sose2025](https://easychair.org/conferences/?conf=sose2025)\n\n# Important Dates:\n\n* **Paper Submission Deadline:**Â **April 15, 2025**\n* **Author Notification:**Â **May 15, 2025**\n* **Final Paper Submission (Camera-ready):**Â **May 22, 2025**\n\nFor more details, visit the conference website:  \n[https://conf.researchr.org/track/cisose-2025/sose-2025](https://conf.researchr.org/track/cisose-2025/sose-2025)\n\nWe look forward to your contributions and participation in IEEEÂ SOSEÂ 2025!\n\nBest regards,  \nSteering Committee, CISOSE 2025",
    "author": "khushi-20",
    "timestamp": "2025-03-18T21:10:39",
    "url": "https://reddit.com/r/bigdata/comments/1jeoq5p/call_for_papers_ieee_sose_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jem106",
    "title": "[CFP] Call for Papers â€“ IEEE JCC 2025",
    "content": "Dear Researchers,\n\nWe are pleased to announce theÂ **16th**Â **IEEE International Conference on Cloud Computing and Services (JCCÂ 2025)**, which will be held fromÂ **July 21-24, 2025**, inÂ **Tucson, Arizona, United States**.\n\nIEEEÂ JCCÂ 2025 is a leading conference focused on the latest developments in cloud computing and services. This conference offers an excellent platform for researchers, practitioners, and industry experts to exchange ideas and share innovative research on cloud technologies, cloud-based applications, and services. We invite high-quality paper submissions on the following topics (but not limited to):\n\n* AI/ML in joint-cloud environments\n* AI/ML for Distributed Systems\n* Cloud Service Models and Architectures\n* Cloud Security and Privacy\n* Cloud-based Internet of Things (IoT)\n* Data Analytics and Machine Learning in the Cloud\n* Cloud Infrastructure and Virtualization\n* Cloud Management and Automation\n* Cloud Computing for Edge Computing and 5G\n* Industry Applications and Case Studies in Cloud Computing\n\n**Paper Submission:**  \nPlease submit your papers via the following link:Â [https://easychair.org/conferences/?conf=jcc2025](https://easychair.org/conferences/?conf=jcc2025)\n\n**Important Dates:**\n\n* **Paper Submission Deadline:**Â March 21, 2025\n* **Author Notification:**Â May 8, 2025\n* **Final Paper Submission (Camera-ready):**Â May 18, 2025\n\nFor additional details, visit the conference website:Â [https://conf.researchr.org/track/cisose-2025/jcc-2025](https://conf.researchr.org/track/cisose-2025/jcc-2025)\n\nWe look forward to your submissions and valuable contributions to the field of cloud computing and services.\n\nBest regards,  \nSteering Committee, CISOSE 2025",
    "author": "khushi-20",
    "timestamp": "2025-03-18T18:43:48",
    "url": "https://reddit.com/r/bigdata/comments/1jem106/cfp_call_for_papers_ieee_jcc_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jeja2a",
    "title": "Call for Papers â€“ IEEE DAPPS 2025",
    "content": "Dear Researchers,\n\nTheÂ **7th IEEE International Conference on Decentralized Applications and Infrastructures (DAPPS 2025)**Â will take place fromÂ **July 21-24, 2025, in Tucson, Arizona, USA**. The conference serves as a premier venue for researchers, practitioners, and industry professionals to discuss innovations in decentralized applications, blockchain, and distributed infrastructure.\n\nIEEE DAPPS 2025 is a premier international forum for researchers and practitioners to exchange innovative ideas, present cutting-edge research, and discuss advancements in decentralized applications, blockchain technologies, and infrastructures. This yearâ€™s conference will cover a wide range of exciting topics, including but not limited to:\n\n* Blockchain &amp; Distributed Ledger Technologies\n* Smart Contracts &amp; Decentralized Finance (DeFi)\n* Security, Privacy, and Trust in Decentralized Systems\n* Scalability, Interoperability, and Performance of DApps\n* Consensus Mechanisms and Protocol Innovations\n* Decentralized AI and Machine Learning\n* Real-World Use Cases &amp; Industry Applications\n\nAll accepted papers will be published in the conference proceedings. You can submit your papers via the following link:Â [https://easychair.org/conferences/?conf=dapps2025](https://easychair.org/conferences/?conf=dapps2025)\n\n**Important Dates:**\n\n* **Paper Submission Deadline:**Â March 21, 2025 (Extended)\n* **Author Notification:**Â May 8, 2025\n* **Final Paper Submission (Camera-ready):**Â May 18, 2025\n\nFor more details about the conference and submission guidelines, please visit the conference website:Â [https://conf.researchr.org/track/cisose-2025/dapps-2025](https://conf.researchr.org/track/cisose-2025/dapps-2025)\n\nThis is an excellent opportunity to contribute to cutting-edge research in decentralized applications and blockchain technologies. We look forward to your submissions!\n\nBest regards,  \nJerry Gao -Â  San Jose State University  \nSteering Committee, CISOSE 2025",
    "author": "khushi-20",
    "timestamp": "2025-03-18T16:31:20",
    "url": "https://reddit.com/r/bigdata/comments/1jeja2a/call_for_papers_ieee_dapps_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jefrfk",
    "title": "Hitachi iQ Powered by Hammerspace and VSP One",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-03-18T13:58:44",
    "url": "https://reddit.com/r/bigdata/comments/1jefrfk/hitachi_iq_powered_by_hammerspace_and_vsp_one/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1je4615",
    "title": "External table path getting deleted on insert overwrite",
    "content": "Hi Folks, i have been seeing this wierd issue after upgrading spark 2 to spark 3. \n\nWhenever any job fails to load data (insert overwrite) in non partitioned external table due to insufficient memory error, on rerun, I get error that hdfs path of the target external table is not present.\nAs per my understanding, insert overwrite only deletes the data and the writes new data and not the hdfs path.\n\nThe insert query is simple insert overwrite select * from source and I have been using spark.sql for it.\n\nAny insights on what could be causing this?\n\nSource and target table details:\nBoth are non partitioned external table with storage as hdfs and file format is parquet. \n\n",
    "author": "Pratyush171",
    "timestamp": "2025-03-18T05:49:46",
    "url": "https://reddit.com/r/bigdata/comments/1je4615/external_table_path_getting_deleted_on_insert/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1je8lyk",
    "title": "Apache Kafka 4.0 released ğŸ‰",
    "content": "",
    "author": "rmoff",
    "timestamp": "2025-03-18T09:09:36",
    "url": "https://reddit.com/r/bigdata/comments/1je8lyk/apache_kafka_40_released/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1je7py5",
    "title": "Need your help with my Masterâ€™s thesis",
    "content": "Hi,\n\nIâ€™m a student from Austria and currently working on my Masterâ€™s thesis, titled \"Requirement Analysis of Data Science as a Service,\" and Iâ€™ve created a survey to gather insights from professionals and enthusiasts in the field. The survey is brief and designed to understand the marked needs for offering Data Science as a Service (DSaaS).\n\nIt would mean a lot if some of you guys working in the field could fill it out. It should take you around 5-10 minutes. I already sent it out in my work/friends circle but unfortunately without a huge response.\n\nHereâ€™s the survey link:Â [https://forms.gle/3Rg7YndJfYTJRgtXA](https://forms.gle/3Rg7YndJfYTJRgtXA)\n\nThank you very much in advance!!!",
    "author": "5381",
    "timestamp": "2025-03-18T08:32:09",
    "url": "https://reddit.com/r/bigdata/comments/1je7py5/need_your_help_with_my_masters_thesis/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1je5xt6",
    "title": "Learn Data Manipulation Using Pandas",
    "content": "Pandas, today's powerful data analysis library acts up to facilitate enhanced data manipulation. Want to know how? Read to comprehend its minutest manouvers and diverse usage with USDSIÂ®.\n\nhttps://preview.redd.it/hlykyis0igpe1.jpg?width=850&amp;format=pjpg&amp;auto=webp&amp;s=92d65bbb5924c7837cc150f6069708401c9b20a6\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-03-18T07:14:29",
    "url": "https://reddit.com/r/bigdata/comments/1je5xt6/learn_data_manipulation_using_pandas/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1je05t2",
    "title": "ğŸ¤– Matrices for Machine Learning with Python",
    "content": "",
    "author": "Veerans",
    "timestamp": "2025-03-18T01:23:01",
    "url": "https://reddit.com/r/bigdata/comments/1je05t2/matrices_for_machine_learning_with_python/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jdkrwf",
    "title": "How to improve my xgboost regression model?",
    "content": "Hello fellas, I have been developing a machine learning model to predict art pieces in my dataset.  \nI have mostly 15000 rows (some rows have Nan values). I set the features as artist, product\\_year, auction\\_year, area, and price, and material of art piece. When I check the MAE it gives me 65% variance to my average test price. And when I check the features by using SHAP, I see that the most effective features are \"area\", \"artist\", and \"material\".  \nI made research about this topic and read that mostly used models that are successful xgboost, and randomforest, and also CNN. However, I cannot reduce the MAE of my xgboost model.  \nAny recommandation is appricated fellas. Thanks and have a nice day.\n\n",
    "author": "No_Development_5561",
    "timestamp": "2025-03-17T12:03:34",
    "url": "https://reddit.com/r/bigdata/comments/1jdkrwf/how_to_improve_my_xgboost_regression_model/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jdav91",
    "title": "DATA SCIENCE AI ROBOTICS THE ULTIMATE TECH TRIO",
    "content": "The future is being built today! Data Science, AI, and Robotics are converging to create a tech revolution that will redefine industries by 2025. From intelligent automation to data-driven breakthroughs, the possibilities are endless. Are you ready to be part of this transformative journey? Letâ€™s unlock the future together! \n\nhttps://preview.redd.it/xki47qaeo8pe1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=2727d3313fbedf3e2c08b8723f998bc3acd88d5d\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-03-17T04:56:23",
    "url": "https://reddit.com/r/bigdata/comments/1jdav91/data_science_ai_robotics_the_ultimate_tech_trio/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jcqkyl",
    "title": "How to Prepare for a Data Engineering Manager Interview?",
    "content": "Hey everyone,\n\n\n\nI recently wrote a **deep dive into the hiring process** for a **Data Engineering Manager role at DFS Group**. It covers:\n\n\n\nğŸ”¹ **SQL Optimization in Snowflake &amp; BigQuery**\n\nğŸ”¹ **Real-time ETL Pipelines (Kafka, Flink, dbt, Airflow)**\n\nğŸ”¹ **Big Data Architecture &amp; Cloud (Azure, Alicloud, GCP)**\n\nğŸ”¹ **Case Study: 360-degree Customer Analytics Platform**\n\nğŸ”¹ **Behavioral Questions &amp; Salary Negotiation Strategies**\n\n\n\nğŸ“Œ **Read it here:** [DFS Group Data Engineering Interview Guide](https://medium.com/p/f238fc6c67bd)\n\n\n\nWhat are some of the **toughest questions** youâ€™ve faced in a **Data Engineering interview**? Letâ€™s discuss below! ğŸš€\n\n\n\n\\#DataEngineering #BigData #CloudComputing #SQL #DataScience",
    "author": "Ok-Bowl-3546",
    "timestamp": "2025-03-16T10:23:23",
    "url": "https://reddit.com/r/bigdata/comments/1jcqkyl/how_to_prepare_for_a_data_engineering_manager/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jcqtgd",
    "title": "The Tableau Conference is just a month away! \n\nğŸ“… Bookmark our session: â€œHow SoFi Automates PowerPoint Reports with Tableau &amp; AIâ€ \n\nğŸ“ Visit our booth in the Data Village. \n\nSee you soon, DataFam!",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-03-16T10:33:23",
    "url": "https://reddit.com/r/bigdata/comments/1jcqtgd/the_tableau_conference_is_just_a_month_away/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jb3mqh",
    "title": "Cloud Data Analytics Is a Scam",
    "content": "",
    "author": "EnvironmentalBox3925",
    "timestamp": "2025-03-14T06:14:59",
    "url": "https://reddit.com/r/bigdata/comments/1jb3mqh/cloud_data_analytics_is_a_scam/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.45,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1jaa2dd",
    "title": "Unleash Insights: Python for Data Analysis",
    "content": "From market analysis to risk assessment and customer segmentation to statistical analysis, Python is the go-to programming language for data science professionals. It has completely transformed the field of data science and made this technology accessible to everyone with its user-friendly interface and vast resources of ready-to-use libraries and data science frameworks. \n\nCheck out our detailed [infographic on Python for data analysis](https://www.usdsi.org/data-science-insights/unleash-insights-python-for-data-analysis) and understand its key features, advantages, popular libraries, and more.\n\nhttps://preview.redd.it/xm0iu3l03goe1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=ccc24cbb2634c241cda1b5698fccd5e6c8770c9c\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-03-13T04:46:16",
    "url": "https://reddit.com/r/bigdata/comments/1jaa2dd/unleash_insights_python_for_data_analysis/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j9mtqt",
    "title": "Emergency Response and Wildfire Real-Time Analysis [Webinar]",
    "content": "",
    "author": "askoshbetter",
    "timestamp": "2025-03-12T08:36:10",
    "url": "https://reddit.com/r/bigdata/comments/1j9mtqt/emergency_response_and_wildfire_realtime_analysis/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j8r5o5",
    "title": "Top 10 Predictions for Data Science from Q1 2025",
    "content": "",
    "author": "sharmaniti437",
    "timestamp": "2025-03-11T06:48:07",
    "url": "https://reddit.com/r/bigdata/comments/1j8r5o5/top_10_predictions_for_data_science_from_q1_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j82wme",
    "title": "Teradata announces it's Enterprise Vector Store",
    "content": "",
    "author": "JanethL",
    "timestamp": "2025-03-10T09:36:25",
    "url": "https://reddit.com/r/bigdata/comments/1j82wme/teradata_announces_its_enterprise_vector_store/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j87idm",
    "title": "Real-Time Alerts for Startups That Just Raised Fundsâ€”Want to Stay in the Loop?",
    "content": "",
    "author": "Last-Payment-3604",
    "timestamp": "2025-03-10T12:46:04",
    "url": "https://reddit.com/r/bigdata/comments/1j87idm/realtime_alerts_for_startups_that_just_raised/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j81kre",
    "title": "Wave of Executive Talent Joins Hammerspace",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-03-10T08:41:58",
    "url": "https://reddit.com/r/bigdata/comments/1j81kre/wave_of_executive_talent_joins_hammerspace/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j7zq3g",
    "title": "Cloudera Data analyst exam certificate",
    "content": "I need to prepare for the cloudera data analyst exam certificate , could you please suggest material to study for this ",
    "author": "Royal-Music4431",
    "timestamp": "2025-03-10T07:20:50",
    "url": "https://reddit.com/r/bigdata/comments/1j7zq3g/cloudera_data_analyst_exam_certificate/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j7mgmq",
    "title": "Need help for my subject for chose use case !",
    "content": "Stockage et recherche de l'information en Big DataÂ : avancÃ©es et dÃ©fits\t",
    "author": "Puzzled-Biscotti-752",
    "timestamp": "2025-03-09T17:47:13",
    "url": "https://reddit.com/r/bigdata/comments/1j7mgmq/need_help_for_my_subject_for_chose_use_case/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j7h8t5",
    "title": "Mastering Ordered Analytics and Window Functions on Big Data Systems",
    "content": "I wish I had mastered ordered analytics and window functions early in my career, but I was afraid because they were hard to understand. After some time, I found that they are so easy to understand.\n\nI spent about 20 years becoming a Teradata expert, but I then decided to attempt to master as many databases as I could. To gain experience, I wrote books and taught classes on each.\n\nIn the link to the blog post below, Iâ€™ve curated a collection of my favorite and most powerful analytics and window functions. These step-by-step guides are designed to be practical and applicable to every database system in your enterprise.\n\nWhatever database platform you are working with, I have step-by-step examples that begin simply and continue to get more advanced. Based on the way these are presented, I believe you will become an expert quite quickly.\n\nI have a list of the top 15 databases worldwide and a link to the analytic blogs for that database. The systems include Snowflake, Databricks, Azure Synapse, Redshift, Google BigQuery, Oracle, Teradata, SQL Server, DB2, Netezza, Greenplum, Postgres, MySQL, Vertica, and Yellowbrick.\n\nEach database will have a link to an analytic blog in this order:\n\nRank  \nDense\\_Rank  \nPercent\\_Rank  \nRow\\_Number  \nCumulative Sum (CSUM)  \nMoving Difference  \nCume\\_Dist  \nLead\n\nEnjoy, and please drop me a reply if this helps you.\n\nHere is a link to 100 blogs based on the database and the analytics you want to learn.\n\n[https://coffingdw.com/analytic-and-window-functions-for-all-systems-over-100-blogs/](https://coffingdw.com/analytic-and-window-functions-for-all-systems-over-100-blogs/) ",
    "author": "NexusDataPro",
    "timestamp": "2025-03-09T13:46:41",
    "url": "https://reddit.com/r/bigdata/comments/1j7h8t5/mastering_ordered_analytics_and_window_functions/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j6u2yu",
    "title": "Sharing My First Big Project as a Junior Data Engineer â€“ Feedback Welcome!",
    "content": "Iâ€™m a junior data engineer, and Iâ€™ve been working on my first big project over the past few months. I wanted to share it with you all, not just to showcase what Iâ€™ve built, but also to get your feedback and advice. As someone still learning, Iâ€™d really appreciate any tips, critiques, or suggestions you might have!\n\nThis project was a huge learning experience for me. I made a ton of mistakes, spent hours debugging, and rewrote parts of the code more times than I can count. But Iâ€™m proud of how it turned out, and Iâ€™m excited to share it with you all.\n\n# How It Works\n\nHereâ€™s a quick breakdown of the system:\n\n1. **Dashboard**: A simple steamlit web interface that lets you interact with user data.\n2. **Producer**: Sends user data to Kafka topics.\n3. **Spark Consumer**: Consumes the data from Kafka, processes it using PySpark, and stores the results.\n4. **Dockerized**: Everything runs in Docker containers, so itâ€™s easy to set up and deploy.\n\n# What I Learned\n\n* **Kafka**: Setting up Kafka and understanding topics, producers, and consumers was a steep learning curve, but itâ€™s such a powerful tool for real-time data.\n* **PySpark**: I got to explore Sparkâ€™s streaming capabilities, which was both challenging and rewarding.\n* **Docker**: Learning how to containerize applications and use Docker Compose to orchestrate everything was a game-changer for me.\n* **Debugging**: Oh boy, did I learn how to debug! From Kafka connection issues to Spark memory errors, I faced (and solved) so many problems.\n\nIf youâ€™re interested, Iâ€™ve shared the project structure below. Iâ€™m happy to share the code if anyone wants to take a closer look or try it out themselves!\n\nhere is my github repo :\n\n[https://github.com/moroccandude/management\\_users\\_streaming/tree/main](https://github.com/moroccandude/management_users_streaming/tree/main)\n\n# Final Thoughts\n\nThis project has been a huge step in my journey as a data engineer, and Iâ€™m really excited to keep learning and building. If you have any feedback, advice, or just want to share your own experiences, Iâ€™d love to hear from you!\n\nThanks for reading, and thanks in advance for your help! ğŸ™",
    "author": "Sea-Big3344",
    "timestamp": "2025-03-08T16:30:15",
    "url": "https://reddit.com/r/bigdata/comments/1j6u2yu/sharing_my_first_big_project_as_a_junior_data/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j62szh",
    "title": "Fivetran vs. Airbyte: Which Data Ingestion Tool Wins?",
    "content": "I just published a breakdown of Fivetran vs. Airbyte on Mediumâ€”two heavyweights in data ingestion. Managed vs. open-source, connectors, pricing, real-time needsâ€”all covered with pros, cons, and examples!\n\nWhich tool (Fivetran or Airbyte) do you rely on for your data pipelines?\n",
    "author": "Illustrious-Quiet339",
    "timestamp": "2025-03-07T15:57:03",
    "url": "https://reddit.com/r/bigdata/comments/1j62szh/fivetran_vs_airbyte_which_data_ingestion_tool_wins/",
    "score": 5,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j5iqun",
    "title": "Factsheet: Data Science Career 2025",
    "content": "Learn about the latest data science industry insights, trends, salary outlooks, interesting facts, and top opportunities in our [Data Science Career Factsheet 2025](https://www.usdsi.org/data-science-insights/resources/factsheet-data-science-careers-in-2025).\n\nhttps://preview.redd.it/mr1mjwru88ne1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=5c87c334d5f7b71861cf822f01fa3ea094d2ed32\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-03-07T01:20:36",
    "url": "https://reddit.com/r/bigdata/comments/1j5iqun/factsheet_data_science_career_2025/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j597lo",
    "title": "Best place to buy firmographic data?",
    "content": "I need firmographic data in fee different countries! \n",
    "author": "location_analytics_9",
    "timestamp": "2025-03-06T16:19:22",
    "url": "https://reddit.com/r/bigdata/comments/1j597lo/best_place_to_buy_firmographic_data/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j495sd",
    "title": "Biggest Issue in SQL - Date Functions and Date Formatting",
    "content": "I used to be an expert in Teradata, but I decided to expand my knowledge and master every database. I've found that the biggest differences in SQL across various database platforms lie in date functions and the formats of dates and timestamps.  \n  \nAs Don Quixote once said, â€œOnly he who attempts the ridiculous may achieve the impossible.â€ Inspired by this quote, I took on the challenge of creating a comprehensive blog that includes all date functions and examples of date and timestamp formats across all database platforms, totaling 25,000 examples per database.  \n  \nAdditionally, I've compiled another blog featuring 45 links, each leading to the specific date functions and formats of individual databases, along with over a million examples.  \n  \nHaving these detailed date and format functions readily available can be incredibly useful. Hereâ€™s the link to the post for anyone interested in this information. It is completely free, and I'm happy to share it.  \n  \n[https://coffingdw.com/date-functions-date-formats-and-timestamp-formats-for-all-databases-45-blogs-in-one/](https://coffingdw.com/date-functions-date-formats-and-timestamp-formats-for-all-databases-45-blogs-in-one/)  \n  \nEnjoy!",
    "author": "NexusDataPro",
    "timestamp": "2025-03-05T10:53:59",
    "url": "https://reddit.com/r/bigdata/comments/1j495sd/biggest_issue_in_sql_date_functions_and_date/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j3z2xv",
    "title": "Curious about startups that just raised funds? Here's a way to get real-time updates and direct contact info. Thoughts?",
    "content": "",
    "author": "No-Baby-6893",
    "timestamp": "2025-03-05T02:12:10",
    "url": "https://reddit.com/r/bigdata/comments/1j3z2xv/curious_about_startups_that_just_raised_funds/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j3btiy",
    "title": "Enhanced multi-value parameters for Job and Company queries - Changelog: jobdataapi.com v4.12 / API version 1.14 ğŸ‘€",
    "content": "",
    "author": "foorilla",
    "timestamp": "2025-03-04T07:20:13",
    "url": "https://reddit.com/r/bigdata/comments/1j3btiy/enhanced_multivalue_parameters_for_job_and/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j3ejf2",
    "title": "Best Big Data Courses on Udemy to learn in 2025",
    "content": "",
    "author": "[deleted]",
    "timestamp": "2025-03-04T09:20:51",
    "url": "https://reddit.com/r/bigdata/comments/1j3ejf2/best_big_data_courses_on_udemy_to_learn_in_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j34dse",
    "title": "The kafka-producer-perf-test tool enables you to produce a large quantity of data to test producer performance for the Kafka cluster.",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-03-03T23:12:40",
    "url": "https://reddit.com/r/bigdata/comments/1j34dse/the_kafkaproducerperftest_tool_enables_you_to/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j2x7n7",
    "title": "Best Place to buy firmographic data ? Techsalerator or Moody's?",
    "content": "",
    "author": "Mental-Advertising83",
    "timestamp": "2025-03-03T16:50:54",
    "url": "https://reddit.com/r/bigdata/comments/1j2x7n7/best_place_to_buy_firmographic_data_techsalerator/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j2f675",
    "title": "Call for Papers: IEEE IMC 2025",
    "content": "**13th IEEE International Conference on Intelligent Mobile Computing (IMC 2025)**\n\n**July 21-24, 2025Tucson, Arizona, USA**\n\nThe IMC 2025, part of the IEEE International Congress on Intelligent and Service-Oriented Systems Engineering (CISOSE 2025), is inviting high-quality research paper submissions! IMC 2025 focuses on cutting-edge advancements in mobile, edge, and cloud computing.\n\n**Topics of Interest**\n\nSubmissions are welcome in areas including, but not limited to:\n\n* Theories, concepts, algorithms, programming models, and methodologies\n* Mobile cloud, intelligent mobile computing, and mobile intelligence\n* Edge computing and fog computing\n* Mobile edge computing (MEC) and multi-access mobile computing\n* Virtualization and containerization for mobile clouds\n* Mobile cloud and mobile computing continuum, offloading, and resource allocation\n* Dynamic resource provisioning, load balancing, and workload management\n* Context-aware resource provisioning and AI-driven resource allocation\n* Data storage and management in mobile environments\n* Mobile clouds and network slicing\n* Orchestration, service discovery, and mobile cloud federations\n* Private and public mobile clouds, and campus networks\n* Mobile clouds and mobile computing with AI and for AI, and mobile AI\n* Mobile agents, digital twins, and service portability and service migration\n* Self-configuration, self-adaptive, self-healing, and AI-based orchestration\n* Performance, latency, scalability, reliability, and quality of service (QoS)\n* Mobile cloud and mobile computing for 5G/6G and non-terrestrial networks (NTN)\n* On-demand mobile computing models and cloud brokering\n* Collaborative mobile intelligence and federated mobile computing\n* Ecosystems, market trends, and business models\n* Security, privacy, trust, and dependability in mobile clouds\n* Energy efficiency and sustainability in mobile cloud computing\n* Mobile cloud computing for social networks and crowdsourcing\n* Mobile cloud computing in healthcare, smart cities, and IoT applications\n\n**Submission Guidelines**\n\nAll accepted papers will be published by IEEE Computer Society Press (EI-Indexed) and included in the IEEE Digital Library.\n\n**Important Dates**\n\n* **PaperÂ Submission Deadline:**Â March 21, 2025\n* **Author Notification:**Â May 7, 2025\n* **FinalÂ Paper Submission (Camera-ready): May 21, 2025**\n\nSubmit your papers here: [https://easychair.org/conferences/?conf=mobilecloudimc25](https://easychair.org/conferences/?conf=mobilecloudimc25)\n\nFor more details, visit: [https://conf.researchr.org/track/cisose-2025/imc-2025](https://conf.researchr.org/track/cisose-2025/imc-2025)\n\nJoin us in shaping the future of intelligent mobile computing!",
    "author": "khushi-20",
    "timestamp": "2025-03-03T02:48:28",
    "url": "https://reddit.com/r/bigdata/comments/1j2f675/call_for_papers_ieee_imc_2025/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j2ddan",
    "title": "Apache Spark Vs Hadoop",
    "content": "Big Data Battle Alert! [Apache Spark vs. Hadoop](https://www.usdsi.org/data-science-insights/apache-spark-vs-hadoop): Which giant rules your data universe?  Spark = Lightning speed (100x faster in-memory processing!) Hadoop = Batch processing king (scalable &amp; cost-effective).Want to dominate your data game?\n\nhttps://preview.redd.it/akdv3kwtgfme1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=ba313e2c0b8be1003589ae083d6d93de82b368ff\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-03-03T00:33:36",
    "url": "https://reddit.com/r/bigdata/comments/1j2ddan/apache_spark_vs_hadoop/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j1k1hg",
    "title": "Call for Papers - IEEE AI Test 2025",
    "content": "Dear Researchers,\n\nWe are pleased to announce theÂ **7th IEEE International Conference on Artificial Intelligence Testing**, which will take place fromÂ **July 21-24, 2025, in Tucson, Arizona, United States**.\n\nAs artificial intelligence (AI) technologies continue to evolve and integrate into various applications, ensuring their reliability, robustness, and security is critical. AI TEST 2025 serves as a premier venue for researchers, practitioners, and industry leaders to exchange insights, methodologies, and innovations in AI testing and validation.\n\nWe invite submissions of original research papers covering AI testing methodologies, tools, and applications. Selected high-quality papers will be invited for extended versions in a special issue of a peer-reviewed journal.\n\n**Topics of Interest (Including but not limited to):**\n\n**AI Testing &amp; Validation**\n\n* Testing AI models and machine learning algorithms\n* Verification, validation, and certification of AI systems\n* Test automation for AI applications\n* Testing generative AI and large language models\n\n**Reliability &amp; Safety of AI Systems**\n\n* Robustness testing of AI models\n* Adversarial attack detection and mitigation\n* Safety assurance for autonomous and AI-driven systems\n\n**AI in Software Testing**\n\n* AI-driven test generation and automation\n* AI for software quality assurance\n* Intelligent debugging and fault localization\n\n**Ethics, Fairness, and Bias in AI Testing**\n\n* Identifying and mitigating bias in AI models\n* Explainability and interpretability testing for AI\n* Regulatory compliance and ethical considerations in AI validation\n\n**AI in Real-World Applications**\n\n* Testing AI in healthcare, finance, cybersecurity, and transportation\n* Performance evaluation of AI-powered decision-making systems\n* Case studies and industry experiences in AI testing\n\nAll submissions must be made through:Â [https://easychair.org/conferences/?conf=aitest2025](https://easychair.org/conferences/?conf=aitest2025)\n\n**Important Dates:**\n\n* Paper Submission: April 01, 2025\n* Notification of Acceptance: May 10, 2025\n* Camera-ready and authorâ€™s registration: June 1, 2025\n\nFor more details, please visit the conference website:Â [https://conf.researchr.org/track/cisose-2025/ai-test2025](https://conf.researchr.org/track/cisose-2025/ai-test2025)\n\nBest Regards,  \nSteering Committee  \nCISOSE 2025",
    "author": "khushi-20",
    "timestamp": "2025-03-01T22:54:49",
    "url": "https://reddit.com/r/bigdata/comments/1j1k1hg/call_for_papers_ieee_ai_test_2025/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j0qg1e",
    "title": "API for job data now with job post descriptions in Markdown + filters down to state/city level",
    "content": "",
    "author": "foorilla",
    "timestamp": "2025-02-28T20:42:41",
    "url": "https://reddit.com/r/bigdata/comments/1j0qg1e/api_for_job_data_now_with_job_post_descriptions/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j0s5qa",
    "title": "Call for Papers â€“ IEEE Big Data Service 2025",
    "content": "We are pleased to invite submissions for the 11th IEEE International Conference on Big Data Computing Service and Machine Learning Applications (BigDataServiceÂ 2025), taking place from July 21-24, 2025, in Tucson, Arizona, USA. The conference provides a premier venue for researchers and practitioners to share innovations, research findings, and experiences in big data technologies, services, and machine learning applications.Â \n\nThe conference welcomes high-quality paper submissions. Accepted papers will be included in the IEEE proceedings, and selected papers will be invited toÂ submitÂ extended versions to a special issue of a peer-reviewed SCI-Indexed journal.Â \n\nTopics of interest include but are not limited to:Â \n\nBig Data Analytics and Machine Learning:\n\n* Algorithms and systems for big data search and analytics\n* Machine learning for big data and based on big data\n* Predictive analytics and simulation\n* Visualization systems for big data\n* Knowledge extraction, discovery, analysis, and presentation\n\nIntegrated and Distributed Systems:Â \n\n* Sensor networks\n* Internet of Things (IoT)\n* Networking and protocols\n* Smart Systems (e.g., energy efficiency systems, smart homes, smart farms)\n\nBig Data Platforms and Technologies:Â \n\n* Concurrent and scalable big data platforms\n* Data indexing, cleaning, transformation, and curation technologies\n* Big data processing frameworks and technologies\n* Development methods and tools for big data applications\n* Quality evaluation, reliability, and availability of big data systems\n* Open-source development for big data\n* Big Data as a Service (BDaaS) platforms and technologies\n\nBig Data Foundations:\n\n* Theoretical and computational models for big data\n* Programming models, theories, and algorithms for big data\n* Standards, protocols, and quality assurance for big data\n\nBig Data Applications and Experiences:\n\n* Innovative applications in healthcare, finance, transportation, education, security, urban planning, disaster management, and more\n* Case studies and real-world implementations of big data systems\n* Large-scale industrial and academic applications\n\nAll papers must beÂ submittedÂ through:Â [https://easychair.org/my/conference?conf=bigdataservice2025](https://easychair.org/my/conference?conf=bigdataservice2025)Â \n\n**Important Dates:**Â \n\n* Abstract Submission Deadline: April 15, 2025Â \n* Paper Submission Deadline: April 25, 2025Â \n* Final Paper and Registration: June 15, 2025Â \n* Conference Dates: July 21-24, 2025Â \n\nFor more details, please visit the conference website:Â [https://conf.researchr.org/track/cisose-2025/bigdataservice-2025](https://conf.researchr.org/track/cisose-2025/bigdataservice-2025#Call-for-Papers)Â \n\nWe look forward to your submissions and contributions. Please feel free to share this CFP with interested colleagues.Â \n\nBest regards,\n\nIEEEÂ BigDataServiceÂ 2025 Organizing Committee",
    "author": "khushi-20",
    "timestamp": "2025-02-28T22:22:20",
    "url": "https://reddit.com/r/bigdata/comments/1j0s5qa/call_for_papers_ieee_big_data_service_2025/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j0wgee",
    "title": "CERTIFIED DATA SCIENCE PROFESSIONAL (CDSPâ„¢)",
    "content": "Advance Your Career with USDSI's Certified Data Science Professional (CDSP) Certification! Master Data Mining, Machine Learning, and Business Analytics through our self-paced program, designed for flexibility and comprehensive learning Join a global network of certified professionals and propel your career to new heights Get Certified. \n\nhttps://preview.redd.it/hxafi2sg02me1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=c7f951ba9cbcdfd4345bea012d85ce2aa2cc047e\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-03-01T03:18:46",
    "url": "https://reddit.com/r/bigdata/comments/1j0wgee/certified_data_science_professional_cdsp/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j05ml1",
    "title": "What new technologies should I follow?",
    "content": "I have about 2 years of experience working on bigdata, have worked mostly only on kafka and clickhouse. What new technologies can I add to my arsenal of big data tools. Also wanted an opinion as to if kafka is actually a popular tool or not in the industry or if it's just popular in my company",
    "author": "CraftyEcho",
    "timestamp": "2025-02-28T04:33:19",
    "url": "https://reddit.com/r/bigdata/comments/1j05ml1/what_new_technologies_should_i_follow/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j02hav",
    "title": "Coursera Plus annual and Monthly subscription 40%off Last two days",
    "content": "",
    "author": "[deleted]",
    "timestamp": "2025-02-28T00:44:48",
    "url": "https://reddit.com/r/bigdata/comments/1j02hav/coursera_plus_annual_and_monthly_subscription/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1j01c7x",
    "title": "Curious about tracking new VC investments for B2B insights? Here's a method to find verified decision-maker contacts!",
    "content": "",
    "author": "Due-Cod-346",
    "timestamp": "2025-02-27T23:23:01",
    "url": "https://reddit.com/r/bigdata/comments/1j01c7x/curious_about_tracking_new_vc_investments_for_b2b/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1izm7mr",
    "title": "AITECH VPN: Decentralized, Secure, and Private Internet Access",
    "content": "\n\nhttps://preview.redd.it/fpxr1xtz1qle1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=12418841b75003822aba3e368e5d80c03a260c40\n\n  Today, one of our biggest concerns as internet users is privacy and security. Although traditional Virtual Private Networks (VPNs) have partially provided a solution to this issue, they cannot provide complete anonymity and an uncensored internet experience due to their centralized structures. u/AITECH uses blockchain technology with its new product AITECH VPN and offers an innovative solution to these problems. For those curious about AITECH IO, you can view all the information including the renewed whitepaper here. Let's continue. With its decentralized structure, NFT-based subscription system and compliance with Web3 security protocols, it provides users with true anonymity, complete security and unlimited internet access. So how will AITECH VPN offer us this?\n\nÂ \n\nNFT-Based Subscription System\n\nAITECH VPN leaves traditional subscription models behind and comes up with an NFT-based system. Users will have NFT to access AITECH VPN. In this way, they will have easy internet access from anywhere they want. They will be free from the central control mechanisms of traditional VPNs. Thanks to an independent VPN subscription, they will not face any problems such as account closures etc. in the future. they will eliminate the risks.\n\nÂ \n\nTrue Anonymity\n\nWhile traditional VPNs usually require an email and password, AITECH VPN works with a Web3-based authentication system. In other words, you do not need to enter any personal information when creating an account. Thus, data leaks, monitoring and security vulnerabilities are prevented.\n\nÂ \n\nMore than 30 Global Server Locations\n\nAITECH VPN offers a fast and uninterrupted internet experience from anywhere in the world with more than 30 optimized servers located on different continents. In this way, you can access the content you want without losing your connection to the outside world even in censored regions.\n\nÂ \n\nWeb3-Grade Security\n\nThanks to blockchain-based security protocols, AITECH VPN users are provided with maximum protection against surveillance, cyber attacks and data breaches. Thanks to its decentralized structure, your data is not stored on a single server and it is not possible for any authority to access it.\n\nÂ \n\nWhy Should You Use AITECH VPN?\n\nAs we progress step by step towards decentralization in the blockchain world, we can use VPN without giving our personal information to anyone. We can use the internet all around the world without being stuck with constantly changing geographical or political restrictions. With AITECH IO technology, we can provide fast and secure connections on high-performance servers. Finally, thanks to its decentralization, we can use it comfortably.\n\nFor more details\n\n[https://docs.aitech.io/products/virtual-private-network](https://docs.aitech.io/products/virtual-private-network)\n\nÂ \n\nAITECH VPN wants to provide its users with a free experience with decentralized technologies that shape the future of the internet. If you wish, you can check the conditions required for a secure internet experience here and register early.\n\n[https://docs.aitech.io/products/virtual-private-network#register-your-interest-now](https://docs.aitech.io/products/virtual-private-network#register-your-interest-now)\n\nBinance Source: [https://www.binance.com/en/square/post/20883222547242](https://www.binance.com/en/square/post/20883222547242)\n\nÂ \n\nThank you",
    "author": "babayaro33",
    "timestamp": "2025-02-27T11:06:58",
    "url": "https://reddit.com/r/bigdata/comments/1izm7mr/aitech_vpn_decentralized_secure_and_private/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1izma4t",
    "title": "Connect Tableau to PowerPoint &amp; Google Slides then automatically generate recurring reports like client reports, monthly reports, QBRs, and financial reports with Rollstack",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-02-27T11:09:51",
    "url": "https://reddit.com/r/bigdata/comments/1izma4t/connect_tableau_to_powerpoint_google_slides_then/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1izkxt4",
    "title": "Last week at ViVE, we hosted a session with Relevate Health's Decision Science &amp; Analytics Lead, VP, Scott Clair, PhD. During the session, we did a deep dive into healthcare data reporting with automation and AI. Today, we're pleased to share the accompanying case study. [Download on LinkedIn]",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-02-27T10:14:45",
    "url": "https://reddit.com/r/bigdata/comments/1izkxt4/last_week_at_vive_we_hosted_a_session_with/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1izpue7",
    "title": "How useful is palantir foundry for fresher who is aspiring to be data scientist/ ML engineer",
    "content": "",
    "author": "BillionaireTitan",
    "timestamp": "2025-02-27T13:38:22",
    "url": "https://reddit.com/r/bigdata/comments/1izpue7/how_useful_is_palantir_foundry_for_fresher_who_is/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1izeli9",
    "title": "Top 5 shifts Reshaping Data Science",
    "content": "AI Revolution 2025: The Future of Data Science is Here! From automated decision-making to ethical AI, the [data science landscape](https://www.usdsi.org/data-science-insights/ai-revolution-2025-the-top-5-shifts-shaping-data-science-destiny) is transforming rapidly. Discover the Top 5 AI-driven shifts that will redefine industries and shape the future.\n\nhttps://preview.redd.it/7i1jyv7qdole1.jpg?width=1000&amp;format=pjpg&amp;auto=webp&amp;s=60553810e5f7042bfabaf9712304ad9f1e893add\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-02-27T05:28:39",
    "url": "https://reddit.com/r/bigdata/comments/1izeli9/top_5_shifts_reshaping_data_science/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iz2nfq",
    "title": "Need help with product name grouping for price comparison website (500k products)",
    "content": "\nI'm working on a website that compares prices for products from different local stores. I have a database of 500k products, including names, images, prices, etc. The problem I'm facing is with search functionality. Because product names vary slightly between stores, I'm struggling to group similar products together.\nI'm currently using PostgreSQL with full-text search, but I can't seem to reliably group products by name. For example, \"Apple iPhone 13 128GB\" might be listed as \"iPhone 13 128GB Apple\" or \"Apple iPhone 13 (128GB)\" or \"Apple iPhone 13 PRO case\" in different stores.\nI've been trying different methods for a week now, but I haven't found a solution. Does anyone have experience with this type of problem? What are some effective strategies for grouping similar product names in a large dataset? Any advice or pointers would be greatly appreciated!!\n",
    "author": "Mali5k",
    "timestamp": "2025-02-26T17:18:42",
    "url": "https://reddit.com/r/bigdata/comments/1iz2nfq/need_help_with_product_name_grouping_for_price/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iz1rnx",
    "title": "Exploring the Impact: Using Data on Newly Funded Startups to Boost Sales",
    "content": "",
    "author": "FairInvite2237",
    "timestamp": "2025-02-26T16:37:15",
    "url": "https://reddit.com/r/bigdata/comments/1iz1rnx/exploring_the_impact_using_data_on_newly_funded/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iyuf2l",
    "title": "Tableau vs. Power BI: âš”ï¸ Clash of the Analytics Titans",
    "content": "",
    "author": "askoshbetter",
    "timestamp": "2025-02-26T11:26:17",
    "url": "https://reddit.com/r/bigdata/comments/1iyuf2l/tableau_vs_power_bi_clash_of_the_analytics_titans/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iy5ers",
    "title": "POI data",
    "content": "To those in real estate: How do you verify if a POI dataset is actually useful for site selection?",
    "author": "location_analytics_9",
    "timestamp": "2025-02-25T13:48:30",
    "url": "https://reddit.com/r/bigdata/comments/1iy5ers/poi_data/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iy4zjk",
    "title": "Free Webinar: Unlocking Global Namespace for Seamless Collaboration",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-02-25T13:31:00",
    "url": "https://reddit.com/r/bigdata/comments/1iy4zjk/free_webinar_unlocking_global_namespace_for/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ixfhgs",
    "title": "Automate and schedule recurring business reports with Rollstack",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-02-24T15:48:13",
    "url": "https://reddit.com/r/bigdata/comments/1ixfhgs/automate_and_schedule_recurring_business_reports/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ix738d",
    "title": "Exploring Real-Time Alerts: How to Spot Startups Right After Funding Rounds",
    "content": "",
    "author": "Plenty_Delivery_4488",
    "timestamp": "2025-02-24T10:07:52",
    "url": "https://reddit.com/r/bigdata/comments/1ix738d/exploring_realtime_alerts_how_to_spot_startups/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iwx46y",
    "title": "CERTIFIED SENIOR DATA SCIENTIST (CSDSâ„¢) BY USDSIÂ®",
    "content": "Elevate your data science career with CSDS by USDSIÂ® Become a leader in the field with advanced skills in data analytics and machine learning. Earn a globally recognized Certification and drive impactful business decisions. Start your journey today and unlock new career opportunities!\n\nhttps://preview.redd.it/3d22nmahp1le1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=e66d0c8778900864f383bb0116e61f810f514b73\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-02-24T01:13:04",
    "url": "https://reddit.com/r/bigdata/comments/1iwx46y/certified_senior_data_scientist_csds_by_usdsi/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iwvtkg",
    "title": "Advice on Bigdata stack",
    "content": "Hello everyone,\n\nI'm new to the world of big data and could use some advice. I'm a DevOps engineer, and my team tasked me with creating a streamlined big data pipeline. We previously used ArangoDB, but it couldnâ€™t handle our 10K RPS requirements. To address this, I built a stack using Kafka, Flink, and Ignite. However, given my limited experience in some areas, there might be inaccuracies in my approach.\n\nAfter poc, we achieved low latency, but I'm now exploring alternative solutions. The developers need to execute queries using JDBC and SQL, which rules out using Redis. Iâ€™m considering the following alternatives:\n\n* **Azure Event Hubs with Flink on VM or Stream Analytics**\n* **Replacing Ignite with Azure SQL Database (In-Memory OLTP)**\n\nWhat do you recommend? Am I missing any key aspects to provide the best solution to this challenge?",
    "author": "lev-13",
    "timestamp": "2025-02-23T23:43:01",
    "url": "https://reddit.com/r/bigdata/comments/1iwvtkg/advice_on_bigdata_stack/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ivnk0t",
    "title": "Pyspark data validation",
    "content": "I'm a data product owner where we create Hadoop tables for our analytics teams to use.  All of our data is monthly processing which has +100 billion rows per table.  As a product owner, I'm responsible in validating the changes our tech team produces and sign off.  Currently, I just write pyspark sql in notebooks using machine learning studio.  This can be a pretty time consuming task in writing sql and executing.  Mainly I end up doing row by row / field to field compares for Production-Test environment for regression testing and ensure what the tech team did is correct.\n\nJust wondering if there is a better way to be doing this or if there's some python package that can be utilized.",
    "author": "corndevil",
    "timestamp": "2025-02-22T10:10:04",
    "url": "https://reddit.com/r/bigdata/comments/1ivnk0t/pyspark_data_validation/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ivbd9f",
    "title": "Hey, I just updated my tool to include international VC rounds and decision-maker contact infoâ€”perfect for anyone in global sales. Let me know if you want to check out a demo!",
    "content": "",
    "author": "Local_Passenger5009",
    "timestamp": "2025-02-21T22:17:55",
    "url": "https://reddit.com/r/bigdata/comments/1ivbd9f/hey_i_just_updated_my_tool_to_include/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iuk6sx",
    "title": "Apache Fury Serialization Framework 0.10.0 released: 2X smaller size for map serialization",
    "content": "",
    "author": "Shawn-Yang25",
    "timestamp": "2025-02-20T23:10:03",
    "url": "https://reddit.com/r/bigdata/comments/1iuk6sx/apache_fury_serialization_framework_0100_released/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1itwn20",
    "title": "Big Data",
    "content": "I am working with big data, approx 50GBs of data collected and stored on databricks each day for last 3 years from a machine in manufacturing plant. 100k Machines send sensor signal data every minute to server but no ECU log. Each machine has ECU that store faults happened in that machine in ECUlog which can only be read by manually connecting external diagnostic device by repairman.\n\nFilteration process should be based on following steps.\n\n* In ECUlog we get diagnosis date and Env data of that machine with fault occured in past few days, we only get diagnosis date, cycle number when diagnosis taken and first cycle number when fault registered for very first time by ECU.\n   * For eg.: machine\\_id, fault\\_ids, diag\\_date, cycle\\_num, Env\\_values and first\\_cycle\\_num where first\\_cycle\\_num &lt; cycle\\_num\n* We need to identify the fault\\_date when fault is registered for very first time by ECU based on first cycle number of machine. So that we can get the sensor data before this first fault occurence in machine to find root cause of fault and its propogation.\n\nWe have more than 5000 of ECUlog readouts for different machines and faults. We have to do it for each log readout. What is best way to analyse and filter such big data?",
    "author": "Reasonable-Spray7334",
    "timestamp": "2025-02-20T04:46:21",
    "url": "https://reddit.com/r/bigdata/comments/1itwn20/big_data/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1it0xjg",
    "title": "THE DATA SCIENCE REVOLUTION PAST PRESENT &amp; BEYOND",
    "content": "Step into the [future of data science](https://www.usdsi.org/data-science-insights/future-of-data-science-10-predictions-you-should-know)! Explore a journey that began with the pioneers of probability and evolved into todayâ€™s dynamic world of AI, big data, and immersive visualizations. As we blend ethics with innovation and cybersecurity with machine learning, the next chapter in data science is here. Embrace change, lead the revolution, and transform your career.\n\nhttps://preview.redd.it/pj5xzq8b62ke1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=d676f22aae7eb7a0a274e3139e34a321df30a67b\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-02-19T01:43:34",
    "url": "https://reddit.com/r/bigdata/comments/1it0xjg/the_data_science_revolution_past_present_beyond/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1isfozf",
    "title": "25 Best AI Agent Platforms to Use in 2025",
    "content": "",
    "author": "Veerans",
    "timestamp": "2025-02-18T08:47:18",
    "url": "https://reddit.com/r/bigdata/comments/1isfozf/25_best_ai_agent_platforms_to_use_in_2025/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1isd7k1",
    "title": "Duda acerca de dÃ³nde estudiar un MÃ¡ster en Data Science o BIG DATA",
    "content": "Estoy evaluando dos programas de posgrado en EspaÃ±a: elÂ **MÃ¡ster en Big Data Analytics de la UC3M**Â y elÂ **MÃ¡ster en Data Science de la Universidad Pontificia de Madrid (UPM)**. Me interesa conocer experiencias de alumni o estudiantes actuales para resolver dudas como:\n\nÂ¿El enfoque teÃ³rico-prÃ¡ctico es equilibrado?\n\nÂ¿CÃ³mo es la conexiÃ³n real con empresas?\n\nÂ¿Vale la pena la inversiÃ³n segÃºn los resultados?\n\nChat GPT me dio esta conclusiÃ³n:  \n**UC3M**: PrÃ¡ctica ligada aÂ **tecnologÃ­a puntera**Â (cloud, IA Ã©tica) y empresas globales. Proyectos mÃ¡s tÃ©cnicos (ej: despliegue de modelos en AWS).\n\n**UPM**: Proyectos suelen centrarse enÂ **sectores locales**Â (ej: retail espaÃ±ol) y uso de herramientas mÃ¡s accesibles (Excel, Power BI). Menor profundidad en ingenierÃ­a de datos.\n\nAgradecerÃ­a cualquier aporte o recomendaciÃ³n.  \nTambiÃ©n podrÃ­a evaluar otras Universidades",
    "author": "Glad-Willow-6138",
    "timestamp": "2025-02-18T06:56:02",
    "url": "https://reddit.com/r/bigdata/comments/1isd7k1/duda_acerca_de_dÃ³nde_estudiar_un_mÃ¡ster_en_data/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iscjug",
    "title": "Selling to startups that just got funded has never been easierâ€”think of it as connecting with fresh prospects who are ready to invest in solid business services. This database makes it simple to find the right contacts!",
    "content": "",
    "author": "DifficultyNo7953",
    "timestamp": "2025-02-18T06:22:58",
    "url": "https://reddit.com/r/bigdata/comments/1iscjug/selling_to_startups_that_just_got_funded_has/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1is0z95",
    "title": "A tool that can simplify and extract data for you - AI scan and summarization",
    "content": "Just finished an app using latest AI model.\n\n[https://apps.apple.com/us/app/insightsscan/id6740463241](https://apps.apple.com/us/app/insightsscan/id6740463241)\n\nI've been working on ios development on and off for around four years. Published a few apps including games, music player, and tools. This is the app I feel most excited when working on it.\n\nIt's an app that uses AI running locally on your phone to explain and summarize texts from images. No need for an internet. Everything stays on your device. Super safe. You can use your camera to capture an image in real time, or select from your photos.\n\nI tried a lot with it myself, scan my mails, scan item labels while shopping. It's pretty fun.\n\nI hope it can provide some value to people and make life a bit easier.\n\nPlease try it out and let me know your thoughts.\n\nOne user recently asked why the app is 1.2G in size and I want to hear what you think.\n\nI chose to include the model itself in this app. It would definitely make the app much size much smaller if I chose to let users download the model after installing this app. I thought about it then decided not to, as the goal for this app is it can be used without internet and I want to keep everything in just one step - download it and you are good to go.\n\nhttps://reddit.com/link/1is0z95/video/6objn2wxwsje1/player\n\n",
    "author": "WeddingWest6062",
    "timestamp": "2025-02-17T18:34:44",
    "url": "https://reddit.com/r/bigdata/comments/1is0z95/a_tool_that_can_simplify_and_extract_data_for_you/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1irpql3",
    "title": "Big Data Book Recommendations for industry?",
    "content": "Hey,\n\nI am looking for some big data book recommendations for industry. \n\nI am starting an internship this summer at a big tech company (not going to disclose exact company, but I think they probably own one of the  top 20 biggest data centers) working on their big data team. I'd like to get some books to read so I'm knowledgable on these topics before starting the internship to help secure RO. \n\nAre there any books that are specifically good for industry? I was thinking the \"Designing Data Intensive Applications\" and \"Enterprise Big Data Lakes\" as two good starting points, but now I see that they have an Apache Iceberg and Data Architecture book. What books (2-4 books) would be most practical to industry and modern practices? ",
    "author": "Acceptable_Safety212",
    "timestamp": "2025-02-17T10:44:58",
    "url": "https://reddit.com/r/bigdata/comments/1irpql3/big_data_book_recommendations_for_industry/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1irf0tt",
    "title": "BUILD A FUTURE-PROOF CAREER IN DATA SCIENCE",
    "content": "At USDSIÂ®, we empower industry leaders to harness data science for strategic impact. What we stand for: in data-driven decision-making. Ethical leadership in an evolving landscape. Building global networks of change-makers. Join us and be part of a community redefining the future of data science. \n\nhttps://preview.redd.it/u4tcxp9zqnje1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=915f15adbc23eaa55a2a4400461b016594f5a110\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-02-17T01:12:20",
    "url": "https://reddit.com/r/bigdata/comments/1irf0tt/build_a_futureproof_career_in_data_science/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iq63bu",
    "title": "Sources to learn NLP and logic in shortest possible time",
    "content": "what to know the best ways and overview",
    "author": "Numerous_Plan_2652",
    "timestamp": "2025-02-15T10:14:43",
    "url": "https://reddit.com/r/bigdata/comments/1iq63bu/sources_to_learn_nlp_and_logic_in_shortest/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ipvwd9",
    "title": "Master Advanced Data Science Leadership Skills",
    "content": "Become a [Certified Lead Data Scientist (CLDS)](https://www.usdsi.org/data-science-certifications/certified-lead-data-scientistb) by USDSI and position yourself as a leader in the world of data science. Master advanced skills in AI, machine learning, and big data to solve complex business problems and drive impactful insights. Unlock high-paying career opportunities and establish yourself as a data science expert!\n\nhttps://preview.redd.it/ha7ve8s749je1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=abcbfd0f4511634550c136d3d63b8621711b696e\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-02-14T23:59:51",
    "url": "https://reddit.com/r/bigdata/comments/1ipvwd9/master_advanced_data_science_leadership_skills/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ipzjfx",
    "title": "Hey fellow bigdata fans, ever wonder who just raised money? I recently stumbled on a tool that shows every funding round and even the decision makers â€“ it's been super handy for my B2B pitches!",
    "content": "",
    "author": "Recent_Shop_1862",
    "timestamp": "2025-02-15T04:35:30",
    "url": "https://reddit.com/r/bigdata/comments/1ipzjfx/hey_fellow_bigdata_fans_ever_wonder_who_just/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ipbuyp",
    "title": "Thoughts on this comment? Curious to hear more thoughts about this comment referencing the relationship between maximizing GPU performance and climate change and the effects it can have.",
    "content": "",
    "author": "hammerspace-inc",
    "timestamp": "2025-02-14T07:32:58",
    "url": "https://reddit.com/r/bigdata/comments/1ipbuyp/thoughts_on_this_comment_curious_to_hear_more/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ipkdse",
    "title": "Data processing and filtering from common crawl",
    "content": "Hey, I'm working on processing and extracting high quality training data from common crawl (10TB+). We have already tried using HuggingFace datatrove on our HPC with great success. The thing is fatatrove stores every in parquet or jsonl... but every step in the pipeline like adding some metadata requires duplicating the data with the added changes. And hence we are looking for a database solution with data processing engine to power our pipeline.\n\nI did some research and was convinced with Hbase+PySpark, since with Hbase we can change the scheme of the columns without requiring a full reminder like in cassandra. But I also read that doing a scan over all the database is slow. And I don't know if this will slowdown our data processing.\n\nWhat are your thoughts and what do you recommend?\n\nThank you!",
    "author": "wisscool",
    "timestamp": "2025-02-14T13:39:27",
    "url": "https://reddit.com/r/bigdata/comments/1ipkdse/data_processing_and_filtering_from_common_crawl/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iphm08",
    "title": "Faster health data analysis with MotherDuck &amp; Preswald",
    "content": "",
    "author": "Amrutha-Structured",
    "timestamp": "2025-02-14T11:41:12",
    "url": "https://reddit.com/r/bigdata/comments/1iphm08/faster_health_data_analysis_with_motherduck/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ipgg9m",
    "title": "I've been using this tool that tracks companies right after they get new funding and even gives you decision-maker detailsâ€”it's really helped me fine-tune my B2B outreach. Thought you might find it as handy as I do!",
    "content": "",
    "author": "BatUnhappy6231",
    "timestamp": "2025-02-14T10:52:20",
    "url": "https://reddit.com/r/bigdata/comments/1ipgg9m/ive_been_using_this_tool_that_tracks_companies/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ip5js8",
    "title": "Ever thought about selling to startups right after they secure funding? I came across a tool that flags fresh funding rounds and even shows key contactsâ€”it really helped me tap into the right opportunities. Might be something to check out if you're looking into this space!",
    "content": "",
    "author": "Content-Age-3583",
    "timestamp": "2025-02-14T00:47:43",
    "url": "https://reddit.com/r/bigdata/comments/1ip5js8/ever_thought_about_selling_to_startups_right/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1inz6lm",
    "title": "Hey everyone, I experimented with reaching out to startups that just raised VC money and it worked wondersâ€”managed to bump my MRR by $5k in a month! If you're curious about a subtle growth hack, give this approach a look.",
    "content": "",
    "author": "Used_Business_919",
    "timestamp": "2025-02-12T12:19:47",
    "url": "https://reddit.com/r/bigdata/comments/1inz6lm/hey_everyone_i_experimented_with_reaching_out_to/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ins2dx",
    "title": "What is your preference for AI storage?",
    "content": "Hello! Curious to hear thoughts on this: Do you use File or Object storage for your AI storage? Or both? Why?",
    "author": "hammerspace-inc",
    "timestamp": "2025-02-12T07:29:15",
    "url": "https://reddit.com/r/bigdata/comments/1ins2dx/what_is_your_preference_for_ai_storage/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1imw0qc",
    "title": "DATA SCIENCE+ AI BUSINESS EVOLUTION",
    "content": "The future of business is data-driven and AI-powered! Discover how the lines between data science and AI are blurringâ€”empowering enterprises to boost model accuracy, reduce time-to-market, and gain a competitive edge. From personalized entertainment recommendations to scalable data engineering solutions, innovative organizations are harnessing this fusion to transform decision-making and drive growth. Ready to lead your business into a smarter era? Letâ€™s embrace the power of data science and AI together.\n\nhttps://preview.redd.it/iuvhar5arhie1.jpg?width=850&amp;format=pjpg&amp;auto=webp&amp;s=030b62cb745a4010fc709a6105c9255e7217c864\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-02-11T03:59:13",
    "url": "https://reddit.com/r/bigdata/comments/1imw0qc/data_science_ai_business_evolution/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1imviou",
    "title": "Why Do So Many B2B Contact Lists Have Outdated Info?",
    "content": "I recently downloaded a B2B contact list from a â€œreliableâ€ source, only to find that nearly **30% of the contacts were outdated**â€”wrong emails, people who left the company, or even businesses that no longer exist.\n\nThis got me thinking:  \nâ“ Why is keeping B2B data accurate such a struggle?  \nâ“ Whatâ€™s the worst experience youâ€™ve had with bad data?\n\nIâ€™d love to hear your thoughtsâ€”especially if youâ€™ve found smart ways to **keep your contact lists clean and updated**.",
    "author": "DBrokerXK",
    "timestamp": "2025-02-11T03:23:34",
    "url": "https://reddit.com/r/bigdata/comments/1imviou/why_do_so_many_b2b_contact_lists_have_outdated/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ilqmmp",
    "title": "Ever wonder who's really controlling the budget? I stumbled upon a tool that neatly lays out every new VC investment with decision maker detailsâ€”pretty interesting if you ask me.",
    "content": "",
    "author": "Objective-Pick-2833",
    "timestamp": "2025-02-09T15:06:50",
    "url": "https://reddit.com/r/bigdata/comments/1ilqmmp/ever_wonder_whos_really_controlling_the_budget_i/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ijnczy",
    "title": "Why You Should Learn Hadoop Before Spark: A Data Engineer's Perspective",
    "content": "Hey fellow data enthusiasts! ğŸ‘‹ I wanted to share my thoughts on a learning path that's worked really well for me and could help others starting their big data journey.\n\n**TL;DR**: Learning Hadoop (specifically MapReduce) before Spark gives you a stronger foundation in distributed computing concepts and makes learning Spark significantly easier.\n\n# The Case for Starting with Hadoop\n\nWhen I first started learning big data technologies, I was tempted to jump straight into Spark because it's newer and faster. However, starting with Hadoop MapReduce turned out to be incredibly valuable. Here's why:\n\n1. **Core Concepts**: MapReduce forces you to think in terms of distributed computing from the ground up. You learn about:\n   * How data is split across nodes\n   * The mechanics of parallel processing\n   * What happens during shuffling and reducing\n   * How distributed systems handle failures\n2. **Architectural Understanding**: Hadoop's architecture is more explicit and \"closer to the metal.\" You can see exactly:\n   * How HDFS works\n   * What happens during each stage of processing\n   * How job tracking and resource management work\n   * How data locality affects performance\n3. **Appreciation for Spark**: Once you understand MapReduce's limitations, you'll better appreciate why Spark was created and how it solves these problems. You'll understand:\n   * Why in-memory processing is revolutionary\n   * How DAGs improve upon MapReduce's rigid model\n   * Why RDDs were designed the way they were\n\n# The Learning Curve\n\nYes, Hadoop MapReduce is more verbose and slower to develop with. But that verbosity helps you understand what's happening under the hood. When you later move to Spark, you'll find that:\n\n* Spark's abstractions make more sense\n* The optimization techniques are more intuitive\n* Debugging is easier because you understand the fundamentals\n* You can better predict how your code will perform\n\n# My Recommended Path\n\n1. Start with Hadoop basics (2-3 weeks):\n   * HDFS architecture\n   * Basic MapReduce concepts\n   * Write a few basic MapReduce jobs\n2. Build some MapReduce applications (3-4 weeks):\n   * Word count (the \"Hello World\" of MapReduce)\n   * Log analysis\n   * Simple join operations\n   * Custom partitioners and combiners\n3. Then move to Spark (4-6 weeks):\n   * Start with RDD operations\n   * Move to DataFrame/Dataset APIs\n   * Learn Spark SQL\n   * Explore Spark Streaming\n\nWould love to hear others' experiences with this learning path. Did you start with Hadoop or jump straight into Spark? How did it work out for you?",
    "author": "codervibes",
    "timestamp": "2025-02-06T22:03:16",
    "url": "https://reddit.com/r/bigdata/comments/1ijnczy/why_you_should_learn_hadoop_before_spark_a_data/",
    "score": 20,
    "num_comments": 6,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ijy76x",
    "title": "Free AI-based data visualization tool for BigQuery",
    "content": "Hi everyone!  \nI would like to share with you a tool that allows you to talk to your BigQuery data, and generate charts, tables and dashboards in a chatbot interface, incredibly straightforward!\n\nIt uses the latest models like O3-mini or Gemini 2.0 PRO  \nYou can check it hereÂ [https://dataki.ai/](https://dataki.ai/)  \nAnd it is completely free :)",
    "author": "fgatti",
    "timestamp": "2025-02-07T08:55:33",
    "url": "https://reddit.com/r/bigdata/comments/1ijy76x/free_aibased_data_visualization_tool_for_bigquery/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ijprfy",
    "title": "ğŸ“Œ Step-by-Step Learning Plan for Distributed Computing",
    "content": "# 1ï¸âƒ£ Foundation (Before Jumping into Distributed Systems) (Week 1-2)\n\nâœ… **Operating Systems Basics** â€“ Process management, multithreading, memory management  \nâœ… **Computer Networks** â€“ TCP/IP, HTTP, WebSockets, Load Balancers  \nâœ… **Data Structures &amp; Algorithms** â€“ Hashing, Graphs, Trees (very important for distributed computing)  \nâœ… **Database Basics** â€“ SQL vs NoSQL, Transactions, Indexing\n\nğŸ‘‰ Yeh basics strong hone ke baad distributed computing ka real fun start hota hai!\n\n# 2ï¸âƒ£ Core Distributed Systems Concepts (Week 3-4)\n\nâœ… **What is Distributed Computing?**  \nâœ… **CAP Theorem** â€“ Consistency, Availability, Partition Tolerance  \nâœ… **Distributed System Models** â€“ Client-Server, Peer-to-Peer  \nâœ… **Consensus Algorithms** â€“ Paxos, Raft  \nâœ… **Eventual Consistency vs Strong Consistency**\n\n# 3ï¸âƒ£ Distributed Storage &amp; Data Processing (Week 5-6)\n\nâœ… **Distributed Databases** â€“ Cassandra, MongoDB, DynamoDB  \nâœ… **Distributed File Systems** â€“ HDFS, Ceph  \nâœ… **Batch Processing** â€“ Hadoop MapReduce, Spark  \nâœ… **Stream Processing** â€“ Kafka, Flink, Spark Streaming\n\n# 4ï¸âƒ£ Scalability &amp; Performance Optimization (Week 7-8)\n\nâœ… **Load Balancing &amp; Fault Tolerance**  \nâœ… **Distributed Caching** â€“ Redis, Memcached  \nâœ… **Message Queues** â€“ RabbitMQ, Kafka  \nâœ… **Containerization &amp; Orchestration** â€“ Docker, Kubernetes\n\n# 5ï¸âƒ£ Hands-on &amp; Real-World Applications (Week 9-10)\n\nğŸ’» Build a distributed system project (e.g., real-time analytics with Kafka &amp; Spark)  \nğŸ’» Deploy microservices with Kubernetes  \nğŸ’» Design large-scale system architectures",
    "author": "codervibes",
    "timestamp": "2025-02-07T00:42:59",
    "url": "https://reddit.com/r/bigdata/comments/1ijprfy/stepbystep_learning_plan_for_distributed_computing/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iivuzb",
    "title": "Data Architecture Complexity",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-02-05T23:02:51",
    "url": "https://reddit.com/r/bigdata/comments/1iivuzb/data_architecture_complexity/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iivkx1",
    "title": "Hey bigdata folks, I just discovered you can now export verified decision-maker emails from every VC-funded startupâ€”itâ€™s a cool way to track companies with fresh capital. Curious to see how it works?",
    "content": "",
    "author": "Legal-Dust9609",
    "timestamp": "2025-02-05T22:44:51",
    "url": "https://reddit.com/r/bigdata/comments/1iivkx1/hey_bigdata_folks_i_just_discovered_you_can_now/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ii3uzd",
    "title": "Create Hive Table (Hands On) with all Complex Datatype",
    "content": "",
    "author": "bigdataengineer4life",
    "timestamp": "2025-02-04T23:26:09",
    "url": "https://reddit.com/r/bigdata/comments/1ii3uzd/create_hive_table_hands_on_with_all_complex/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ihis4y",
    "title": "IT hiring and salary trends in Europe (18'000 jobs, 68'000 surveys)",
    "content": "Like every year, weâ€™ve compiled a report on the European IT job market.\n\nWe analyzed **18'000+ IT job offers** and surveyed **68'000 tech professionals** to reveal insights on salaries, hiring trends, remote work, and AIâ€™s impact.\n\nNo paywalls, just raw PDF: [https://static.devitjobs.com/market-reports/European-Transparent-IT-Job-Market-Report-2024.pdf](https://static.devitjobs.com/market-reports/European-Transparent-IT-Job-Market-Report-2024.pdf)",
    "author": "One-Durian2205",
    "timestamp": "2025-02-04T07:24:11",
    "url": "https://reddit.com/r/bigdata/comments/1ihis4y/it_hiring_and_salary_trends_in_europe_18000_jobs/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ihe01u",
    "title": "WANT TO CREATE POWERFUL INTERACTIVE DATA VISUALIZATIONS?",
    "content": "Unlock the power of interactive data visualization with D3.js! From complex datasets to visually engaging graphics, D3.js makes it possible to craft dynamic, user-friendly visual experiences. Want to level up your data visualization skills? Check out our latest blog!\n\nhttps://preview.redd.it/qhp4ilhpd3he1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=3b0aff6efc969face1357816ab08a276658afdcc\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-02-04T02:34:05",
    "url": "https://reddit.com/r/bigdata/comments/1ihe01u/want_to_create_powerful_interactive_data/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1igv63a",
    "title": "[Community Poll] Is your org's investment in Business Intelligence SaaS going up or down in 2025?",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-02-03T10:52:56",
    "url": "https://reddit.com/r/bigdata/comments/1igv63a/community_poll_is_your_orgs_investment_in/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iguzty",
    "title": "Big data explanations?",
    "content": "hey , does anyone knows resources for big data course or anyone that explains the course in detail? (especially Cambridge slides) iâ€™m lost ",
    "author": "Raghadlil",
    "timestamp": "2025-02-03T10:46:00",
    "url": "https://reddit.com/r/bigdata/comments/1iguzty/big_data_explanations/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1igkgpt",
    "title": "7 Real-World Examples of How Brands Are Using Big Data Analytics",
    "content": "",
    "author": "Veerans",
    "timestamp": "2025-02-03T01:23:03",
    "url": "https://reddit.com/r/bigdata/comments/1igkgpt/7_realworld_examples_of_how_brands_are_using_big/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ifa0si",
    "title": "Crash Course on Developing AI Applications with LangChain",
    "content": "",
    "author": "AMDataLake",
    "timestamp": "2025-02-01T09:29:25",
    "url": "https://reddit.com/r/bigdata/comments/1ifa0si/crash_course_on_developing_ai_applications_with/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1if0nn3",
    "title": "Best Big Data Courses on Udemy for Beginners to advanced",
    "content": "",
    "author": "[deleted]",
    "timestamp": "2025-01-31T23:51:15",
    "url": "https://reddit.com/r/bigdata/comments/1if0nn3/best_big_data_courses_on_udemy_for_beginners_to/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iek4hg",
    "title": "The Numbers behind Uber's Big Data Stack",
    "content": "I thought this would be interesting to the audience here.\n\nUber is well known for its scale in the industry.\n\nHere are the latest numbers I compiled from a plethora of official sources:\n\n* **Apache Kafka**:\n  * 138 million messages a second\n  * 89GB/s (7.7 Petabytes a day)\n  * 38 clusters\n* **Apache Pinot**:\n  * 170k+ peak queries per second\n  * 1m+ events a second\n  * 800+ nodes\n* **Apache Flink**:\n  * 4000 jobs processing 75 GB/s\n* **Presto**:\n  * 500k+ queries a day\n  * reading 90PB a day\n  * 12k nodes over 20 clusters\n* **Apache Spark**:\n  * 400k+ apps ran every day\n  * 10k+ nodes that use **&gt;95%** of analyticsâ€™ compute resources in Uber\n  * processing hundreds of **petabytes** a day\n* **HDFS**:\n  * Exabytes of data\n  * 150k peak requests per second\n  * tens of clusters, 11k+ nodes\n* **Apache Hive**:\n  * 2 million queries a day\n  * 500k+ tables\n\nThey leverage a Lambda Architecture that separates it into two stacks - a real time infrastructure and batch infrastructure.\n\nPresto is then used to bridge the gap between both, allowing users to write SQL to query and join data across all stores, as well as even create and deploy jobs to production!\n\nA lot of thought has been put behind this data infrastructure, particularly driven by their complex requirements which grow in opposite directions:\n\n1. **Scaling Data**Â - total incoming data volume is growingÂ **at an exponential rate**Replication factor &amp; several geo regions copy data.Canâ€™t afford to regress on data freshness, e2e latency &amp; availability while growing.\n2. **Scaling Use Cases**Â - new use cases arise from various verticals &amp; groups, each with competing requirements.\n3. **Scaling Users**Â - the diverse users fall on a big spectrum of technical skills. (some none, some a lot)\n\nI have covered more about Uber's infra, including **use cases for each technology**, [in my 2-minute-read newsletter](https://2minutestreaming.beehiiv.com/p/uber-data-infrastructure-scale-numbers) where I concisely write interesting Big Data content.",
    "author": "2minutestreaming",
    "timestamp": "2025-01-31T10:28:23",
    "url": "https://reddit.com/r/bigdata/comments/1iek4hg/the_numbers_behind_ubers_big_data_stack/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1idrbo1",
    "title": "[Community Poll] Which BI Platform will you use most in 2025?",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-01-30T09:48:23",
    "url": "https://reddit.com/r/bigdata/comments/1idrbo1/community_poll_which_bi_platform_will_you_use/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1idrbni",
    "title": "[Community Poll] Which BI Platform will you use most in 2025?",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-01-30T09:48:22",
    "url": "https://reddit.com/r/bigdata/comments/1idrbni/community_poll_which_bi_platform_will_you_use/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1idr4y5",
    "title": "[Community Poll] Are you actively using AI for business intelligence tasks?",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-01-30T09:40:22",
    "url": "https://reddit.com/r/bigdata/comments/1idr4y5/community_poll_are_you_actively_using_ai_for/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1idr4yd",
    "title": "[Community Poll] Are you actively using AI for business intelligence tasks?",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-01-30T09:40:22",
    "url": "https://reddit.com/r/bigdata/comments/1idr4yd/community_poll_are_you_actively_using_ai_for/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1id04c8",
    "title": "ğŸ¤” ğ—œğ˜€ ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ˜ƒğ—² ğ—”ğ—œ ğ—´ğ—¼ğ—¶ğ—»ğ—´ ğ˜ğ—¼ ğ˜ğ—®ğ—¸ğ—² ğ—¼ğ˜ƒğ—²ğ—¿ ğ— ğ—Ÿ ğ—¼ğ—¿ ğ——ğ—®ğ˜ğ—® ğ—¦ğ—°ğ—¶ğ—²ğ—»ğ—°ğ—² ğ—·ğ—¼ğ—¯s?",
    "content": "I donâ€™t think so. Instead, itâ€™s here to **free** data scientist and ML engineers ğ—³ğ—¿ğ—¼ğ—º ğ˜ğ—²ğ—±ğ—¶ğ—¼ğ˜‚ğ˜€, ğ—¿ğ—²ğ—½ğ—²ğ˜ğ—¶ğ˜ğ—¶ğ˜ƒğ—² ğ˜ğ—®ğ˜€ğ—¸ğ˜€â€”so you can focus on higher-value work like ğ—¯ğ˜‚ğ—¶ğ—¹ğ—±ğ—¶ğ—»ğ—´ ğ—¯ğ—²ğ˜ğ˜ğ—²ğ—¿ ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ˜€, ğ˜‚ğ—»ğ—°ğ—¼ğ˜ƒğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ—¶ğ—»ğ˜€ğ—¶ğ—´ğ—µğ˜ğ˜€ ğ—³ğ—¿ğ—¼ğ—º ğ˜‚ğ—»ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—²ğ—± ğ—±ğ—®ğ˜ğ—® ğ—³ğ—®ğ˜€ğ˜ğ—²ğ—¿, ğ—®ğ—»ğ—± ğ—±ğ—¿ğ—¶ğ˜ƒğ—¶ğ—»ğ—´ ğ—ºğ—¼ğ—¿ğ—² ğ—¶ğ—ºğ—½ğ—®ğ—°ğ˜ ğ—³ğ—¼ğ—¿ ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—¼ğ—¿ğ—´ ğ—®ğ—»ğ—± ğ—°ğ˜‚ğ˜€ğ˜ğ—¼ğ—ºğ—²ğ—¿ğ˜€.\n\nCheck out this Medium article on how **Google**, **Teradata**, and **Gemini** are transforming enterprise data workflows and insights with Generative AI:\n\nğŸ”—https://medium.com/google-cloud/how-generative-ai-transforms-enterprise-data-insights-with-google-gemini-and-teradata-382b7e274af8\n\nWould love to hear your thoughtsâ€”ğ—µğ—¼ğ˜„ ğ—±ğ—¼ ğ˜†ğ—¼ğ˜‚ ğ˜€ğ—²ğ—² ğ—šğ—²ğ—»ğ—”ğ—œ ğ˜€ğ—µğ—®ğ—½ğ—¶ğ—»ğ—´ ğ˜ğ—µğ—² ğ—³ğ˜‚ğ˜ğ˜‚ğ—¿ğ—² ğ—¼ğ—³ ğ—±ğ—®ğ˜ğ—® ğ˜€ğ—°ğ—¶ğ—²ğ—»ğ—°ğ—² ğ—®ğ—»ğ—± ğ— ğ—Ÿ? ğŸ‘‡",
    "author": "JanethL",
    "timestamp": "2025-01-29T11:02:25",
    "url": "https://reddit.com/r/bigdata/comments/1id04c8/ğ—œğ˜€_ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ˜ƒğ—²_ğ—”ğ—œ_ğ—´ğ—¼ğ—¶ğ—»ğ—´_ğ˜ğ—¼_ğ˜ğ—®ğ—¸ğ—²_ğ—¼ğ˜ƒğ—²ğ—¿_ğ— ğ—Ÿ_ğ—¼ğ—¿_ğ——ğ—®ğ˜ğ—®/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1icrktx",
    "title": "Basic Components That Make Up Data Science",
    "content": "The data science domain is huge and if you want to make a career in data science, then you need to be aware of the various components that make up this widely used technology including data, programming languages, machine learning, and more. \n\nhttps://preview.redd.it/gfspzpv83xfe1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=ec335095a186381acd3502d4e3cf46d66e233a68\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-01-29T04:20:48",
    "url": "https://reddit.com/r/bigdata/comments/1icrktx/basic_components_that_make_up_data_science/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1icq8a1",
    "title": "Hey everyone! I just found an amazing way to total B2B leads: hit up the recently funded startups! You can grab decision maker contact info super quick right after each funding round. If youâ€™re curious, I can share a demo! Letâ€™s connect!",
    "content": "",
    "author": "Working-Union-3630",
    "timestamp": "2025-01-29T02:42:54",
    "url": "https://reddit.com/r/bigdata/comments/1icq8a1/hey_everyone_i_just_found_an_amazing_way_to_total/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1iclhvv",
    "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
    "content": "",
    "author": "Loose-Ad3323",
    "timestamp": "2025-01-28T21:16:02",
    "url": "https://reddit.com/r/bigdata/comments/1iclhvv/efficiently_modeling_long_sequences_with/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ic4ww3",
    "title": "Best cert. for entry into big data field",
    "content": "As I've described. I'm looking to see what would be the best certification for entry into big data field. I'm currently working as IT Auditor and hope to use that as a stepping stone.",
    "author": "[deleted]",
    "timestamp": "2025-01-28T09:04:26",
    "url": "https://reddit.com/r/bigdata/comments/1ic4ww3/best_cert_for_entry_into_big_data_field/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ic8qmm",
    "title": "[Poll - LinkedIn] Which BI platform will you use most in 2025?",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-01-28T11:41:05",
    "url": "https://reddit.com/r/bigdata/comments/1ic8qmm/poll_linkedin_which_bi_platform_will_you_use_most/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ic8qlw",
    "title": "[Poll - LinkedIn] Which BI platform will you use most in 2025?",
    "content": "",
    "author": "Rollstack",
    "timestamp": "2025-01-28T11:41:04",
    "url": "https://reddit.com/r/bigdata/comments/1ic8qlw/poll_linkedin_which_bi_platform_will_you_use_most/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ic6u3x",
    "title": "Hey, youâ€™re in sales? Youâ€™ve got to check out this tool that tracks companies that just got funding! It even highlights who's calling the shots. It honestly makes targeting leads way easier. Just give it a spin, itâ€™s free!",
    "content": "",
    "author": "Mean_Stock_8736",
    "timestamp": "2025-01-28T10:23:38",
    "url": "https://reddit.com/r/bigdata/comments/1ic6u3x/hey_youre_in_sales_youve_got_to_check_out_this/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ibzo83",
    "title": "HOW TO BUILD YOUR ORGANIZATION DATA MATURE?",
    "content": "Take your organization from data exploring to #data transformed with this comprehensive guide to data maturity. Discover the four key elements that determine data maturity and how to develop a data-driven culture within your organization. Start your journey to #datatransformation with this insightful guide. Become USDSIÂ® Certified to lead your team in creating a data-driven culture. \n\nhttps://reddit.com/link/1ibzo83/video/n0p2wzn02qfe1/player\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-01-28T04:41:17",
    "url": "https://reddit.com/r/bigdata/comments/1ibzo83/how_to_build_your_organization_data_mature/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ibjgrs",
    "title": "Where Can we buy B2B Data?I found Infobelpro to be the best so far but  checking!",
    "content": "",
    "author": "DBrokerXK",
    "timestamp": "2025-01-27T13:41:07",
    "url": "https://reddit.com/r/bigdata/comments/1ibjgrs/where_can_we_buy_b2b_datai_found_infobelpro_to_be/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i9xlg4",
    "title": "You Need to Know About RWA Inc and RWAI",
    "content": "This week, RWA Inc. dropped some incredible updates! The platform, which makes investment opportunities more accessible by tokenizing real-world assets, is bridging the gap between traditional finance and decentralized technology. And the Launchpad platform is at the heart of it all. Launchpad simplifies the process of launching new projects, raising capital, and tokenization, making it way easier for both entrepreneurs and investors.\n\n# What is RWAI, and What Does It Do?\n\nRWAI, short for **Research, Reporting, and Launch AI Agent**, is an AI tool developed by RWA Inc. Its main goal? To make the research, reporting, and launch processes for projects faster and easier. In short, itâ€™s a helpful companion for both project creators and investors. Here's what RWAI brings to the table:\n\n* It analyzes project details thoroughly and provides users with clear and precise information.\n* It speeds up launch processes, enabling projects to get started quickly.\n* It fosters greater engagement within the community, making the platform more vibrant and dynamic.\n\nRWAIâ€™s roadmap includes some standout features:\n\n1. **Research and Reporting Module:** Allows users to analyze projects in detail and make well-informed investment decisions.\n2. **Launch Module:** Optimizes the launch process so both project owners and investors can actively participate in the most effective way possible.\n3. **Community Engagement:** Offers tools and activities to ensure the community is more involved in projects.\n\nRWAI truly aims to provide a practical and seamless experience for its users.\n\n# The Benefits of Staking\n\nStaking $RWA tokens on the RWA Inc. platform offers users a range of perks that go beyond just earning rewards. Hereâ€™s what you get:\n\n* **Platform Access and Level Upgrades:** By staking your $RWA tokens, you unlock access to various platform features. The more you stake, the higher your tier level, which comes with extra benefits.\n* **Access to Investment Opportunities:** Staking users gain access to unique investment options on the platform, such as real estate-backed tokens, private equity, and commodities. Regularly check the platform for new opportunities!\n* **Participation in Token Sales:** Be part of upcoming token sales and exclusive crowdfunding events. Enjoy guaranteed allocation rounds and \"first-come, first-serve\" opportunities.\n* **Rewards and Exclusive Privileges:** As you stake more, you earn Tier Points that unlock additional rewards and special privileges.\n\nStaking is more than just passive incomeâ€”itâ€™s your gateway to investment opportunities and active participation in the ecosystem.\n\n# DAO Labs and the First ILO Success\n\nDAO Labs hosted its first-ever ILO (Initial Labor Offering) for RWA Inc. on its platform, and it was a massive success! As social miners, we had a front-row seat to witness this milestone. This launch clearly showcased DAO Labs' community-focused vision.\n\nThrough this process, we saw just how impactful community-driven projects can be. DAO Labs has set a strong example for future project launches and has become a solid reference point for the community.",
    "author": "Fahim61891012",
    "timestamp": "2025-01-25T14:30:38",
    "url": "https://reddit.com/r/bigdata/comments/1i9xlg4/you_need_to_know_about_rwa_inc_and_rwai/",
    "score": 6,
    "num_comments": 1,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1ia7fn6",
    "title": "I aspire to advance my career in Big Data",
    "content": "Hi everyone,\n\nI graduated in 2022 and currently have 2.5 years of experience in the big data domain. Most of my work involves developing complex Spark-Scala-based procedures and functions tailored to client requirements. I also have some experience with Bash scripting to create reconciliation scripts, as we primarily store data in Hive databases.\n\nThe tools and technologies I am proficient in include:\n\nApache Spark,Kafka,Hadoop,Hive,HBase\nScala programming,MS SQL,Bitbucket\n,IntelliJ,Git,Python\n\nAlthough my team also works on Power BI report generation, I haven't had direct exposure to it yet.\n\nI enjoy working in this domain and am eager to expand my knowledge for better career opportunities and growth. Which additional tools or technologies should I learn, or in which of my current skills should I deepen my expertise, to advance my career in big data?\n",
    "author": "Calm_Yogurt_7560",
    "timestamp": "2025-01-25T23:05:38",
    "url": "https://reddit.com/r/bigdata/comments/1ia7fn6/i_aspire_to_advance_my_career_in_big_data/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i9ncgg",
    "title": "For those who love Spark and big data performance, checkout our weekly substack!",
    "content": "Hey all!\n\nWeâ€™ve launched a Substack calledÂ **Big Data Performance**, where weâ€™re publishing weekly posts on all things big data and performance.\n\nThe idea is to share practical tips, and not just fluff.\n\nThis is a community-driven effort by a few of us passionate about big data. If that sounds interesting, check it out and consider subscribing:If you work with Spark or other big data tools, this might be right up your alley.\n\nSo far, weâ€™ve covered:\n\n* **Making Spark jobs more readable**: Best practices to write cleaner, maintainable code.\n* **Scaling ML inference with Spark**: Tips on inference at scale and optimizing workflows.\n\nThis is a community-driven effort by a few of us passionate about big data. If that sounds interesting, check it out and consider subscribing:  \nğŸ‘‰Â [Big Data Performance Substack](https://bigdataperformance.substack.com/)\n\nWeâ€™d love to hear your feedback or ideas for topics to cover next.\n\nCheers!",
    "author": "Vegetable_Home",
    "timestamp": "2025-01-25T06:51:19",
    "url": "https://reddit.com/r/bigdata/comments/1i9ncgg/for_those_who_love_spark_and_big_data_performance/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i97if9",
    "title": "January Product Updates from BI Report Generation Platform, Rollstack",
    "content": "Hi data nerds,\n\nHere are the latest updates from [Rollstack](https://www.rollstack.com)â€”a platform designed to connect your favorite BI tools (Power BI, Tableau, Looker, Metabase, and Google Sheets) to your presentation software for automatic report generation. If youâ€™re juggling QBRs, client reports, or departmental updates, you might find something here that simplifies your routine.\n\n**January 2025 Updates**\n\n**Power BI Integration Has Arrived**\n\nBy rolling out this feature, PBI teams now gain access to the same AI-driven reporting that Tableau, Looker, and Metabase have offered since our early daysâ€”cutting tens of thousands of hours from report generation. Want to explore further or book a demo?\n\nLearn more and schedule a demo:  [Power BI integration](https://www.rollstack.com/integrations/power-bi)! \n\n**AI Insights Are Now Open to All**\n\nRollstack AI is now open to everyone, giving business professionals an advanced way to generate customized, relevant insights within their presentations and documents. Teams can edit slide commentaries, titles, and moreâ€”while preserving the deckâ€™s structureâ€”so stakeholders can reach decisions faster and with greater clarity.\n\nLearn more: [AI insights and native charts](https://www.rollstack.com/articles/ai-insights-and-native-charts-for-business-intelligence-report-generation)\n\n**Native Charts for PowerPoint and Google Slides**\n\nIf youâ€™d rather use PowerPoint or Google Slides charts instead of those in your BI tool, you can now convert them into fully editable versions in your presentation software. They include an accompanying spreadsheet, letting you take a closer look at the source data whenever you need.\n\nCheck it out: [AI insights and native charts article](https://www.rollstack.com/articles/ai-insights-and-native-charts-for-business-intelligence-report-generation)\n\n# How to Access These Features\n\n* If you already have a Rollstack account, you can find everything you need in [the app](https://app.rollstack.com/) or message us on Slack/Teams.\n* If Rollstack is new to you, [book a quick tour](https://www.rollstack.com/try-rollstack-automated-storytelling-with-data-ai) to explore AI insights and native charts in action.\n\nThanks for reading, and we hope youâ€™ll explore these new Rollstack features. If you have any questions, let us know in the comments. Your feedback genuinely helps us shape what comes next!\n\n*â€”Team Rollstack*",
    "author": "Rollstack",
    "timestamp": "2025-01-24T15:11:18",
    "url": "https://reddit.com/r/bigdata/comments/1i97if9/january_product_updates_from_bi_report_generation/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i9jr5w",
    "title": "AI Data Scientists The Game Changer You Need to Know",
    "content": "The Rise of the AI Data Scientist! AI Data Scientists are leading the way in transforming raw data into powerful insights. Their expertise in both AI and data science is creating ground breaking solutions across industries. Ready to become part of this exciting evolution?\n\nhttps://preview.redd.it/fw16nweh44fe1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=4b00756f53a4389fc2ba992921319324759d9470\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-01-25T02:56:07",
    "url": "https://reddit.com/r/bigdata/comments/1i9jr5w/ai_data_scientists_the_game_changer_you_need_to/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i945ki",
    "title": "Hey friends, if you're looking for a solid tactics to level up your biz, check this out: target startups that just Wildly landed VC funds! I mean, the growth potential is awesome! I pulled in an extra $5k MRR in just one month using this approach. It really does workâ€”you wonâ€™t believe how simple it",
    "content": "",
    "author": "Ok-Psychology-6350",
    "timestamp": "2025-01-24T12:46:48",
    "url": "https://reddit.com/r/bigdata/comments/1i945ki/hey_friends_if_youre_looking_for_a_solid_tactics/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i926uf",
    "title": "Whatâ€™s the best open-source tool for fast PostgreSQL reporting (Docker-friendly and responsive)?",
    "content": "Hey everyone!\n\nIâ€™m working with a 22GB PostgreSQL database (Bitnami/PostgreSQL:16.2.0) and need to generate quick reports, such as linking patients to specific types of consultations.\n\nIâ€™m looking for an **open-source tool**, preferably Docker-ready, that allows me to:\n\n1. Create reports with graphs that look great on both mobile devices and TVs.\n2. Publish visualizations either publicly or privately (with login/password).\n3. Integrate via API (if possible, for easier automation).\n\nI need something easy to use, especially for someone comfortable writing SQL queries in PostgreSQL. Whatâ€™s new in the market thatâ€™s simple yet powerful?\n\nThanks a lot! ğŸ™Œ",
    "author": "hermesalvesbr",
    "timestamp": "2025-01-24T11:24:00",
    "url": "https://reddit.com/r/bigdata/comments/1i926uf/whats_the_best_opensource_tool_for_fast/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i8qh58",
    "title": "Learn how to implement leader election and failover using Zookeeper, .NET Core, and Docker. This article demonstrates building a distributed system with automatic leader election, handling failures gracefully to ensure high availability and fault tolerance.",
    "content": "",
    "author": "Vasilkosturski",
    "timestamp": "2025-01-24T00:57:25",
    "url": "https://reddit.com/r/bigdata/comments/1i8qh58/learn_how_to_implement_leader_election_and/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i7z1gd",
    "title": "How does HDFS write work?",
    "content": "Hi all \nI have always wondered how Hadoop executes Hdfs put and get command for writing and reading distributed data seamlessly though it involves complex processes in the background. Hence this blog, expecting constructive feedback and criticism :)",
    "author": "DazzlingTelevision52",
    "timestamp": "2025-01-23T01:40:05",
    "url": "https://reddit.com/r/bigdata/comments/1i7z1gd/how_does_hdfs_write_work/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i79nxh",
    "title": "AI DATA SCIENTIST- A NEW CLASS OF SPECIALIST ROUTINE",
    "content": "Gain an insight into the life of a **data science** professional as you understand the top skills needed including **data labeling, AI, and machine learning**. Read now!\n\nhttps://preview.redd.it/cuc9iyzedjee1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=56c83cdee2d1e155d2191ece9711b1e57e505de9\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-01-22T05:08:40",
    "url": "https://reddit.com/r/bigdata/comments/1i79nxh/ai_data_scientist_a_new_class_of_specialist/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i62y89",
    "title": "Hey friends! Have you heard about this awesome tool for business analysts in the VC scene? It streams live data on all the startups that scored VC funding globally, with loads of historical info! If you're curious or want to try it out, just drop a comment!",
    "content": "",
    "author": "PotentialCorrect4342",
    "timestamp": "2025-01-20T15:23:01",
    "url": "https://reddit.com/r/bigdata/comments/1i62y89/hey_friends_have_you_heard_about_this_awesome/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i5ohu9",
    "title": "Explore How Python is Revolutionizing Healthcare Technology",
    "content": "Python is transforming healthcare technology, and here's why itâ€™s the perfect fit! From data analysis to machine learning, Python is the go-to language for healthcare innovation. Curious to see how itâ€™s changing the game?\n\nhttps://preview.redd.it/543rf79n05ee1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=d89e25b31da4989119ea82ff055be98d70feae81\n\n",
    "author": "sharmaniti437",
    "timestamp": "2025-01-20T04:52:04",
    "url": "https://reddit.com/r/bigdata/comments/1i5ohu9/explore_how_python_is_revolutionizing_healthcare/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i5693h",
    "title": "Hey friends, youâ€™ve got to check out this amazing tool that tracks VC investments in real-time! ğŸŒğŸ’¸ Itâ€™s super useful for seeing which companies are getting funding and even offers detailed insights into industries and key players. A fantastic resource if you're diving into the VC world!",
    "content": "",
    "author": "Limp_Hyena_244",
    "timestamp": "2025-01-19T12:10:56",
    "url": "https://reddit.com/r/bigdata/comments/1i5693h/hey_friends_youve_got_to_check_out_this_amazing/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i4vwb3",
    "title": "Hey friends, if you're curious about the VC world, I just found this amazing live investment tracker that shows all the VC funding happening globally! It's super insightful for data analysis on companies and decision makers. A game-changer if you're looking to learn the ins and outs of venture capit",
    "content": "",
    "author": "LumpyCandidate3032",
    "timestamp": "2025-01-19T03:47:58",
    "url": "https://reddit.com/r/bigdata/comments/1i4vwb3/hey_friends_if_youre_curious_about_the_vc_world_i/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i4j95u",
    "title": "Solidus AI Tech - Among Binance's Top 5 Alpha Projects!",
    "content": "Everything starts againÂ TRUMPÂ launching her own token before she becomes president andÂ BTCÂ is a good start and we should not forget artificial intelligence projects\n\nSolidus AI TechÂ u AITECHÂ has solidified its leadership in Web3 andÂ AIÂ innovations and gained the trust of global investors by being ranked among Binance's Top 5 Alpha Projects.\n\nhttps://preview.redd.it/30gbjjvswtde1.png?width=1750&amp;format=png&amp;auto=webp&amp;s=514c3218a7027b993cafefd80544c267f6c6999d\n\nWhy This Matters\n\nVisibility and Recognition:Â AITECH's recognition by Binance puts the project on the radar of global investors and increases investor confidence.\n\nAdoption and Growth: Such recognition can accelerate Solidus AI Tech's adoption and support growth in its ecosystem.\n\nLeadership: Being featured on a major platform like Binance helps Solidus AI Tech position itself as a leader inÂ Web3Â AI innovations.\n\nWhat's Next ?\n\nThis achievement increases the potential for Solidus AI Tech to attract more collaboration and investment in its future projects. The Solidus AI Tech community celebrates this significant milestone and looks forward to the future.",
    "author": "murat-calskan",
    "timestamp": "2025-01-18T15:31:14",
    "url": "https://reddit.com/r/bigdata/comments/1i4j95u/solidus_ai_tech_among_binances_top_5_alpha/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "bigdata",
    "post_id": "1i3dkwk",
    "title": "Cancer Immunotherapy &amp; Big Data/AI Technology",
    "content": "Cancer touches millions of lives, and the journey to better treatments is one we take together. On January 23rd, 2025, at 11:00 AM EDT / 09:30 PM IST, join us for a thought-provoking webinar, The Intersection of Cancer Immunotherapy &amp; Big Data/AI Technology. \n\nLink to Register: [https://www.senzmate.com/publish/webinar-7/](https://www.senzmate.com/publish/webinar-7/)",
    "author": "Icy-Distribution7231",
    "timestamp": "2025-01-17T03:38:20",
    "url": "https://reddit.com/r/bigdata/comments/1i3dkwk/cancer_immunotherapy_big_dataai_technology/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  }
]