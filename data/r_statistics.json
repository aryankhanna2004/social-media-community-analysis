[
  {
    "subreddit": "statistics",
    "post_id": "1ocg622",
    "title": "[career] What is the pathway to remote work? Stats major / cs minor",
    "content": "I got into stats/cs with the dream working remote, going back home to Puerto Rico, and helping rebuild my community. I am a 3rd year at FSU with good grades, but no internship and no work experience. What is the pathway to remote work? Is it realistic within 2-3 years? Thanks! ",
    "author": "EnvironmentOne6753",
    "timestamp": "2025-10-21T08:33:59",
    "url": "https://reddit.com/r/statistics/comments/1ocg622/career_what_is_the_pathway_to_remote_work_stats/",
    "score": 7,
    "num_comments": 4,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ocjob8",
    "title": "[question] how should I analyse repeated likert scale data?",
    "content": "I have a set of 1000 cases, each has been reviewed using a likert scale. (I also  have some cases duplicated to have inter rater agreement. But not worrying about that for now).\n\nHow can I analyse this and take into account the clustering on the reviewer? ",
    "author": "ManagementObvious631",
    "timestamp": "2025-10-21T10:46:09",
    "url": "https://reddit.com/r/statistics/comments/1ocjob8/question_how_should_i_analyse_repeated_likert/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ocq65k",
    "title": "[Question] Can someone help me answer a math question from my dream?",
    "content": "So this sounds stupid, but I dreamt this last night, woke up, and was very confused cuz I feel dumb. The following is a real interaction that I dreamt, and idk what to make of it.\n\nMy dream self was arguing with someone, and I said *\"dude the odds of winning that lottery are like 1 in a million\"* and the dream person I spoke to said* \"Actually, it's 50/50. You have a 1 in 2 chance. So it's 1 in 2\".*\n\nI said to the dream person *\"Well I wish! But we both know that's not true haha\".*\n\nAnd the dream person in the dream said *\"Well think about it: You get one chance to pick a number out of a million. That means 999,999 other numbers won't be picked\"*\n\nMe: *\"Right...?\"*\n\nThe dream person: *\"So If you didn't win and I ask the question 'did you win?', your response would be 'no', right?\"*\n\nMe: *\"Of course\".*\n\nThe dream person: *\"So imagine marking all of those 999,999 numbers with the word 'no'. Suddenly, if everything else is a 'no', then they can all just be considered one entity, or one real number\".*\n\nMe: *\"I guess...?\"*\n\nThe dream person: *\"That means the 1 in that 999,999 suddenly becomes a 'yes', which means despite it being small it technically has the same weight as the 'no', as there can only be a yes or no in this situation. \n\nSo 1 and a million odds is really just 50/50. You either got it or you didn't.\"*\n\nMe: *\"What the f-?!?!\"*\n\nSo yeah... basically I've been thinking about this all day. No I don't dream of anything remotely like this lol, I've just been trying to understand if thar logic makes sense. I myself didn't think of this deliberately - my conscienceness did üòÖ",
    "author": "JesseTheClassy",
    "timestamp": "2025-10-21T14:49:28",
    "url": "https://reddit.com/r/statistics/comments/1ocq65k/question_can_someone_help_me_answer_a_math/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ocist6",
    "title": "[Question] One-way ANOVA bs multiple t-tests",
    "content": "Something I am unclear about. If I run a One-Way ANOVA with three different levels on my IV and the result is significant, does that mean that at least one pairwise t-tests will be significant if I do not correct for multiple comparisons (assuming all else is equal)? And if the result is non-significant, does it follow that none of the pairwise t-tests will be significant?\n\nPut another way, is there a point to me doing a One-Way ANOVA with three different levels on my IV or should I just skip to the pairwise comparisons in that scenario? Does the one-way ANOVA, in and of itself, provide protection against Type 1 error?\n\nEdit: excuse the typo in the title, I meant ‚Äúvs‚Äù not ‚Äúbs‚Äù",
    "author": "ihateirony",
    "timestamp": "2025-10-21T10:13:30",
    "url": "https://reddit.com/r/statistics/comments/1ocist6/question_oneway_anova_bs_multiple_ttests/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1occx64",
    "title": "[Q] The impact of sample size variability on p-values",
    "content": "How big of an effect has sample size variability on p-values? Not sample-size itself, but its variability? This keeps bothering me, but let me lead with an example to explain what I have in mind.\n\nLet's say I'm doing a clinical trial having to do with leg amputations. Power calculation says I need to recruit 100 people. I start recruiting but of course it's not as easy as posting a survey on MTurk: I get patients when I get them. After a few months I'm at 99 when a bus accident occurs and a few promising patients propose to join the study at once. Who am I to refuse extra data points? So I have 108 patients and I stop recruitment.\n\nNow, due to rejections, one of them choking on an olive and another leaving for Tailand with their lover, I lose a few before the end of the experiment. When the dust settles I have 96 data points. I would have prefered more, but it's not too far from my initial requirements. I push on, make measurements, perform statistical analysis using NHST (say, a t-test with n=96) and get the holy p-value of 0.043 or something. No multiple testign or anything, I knew exactly what I wanted to test and I tested it (let's keep things simple).\n\nNow the problem: we tend to say that this p-value is the probability of observing data as extreme or more than what I observed in my study, but that's missing a few elements, namely all the assumptions that are baked into sampling and the tests etc. In particular, since the t-test assumes a fixed sample size (as required for the calculation), my p-value is \"the probability of observing data as extreme or more than what I observed in my study assuming n=97 assuming the NH is true\".\n\nIf someone wanted to reproduce my study however, even using the exact same recruitment rules, measurement techniques and statistical analysis, it is not guaranted that they'd have exactly 97 patients. So the p-value corresponding to \"the probability of observing data as extreme or more than what I observed in my study following the same methodology\" would be different from the one I computed which assumes n=97. The \"real\" p-value, the one that corresponds to actually reproducing the experiment as a whole, would probably be quite different from the one I computed following common practices as it should include the uncertainty on the sample size: differences in sample size obviously impact what result is observed, so the variability of the sample size should impact the probability of observing such result or more extreme.\n\nSo I guess my question is: how big of an effect would that be? I'm not really sure how to approach the problem of actually computing the more general p-value. Does it even make sense to worry about this different kind of p-value? It's clear that nobody seems to care about it, but is that because of tradition or because we truly don't care about the more general interpretation? I think that this generalized interpretation of \"if we were to redo the experiment we'd be that much likely to observe at least as extreme data\" is closer to intuition than the restricted form we compute in practice but maybe I'm wrong.\n\nWhat do you think?",
    "author": "cym13",
    "timestamp": "2025-10-21T06:26:27",
    "url": "https://reddit.com/r/statistics/comments/1occx64/q_the_impact_of_sample_size_variability_on_pvalues/",
    "score": 2,
    "num_comments": 11,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1oc93xx",
    "title": "Is it worth it to do a research project under an anti-bayesian if I want to go into bayesian statistics? [Q][R]",
    "content": "Long story short, for my undergraduate thesis I don't really have the opportunity to do bayesian stats, as there isn't a bayesian supervisor available.\n\nI am quite close and have developed a really good relationship with my professor, who unfortunately is a very vocal anti-bayesian.\n\nWould doing non-bayesian semiparametric research be beneficial for bayesian research later on? For example if I want to do my PhD using bayesian methods.\n\nTo be clear, since im at undergrad level the project is gonna be application-focused.",
    "author": "gaytwink70",
    "timestamp": "2025-10-21T03:15:57",
    "url": "https://reddit.com/r/statistics/comments/1oc93xx/is_it_worth_it_to_do_a_research_project_under_an/",
    "score": 3,
    "num_comments": 18,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1oc3rok",
    "title": "[Discussion] I wrote about the Sinkhorn-Knopp algorithm for Optimal Transport Problems. Let me know what you think",
    "content": "Sinkhorn-Knopp is an algorithm used to ensure the rows and columns of a matrix sum to 1, like in a probability distribution. It's an active area of research in Statistics. The interesting thing is it gets you probabilities, much like Softmax would.  \nHere's the [article](https://leetarxiv.substack.com/p/sinkhorn-knopp-algorithm-24d).",
    "author": "DataBaeBee",
    "timestamp": "2025-10-20T21:41:29",
    "url": "https://reddit.com/r/statistics/comments/1oc3rok/discussion_i_wrote_about_the_sinkhornknopp/",
    "score": 8,
    "num_comments": 3,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ocax12",
    "title": "[R] A simple PMF estimator on large supports",
    "content": "When working on various recommender systems, it always was weird to me that creating dashboards or doing feature engineering is hard with integer-valued features that are heavily tailed and have large support, such as # of monthly visits on a website, or # monthly purchases of a product.\n\n  \nSo I decided to do a one small step towards tackling the problem. I hope you find it useful:  \n[https://arxiv.org/abs/2510.15132](https://arxiv.org/abs/2510.15132)",
    "author": "alexsht1",
    "timestamp": "2025-10-21T04:56:18",
    "url": "https://reddit.com/r/statistics/comments/1ocax12/r_a_simple_pmf_estimator_on_large_supports/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1obxvdw",
    "title": "[Q] Binomial GLMM Model Pruning/Validation/Selection - How to find the \"best\" model?",
    "content": "As one part of my masters thesis, I'm attempting to model tree failure probability (binary- Unlikely/Elevated) vs. tree-level and site-level predictors; 3 separate models, one for each species. Unfortunately 3 stats classes in the past 2 years did not go into much depth on this topic. I originally had a 4-category response variable, but reduced to 2 due to low power/ # obs in some categories. So I originally started with ordinal CLMs/CLMMs (ordinal package) and ordinal BRMs (Bayesian regression models, brms package), but switched to GLMMs (glmmTMB) after moving to binary outcomes. As an example, here are 3 versions of the Douglas-fir model:\n\n    m_fail_PSME &lt;- clmm(\n      Fail.like ~ Built.Unbuilt + z_logDBH + z_CR + z_Mean_BAI_10 +\n        z_BA.m2.ha + z_SM_site + z_vpdmax + z_Architectural_sum + z_Physical_sum + \n        z_Biological_sum + (1 | Site),\n      data = psme_data, link = \"logit\", Hess = TRUE, na.action = na.omit)\n    b_ord_psme &lt;- brm(\n      Fail.like ~ Built.Unbuilt + z_logDBH + z_CR + z_Mean_BAI_10 +\n        z_BA.m2.ha + z_SM_site + z_vpdmax +\n        z_Architectural_sum + z_Physical_sum + z_Biological_sum + (1 | Site), data   = psme_data,  \n       family = cumulative(link = \"logit\"), chains = 4, iter = 2000, cores = 4, seed   = 2025)\n    m_risk_PSME &lt;- glmmTMB(\n      Fail.bin ~ Built.Unbuilt + z_logDBH + z_CR + z_logMean_BAI_10 +\n        z_BA.m2.ha + z_SM_site + z_vpdmax +\n        z_Architectural_sum + z_Physical_sum + z_Biological_sum + (1 | Site),\n      data   = psme_data, family = binomial(), REML   = FALSE)\n\nI've done linear mixed effects models to answer my other research questions and have a pretty solid understanding of how to find the \"best\" model with LMEs, but not with binomial GLMMs. Is the model selection process similar (e.g., drop 1, refit, check significance, check AIC, etc.)? Must you use DHARMa simulated residuals for diagnostics?\n\nAlso, what are the best tests/plots for reporting final results with this type of model?\n\nThanks",
    "author": "EndBrave3332",
    "timestamp": "2025-10-20T16:54:50",
    "url": "https://reddit.com/r/statistics/comments/1obxvdw/q_binomial_glmm_model_pruningvalidationselection/",
    "score": 9,
    "num_comments": 2,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1obwtqt",
    "title": "[Q] What is the expected value for the sum of random complex numbers?",
    "content": "Hi, ran across this problem which looks like it should have a relatively easy solution but I cant find it...\nWhat is the expected value for the sum of e^i(theta n) where theta n is a uniform random value 0 to 2pi? If n is large, it would be zero. That part is obvious. But if n is small, say 2, it would be 1. I can visualize the relationship (as n increases the expectation goes to 0) but cant describe the relationship mathematically.  Is there a proof or paper on this? Any help would be greatly appreciated.",
    "author": "stockBot9000",
    "timestamp": "2025-10-20T16:07:46",
    "url": "https://reddit.com/r/statistics/comments/1obwtqt/q_what_is_the_expected_value_for_the_sum_of/",
    "score": 3,
    "num_comments": 19,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1obxqyf",
    "title": "[Q] How do I interpret these confidence intervals?",
    "content": "I have two samples of a part (A and B) and am doing a test to failure on them. Part A has a failure rate of 3.6% with a 95% CI of \\[0.4%, 12.5%\\]. Part B has a failure rate of 16.5% with a 95% CI of \\[11.7%, 22.3%\\].\n\nThe null hypothesis is that the two parts are the same. My first instinct is to fail to reject the null hypothesis because the confidence intervals overlap. However, my second thought is it would take some incredibly bad luck to have the true failure rate of Part A at the top of its CI AND Part B to be at the bottom of its CI.\n\nWhich is the best interpretation of these results? Should I instead use a third option of a Student-T test but for binomial distributions?",
    "author": "Elandril_Alch",
    "timestamp": "2025-10-20T16:49:20",
    "url": "https://reddit.com/r/statistics/comments/1obxqyf/q_how_do_i_interpret_these_confidence_intervals/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1obtx78",
    "title": "[Q] What are some common pitfalls and errors when testing composite nulls?",
    "content": "Open question to the contrast of simple hypothesis to composite hypothesis testing.\n\nWhat are some common pitfalls and erros related to composite null testing you have seen or know about?\n\n",
    "author": "Stochastic_berserker",
    "timestamp": "2025-10-20T14:09:57",
    "url": "https://reddit.com/r/statistics/comments/1obtx78/q_what_are_some_common_pitfalls_and_errors_when/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1obc9wz",
    "title": "[Question] What specific questions and advantages does functional data analysis have over traditional methods, and when do you use it over said methods?",
    "content": "A while ago I asked in this subreddit about interpretable methods for time-series classification and was suggested to look into functional data analysis (FDA).  I've spent the past week looking into it and am still extremely confused about what advantages FDA has over other methods particularly when it comes to problems that can be modeled as being generated by some physical process.\n\nFor example, suppose I have some time-series data generated a combination of 100 sine functions. If I didn't know this in advance (which is the point of FDA), had limited, sparse, and noisy observations, and wanted to apply an FDA method to the problem, as far as I can tell, this is what I would do:\n\n1. Assume that the data is generated by some basis (fourier/b-splines/wavelets)\n2. Solve a system of equations to find out the coefficient of the basis functions\n\nThen, depending on my task:\n\n3. Apply functional PCA to figure out which one of those basis functions really affects the data.\n4. Using domain knowledge, interpret the principal components\n\nor \n\n3. Apply functional regression to answer questions like 'how does a patient's heart rate over a 24-hour period influence their blood pressure?'\n4. Use functional regression model to do....something that's better than what can be done with traditional methods\n\nOR\n\nsomething else that can supposedly be done better than traditional methods\n\n\nWhat I'm not understanding is why we'd use functional data analysis anywhere at all. The hard part (FPCA interpretation) is still left up to the domain expert and I believe it's just as hard as interpreting, for example, a deep learning model that performs equally well on the data. I also have some qualms about arbitrarily applying wavelets/fourier functions/splines as basis functions, rather arbitrarily. I know the point is that your generating process is smooth, but I'm still kind of unconvinced by why this is a better method at all. Could someone give me insight on the problem?",
    "author": "IllustriousPeanut509",
    "timestamp": "2025-10-19T23:04:22",
    "url": "https://reddit.com/r/statistics/comments/1obc9wz/question_what_specific_questions_and_advantages/",
    "score": 15,
    "num_comments": 7,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1obq6qz",
    "title": "[Q] Sampling within a defined Sample Size",
    "content": "Our Stats SME at the company recently left and we are trying to develop a sampling system for a different type of component that we receive from our suppliers. \n\n\n\n**For other components**: We inspect a pre-defined number of samples from the received lot, and that sample size is based on the risk involved and whether it is destructive or non-destructive testing. For example, we might receive a lot of 500 parts, select 30 samples from the lot, and measure a few dimensions on each sample. The dimensions that are measured are based on what are the most key characteristics to functionality. \n\n\n\n**For this component**: It is an instruction booklet with artwork/text inside. These are long and include several different languages, so we want to develop a method/sampling rationale to only inspect a few pages to make sure color, graphics, bleed-through, etc. all match the requirements. No page or requirement aspect is more key than the others. \n\n\n\n**Question**: How are samples of a sample usually incorporated into sampling plans? For example, if we receive a lot of 500 booklets, and each booklet has 250 pages, and our sampling requirement is n=30, how can that be broken up into how many pages per booklet we should inspect? Inspecting just 30 pages from 1 booklet or 5 pages across 6 booklets doesnt seem right, but all 250 pages from 30 booklets is also unreasonable. Is there some way to tie in a sampling plan to statistically understand \"if we sample x number of pages from each booklet, and x number of booklets from a lot, then the lot's probability of conformance is x% at 95% confidence\" or something like that?\n\n\n\nI'm a bit lost on where to even start so any guidance people can offer in terms of what inputs we need to understand first, or if there's a term for this type of method/calculation that I can look into, would be really great.",
    "author": "Archfielded",
    "timestamp": "2025-10-20T11:51:34",
    "url": "https://reddit.com/r/statistics/comments/1obq6qz/q_sampling_within_a_defined_sample_size/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1obfrzb",
    "title": "Thesis idea [Question]",
    "content": "\nHello everyone, I hope you are doing well...\nI am a financial maths master student and I have been figuring out ideas for my master's degree thesis.\nWhat i know for sure is that i want it to be mainly about time series forecasting (revenue most likely)\nAnd to make it more interesting i want to use garch to model volatility of residuals and then simulate this volatility with monte carlo, and to finish it up i would add the forecasted value from the best time series forecasting model at each point in time to the simulated residuals therefore i would pull out confidence intervals and VaR CVaR...etc\n\nThis is purely Theoretical but i'd love to get an expert opinion on the subject.\nHave a good day!",
    "author": "Dillon_37",
    "timestamp": "2025-10-20T03:15:46",
    "url": "https://reddit.com/r/statistics/comments/1obfrzb/thesis_idea_question/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1oawlvz",
    "title": "[Q] Struggling with stochastics",
    "content": "Hello,\n\nI have just started my master's in Statistical Science with a bachelor's in Sociology and one of the first mandatory modules we need to take is Stochastics. I am really struggling with all the notations and the general mathematical language as I have not learned anything of this sort in my bachelor's degree. I had several statistics courses but they were more applied statistics, we did not learn probability theory or measure theory at all. Do you think it's possible for me to catch up and understand the basics of stochastic analysis? I am really worried about my lack of prior understanding on this topic. I am trying to read some books but it still feels very foreign...",
    "author": "kertuotis",
    "timestamp": "2025-10-19T11:09:20",
    "url": "https://reddit.com/r/statistics/comments/1oawlvz/q_struggling_with_stochastics/",
    "score": 10,
    "num_comments": 18,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1oaujnl",
    "title": "[Question] Compiling vehicle accident (fatal, multi car collision, etc) stats for a specific interstate?",
    "content": "I have been seeing a lot of extremely horrific incidents on my local interstate (I-80/94 near Chicago) in the last few years. However in 2025 It's become weekly in my commute. It's extremely unsettling how HURT people are getting.\n\nThere is a large, continuous construction project we did not vote on (privatized). Roads go to extremely narrow corridors being heavily worked on in 10 mile sprints. Drivers are distracted so it's a mess. Semi's will swerve to avoid barriers and cause multi car crashes a few times a month.\n\n**After having to jump out of my car to help a woman who crushed her chest during a five car pile up, I decided I wanted to start looking into some data as a responsible citizen.**\n\nProblem is I can't find a governing body or source that tracks accidents on interstates over time. What the heck? Is there a reasonable way to compile this data?!?!? How do we figure out how safe these privatized interstates are???\n\n**TL:DR  Where can I find auto crash (fatal/severe injury) data for the I-80/94 interstate year by year????**\n\nThanks guys you're all so cool to me",
    "author": "lizarddan",
    "timestamp": "2025-10-19T09:48:57",
    "url": "https://reddit.com/r/statistics/comments/1oaujnl/question_compiling_vehicle_accident_fatal_multi/",
    "score": 8,
    "num_comments": 3,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1oaz7qx",
    "title": "[Q] Looking for StatXact User Manual PDF",
    "content": "Hey everyone!  \nDoes anyone happen to have a pdf copy of the user manual for StatXact? I‚Äôd really appreciate any version you can share, though the most recent edition would be ideal. I‚Äôve searched around but haven‚Äôt been able to find a proper PDF or online copy anywhere.\n\nThanks in advance!",
    "author": "7Caliostro7",
    "timestamp": "2025-10-19T12:50:08",
    "url": "https://reddit.com/r/statistics/comments/1oaz7qx/q_looking_for_statxact_user_manual_pdf/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1oaz9ff",
    "title": "Detecting Time Series peaks and troughs [Q]",
    "content": "Is there any algorithm which can do this for data like Stock Prices?",
    "author": "jenpalex",
    "timestamp": "2025-10-19T12:51:57",
    "url": "https://reddit.com/r/statistics/comments/1oaz9ff/detecting_time_series_peaks_and_troughs_q/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1oac8h3",
    "title": "[Question] Capturing peaks in time series forecast",
    "content": "I'm trying to forecast peak load with a time series model with exogenous variables (weather, some economic variables, month variables, weekday/weekend effects, etc). I'm using a python stats models SARIMAX model with some AR/MA terms but nothing beyond that, hoping that the inclusion of daily weather and some month/season indicators builds in most seasonal effects.\n\nI'm seeing a consistent pattern in my in sample residuals where peak load times (winter days in this instance) have a lot higher/more variable residuals than during base load times. I've tried engineering some different interaction terms/nonlinear weather effects without much change.\n\nI think the crux of the issue is that my model is fitting too much to the non-winter days, causing it to suffer accuracy in the peak load times. The stats models SARIMAX implementation seems to use MLE. I'm trying to find the most painless solution between modifying the objective function/weighting the data so that my model can be more accurate in capturing peaks. \n\nIf you have suggestions for other libraries/models (e.g I've considered WLS but haven't found much in the literature of it being used for this task) please let me know as well! \n\n\nThanks!",
    "author": "Yarn84llz",
    "timestamp": "2025-10-18T17:42:01",
    "url": "https://reddit.com/r/statistics/comments/1oac8h3/question_capturing_peaks_in_time_series_forecast/",
    "score": 9,
    "num_comments": 5,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1oa7mwa",
    "title": "[E] Applying to PhD programs in the US, how do I go about expressing research interests?",
    "content": "I‚Äôm applying to PhD programs from undergrad, and am really struggling with figuring out how to express what methods or sub fields I‚Äôm interested in, And to what level of detail are committees expecting?\n\nThe programs I am applying to are application and method focused, so most professors within the department do applied stats research. \n\nFor example, I‚Äôm interested in (broadly) uncertainty quantification/interpretable machine learning for scientific discovery in the fields of earth science and biology. \n\nI‚Äôm not sure if this is too specific/too broad for applications, because I don‚Äôt have any explicit experience in this. My research experiences are in these domains but not strictly technical/relevant. \n\nI could mention Bayesian neural networks or physics informed ML, which do seem interesting to me, but it seems very specific and I don‚Äôt want to try to speak on these technical things that I don‚Äôt really have any experience with. ",
    "author": "Voldemort57",
    "timestamp": "2025-10-18T14:15:44",
    "url": "https://reddit.com/r/statistics/comments/1oa7mwa/e_applying_to_phd_programs_in_the_us_how_do_i_go/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1oan1af",
    "title": "Is an applied statistics PhD less prestigious than a methodological/theoretical statistics PhD? [Q][R]",
    "content": "According to ChatGPT it is, but im not gonna take life advice from a robot.\n\nThe argument is that applied statisticians are consumers of methods while theoretical statisticians are producers of methods. The latter is more valuable not just because of its generalizability to wider fields, but just due to the fact that it is quantitavely more rigorous and complete, with emphasis on proofs and really understanding and showing how methods work. It is higher on the academic hierarchy basically.\n\nAlso another thing is I'm an international student who would need visa sponsorship after graduation. Methodological/thoeretical stats is strongly in the STEM field and shortage list for occupations while applied stats is usually not (it is in the social science category usually). \n\nI am asking specifically for academia by the way, I imagine applied stats does much better in industry.",
    "author": "gaytwink70",
    "timestamp": "2025-10-19T04:15:06",
    "url": "https://reddit.com/r/statistics/comments/1oan1af/is_an_applied_statistics_phd_less_prestigious/",
    "score": 0,
    "num_comments": 42,
    "upvote_ratio": 0.46,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1oacepm",
    "title": "[Question] Conjoint analysis problem with statistical power",
    "content": "We ran a conjoint experiment with 8 tasks across 1,300 respondents. Based on a pretty popular paper in our field, we ran the conjoint experiment with a randomized age variable in the conjoint, where the age could be any of the 26 integers. Rather than that, the other attributes shown across the tasks have at most 12 attributes (which is our main treatment).\n\n  \nOne of the reviewers of our paper said that this is a fatal problem since there are approximately 30,000 total scenarios but only about 20,800 were shown. The reviewer added that this age attribute resulted in too many empty cells.\n\nWhat do you all think? Can we argue, when calculating the statistical power, that the attribute with the most levels is 12 rather than 26?\n\nThank you!",
    "author": "opposity",
    "timestamp": "2025-10-18T17:50:42",
    "url": "https://reddit.com/r/statistics/comments/1oacepm/question_conjoint_analysis_problem_with/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1oa2i4j",
    "title": "[Software] For an app which is focused on tracking and logging personal metrics (or timed phenomenon) what could be some truly useful statistical measures?",
    "content": "I'm working on an app in which I log items, and then display them as graphs. This all started after my wife jokingly accused me of taking 1-hour long showers (not true!) - so I set out to prove her wrong [https://imgur.com/a/PihQc20](https://imgur.com/a/PihQc20)\n\nThen I realized that I could go quite far with this, by providing various types of trackers, and different ways of exporting the data out, to be further correlated with environmental or fitness data.\n\nFor example, I also track my subjective level of well-being, multiple times a day (which I intend to normalize) and determine correlations between when I feel the way I do, and how it is correlated to my other health metrics, such as RHR, HRV, Sleep, etc.\n\nMy question for the community is this: How can I make my correlations section more useful? Any advice? What are some items which would truly reveal meaningful insights that a person could use, day to day? (or perhaps, as an aid to something they already do, professionally)\n\n[https://imgur.com/a/aCeEljQ](https://imgur.com/a/aCeEljQ)\n\nüôè Thank you! Appreciate any guidance.\n\n",
    "author": "markraidc",
    "timestamp": "2025-10-18T10:57:10",
    "url": "https://reddit.com/r/statistics/comments/1oa2i4j/software_for_an_app_which_is_focused_on_tracking/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o9bkj6",
    "title": "Is bayesian nonparametrics the most mathematically demanding field of statistics? [Q]",
    "content": "",
    "author": "gaytwink70",
    "timestamp": "2025-10-17T13:01:00",
    "url": "https://reddit.com/r/statistics/comments/1o9bkj6/is_bayesian_nonparametrics_the_most/",
    "score": 92,
    "num_comments": 43,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o9extz",
    "title": "Variational Inference [Career]",
    "content": "Hey everyone. I'm an undergraduate statistics student with a strong interest in probability and Bayesian statistics. Lately, But lately, I‚Äôve been really enjoying studying nonlinear optimization applied to inverse problems. I‚Äôm considering pursuing a master‚Äôs focused on optimization methods (probably incremental gradient techniques) for solving variational inference problems, particularly in computerized tomography.\n\nDo you think this is a promising research topic, or is it somewhat outdated? Thanks!",
    "author": "[deleted]",
    "timestamp": "2025-10-17T15:17:17",
    "url": "https://reddit.com/r/statistics/comments/1o9extz/variational_inference_career/",
    "score": 25,
    "num_comments": 16,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o9yvrh",
    "title": "Confused about possible statistical error [Q]",
    "content": "So i got my reading test results back yesterday and spotted a little gem of an error there. It says that for reading attribute x i belong in the 45th percentile, meaning below average skill. However my score is higher than median score, My score 23/25, average 22.56/25. Is this even mathematically possible or what bc the math aint mathing to me. For context this is a digitally done reading comprehension test for highschool 1st years in finland\n\nEDIT: \nChanged median to average, mistranslation on my part",
    "author": "Brutustheman",
    "timestamp": "2025-10-18T08:32:25",
    "url": "https://reddit.com/r/statistics/comments/1o9yvrh/confused_about_possible_statistical_error_q/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o8wydb",
    "title": "[Question] Whats the best introductory book about Monte Carlo methods?",
    "content": "Im looking for a good book about Monte Carlo simulations. Everything I found so far only throws in a lot of imaginary problems that are solved by an abstract MC method. To my surprise they never talk about the cons and pros of the method, and especially about the accuracy, about how to find out how many iterations need to be done, how to tell if the simulation converged, etc. Im mainly interested in the latter question.\n\nThe closest thing I found so far to what Im looking for is this: [https://books.google.hu/books?id=Gr8jDwAAQBAJ&amp;printsec=copyright&amp;redir\\_esc=y#v=onepage&amp;q&amp;f=false](https://books.google.hu/books?id=Gr8jDwAAQBAJ&amp;printsec=copyright&amp;redir_esc=y#v=onepage&amp;q&amp;f=false)",
    "author": "Morpheus_the_fox",
    "timestamp": "2025-10-17T02:43:47",
    "url": "https://reddit.com/r/statistics/comments/1o8wydb/question_whats_the_best_introductory_book_about/",
    "score": 43,
    "num_comments": 10,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o96e6t",
    "title": "[Question] Will my method for sampling training data cause training bias?",
    "content": "I‚Äôm an actuary at a health insurance company and as a way to assist the underwriting process am working on a model to predict paid claims for employer groups in a future period. I need help determining if my training data is appropriate.\n\nI have 114 groups, they all have at least 100 members with an average of 700 members. I feel like I don‚Äôt have enough groups to create a robust model using a traditional training/testing data 70/30 split. So what I‚Äôve done is I disaggregated the data so that it‚Äôs at the member level (there are ~82k members), then I simulated 10,000 groups of random sizes (the sizes follow an exponential distribution to approximate my actual group size distribution), then I randomly sampled the members into the groups with replacement, finally I aggregate the data up to the group level to get a training data set.\n\nWhat concerns me: the model is trained and tested on effectively the same underlying membership - potentially causing training bias.\n\nWhy I think this works: none of the simulated groups are specifically the same as my real groups. The underlying membership is essentially a pool of people that could reasonably reflect any new employer group we insure. By mixing them up into simulated groups and then aggregating the data I feel like I‚Äôve created plausible groups.",
    "author": "g_rogers",
    "timestamp": "2025-10-17T09:43:04",
    "url": "https://reddit.com/r/statistics/comments/1o96e6t/question_will_my_method_for_sampling_training/",
    "score": 7,
    "num_comments": 3,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o8w3v6",
    "title": "Help a student understand Real life use of the logistic distribution [R] [E]",
    "content": "Hey everyone,\n\nI‚Äôm a student currently prepping for a probability presentation, and my topic is the logistic distribution,  specifically its applications in the actuarial profession.\n\nI‚Äôve done quite a bit of research, but most of what I‚Äôm finding is buried in heavy theoretical or statistical jargon that‚Äôs been tough for me to get any genuine understanding of other than copy paste memorize. \n\nIf any actuaries here have actually used the logistic distribution (or seen it used in practice), could you please share how or where it fits into your work? Like whether it‚Äôs used in modeling, risk assessment, survival analysis, or anything else that‚Äôs not just abstract theory.\n\nAny pointers, examples, or even simplified explanations would be greatly appreciated. \n\nThanks in advance!\n\n",
    "author": "bleediepie",
    "timestamp": "2025-10-17T01:49:41",
    "url": "https://reddit.com/r/statistics/comments/1o8w3v6/help_a_student_understand_real_life_use_of_the/",
    "score": 9,
    "num_comments": 8,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o9cniq",
    "title": "Disaggregating histogram under constraint [Question]",
    "content": "I have a histogram with bin widths of (say) 5. The underlying variable is discrete with intervals of 1. I need to estimate the underlying distribution in intervals of 1. \n\nI had considered taking a pseudo-sample and doing kernel density estimation, but I have the constraint that the modelled distribution must have the same means within each of the original bin ranges. In other words re-binning the estimated distribution should reconstruct the original histogram exactly.\n\nObviously I could just assume the distribution within each bin is flat which makes this trivial, but I need the estimated distribution to be ‚Äúsmooth‚Äù.\n\nDoes anyone know how I can do this? ",
    "author": "hazzaphill",
    "timestamp": "2025-10-17T13:43:05",
    "url": "https://reddit.com/r/statistics/comments/1o9cniq/disaggregating_histogram_under_constraint_question/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o8yhtl",
    "title": "[Question] statistical test between 2 groups with categorical variables",
    "content": "Hi guys,\n\nI basically have 2 groups of users, where each tested 2 different things.\n\nI have a categorical variable (non-ordered) and I would like to test if there is a statistically significant difference between them.\n\nSample sizes are not so similar.\n\nI was thinking of using chi-squared. Is this the correct test?\n\nWhat other approaches should I consider?\n\nThank you for your help!",
    "author": "Opposite_Reporter_86",
    "timestamp": "2025-10-17T04:14:59",
    "url": "https://reddit.com/r/statistics/comments/1o8yhtl/question_statistical_test_between_2_groups_with/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o8xtt1",
    "title": "[Question] Time Intervall Problem",
    "content": "I am working on a problem and I can not find a solution or I am not sure, that my solution is correct.\n\nLet's say we have two events that occur on average for some seconds per hour.\n\nEvent\\_A lasts 10 seconds per hour.\n\nEvent\\_B lasts 5 seconds per hour.\n\nI want to figure what the chance is that both events have any overlap.\n\nMy idea is: 10/3600 \\* 5/3600.\n\nMy interpretation is, that the first even is active for a time fraction of an hour, and the chance that the second even happens at the same time during the active time is 5/3600 thus the fomula above.\n\nPlease help me to think this through.\n\n  \nEdit: Promise its not homework. Multiple people are thinking about this and we have different opinions.",
    "author": "super_brudi",
    "timestamp": "2025-10-17T03:37:19",
    "url": "https://reddit.com/r/statistics/comments/1o8xtt1/question_time_intervall_problem/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o8w5s4",
    "title": "[Question] Is there something wrong with this calculator?",
    "content": "I have a statistics exam is less than a week and my calculator is giving me the wrong values for binomial distributions. This one problem has the following information 16 trials, 0,1 probability and an x value between 3 and 16. I get 0,51 on my calculator but the answer is supposed to be 0,4216. I typed in binomcdf and put in the right info but still I'm getting wrong values.",
    "author": "Destiny_Villain",
    "timestamp": "2025-10-17T01:53:11",
    "url": "https://reddit.com/r/statistics/comments/1o8w5s4/question_is_there_something_wrong_with_this/",
    "score": 1,
    "num_comments": 7,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o8blsc",
    "title": "[Question] Should I transform data if confidence intervals include negative values in a set where negative values are impossible (i.e. age)? SPSS",
    "content": "Basically just the question. My confidence interval for age data is -120 to 200. Do I just accept this and move on? I wasn‚Äôt given many detailed instructions and am definitely not proficient in any of this. Thank you!!",
    "author": "throwitlikeapoloroid",
    "timestamp": "2025-10-16T09:56:24",
    "url": "https://reddit.com/r/statistics/comments/1o8blsc/question_should_i_transform_data_if_confidence/",
    "score": 5,
    "num_comments": 13,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o7b2rs",
    "title": "Love statistics, hate AI [D]",
    "content": "I am taking a deep learning course this semester and I'm starting to realize that it's really not my thing. I mean it's interesting and stuff but I don't see myself wanting to know more after the course is over.\n\nI really hate how everything is a black box model and things only work after you train them aggressively for hours on end sometimes. Maybe it's cause I come from an econometrics background where everything is nicely explainable and white boxes (for the most part).\n\nTransformers were the worst part. This felt more like a course in engineering than data science.\n\nIs anyone else in the same boat?   \n  \nI love regular statistics and even machine learning, but I can't stand these ultra black box models where you're just stacking layers of learnable parameters one after the other and just churning the model out via lengthy training times. And at the end you can't even explain what's going on. Not very elegant tbh.",
    "author": "gaytwink70",
    "timestamp": "2025-10-15T06:31:14",
    "url": "https://reddit.com/r/statistics/comments/1o7b2rs/love_statistics_hate_ai_d/",
    "score": 341,
    "num_comments": 88,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o8awbl",
    "title": "Rigoureness &amp; Nominal correlation [Question]",
    "content": "Hello, I was said to come here for help ;)\n\nSo I have a question / problem.\n\nIn detaiÃÇ : \nI have a dataset an I would like to correlate two, even 3 to see how the 3rd one influence the others 2 variables . The thing is this is nominal ( non ordinal, non binary data so I cant do dummies). I manage to at least have a pivot table to seek the frequencies of\neach specific situations but I am wondering now, I could calculate the chi square based on the frequency of let's say variable A1 that is associated with B1 in the dataset ( so using this frequency as objected one ) and using the whole frequency of only A1 as the expected one. But I am afraid of the rigorous impact. I thought abt % as well but as I read it seems not good to try correlation on % based\nvalues.\n\nSo if you have any nominal categorical data correlation techniques that would help or if know about rigoureness.\n\nI am not that familiar data treatment but I was thinking maybe a python kinda stuff could work ? For now on I am only on excel lost with my frequencies\nI hope this is clear.\n\n\nThanks for your answer",
    "author": "Taooishere",
    "timestamp": "2025-10-16T09:30:56",
    "url": "https://reddit.com/r/statistics/comments/1o8awbl/rigoureness_nominal_correlation_question/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o7y8od",
    "title": "Did poorly on first exam back [Discussion]",
    "content": "After a freshman year of trying lots of different classes and reflecting over the summer I finally thought I found the major for me, Statistics, however I just had my first exam for my statistical modeling class for simple linear regression. I was so confident during it, almost every question I knew how to answer it and was sure I would get an A on it. I got a 66 on it. I got literally all the math right but so many of the questions I got 1 or 2 points deducted because a word choice or two wasn‚Äôt fully accurate or didn‚Äôt totally describe what was going on. To be fair the final few questions I had a weak spot in my knowledge, I completely spaced on how to spot confidence vs predicted intervals which is embarrassing, but it‚Äôs more about how if I just used a few different words the final grade would be way higher. Fortunately, exams are only 33% of the grade and of the 4 he drops the lowest one but now my margin for error on the exams is very small and multiple linear regression is much harder Ive been fascinated with this class and enjoy it every day and thought I had matched my academic interests with what I‚Äôm good at. I just want to get an A in a hard class for once.\n\nI had a bunch of dumb mistakes too, like I put Beta 1 as hours instead of minutes as it was listed in the problem which lost me points, I forgot to put the ^ over the Y once. (I had to give the exam back to my professor  and I don‚Äôt remember a lot of specific writings I got points off for",
    "author": "Global-Hat-1139",
    "timestamp": "2025-10-15T22:48:41",
    "url": "https://reddit.com/r/statistics/comments/1o7y8od/did_poorly_on_first_exam_back_discussion/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o7lk30",
    "title": "[E] Career Inquiry",
    "content": "I was a statistics major because it is my dream job to become a statistican but sadly personal problem happen and it caused me to transfer out and went to a school that does not offer statistics as its program. Now I am taking BS mathematics. Can I still be a statistician and if yes, what are the pros and cons.",
    "author": "Patient-Sun2074",
    "timestamp": "2025-10-15T13:03:08",
    "url": "https://reddit.com/r/statistics/comments/1o7lk30/e_career_inquiry/",
    "score": 6,
    "num_comments": 7,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o7jxr5",
    "title": "Econ and stats books [Education]",
    "content": "Hi, I would like to apply to university for economics and stats/ maths, stats and economics and stats, and I am looking to read some books to talk about in my interviews and essay does anyone have any recommendations",
    "author": "Leading-Department11",
    "timestamp": "2025-10-15T12:01:39",
    "url": "https://reddit.com/r/statistics/comments/1o7jxr5/econ_and_stats_books_education/",
    "score": 8,
    "num_comments": 2,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o7t6q3",
    "title": "[Question] Can someone help me understand the difference between these two ANOVAs? (\"species by treatment\" vs \"treatment by species\")",
    "content": "Hello everyone. I am a graduate student researcher. For my master's I gave a bunch of different wetland plants three different amounts of polluted water -- no pollution (0%), 30%, and 70%. Now I am doing statistics on those results (in this case, the amount of metal within the plants' tissues).\n\nThe thing is, I am bad at statistics and my brain is very confused. A statistician has been kind of tutoring me and I've been learning but its been slow going. \n\nSo here's the thing I don't understand-- I've used Jump to do ANOVAs comparing both my five plant species, and the three treatment groups. Here's a picture of the Tukey tables from those: [https://ibb.co/FLKFzYTh](https://ibb.co/FLKFzYTh)\n\nWhat is exactly the difference between \"treatment by species\" and \"species by treatment?\" He had me transform the data logarithmically because the \"Residual by Predicted Plot\" made a cone shape which apparently is \"bad.\" Then he had me do ANOVAs with \"treatment by species\" and \"species by treatment.\" The thing is I don't actually understand the difference between those two things... I asked my tutor today at the end of our meeting and he explained but I just was nodding with a blank stare because I knew we were out of time. This stuff is like black magic to me, any help would be very appreciated! \n\nSo in short, my tutor had me do an ANOVA in Jump where the \"Y\" was Log(Al-L) (that stands for \"Aluminum in Leaves\" data) of \"Treatment by Species\" and then \"Species by Treatment\" and I don't actually know why he had me do any of those things or what the difference between those two groups is. D:\n\nThank you so much and have a nice day! ",
    "author": "Rejoicing_Tunicates",
    "timestamp": "2025-10-15T18:22:22",
    "url": "https://reddit.com/r/statistics/comments/1o7t6q3/question_can_someone_help_me_understand_the/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o6jhob",
    "title": "[Q] Bayesian phd",
    "content": "Good morning, \nI'm a master student at Politecnico of Milan, in the track Statistical Learning. My interest are about Bayesian Non-Parametric framework and MCMC algorithm with a focus also on computational efficiency. \nAt the moment, I have a publication about using Dirichlet Process with Hamming kernel in mixture models and my master thesis is in the field of BNP but in the framework of distance-based clustering. \nNow, the question, I'm thinking about a phd and given my \"experience\" do you have advice on available professors or universities with phd in the field? \n\nThanks in advance to all who wants to respond, sorry if my english is far from being perfect.",
    "author": "Gyozesaifa",
    "timestamp": "2025-10-14T09:00:14",
    "url": "https://reddit.com/r/statistics/comments/1o6jhob/q_bayesian_phd/",
    "score": 23,
    "num_comments": 50,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o74lf6",
    "title": "[E] Chi squared test",
    "content": "Can someone explain it in general and how to achive on ecxel (need for an exam)",
    "author": "M3SM3",
    "timestamp": "2025-10-15T00:32:29",
    "url": "https://reddit.com/r/statistics/comments/1o74lf6/e_chi_squared_test/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.45,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o6iukm",
    "title": "[Research] Free AAAS webinar this Friday: \"Seeing through the Epidemiological Fallacies: How Statistics Safeguards Scientific Communication in a Polarized Era\" by Prof. Jeffrey Morris, The Wharton School, UPenn.",
    "content": "[Here's the free registration link.](https://community.amstat.org/discussion/aaas-section-u-statistics-invitation-to-aaas-webinar-with-professor-jeffrey-s-morris-1#bmc1304664-4892-40ac-a058-e4102457eda4) The webinar is **Friday (10/17) from 2:00-3:00 pm ET**. *Membership in AAAS is not required.*\n\n\n\n**Abstract:** \n\nObservational data underpin many biomedical and public-health decisions, yet they are easy to misread, sometimes inadvertently, sometimes deliberately, especially in fast-moving, polarized environments during and after the pandemic. This talk uses concrete COVID-19 and vaccine-safety case studies to highlight foundational pitfalls: base-rate fallacy, Simpson‚Äôs paradox, post-hoc/time confounding, mismatched risk windows, differential follow-up, and biases driven by surveillance and health-care utilization.  \n  \nIllustrative examples include:  \n\n\n1. Why a high share of hospitalized patients can be vaccinated even when vaccines remain highly effective.\n2. Why higher crude death rates in some vaccinated cohorts do not imply vaccines cause deaths.\n3. How policy shifts confound before/after claims (e.g., zero-COVID contexts such as Singapore), and how Hong Kong‚Äôs age-structured coverage can serve as a counterfactual lens to catch a glimpse of what might have occurred worldwide in 2021 if not for COVID-19 vaccines.\n4. How misaligned case/control periods (e.g., a series of nine studies by RFK appointee David Geier) can manufacture spurious associations between vaccination and chronic disease.\n5. How a pregnancy RCT‚Äôs ‚Äúbirth-defect‚Äù table was misread by ACIP when event timing was ignored.\n6. Why apparent vaccine‚Äìcancer links can arise from screening patterns rather than biology.\n7. What an unpublished ‚Äúunvaccinated vs. vaccinated‚Äù cohort (‚ÄúAn Inconvenient Study‚Äù) reveals about non-comparability, truncated follow-up, and encounter-rate imbalances, despite being portrayed as a landmark study of vaccines and chronic disease risk in a recent congressional hearing.\n\nI will outline a design-first, transparency-focused workflow for critical scientific evaluation, including careful confounder control, sensitivity analyses, and synthesis of the *full* literature rather than cherry-picked subsets, paired with plain-language strategies for communicating uncertainty and robustness to policymakers, media, and the public. I argue for greater engagement of statistical scientists and epidemiologists in high-stakes scientific communication.",
    "author": "CommentSense",
    "timestamp": "2025-10-14T08:36:27",
    "url": "https://reddit.com/r/statistics/comments/1o6iukm/research_free_aaas_webinar_this_friday_seeing/",
    "score": 17,
    "num_comments": 7,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o6wjzf",
    "title": "Calculating expected loss / scenarios for a bonus I am about to play for [discussion]",
    "content": "Hi everyone,\n\nNeed some help as AI tools are giving different answers. REALLY appreciate any replies here, in depth or surface level. This involves risk of ruin, expected playthrough before ruin and expected loss overall.\n\nI am going to be playing on a video poker machine for a $2-$3k value bonus. I need to wager $18,500 to unlock the bonus.\n\nI am going to be playing 8/5 Jacks or Better poker (house edge of 2.8%), with $5 per hand, 3 hands dealt per hand for $15 per hand wager. The standard deviation is 4.40 units, and the correlation between hands is assumed at 0.10.\n\nMy scenario I am trying to ruin is I set a max stop loss of $600. When I hit the $600 stop loss, I switch over to the video blackjack offered, $5 per hand, terrible house edge of 4.6% but much low variance to accomplish the rest of the playthrough.\n\nI am trying to determine what is the probability that I achieve the following before hitting the $600 stop loss in Jacks or Better 8/5:\n$5000+ playthrough \n$10,000+ playthrough\n$15,000+ playthrough\n$18,500, 100% playthrough?\n\nWhat is the expected loss for the combined scenario of $600 max stop loss in video poker, with continuing until $18,500 playthrough in the video poker? What is the probability of winning $1+, losing $500+, losing $1000+, losing $1500+ for this scenario.\n\nI expect average loss to be around $1000. If I played the video poker for the full amount, I‚Äôd lose on average $550. However the variance is extreme and you‚Äôd have a 10%+ of losing $2000+. If I did blackjack entirely I‚Äôd lose ~$900 but no chance of winning.\n\nAppreciate any mathematical geniuses that can help here!",
    "author": "Time-Philosophy0323",
    "timestamp": "2025-10-14T17:25:53",
    "url": "https://reddit.com/r/statistics/comments/1o6wjzf/calculating_expected_loss_scenarios_for_a_bonus_i/",
    "score": 0,
    "num_comments": 14,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o70ohh",
    "title": "[Q] Optimization problem",
    "content": "We want to minimize the risk of your portfolio while achieving a 10% return on your ‚Çπ20 lakh investment. The decision variables are the weights (percentages) of each of the 200 stocks in your portfolio. The constraints are that the total investment can't exceed ‚Çπ20 lakh, and the overall portfolio return must be at least 10%. We're also excluding stocks with negative returns or zero growth.",
    "author": "SuChIr_chad",
    "timestamp": "2025-10-14T20:42:03",
    "url": "https://reddit.com/r/statistics/comments/1o70ohh/q_optimization_problem/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o639l9",
    "title": "[Q][S] How was your experience publishing in Journal of Statistical Software?",
    "content": "I‚Äôm currently writing a manuscript for an R package that implements methods I published earlier. The package is already on CRAN, so the only remaining step is to submit the paper to JSS. However, from what I‚Äôve seen in past publications, the publication process can be quite slow, in some cases taking two years or more. I also understand that, after submitting a revision, the editorial system may assign a new submission number, which effectively ‚Äúresets‚Äù the timestamp, that means the ‚ÄúSubmitted / Accepted / Published‚Äù dates printed on the final paper may not accurately reflect the true elapsed time.\n\nDoes anyone here have recent experience (in the last few years) with JSS‚Äôs publication timeline? I‚Äôd appreciate hearing how long the process took for your submission (from initial submission to final publication).",
    "author": "Unhappy_Passion9866",
    "timestamp": "2025-10-13T19:11:18",
    "url": "https://reddit.com/r/statistics/comments/1o639l9/qs_how_was_your_experience_publishing_in_journal/",
    "score": 12,
    "num_comments": 9,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o69unx",
    "title": "[Question] How can I find practice questions with solutions for Introductory statistics?",
    "content": "Meanwhile I am learning by myself introductory statistics in order to start with data analysis. I am using a video course and the book \"Statistics for Business and Economics\". The problem is the exercise questions in this book are often unnecessaryly long and doesnt have solutions at all. I have looked for other books but couldnt find any. I just need more theory based and clear questions with solutions to practice. Do you have any suggestions?",
    "author": "cahit135",
    "timestamp": "2025-10-14T01:26:21",
    "url": "https://reddit.com/r/statistics/comments/1o69unx/question_how_can_i_find_practice_questions_with/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o6mxu3",
    "title": "[Research]Thesis ideas ?",
    "content": "",
    "author": "NC1_123",
    "timestamp": "2025-10-14T11:04:59",
    "url": "https://reddit.com/r/statistics/comments/1o6mxu3/researchthesis_ideas/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o5m8yp",
    "title": "[Discussion] What I learned from tracking every sports bet for 3 years: A statistical deep dive",
    "content": "I‚Äôve been keeping detailed records of my sports betting activity for the past three years and wanted to share some statistical analysis that I think this community might appreciate. The dataset includes over 2,000 individual bets along with corresponding odds, outcomes, and various contextual factors.  \n  \nThe dataset spans from January 2022 to December 2024 and includes 2,047 bets. The breakdown by sport is NFL at 34 percent, NBA at 31 percent, MLB at 28 percent, and Other at 7 percent. Bet types include moneylines (45 percent), spreads (35 percent), and totals (20 percent). The average bet size was $127, ranging from $25 to $500. Here are the main research questions I focused on: Are sports betting markets efficient? Do streaks or patterns emerge beyond random variation? How accurate are implied probabilities from betting odds? Can we detect measurable biases in the market?  \n  \nFor data collection, I recorded every bet with its timestamp, odds, stake, and outcome. I also tracked contextual information like weather conditions, injury reports, and rest days. Bet sizing was consistent using the Kelly Criterion. I primarily used Bet105, which offers consistent minus 105 juice, helping reduce the vig across the dataset. Several statistical tests were applied. To examine market efficiency, I ran chi-square goodness of fit tests comparing implied probabilities to actual win rates. A runs test was used to examine randomness in win and loss sequences. The Kolmogorov-Smirnov test evaluated odds distribution, and I used logistic regression to identify significant predictive factors.  \n  \nFor market efficiency, I found that bets with 60 percent implied probability won 62.3 percent of the time, those with 55 percent implied probability won 56.8 percent, and bets around 50 percent won 49.1 percent. A chi-square test returned a value of 23.7 with a p-value less than 0.001, indicating statistically significant deviation from perfect efficiency. Regarding streaks, the longest winning streak was 14 bets and the longest losing streak was 11 bets. A runs test showed 987 observed runs versus an expected 1,024, with a Z-score of minus 1.65 and a p-value of 0.099. This suggests no statistically significant evidence of non-randomness.  \n  \nLooking at odds distribution, most of my bets were centered around the 50 to 60 percent implied probability range. The K-S test yielded a D value of 0.087 with a p-value of 0.023, indicating a non-uniform distribution and selective betting behavior on my part. Logistic regression showed that implied probability was the most significant predictor of outcomes, with a coefficient of 2.34 and p-value less than 0.001. Other statistically significant factors included being the home team and having a rest advantage. Weather and public betting percentages showed no significant predictive power.  \n  \nAs for market biases, home teams covered the spread 52.8 percent of the time, slightly above the expected 50 percent. A binomial test returned a p-value of 0.034, suggesting a mild home bias. Favorites won 58.7 percent of moneyline bets despite having an average implied win rate of 61.2 percent. This 2.5 percent discrepancy suggests favorites are slightly overvalued. No bias was detected in totals, as overs hit 49.1 percent of the time with a p-value of 0.67. I also explored seasonal patterns. Monthly win rates varied significantly, with September showing the highest win rate at 61.2 percent, likely due to early NFL season inefficiencies. March dropped to 45.3 percent, possibly due to high-variance March Madness bets. July posted 58.7 percent, suggesting potential inefficiencies in MLB markets. An ANOVA test returned F value of 2.34 and a p-value of 0.012, indicating statistically significant monthly variation.  \n  \nFor platform performance, I compared results from Bet105 to other sportsbooks. Out of 2,047 bets, 1,247 were placed on Bet105. The win rate there was 56.8 percent compared to 54.1 percent at other books. The difference of 2.7 percent was statistically significant with a p-value of 0.023. This may be due to reduced juice, better line availability, and consistent execution. Overall profitability was tested using a Z-test. I recorded 1,134 wins out of 2,047 bets, a win rate of 55.4 percent. The expected number of wins by chance was around 1,024. The Z-score was 4.87 with a p-value less than 0.001, showing a statistically significant edge. Confidence intervals for my win rate were 53.2 to 57.6 percent at the 95 percent level, and 52.7 to 58.1 percent at the 99 percent level. There are, of course, limitations. Selection bias is present since I only placed bets when I perceived an edge. Survivorship bias may also play a role, since I continued betting after early success. Although 2,000 bets is a decent sample, it still may not capture the full market cycle. The three-year period is also relatively short in the context of long-term statistical analysis. These findings suggest sports betting markets align more with semi-strong form efficiency. Public information is largely priced in, but behavioral inefficiencies and informational asymmetries do leave exploitable gaps. Home team bias and favorite overvaluation appear to stem from consistent psychological tendencies among bettors. These results support studies like Klaassen and Magnus (2001) that found similar inefficiencies in tennis betting markets.  \n  \nFrom a practical standpoint, these insights have helped validate my use of the Kelly Criterion for bet sizing, build factor-based betting models, and time bets based on seasonal trends. I am happy to share anonymized data and the R or Python code used in this analysis for academic or collaborative purposes. Future work includes expanding the dataset to 5,000 or more bets, building and evaluating machine learning models, comparing efficiency across sports, and analyzing real-time market movements.  \n  \nTLDR: After analyzing 2,047 sports bets, I found statistically significant inefficiencies, including home team bias, seasonal trends, and a measurable edge against market odds. The results suggest that sports betting markets are not perfectly efficient and contain exploitable behavioral and structural biases.",
    "author": "BeneficialTomato7562",
    "timestamp": "2025-10-13T08:06:25",
    "url": "https://reddit.com/r/statistics/comments/1o5m8yp/discussion_what_i_learned_from_tracking_every/",
    "score": 46,
    "num_comments": 19,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o5udx0",
    "title": "[E] Which major is most useful?",
    "content": "Hey, I have a background in research economics (macroeconometrics and microeconometrics). I now want to profile myself for jobs as a (health)/bio statistician, and hence I'm following an additional master in statistics. There are two majors I can choose from; statistical science (data analysis w python, continuous and categorical data, statistical inference, survival and multilevel analysis) and computational statistics (databases, big data analysis, AI, programming w python, deep learning). Do you have any recommendation about which to choose? \nAditionally, I can choose 3 of the following courses: survival analysis, analysis of longitudinal and clustered data, causal machine learning, bayesian stats, analysis of high dimensional data, statistical genomics, databases. Anyone know which are most relevant when focusing on health?",
    "author": "Bartastico",
    "timestamp": "2025-10-13T12:57:51",
    "url": "https://reddit.com/r/statistics/comments/1o5udx0/e_which_major_is_most_useful/",
    "score": 15,
    "num_comments": 16,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o5itug",
    "title": "[career] Question about the switching from Economics to Statistics",
    "content": "[](https://www.reddit.com/r/statistics/?f=flair_name%3A%22Career%22)Posting on behalf of my friend since he doesn‚Äôt have enough karma.\n\nHe completed his BA in Economics (top of his class) from a reputed university in his country consistently ranked in the top 10 for economics. His undergrad coursework included:\n\n* Microeconomics, Macroeconomics, Money &amp; Banking, Public Economics\n* Quantitative Methods, Basic Econometrics, Operation Research (Paper I &amp; II)\n* Statistical Methods, Econometrics (Paper I &amp; II), Research Methods, Dissertation\n\nHe then did his MA in Economics from one of the top economics colleges in the country, again finishing in the Top 10 of his class His master‚Äôs included advanced micro, macro, game theory, and econometrics-heavy quantitative coursework.\n\nHe‚Äôs currently pursuing an MSc in eme at LSE. His GRE score is near perfect. Originally, his goal was a PhD in Economics, but after getting deeper into the mathematical side, he‚Äôs want to go in pure Statistics and now wants to switch fields and apply for a PhD in Statistics ideally at a top global program\n\nSo the question is ‚Äî can someone with a strong economics background like this successfully transition into a Statistics PhD",
    "author": "Apprehensive_Box7681",
    "timestamp": "2025-10-13T05:51:04",
    "url": "https://reddit.com/r/statistics/comments/1o5itug/career_question_about_the_switching_from/",
    "score": 8,
    "num_comments": 6,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o56b35",
    "title": "[Q] Recommendations for virtual statistics courses at an intermediate or advanced level?",
    "content": "I'd like to improve my knowledge of statistics, but I don't know where a good place is that's virtual and doesn't just teach the basics, but also intermediate and advanced levels.",
    "author": "NombreDeUsuario0038",
    "timestamp": "2025-10-12T18:15:21",
    "url": "https://reddit.com/r/statistics/comments/1o56b35/q_recommendations_for_virtual_statistics_courses/",
    "score": 21,
    "num_comments": 6,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o5pbwa",
    "title": "[Question] Verification scheme for scraped data",
    "content": "",
    "author": "-DoubleWide-",
    "timestamp": "2025-10-13T09:57:08",
    "url": "https://reddit.com/r/statistics/comments/1o5pbwa/question_verification_scheme_for_scraped_data/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o58lgu",
    "title": "[Q] How do statistic softwares determine a p-value if a population mean isn‚Äôt known?",
    "content": "I‚Äôm thinking about hypothesis testing and I feel like I forgot about a step in that determination along the way. ",
    "author": "dayb4august",
    "timestamp": "2025-10-12T20:07:11",
    "url": "https://reddit.com/r/statistics/comments/1o58lgu/q_how_do_statistic_softwares_determine_a_pvalue/",
    "score": 7,
    "num_comments": 19,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o5amia",
    "title": "[Career] Best way to identify masters programs to apply to? (Statistics MS, US)",
    "content": "Hi,\n\n\n\nI‚Äôve always been interest in stats, but during undergrad I was focused on getting a job straight out, and chose consulting. I‚Äôve become disinterested in the business due to how wishy washy the work can be. Some of the stuff I‚Äôve had to hand off has driven me nuts. So my main motivation is to understand enough to apply robust methods to problems (industry agnostic right now. I‚Äôd love to have a research question and just exhaustively work through it from an appropriate statistical framework. Because of this, I‚Äôm strongly considering going back to school with a full focus on statistics (specifically not data science).\n\n¬†\n\nI‚Äôve been researching some programs (e.g., GA tech, UGA, UNC, UCLA), but firstly am having a hard time truly distinguishing between them. What makes programs good, how much does the name matter, are there ‚Äúlower profile‚Äù schools that have a really strong program?\n\n¬†\n\nI‚Äôm also unclear on which type or tier of school would be considered a reach vs realistic.\n\n¬†\n\nDescriptors:\n\n1. Undergrad: 3.85 GPA Emory University, BBA Finance + Quantitative sciences (data + decision sciences)\n2. Relevant courses: Linear Algebra (A-), Calculus for data science (A-, included multivariable functions/integration, vectors, taylor series, etc.), Probability and statistics (B+), Regression Analysis (A), Forecasting (A, non-math intensive business course applying time series, ARIMA, classification models, survival analysis, etc.), natural language processing seminar (wrote continuously on a research project without publishing but presenting at low stakes event)\n3. GRE: 168 quant 170 verbal\n4. Work experience: 1 year at a consulting firm working on due diligence projects with little deep data work. Most was series of linear regressions and some monte carlo simulations.\n5. Courses I‚Äôm lacking: real analysis, more probability courses¬†\n\nThanks for any advice!",
    "author": "Kevinisaname",
    "timestamp": "2025-10-12T21:52:46",
    "url": "https://reddit.com/r/statistics/comments/1o5amia/career_best_way_to_identify_masters_programs_to/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o5xt3i",
    "title": "[Discussion] I've been forced to take elementary stats in my 1st year of college and it makes me want to kms &lt;3  How do any of you live like this",
    "content": "i dont care if this gets taken down, this branch of math is A NIGHTMARE.. ID RATHER DO GEOMETRY. I messed up the entire trigonometry unit in my financial algebra class but IT WAS STILL EASIER THAN THIS. ID GENUINELY RATHER DO GEOMETRY IT IS SO MUCH EASIER, THIS SHIT SUCKS SO HARD.. None of it makes any sense. The real-world examples arent even real world at all, what do you mean the percentage of picking a cow that weighs infinite pounds???????? what do you mean mean of sample means what is happening. its all a bunch of hypothetical bullshit. I failed algebra like 3 times, and id rather have to take another algebra class over this BULLSHIT. \n\nEdit: I feel like I'm in hell. Writing page after page of bullshit nonsense notes. This genuinely feels like they were pulling shit out they ass when they made this math. I am so close to giving up forever",
    "author": "Artistic_Pineapple80",
    "timestamp": "2025-10-13T15:05:55",
    "url": "https://reddit.com/r/statistics/comments/1o5xt3i/discussion_ive_been_forced_to_take_elementary/",
    "score": 0,
    "num_comments": 14,
    "upvote_ratio": 0.21,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o4huia",
    "title": "[D] What work/textbook exists on explainable time-series classification?",
    "content": "I have some background in signal processing and time-series analysis (forecasting) but I'm kind of lost in regards to explainable methods for time-series methods.\n\nIn particular, I'm interested in a general question:\n\nSuppose I have a bunch of time series s1, s2, s3,....sN. I've used a classifier to classify them into k groups. (WLG k=2). How do I know what parts of each time series caused this classification, and why? I'm well aware that the answer is 'it depends on the classifier' and the ugly duckling theorem, but I'm also quite interested in understanding, for example, what sorts of techniques are used in finance. I'm working under the assumption that in financial analysis, given a time-series of, say, stock prices, you can explain sudden spikes in stock prices by saying 'so-and-so announced the sale of 40% stock'. But I'm not sure how that decision is made. What work can I look into?",
    "author": "IllustriousPeanut509",
    "timestamp": "2025-10-11T23:27:12",
    "url": "https://reddit.com/r/statistics/comments/1o4huia/d_what_worktextbook_exists_on_explainable/",
    "score": 14,
    "num_comments": 8,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o3r9ek",
    "title": "[Q] Unable to link data from pre- and posttest",
    "content": "Hi everyone! I need your help.\n\nI conducted a student questionnaire (likert scale) but unfortunately did so anonymously and am unable to link the pre- and posttest per person. In my dataset the participants in the pre- and posttest all have new id‚Äôs, but in reality there is much overlap between the participants in the pretest and those in the posttest.\n\nAm i correct that i should not really do any statistical testing (like repeated measures anova) as i would have to be able to link pre- and posttest scores per person?\n\nAnd for some items, students could answer ‚Äònot applicable‚Äô. For using chi-square to see if there is a difference in the amount of times ‚Äònot applicable‚Äô was chosen i would also need to be able to link the data, right? As i should not use the pre- and posttest as independent measures?\n\nThanks in advance!",
    "author": "wolfmotherrrrr",
    "timestamp": "2025-10-11T02:30:31",
    "url": "https://reddit.com/r/statistics/comments/1o3r9ek/q_unable_to_link_data_from_pre_and_posttest/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o4bouj",
    "title": "My uneducated take on Marylin Savants framing of the Monty Hall problem. [Discussion]",
    "content": "From my understanding Marylin Savants explanation is as follows; When you first pick a door, there is a 1/3 chance you chose the car. Then the host (who knows where the car is) always opens a different door that has a goat and always offers you the chance to switch. Since the host will never reveal the car, his action is not random, **it is giving you information**. Therefore, your original door still has only a 1/3 chance of being right, but the entire 2/3 probability from the two unchosen doors is now concentrated onto the single remaining unopened door. So by switching, you are effectively choosing the option that held a 2/3 probability all along, which is why switching wins twice as often as staying.\n\nClearly switching increases the odds of winning. The issue I have with this reasoning is in her claim that‚Äôs the host is somehow ‚Äúrevealing information‚Äù and that this is what produces the 2/3 odds. That seems absurd to me. The host is constrained to always present a goat, therefore his actions are uninformative. \n\nConsider a simpler version: suppose you were allowed to pick two doors from the start, and if either contains the car, you win. Everyone would agree that‚Äôs a 2/3 chance of winning. Now compare this to the standard Monty Hall game: you first pick one door (1/3), then the host unexpectedly allows you to switch. If you switch, you are effectively choosing the other two doors. So of course the odds become 2/3, but **not because the host gave new information**. The odds increase simply because you are now selecting two doors instead of one, just in two steps/instances instead of one as shown in the simpler version. \n\nThe only way the hosts action could be informative is **if** he presented you with car upon it being your first pick. In that case, if you were presented with a goat, you would know that you had not picked the car and had definitively picked a goat, and by switching you would have a 100% chance of winning.\n\n\nC.! ‚Üí (G ‚Üí G)\n\nG. ‚Üí (C! ‚Üí G)\n\nG. ‚Üí (G ‚Üí C!)\n\nLooking at this simply, the hosts actions are irrelevant as he is constrained to present a goat regardless of your first choice. The 2/3 odds are simply a matter of choosing two rather than one, regardless of how or why you selected those two. \n\nIt seems Savant is hyper-fixating on the host‚Äôs behavior in a similar way to those who wrongly argue 50/50 by subtracting the first choice. Her answer (2/3) is correct, but her explanation feels overwrought and unnecessarily complicated. ",
    "author": "Kage_anon",
    "timestamp": "2025-10-11T17:48:58",
    "url": "https://reddit.com/r/statistics/comments/1o4bouj/my_uneducated_take_on_marylin_savants_framing_of/",
    "score": 0,
    "num_comments": 175,
    "upvote_ratio": 0.28,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o3eitf",
    "title": "[Q] Anyone experienced in state-space models",
    "content": "Hi, i‚Äôm stat phd, and my background is Bayesian. I recently got interested in state space model because I have a quite interesting application problem to solve with it. If anyone ever used this model (quite a serious modeling), what was your learning curve like and usually which software/packages did you use?",
    "author": "cool-whip-0",
    "timestamp": "2025-10-10T15:05:40",
    "url": "https://reddit.com/r/statistics/comments/1o3eitf/q_anyone_experienced_in_statespace_models/",
    "score": 16,
    "num_comments": 19,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o3nirq",
    "title": "[Question] Cronbach's alpha for grouped binary conjoint choices.",
    "content": "For simplicity, let's assume I run a conjoint where each respondent is shown eight scenarios, and, in each scenario, they are supposed to pick one of the two candidates. Each candidate is randomly assigned one of 12 political statements. Four of these statements are liberal, four are authoritarian, and four are majoritarian. So, overall, I end up with a dataset that indicates, for each respondent, whether the candidate was picked and what statement was assigned to that candidate.\n\nIn this example, may I calculate Cronbach's alpha to measure the consistency between each of the treatment groups? So, I am trying to see if I can compute an alpha for the liberal statements, an alpha for the authoritarian ones, and an alpha for the majoritarian ones.",
    "author": "opposity",
    "timestamp": "2025-10-10T22:36:33",
    "url": "https://reddit.com/r/statistics/comments/1o3nirq/question_cronbachs_alpha_for_grouped_binary/",
    "score": 4,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o32bbh",
    "title": "[Question] Conditional inference for partially observed set of binary variables?",
    "content": "I have the following setup:\n\nI'm running a laundry business. I have a set of method **M** to remove stain on clothes. Each stain have their own characteristics though, so I hypothesized that there will be relationship like \"if it doesn't work on m\\_i, it should work on m\\_j\". I have the record of the stains and their success rate on some methods. Unfortunately, the stain vs methods experiment are not exhaustive. Most stains are only tested on subset of **M**. One day, I came across a new kind of stain. I tested it on some methods **O** ‚äÜ **M** once, so I have a binary data (success/not) of size |**O**|. Now I'm curious, what would be the success rate for the other methods **U** = **M**\\\\**O** given the observation of methods in **O**? Since the observation are just binary data instead of success rate, is it still possible to do inference?\n\nAlthough the dataset samples are incomplete (each sample only have values for subset of M), I think it's at least enough to build the joint data of pairwise variables in M. However, I don't know what kind of bivariate distribution I can fit to the joint data. \n\nIn Gaussian models, to do this kind of conditional inference, we have a closed formula that only involves the observation, marginals, and the joint multivariate gaussian distribution of the data. In this case however, since we are working with success rate, the variables are bounded in \\[0,1\\], so it can't be gaussian, I'm thinking that it should be Beta?? What kind of transformation for these data do you think is ok so that we can fit gaussian? what are the possible losses when we do such transformation?\n\nIf we proceed with non-gaussian model, what kind of joint distribution that we can use such that it's possible to calculate the posterior given that we only have the pairwise joint distribution?  \n",
    "author": "gumball3point",
    "timestamp": "2025-10-10T07:23:34",
    "url": "https://reddit.com/r/statistics/comments/1o32bbh/question_conditional_inference_for_partially/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o2nlko",
    "title": "[Discussion] can some please tell me about Computational statistics?",
    "content": "Hay guys can someone with experience in Computational statistics give me a brief deep dive of the subjects of Computational statistics and the diffrences it has compared to other forms of stats, like when is it perferd over other forms of stats, what are the things I can do in Computational statistics that I can't in other forms of stats, why would someone want to get into Computational statistics so on and so forth. Thanks. ",
    "author": "deesnuts78",
    "timestamp": "2025-10-09T18:18:08",
    "url": "https://reddit.com/r/statistics/comments/1o2nlko/discussion_can_some_please_tell_me_about/",
    "score": 21,
    "num_comments": 32,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o2jx5c",
    "title": "[Q] Statistics PhD and Real Analysis?",
    "content": "I'm planning on applying to statistics PhDs for fall 2025, but I feel like I've kind of screwed myself with analysis.\n\nI spoke to some faculty last year (my junior year) and they recommended trying to complete a mathematics double major in 1.5 semesters, as I finished my statistics major junior year. I have been trying to do that, but I'm going insane and my coursework is slipping. I had to take statistical inference and real analysis this semester at the same time which has sucked to say the least. I am doing mediocre in both classes, and am at real risk of not passing analysis. I'm thinking of withdrawing so I can focus on inference (it's only offered in the fall), then taking analysis again next semester. My applied statistics coursework is fantastic and I have all As, as well as have done very well in linear algebra-based mathematics courses and applied mathematics courses. I'm most interested in researching applied statistics, but I do understand theory is very important.\n\n  \nBasically my question is how cooked am I if I decide to withdraw from analysis and try again next semester. I don't plan on withdrawing until the very last minute so I can learn as much as possible, but plan on prioritizing inference for the rest of the semester. The programs I'm looking at do not heavily emphasize theory, but I know lacking analysis or failing analysis looks extremely bad. ",
    "author": "[deleted]",
    "timestamp": "2025-10-09T15:28:06",
    "url": "https://reddit.com/r/statistics/comments/1o2jx5c/q_statistics_phd_and_real_analysis/",
    "score": 15,
    "num_comments": 11,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o2l8vr",
    "title": "[Discussion] Should I reach out to professors for PhD applications?",
    "content": "I am applying to PhD programs in Statistics and Biostatistics, and am unsure if it is appropriate to reach out to professors prior to applying in order to get on their radar and express interest in their work. I‚Äôm interested in applied statistical research and statistical learning. I‚Äôm applying to several schools and have a couple professors at each program that I‚Äôd like to work under if I am admitted to the program. \n\nMost of my programs suggest we describe which professors we‚Äôd want to work with in our statements of purpose, but don‚Äôt say anything about reaching out before hand. \n\nAlso, some of the programs are rotation based, and you find your advisor during those year 1-2 rotations. ",
    "author": "Voldemort57",
    "timestamp": "2025-10-09T16:27:09",
    "url": "https://reddit.com/r/statistics/comments/1o2l8vr/discussion_should_i_reach_out_to_professors_for/",
    "score": 14,
    "num_comments": 13,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o28mwf",
    "title": "[question] How to deal with low Cronbach‚Äôs alpha when I can‚Äôt change the survey?",
    "content": "I‚Äôm analyzing data from my master‚Äôs thesis survey (3 items measuring Extraneous Cognitive Load). The Cronbach‚Äôs alpha came out low (~0.53).\nThese are the items:\n1-When learning vocabulary through AI tools, I often had to sift through a lot of irrelevant information to find what was useful.\n\n2-The explanations provided by AI tools were sometimes unclear.\n\n3-The way information about vocabulary was presented by AI tools made it harder to understand the content\n\nThe problem is: I can‚Äôt rewrite the items or redistribute the survey at this stage.\n\nWhat are the best ways to handle/report this? Should I just acknowledge the limitation, or are there accepted alternatives (like other reliability measures) I can use to support the scale?",
    "author": "Comfortable-Fox-4563",
    "timestamp": "2025-10-09T08:15:01",
    "url": "https://reddit.com/r/statistics/comments/1o28mwf/question_how_to_deal_with_low_cronbachs_alpha/",
    "score": 11,
    "num_comments": 16,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o2r0r1",
    "title": "[Question] Regression - interpreting parallel slopes",
    "content": "OK, let's say you examine two closely related species for two covarying characters. Like body mass (X) and tibial thickness (Y). You have a reason to suspect a different body/mass-tibia relationship - say there is an identified behavioral difference between the two quadrupedal taxa - maybe one group spends much of it's day facultatively bipedal to feed on higher branches in trees. \n\nYou run a regresision on the tibia/body mass data for both species to see if the slopes of the two regressions are significantly different. However, the two species have parallel slopes, but significantly different Y intercepts. What is the interpretation of the Y intercept difference? That at the evolutionary divergence tibial thickness changed (evolutionarily) due to the behavioral change, but that the overall genetic linkage between body mass and tibial robusticity remains constant?",
    "author": "azroscoe",
    "timestamp": "2025-10-09T21:07:25",
    "url": "https://reddit.com/r/statistics/comments/1o2r0r1/question_regression_interpreting_parallel_slopes/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o2z7pb",
    "title": "[Question] Why can statisticians blindly accept random results?",
    "content": "I'm currently doing honours in maths (kinda like a 1 year masters degree) and today we had all the maths and stats honours students presenting their research from this year. Watching these talks made me remember a lot things I thought from when I did a minor in mathematical statistics which I never got a clear answer for.\n\nMy main problem with statistics I did in undergrad is that statisticians have so many results that come from thin air. Why is the Central limit theorem true? Where do all these tests (like AIC, ACF etc) come from? What are these random plots like QQ plots?\n\nI don't mind some slight hand-waving (I agree some proofs are pretty dull sometimes) but the amount of random results statistics had felt so obscure. This year I did a research project on splines and used this thing called smoothing splines. Smoothing splines have a \"smoothing term\" which smoothes out the function. I can see what this does but WHERE THE FUCK DOES IT COME FROM. It's defined as the integral of f''(x)\\^2 but I have no idea why this works. There's so many assumptions and results statisticians pull from thin air and use mindlessly which discouraged me pursuing statistics.\n\nI just want to ask statisticians how you guys can just let these random bs results slide and go on with the rest of the day. To me it feels like a crime not knowing where all these results come from.",
    "author": "felixinnz",
    "timestamp": "2025-10-10T05:13:52",
    "url": "https://reddit.com/r/statistics/comments/1o2z7pb/question_why_can_statisticians_blindly_accept/",
    "score": 0,
    "num_comments": 22,
    "upvote_ratio": 0.19,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o27cqr",
    "title": "[Question] Is binomial law relevant to estimate CPU contention and slowdown across processes?",
    "content": "Here is an example of the problem I want to solve: a server with 4 CPUs is running 8 processes waiting for IOs 66% of the time.\n\nI am convinced that using a binomial law is the solution. But I haven't done any statistics for years, so I can't be 100% sure. Here are the details of my solution.\n\nSo, 8 processes using CPU 33% (1-66%) of the time: `Binomial(n = 8, p = 1/3)`. Then, I'm looking for:\n\n        P(X &gt; 4)\n        = 1 - P(X &lt;= 4)\n        = 1 - P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4)\n\nIn a spreadsheet, I use the formula `=1-BINOMDIST(4, 8, 1/3, TRUE)` which returns 0.0879. So for \\~9% of the time, there is a CPU contention. First question, is it correct?\n\nAdding more processes improves throughput but degrades latency because of CPU contention. So I want to know of how the % of slowdown. I feel like it's 9% slower, since processes are waiting for a CPU 9% of their time. But when I compute with more than 32 processes the CPU contention is ceiling at 100%. It's obvious since a probability of more than 100% is a non sens. Either, this percentage is not an indicator of the latency increase, or it does not work above 100%.\n\n|Processes|CPU contention|\n|:-|:-|\n|8|9%|\n|16|68%|\n|24|95%|\n|32|99%|\n|33|100%|\n|64|100%|\n\nMy last idea is to weight by the number of waiting processes, still with the same example of 4 CPUs and 8 processes: \n\n    P(X=5) + P(X=6) * 2 + P(X=7) * 3 + P(X=8) * 4\n    = BINOMDIST(5,8,1/3,FALSE) + BINOMDIST(6,8,1/3,FALSE)*2 + BINOMDIST(7,8,1/3,FALSE)*3 + BINOMDIST(8,8,1/3,FALSE)*4\n    = 0.1103490322\n    ~= 11%\n\nSecond question, is it correct to weight each distribution of the binomial law by the number of waiting processes to estimate the % of latency increase?",
    "author": "xilase",
    "timestamp": "2025-10-09T07:26:04",
    "url": "https://reddit.com/r/statistics/comments/1o27cqr/question_is_binomial_law_relevant_to_estimate_cpu/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o2bhf6",
    "title": "[Q] Treating stimuli vs. scale items as random factors",
    "content": "I work a lot with scale measures (e.g., personality traits, political orientation, etc.). Like most people, I usually either create a summary score (e.g., the mean or sum of item responses) or use factor analysis/latent variable modeling.\n\nLately, I‚Äôve been doing more research that involves stimuli. For example, I might have participants rate sets of faces (say, on perceived competence) that vary in attractiveness. For these studies, I use linear mixed-effects (LME) models, treating both participants and stimuli as random factors.\n\nI understand why LMEs make sense for stimulus-rating designs. The stimuli are sampled from a larger population of possible exemplars. But what‚Äôs been bugging me is why we don‚Äôt use LMEs for scale measures. Aren‚Äôt the 10 items on a personality scale also a kind of sample from a much broader population of possible items that could have been used to measure that construct?\n\nSo why is it acceptable to average or factor-analyze those item responses, but not acceptable to simply average competence ratings across a set of ‚Äúattractive faces‚Äù?\n\nDoes anyone have any sources they could guide me to that cover this or related issues? Sorry if my question is convoluted. ¬†",
    "author": "diediedie_mydarling",
    "timestamp": "2025-10-09T10:01:23",
    "url": "https://reddit.com/r/statistics/comments/1o2bhf6/q_treating_stimuli_vs_scale_items_as_random/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o1wzcx",
    "title": "[Question] statistical tests and probability distributions",
    "content": "I was reading some statistical tests ( t test , ANOVA etc ) and I wanted to know how it is connected to probability distributions ( t and F distribution). It seems to me that they came up with these tests using some properties of the respective probability distributions and I would like to understand that. It seems vague to me when they ask to compute a t statistic and look at the p value based on the degrees of freedom üòµ‚Äçüí´",
    "author": "Confused-Monkey91",
    "timestamp": "2025-10-08T21:43:10",
    "url": "https://reddit.com/r/statistics/comments/1o1wzcx/question_statistical_tests_and_probability/",
    "score": 5,
    "num_comments": 5,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o1gfsf",
    "title": "[Q] Understanding potential errors in P value more clearly",
    "content": "Hi! In light of the political climate, I'm trying to understand reading research a little bit better. I'm stuck on p values. What can be interpreted from a significantly low p value and how can we be sure that that said p value is not a result of \"bad research\" or error (excuse my layman language). ",
    "author": "Shoddy_Economy4340",
    "timestamp": "2025-10-08T10:08:53",
    "url": "https://reddit.com/r/statistics/comments/1o1gfsf/q_understanding_potential_errors_in_p_value_more/",
    "score": 10,
    "num_comments": 16,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o1w8xd",
    "title": "How anomalous is my dating history? [Discussion]",
    "content": "I was sitting here and reflecting on my past and relationships, and suddenly I realized that 6 of the 7 women I have called my girlfriend or partner since I was 15 had a diagnosis for Bipolar Disorder while I was dating them. I recently learned only a very small portion (2.8%) of the population has a medical diagnosis for BPD.\n\nThis means that my dating history is anomalous, as these numbers outpace random chance.\n\nNow, I'm terrible at this specific form of mathematics, as I haven't done it in...oh...12 years? So I was wondering if it would be able to see just what the odds were for me to have had a 6 of 7 streak with BPD partners? It could be fun???\n\nI see rule 1 about homework questions, but this isn't homework...so I hope this is inbounds to ask for help with.",
    "author": "Revolutionary-420",
    "timestamp": "2025-10-08T21:01:16",
    "url": "https://reddit.com/r/statistics/comments/1o1w8xd/how_anomalous_is_my_dating_history_discussion/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o14arf",
    "title": "[Question] Comparing the averages of two unmatched groups?",
    "content": "I have a set of test subjects for which I have matched pre/post data. Unfortunately my control group is unmatched so I only have average pre/post data. I assume the best way to proceed is to compare the average change of the test subjects with the average change of the control subjects, but what is the best statistical test for this? Thanks!",
    "author": "wimsey_pimsey",
    "timestamp": "2025-10-08T00:50:03",
    "url": "https://reddit.com/r/statistics/comments/1o14arf/question_comparing_the_averages_of_two_unmatched/",
    "score": 3,
    "num_comments": 6,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o0e10g",
    "title": "[Question] Is Epistemic Network Analysis (ENA) statistically sound?",
    "content": "Epistemic Network Analysis (ENA) is a quantitative method used to study how people connect ideas, concepts, or forms of knowledge within complex thinking or learning tasks. It is a relatively recent method (2016) which is being widely used in my field of research, which is learning analytics. \n\nBut I've always felt something off about the statistics &amp; math behind this method but I am not exactly able to point out what. I just wanted to get more opinions on this, is the statistical foundation of this method robust or not?\n\nLink to the main paper on the method: https://files.eric.ed.gov/fulltext/EJ1126800.pdf",
    "author": "Quinnybastrd",
    "timestamp": "2025-10-07T05:56:47",
    "url": "https://reddit.com/r/statistics/comments/1o0e10g/question_is_epistemic_network_analysis_ena/",
    "score": 13,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o0us2k",
    "title": "[Question] 2 variable statistics vs 1 variable difference statistics",
    "content": "How do you best determine if you need to use 2 variable statistics or if applying 1 variable statistics to the difference of two means is more appropriate? In some cases it's very obvious, such as when 2 data sets are about different things and you want to check for correlations or when the question itself is about if one is bigger, but other times you see things being analyzed using what seems to be the opposite method that what you might think. What are some good ways to determine which method is most appropriate?",
    "author": "void2258",
    "timestamp": "2025-10-07T16:27:53",
    "url": "https://reddit.com/r/statistics/comments/1o0us2k/question_2_variable_statistics_vs_1_variable/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o0iv8a",
    "title": "[Q] Generating Copula data",
    "content": "\n\nHey.\n\nI am constructing a Survival model for correlated competing risks.\n\nIts all working!!! But i chose the worst way of doing stuff, and i want to correct course, but turns out i am having a hard time.\n\nI originally generated  data from marginal copula C(Fx,Fy), and in my likelihood i used Sxy= 1-Fx-Fy+C(Fx,Fy) as the censored bit.\n\n\nBut i want to be able to include k risks.... and extending S into Sxyw.. is hard and gets messy in the choices i made.\n\nSooo i want to use Sxy as C(Sx,Sy).... which extrapolates easily to k risks.....\n\n\nBut how do i generate data from this??\n\nI get that if Sxy =C(Sx,Sy) then Fxy= 1-Sx-Sy+C(Sx,Sy).\n\nDo i only need to do 1-u and 1-v to when u and v come from C(u,v)?",
    "author": "PigletySquidy",
    "timestamp": "2025-10-07T09:01:51",
    "url": "https://reddit.com/r/statistics/comments/1o0iv8a/q_generating_copula_data/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o0a6z1",
    "title": "[Question] Approximate total given top count",
    "content": "say there is an activity in an online game  where people can gain points infinitely by participating, linearly. Given the total number of participants as well as the points of the top 1-100 participants, how can i approximate the total amount of points earned by all participants? ",
    "author": "whydonlinre",
    "timestamp": "2025-10-07T02:40:03",
    "url": "https://reddit.com/r/statistics/comments/1o0a6z1/question_approximate_total_given_top_count/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nzsfa9",
    "title": "[Education] How do I start learning stats from the basics?",
    "content": "Hi, i know there might be 100s of post with the same question but still taking a chance.\nThese are the topics which I want to learn but the problem is i have zero stats knowledge. How do I start ? Is there any YT channels you can suggest with these particular topics or how do I get the proper understanding of these topics? Also I want to learn these topics on Excel. \nThanks for the help in advance. \nI can also pay to any platform if the teaching methods are nice and syllabus is the same. \n\nProbability Distributions\nSampling Distributions\nInterval Estimation\nHypothesis Testing\n\nSimple Linear Regression\nMultiple Regression Models\nRegression Model Building\nStudy Break\nRegression Pitfalls\nRegression Residual Analysis",
    "author": "AnxietyPhysical",
    "timestamp": "2025-10-06T12:17:21",
    "url": "https://reddit.com/r/statistics/comments/1nzsfa9/education_how_do_i_start_learning_stats_from_the/",
    "score": 15,
    "num_comments": 32,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o0kjdk",
    "title": "Is time series analysis a speciality of statistics or economics? [Q][R]",
    "content": "Given that most observational time series data are economic in nature. Also a lot of the time series models (VAR, GARCH) are really only applicable for economic data.",
    "author": "gaytwink70",
    "timestamp": "2025-10-07T10:02:30",
    "url": "https://reddit.com/r/statistics/comments/1o0kjdk/is_time_series_analysis_a_speciality_of/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nzt9yw",
    "title": "[Career] Business major -&gt; Msc Statistics? Advice needed",
    "content": "Hi, I‚Äôm a international student majoring in a Business major (Marketing specifically) but looking to pivot into Statistics.\n\nSo far I‚Äôve voluntarily taken Linear Algebra, Calculus II, Probability, Mathematical Statistics, and Optimization (none of these are required in my major). I also have one paper in finance microstructure published in an A-rank ABDC journal that includes some postgraduate-level quant work.\n\nMy goal is to do a PhD in stats/quantitative/operations research.\n\nIs it realistic for someone without a math/stats major to get into a top-tier Master program like Imperial‚Äôs or Oxbridge‚Äôs? If so, which additional math courses are must-takes to stay competitive?",
    "author": "namanhz",
    "timestamp": "2025-10-06T12:48:28",
    "url": "https://reddit.com/r/statistics/comments/1nzt9yw/career_business_major_msc_statistics_advice_needed/",
    "score": 5,
    "num_comments": 11,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1o0a07f",
    "title": "[Q]Which masters?",
    "content": "Which masters subject would pair well with statistics if I wanted to make the highest pay without being in a senior position?",
    "author": "Flashy_Ad_8247",
    "timestamp": "2025-10-07T02:27:49",
    "url": "https://reddit.com/r/statistics/comments/1o0a07f/qwhich_masters/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.39,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nzshk0",
    "title": "[Q]: Odds &amp; Probabilities and Predictive Analysis",
    "content": "Hello Math Lords of Reddit,\n\nI have a question regarding odds and probabilities and I am having a hard time wrapping my head around this concept.\n\nI know that previous events affect future outcomes when they are dependent events (such as selecting a cards and removing them from a deck) and generally, independent events are not affected by previous events. But what about when something is happening multiple times in succession? Such as when rolling two dice, if I were to ask what are the odds of rolling a 7 five times in a row the result would be(1/12^5 =0.00000402 or 0.000402%) \n\nBut if a 7 were to roll 4 times in a row and you were to ask someone what are the odds that I roll a 7 again? They would tell you it is 1/12 since rolling dice are supposed to be independent events. \n\nSo this is where I am having confusion. How can both be true? That the odds of rolling a 7 five times in a row is 0.000402% but then rolling the next 7 after the fourth is still 1/12?",
    "author": "narweezy305",
    "timestamp": "2025-10-06T12:19:42",
    "url": "https://reddit.com/r/statistics/comments/1nzshk0/q_odds_probabilities_and_predictive_analysis/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nz5cc8",
    "title": "Book Recommendations for Regression Analysis [Education]",
    "content": "Hi, I would appreciate any book recommendations regression analysis of this sort of format: motivation (why was this model conceived), derivation (ideally a calculus based approach, without probability theory, heavy real analysis, or lengthy proofs), applications (while discussing the limitations of the model), and then exercises (ideally a mixture of modeling exercises and theoretical ones as well).\n\nI would love for the book to cover linear regression, ANOVA, and logistic regression if possible. More would be a bonus!\n\nMy formal education isn't in math, but I am well versed in vector calculus, linear algebra, and elementary probability and statistics and am highly motivated to self study.\n\nAny recommendations would be appreciated!",
    "author": "First_Spell_4839",
    "timestamp": "2025-10-05T17:55:16",
    "url": "https://reddit.com/r/statistics/comments/1nz5cc8/book_recommendations_for_regression_analysis/",
    "score": 31,
    "num_comments": 19,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nz0hmw",
    "title": "[Question] Need help with Selection Bias",
    "content": "Hello I could really use someone's help with this issue. Basically, I have a HUGE dataset, and the point of the analysis is to figure out what percent of the US population is bilingual. However, I STRONGLY suspect that people who are bilingual are significantly more likely to have taken this survey based on the way the survey was advertised, thus giving me bad results.\n\nMy question is, is this study completely ruined and unfixable? Here's what I've thought of for fixing it: Starting with post-stratification weighting. However, this doesn't really fix the issue because the bias isn't caused by demographics (an 18 yo female who took the study is more likely to be bilingual than an 18 yo female in the general population). So I thought maybe I would try Bayesian Logistic Regression modeling, as this introduces priors and is supposed to be helpful with selection bias issues. However, what would I do for my priors? If my priors are the percent of each demographic that are bilingual based on past studies, isn't this begging the question?\n\nAny suggestions?",
    "author": "lightbulb20seven",
    "timestamp": "2025-10-05T14:20:22",
    "url": "https://reddit.com/r/statistics/comments/1nz0hmw/question_need_help_with_selection_bias/",
    "score": 8,
    "num_comments": 3,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nyl8uw",
    "title": "[E] What minor to choose between Math and Econ as a Stat Major?",
    "content": "What minor should i choose between Econ and Math?  I am in a stat major course. I I dont have any specific idea, but that being said, I do like game thoewry and know that  it has a lot of application in ML stuff....\n\ngoal: well, as of now, I did publish a paper in econometrics side, but I am really open to anything. I will be targeting some good rnd jobs after getting my phd tho..But i am interested in a variety of topics: Game theory, and ML and and lots of stat obv, along will some stochiastic topics....\n\n  \nHere aare the eco and math sylabi, please look for \",minor\" courses..\n\n[eco](https://www.presiuniv.ac.in/web/Four%20Years%20UG%20Syllabus%20-%20with%20researchRevised%20Economics2025.pdf) \n\n[math](https://www.presiuniv.ac.in/web/NEP_Updated_13.01.25.pdf)",
    "author": "The_Hocus_Focus",
    "timestamp": "2025-10-05T04:00:46",
    "url": "https://reddit.com/r/statistics/comments/1nyl8uw/e_what_minor_to_choose_between_math_and_econ_as_a/",
    "score": 11,
    "num_comments": 19,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ny5yym",
    "title": "Resource recommendations [discussion]",
    "content": "Hey y'all I'm looking for some advanced statistics resources to freshen up my statistics as I apply for data analyst and data science roles! Books, study guides, websites would be great.\n\nThank you ",
    "author": "Champagnemusic",
    "timestamp": "2025-10-04T14:23:37",
    "url": "https://reddit.com/r/statistics/comments/1ny5yym/resource_recommendations_discussion/",
    "score": 14,
    "num_comments": 4,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nxndzt",
    "title": "[Question]. statistically and mathematically, is age discrete or continuous?",
    "content": "I know this might sound dumb but it had been an issue for me lately, during statistics class someone asked the doc if age was discrete or continuous and tge doc replied of it being discrete, fast forward to our first quiz he brought a question for age, it being discrete or continuous. I myself and a bunch of other good studens put discrete recalling his words and thinking of it in terms that nobody takes age with decimals just for it to get marked wrong and when I told him about it he denied saying so.\nI went ahead and asked multiple classmates and they all agreed that he did in fact say that it's discrete during class.\nnow I'm still confused, is age in statistics and general math considered discrete or continuous? I still consider it as discrete because when taking age samples they just take it as discrete numbers without decimals or months if some wanted to say, it's all age ranges or random ages. while this is is argument against his claim.\nhope I didn't talk too much.\n\nedit: I know it depends on the preferred model but what is it considered as generally ",
    "author": "murasaki_yami",
    "timestamp": "2025-10-04T00:15:46",
    "url": "https://reddit.com/r/statistics/comments/1nxndzt/question_statistically_and_mathematically_is_age/",
    "score": 74,
    "num_comments": 77,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nxu59y",
    "title": "Suggest some of the best website,YouTube channels,books etc to learn statistics for ug level [Question]",
    "content": "",
    "author": "Popular_Lettuce7084",
    "timestamp": "2025-10-04T06:34:01",
    "url": "https://reddit.com/r/statistics/comments/1nxu59y/suggest_some_of_the_best_websiteyoutube/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nxk9kl",
    "title": "[Question] Is there a special term or better way to phrase \"the maximum lowest outcome\"?",
    "content": "As an example, let's say I'm picking 10 marbles from a bag of 100 marbles. The marbles can come in the colors red, blue, green, and yellow, and there are 25 marbles of each color. In this situation, I want to randomly pick 10 marbles from the bag with the hopes of grabbing the highest number of marbles of the same color.\n\nObviously, the highest number of marbles that could be of one color is 10 while the lowest number of same-color marbles is 1, or even technically 0. But the question I want to learn how to phrase is essentially equivalent to what is the worst possible outcome in this situation?\n\nTo my understanding, the worst combination of marble colors in my example would be 3/3/2/2 or 3/3/3/2, so the numerical answer is 3, because that's the \"maximum lowest number\" of same color marbles. So, how should I phrase the question that would give me the prior answer in a way that is more specific than \"whats the worst outcome\" but more generalized than explaining literally the entire example set-up?\n\nTldr; Is there a specific term/phrase or a better way to describe the maximum lowest possible outcome of a combination?\n\nThanks!",
    "author": "akero360",
    "timestamp": "2025-10-03T21:13:07",
    "url": "https://reddit.com/r/statistics/comments/1nxk9kl/question_is_there_a_special_term_or_better_way_to/",
    "score": 10,
    "num_comments": 13,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ny2lnz",
    "title": "[Question] AP Stats Question, pls help",
    "content": "A final exam for a college algebra class had 60 questions. For all the students that took the\nexam, the mean number of questions answered correctly is 54, with a standard deviation of 12.\nWould it be reasonable to assume that the distribution of the number of questions answered correctly is approximately normal? Explain.\n\nCan someone help explain this?üòì\n\n",
    "author": "Define_22",
    "timestamp": "2025-10-04T12:08:53",
    "url": "https://reddit.com/r/statistics/comments/1ny2lnz/question_ap_stats_question_pls_help/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nxh38c",
    "title": "[Discussion] Measures of Central Tendency for Levels of Measurement",
    "content": "I'm currently enrolled in an advanced statistical analysis course for my postgrad in applied statistics. Since high school, I've taken quite an interest in research and statistics. I've familiarized myself with the basics, especially in descriptive statistics. \n\nBut recently, I've learned a major error that I've been making since high school up until my undergrad thesis: **using mean to analyze ordinal data**, i.e., Likert scale. Apparently, since the data are ordinal, it would make more sense to use the median to analyze the data. Even in my current job, my manager has set an action standard using average liking scores to determine recommendations for our projects. The scales we've been using for data gathering were ordinal-often Likert scales for our initial tests. \n\n  \nThis is a particularly new learning for me. Any thoughts on this? Or can you suggest any reference I could read that supports this?\n\n ",
    "author": "slapmenanami",
    "timestamp": "2025-10-03T18:29:34",
    "url": "https://reddit.com/r/statistics/comments/1nxh38c/discussion_measures_of_central_tendency_for/",
    "score": 4,
    "num_comments": 9,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nwxdi6",
    "title": "[Question] Trouble with convergence in a mixed model in R",
    "content": "I'm trying to analyse some behavioural data. I have a large dataset which shows how the behaviour varies with time and the population of origin, and for a subset of that data I also have measurements of other traits that are predicted to explain the behaviour.\n\nFor the first (larger) model I included time and population as fixed effects, and I found that time significantly explained the behaviour, and that while population wasn't significant, there was a sig. interaction between time and the population of origin, which was explained by much lower readings in a single population toward the end of the observation period (as shown by a tukey post-hoc).\n\nNow I'm trying to model the additional traits that are predicted to explain the behaviour.  The other traits also vary across time and population, so I want to include the new variables as fixed effects, and time &amp; pop as random effects in order to remove that correlation. However, including population in the model causes a convergence error (because only one group is different to all the others).\n\nSo what do I do? I can't just ignore the interaction or the group driving it, but I also cannot see how to include it in my model.\n\nI'm working in R with generalised linear mixed models from lme4. Time (i.e. the month of observation) and population are encoded as factors, while the additional variables are continuous. Each measured individual was randomly sampled at only one time point. \n\nI've tried encoding the random effects variously as ... + (1|month) + (1|population), or ... +(1|month:population). Neither helped with the convergence issue.\n\nI'm aware that this is probably a stupid question and betrays a lack of basic understanding. Yeah. But any advice you can give would be appreciated :)",
    "author": "c_aterpillar",
    "timestamp": "2025-10-03T05:10:49",
    "url": "https://reddit.com/r/statistics/comments/1nwxdi6/question_trouble_with_convergence_in_a_mixed/",
    "score": 6,
    "num_comments": 9,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nwe2fv",
    "title": "[Education] Great YouTube channel for learning stats fundamentals",
    "content": "Hey folks, \n\nI just wanted to drop in and recommend a Youtube channel that really helped me to polish off some basic concepts of Stats. \n\nWhen I started with stats in uni, I was overwhelmed by the number of topics and the formulas. Then someone recommended me this channel, and I never looked back. Aced all my classes, and now I am seriously considering a career that is heavy on statistics.\n\nChannel name : Bandon Foltz\n\nLink : [https://www.youtube.com/@BrandonFoltz](https://www.youtube.com/@BrandonFoltz)  \n",
    "author": "YellowKeyYield",
    "timestamp": "2025-10-02T12:53:22",
    "url": "https://reddit.com/r/statistics/comments/1nwe2fv/education_great_youtube_channel_for_learning/",
    "score": 45,
    "num_comments": 1,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nwv89n",
    "title": "[Question] I want to do a Multi-level-model in a Meta-Analysis for my masters thesis",
    "content": "I collected 44 Studies that fit my research question, about occupational death. I wrote SQLite Code in R to get a Databank of four tables. One with all the studies, one with the impact factors of the journals, one with the models of the studies and the last one with the effects of the models.\n\nI collected all the empirical analysis that used HR (Hazard Ratio), OR (odds ratio), SMR (standardized mortality ratio) and RR (relative risk) and calculated se, z- and p-value for them logarithmic and linear for ERR (Excess Relative Risk) effects.\n\nI wanted to do models with the log effects and the linear separate. The two models I wanted to calculate should look like this:\n\n1. effects ‚àà models ‚àà data origin\n2. effects ‚àà  models ‚àà studies ‚àà author\n\nThe next step would be a cross-validation of the two models and using mixed-effects (random and fixed)\n\nI got my database but I'm struggeling with the R-code for a good multi-level  \nThe foret plot attached is the result of the first model without random effects.  \n[https://imgur.com/a/iJvUITx](https://imgur.com/a/iJvUITx)\n\nEvery thought and help is appreciated and sorry for poor english.",
    "author": "Lascho94",
    "timestamp": "2025-10-03T03:18:52",
    "url": "https://reddit.com/r/statistics/comments/1nwv89n/question_i_want_to_do_a_multilevelmodel_in_a/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nwmhjn",
    "title": "[Question] Which line items should I exclude from these financial statements to apply Benford's Law for fraud detection?",
    "content": "\r\nHey r/statistics\n\r\nI'm diving into some forensic accounting work and want to run a Benford's Law analysis on a set of financial statements to check for anomalies/fraud. I've got this Google Sheet with balance sheet, income statement, and maybe cash flow data: [The Google Sheet link is in the comments below.]\r\n\r\nFor those unfamiliar, Benford's Law looks at the distribution of leading digits in numerical data (expecting more 1s than 9s, etc.), but it only works well on \"naturally occurring\" numbers from transactions. So, I know I need to filter out stuff like totals, percentages, negatives, zeros, and rounded estimates to avoid skewing the results.\r\n\r\nQuick question: Based on standard practice, which specific line items or types of accounts in typical financial statements should I **remove** before running the analysis? For example:\r\n- All subtotals and grand totals (obvious, but confirm)?\r\n- Deferred revenue or accrued expenses (since they might be estimates)?\r\n- Equity sections or non-operating items?\r\n- Anything from the cash flow statement?\r\n\r\nIf you've got a checklist or tool (like in Excel/Python) for cleaning data for Benford's, share away! Also, any tips on handling multi-year data or currency conversions?\r\n\r\nThanks in advance ‚Äì trying to get this right for a real case.\r\n\r\n",
    "author": "The-Utimate-Vietlish",
    "timestamp": "2025-10-02T18:48:41",
    "url": "https://reddit.com/r/statistics/comments/1nwmhjn/question_which_line_items_should_i_exclude_from/",
    "score": 4,
    "num_comments": 11,
    "upvote_ratio": 0.61,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nw8ew8",
    "title": "[Education] Resources to pass college statistics?",
    "content": "I need to pass statistics but I have a rocky background with math.\n\nI attempted the class once and made to week 4 easy but the txt book got confusing and my need to read each chapter a million times set me back so dropped. \n\nAny tips on resources to use or where to start?\n\nUnit 1: Sampling data\nUnit 2: Descriptive statistics\nUnit 3: Linear Regression &amp; Correlation \nUnit 4: Normal Distribution &amp; CLT\nUnit S1: Bootstrap CI\nUnit 5: Confidence Intervals\nUnit 6: Hypothesis Testing Preliminaries \nUnit 7: Hypothesis Testing for Proportion (categorical data)\nUnit 8: Hypothesis Testing for Means\nUnit 9: Chi-Square Test of Independence \nUnit S2: Randomization Tests",
    "author": "buzzdeletedigit",
    "timestamp": "2025-10-02T09:23:44",
    "url": "https://reddit.com/r/statistics/comments/1nw8ew8/education_resources_to_pass_college_statistics/",
    "score": 7,
    "num_comments": 5,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nwmneh",
    "title": "[D] Matching controls to treatments with low participation rate in healthcare intervention project",
    "content": "Is there a way to propensity score match treatments to controls in observational data if only a small percentage of eligible members in the treatment group have elected to participate in the intervention program?\n\nMy employer doesn't have good data for predicting who will choose to participate, making it difficult to select controls with similar propensity scores.\n\nThe best solution at the moment is a variation of intention-to-treat for observational data, where all participants &amp; non-participants in the treatment group are lumped together and compared with the eligible control population. This makes a (reasonable) assumption the controls have a similar proportion of people who would be motivated to participate in the healthcare intervention. \n\nITT reduces bias but also dilutes the treatment group with non-participants. Is there a way around this?",
    "author": "RobertWF_47",
    "timestamp": "2025-10-02T18:56:19",
    "url": "https://reddit.com/r/statistics/comments/1nwmneh/d_matching_controls_to_treatments_with_low/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nwudzb",
    "title": "What's the point in learning university-level math when you will never actually use it? [Q]",
    "content": "I know it's important to understand the math concepts, but I'm talking about all the manual labor you're forced to go through in a university-level math course. For example, going through the painfully tedious process to construct a spline, do integration by parts multiple times, calculate 4th derivatives of complicted functions by hand in order to construct a taylor series, do Gauss-Jordan elimination manually to find the inverse of a matrix, etc. All those things are done quick and easy using computer programs and statistical packages these days.\n\nUnless you become a math teacher, you will never actually use it. So I ask, what's the point of all this manual labor for someone in statistics?",
    "author": "gaytwink70",
    "timestamp": "2025-10-03T02:26:48",
    "url": "https://reddit.com/r/statistics/comments/1nwudzb/whats_the_point_in_learning_universitylevel_math/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nw39m6",
    "title": "[Q] Need help choosing a stats learning path",
    "content": "I work in e-commerce and I want to strengthen my statistics foundations for things like A/B testing, hypothesis testing, regression, forecasting, and general business analytics. I don‚Äôt need very heavy math proofs but I want good intuition, a wide range of tools, and examples that make sense for business.\n\nThe books I am looking at are:\n\n‚Ä¢Cartoon Guide to Statistics (for a light start)\n‚Ä¢OpenIntro Statistics (for basics)\n‚Ä¢Applied Statistics in Business &amp; Economics (Doane &amp; Seward) or Business Statistics: For Contemporary Decision Making (Ken Black)\n‚Ä¢Practical Statistics for Data Scientists or Think Stats (3rd edition)\n‚Ä¢Statistical Methods in Online A/B Testing (Georgiev)\n‚Ä¢Trustworthy Online Controlled Experiments (Kohavi)\n‚Ä¢Maybe All of Statistics, The Art of Statistics, or Causal Inference in Statistics as extra references\n\nRight now for example, in my company we have a loyalty program. Next year they want to increase the spend thresholds for the tiers. I feel like this is the kind of problem where I could use statistics to test if the change would be good or not, since I have customer data and tier information.\n\nMy questions are:\n1.For the general applied stats book, should I go with Doane &amp; Seward or Ken Black\n2.Do you think online courses like Coursera or Udemy would be a better choice for me than going through these books\n3.Does this stack look balanced for someone in e-commerce or am I making it too heavy\n\nWould really appreciate your advice.",
    "author": "Admirable_Door4350",
    "timestamp": "2025-10-02T06:06:34",
    "url": "https://reddit.com/r/statistics/comments/1nw39m6/q_need_help_choosing_a_stats_learning_path/",
    "score": 4,
    "num_comments": 6,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nvpwmv",
    "title": "[Q] Stats vs DS",
    "content": "I‚Äôm choosing between Georgia Tech‚Äôs MS in Statistics and UMich Master‚Äôs in Data Science. I really like stats -- my undergrad is in CS, but my job has been pushing me more towards applied stats, so I want to follow up with a masters. The problem I'm deciding between is if UMich‚Äôs program is more ‚Äúfluffy‚Äù content -- i.e., import sklearn into a .ipynb -- compared to a proper, rigorous stats MS like at GTech. Simultaneously, the name recognition of UMich might make it so it doesn't even matter.\n\nFor someone whose end goal is a high-level Data Scientist or Director level at a large company, which degree would you recommend? If you‚Äôve taken either program, super interested to hear thoughts. Thanks all!",
    "author": "Acrobatic-Boot-3843",
    "timestamp": "2025-10-01T17:55:02",
    "url": "https://reddit.com/r/statistics/comments/1nvpwmv/q_stats_vs_ds/",
    "score": 20,
    "num_comments": 23,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nvoixd",
    "title": "[Q] i have a probably quick and easy question about breaking down the probability of a side bet at a casino i go too",
    "content": "Hello everyone,\n\nCan someone take me through the working out and result for this side bet at a casino.\n\nOk so the game is blackjack and tbere are 6 decks in play.\n\nThe side bet requires the player to get either an ace and jack of hearts OR an ace and jack of diamonds plus the dealer needs to hit any blackjack (any ace combined with any 10 value card, thus being any king, queen, jack, or ten).\n\nI am curious to know the odds (1 in X hands)\n\nCheers",
    "author": "devin93uk",
    "timestamp": "2025-10-01T16:50:34",
    "url": "https://reddit.com/r/statistics/comments/1nvoixd/q_i_have_a_probably_quick_and_easy_question_about/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nvljix",
    "title": "[Research] Which test?",
    "content": "Conducting a study where I investigate how anxiety and shyness correlate with flirting behaviors/attitudes. Participants‚Äô scores on an anxiety scale and a shyness scale will correlate to their responses on a flirting survey. Which test should I use for the data? A t-test? An f-test (ANOVA)?",
    "author": "CCMacchiatto",
    "timestamp": "2025-10-01T14:44:56",
    "url": "https://reddit.com/r/statistics/comments/1nvljix/research_which_test/",
    "score": 0,
    "num_comments": 18,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nurdot",
    "title": "[R] Forecasting Outcome Variable with Artificial \"Supply\" Constraint",
    "content": "Hello,\n\nSo I'm trying to build out a predictive model to forecast future ticket sales for comedy shows, trained on the comedians' historical ticket sales performance. Currently, I'm just using a linear model, with the comedians' podcast viewership by metropolitan area and a control for venue capacity as independent variables. There is a clear linear relationship between the comedian's podcast views and the comedian's ticket sales. That relationship only grows more robust when making population adjustments (e.g., views per capita).\n\nOne hurdle I keep running into is that the¬†**ticket sales outcomes are artificially constrained by the capacity of the venue**. The modal show is a \"sell out.\" Subsequently, the model I'm developing -- while robust -- tends to be really conservative, hovering around the venue's capacity. Ideally, this model would help indicate where sales might even exceed capacity.\n\nAre there any methods appropriate for this type of analytics? One with an artificial supply constraint such as venue capacity? I've looked into the tobit model, which I think is a good place to start? But is there anything else I should poke around into to help me develop this project?\n\nI might also explore modeling out \"Percent of tickets sold\" rather than nominal ticket sales, though that has proven to be less robust in some early analyses.\n\nThanks!",
    "author": "Objective-You-7291",
    "timestamp": "2025-09-30T15:32:25",
    "url": "https://reddit.com/r/statistics/comments/1nurdot/r_forecasting_outcome_variable_with_artificial/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nuota9",
    "title": "[Software] Simple Query stats tool",
    "content": "Hello,\n\nI was curious if anyone here would be willing to give my tool a look. It's completely free, and still new and not feature complete yet but a good MVP I think. I think the audience here is probably more advanced than the intended audience but would appreciate your points of view.\n\nYou can find it here: [https://simplequery.io](https://simplequery.io)",
    "author": "justbane",
    "timestamp": "2025-09-30T13:48:37",
    "url": "https://reddit.com/r/statistics/comments/1nuota9/software_simple_query_stats_tool/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nutce0",
    "title": "[Q] , [E]; can I use MAD instead of simple standard deviation to calculate SEM?",
    "content": "Hi guys. Was wondering if the Sem (Standard error of the mean) can be calculated using MAD instead of simple standard deviation because sem = s/root n takes a lot of time in some labs where I need to do an error analysis. Also just wanted to say mean absolute deviation, I have a feeling y‚Äôall already know but a STAT major in r/homework help thought it was median so idk if it means something else post- high school ",
    "author": "MischievousPenguin1",
    "timestamp": "2025-09-30T17:00:13",
    "url": "https://reddit.com/r/statistics/comments/1nutce0/q_e_can_i_use_mad_instead_of_simple_standard/",
    "score": 3,
    "num_comments": 9,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nusulq",
    "title": "[Q] there is a radio station doing a promotion where you are picking three winners against the spread. If you pick three winners your name is advanced to a weekly drawing.  It would be the same as picking the outcome of a coin toss correctly three times in a row.",
    "content": "I was thinking of going in cahoots with my wife and making opposite picks.  So if I pick HHH and she picks TTT, would we have a better chance of one of us winning the weekly contest?  The way I see it, between the two of us, we will always win 2 out of three and it would come down to a 50/50 situation instead of a one in three situation.",
    "author": "DirtyDizzyPickle20",
    "timestamp": "2025-09-30T16:37:08",
    "url": "https://reddit.com/r/statistics/comments/1nusulq/q_there_is_a_radio_station_doing_a_promotion/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ntk4a8",
    "title": "[Q] Are traditional statistical methods better than machine learning for forecasting?",
    "content": "I have a degree in statistics but for 99% of prediction problems with data, I've defaulted to ML. Now, I'm specifically doing forecasting with time series, and I sometimes hear that traditional forecasting methods still outperform complex ML models (mainly deep learning), but what are some of your guys' experience with this?",
    "author": "CIA11",
    "timestamp": "2025-09-29T07:52:26",
    "url": "https://reddit.com/r/statistics/comments/1ntk4a8/q_are_traditional_statistical_methods_better_than/",
    "score": 113,
    "num_comments": 47,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ntcxqj",
    "title": "In your opinion, what‚Äôs the most important real-world breakthrough that was driven by statistical methods? [Q]",
    "content": "",
    "author": "External_Mobile_4593",
    "timestamp": "2025-09-29T01:51:03",
    "url": "https://reddit.com/r/statistics/comments/1ntcxqj/in_your_opinion_whats_the_most_important/",
    "score": 91,
    "num_comments": 37,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nttwtg",
    "title": "[Q] Has anyone any experience with classical methods for assessment?",
    "content": "I am designing a test that will be taken by thousands of people to measure their numeracy ability, the outcome for each will be low, medium or high numeracy. The question items are multiple choice and written to reflect an existing numeracy skill framework. So the test will have 20 low numeracy ability questions, 20 medium questions and 20 high. The outcome is to decide which category best describes the person. Are there any classical statistical methods that can help with this categorisation problem? I am familiar with some IRT methods but would like to ask other statisticians if they have any ideas for a reasonably simple method for classifying based on responses to these three different difficulty questions or assessing the reliability of the categorisation.",
    "author": "cowboysted",
    "timestamp": "2025-09-29T14:00:01",
    "url": "https://reddit.com/r/statistics/comments/1nttwtg/q_has_anyone_any_experience_with_classical/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nttosw",
    "title": "[Q] Rounding question",
    "content": "We have a survey where we asked people what rents they charged for an apartment. We knew from focus groups they would not give us an exact number, so we provided ranges (e.g. $1000-$1,500 per month). We have to do some statistics on their answers but for government reporting reasons, we need to break the range down to exact numbers again. (For example, the government wants to know how many people charged more then $1,400 a month in rent.) What do you recommend?\n\nAnd if this is best posted in a different subreddit, let me know. Thanks",
    "author": "catdogfish4",
    "timestamp": "2025-09-29T13:51:27",
    "url": "https://reddit.com/r/statistics/comments/1nttosw/q_rounding_question/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ntdmdq",
    "title": "[Question] Biostatistics books",
    "content": "I finished my PhD in Pharmacoepidemiology 8 years ago. Since then I have worked as a data scientist. I would like to find my way back into epidemiology/public health research. During my PhD I mostly learned the statistics that were used for my research. I would therefore like to have a better foundation in biostatistics. Which biostatistics book would you recommend for someone with basic epidemiological and statistical knowledge? So far I found the books below. Which is best or would you recommend a similar book?\n\n* Biostatistics: A Foundation for Analysis in the Health Sciences by Wayne W. Daniel &amp; Chadd L. Cross\n* Introduction to Biostatistics and Research Methods by P.S.S. Sundar Rao\n* Fundamentals of Biostatistics by Bernard Rosner\n\nThank you!",
    "author": "sancho_panza66",
    "timestamp": "2025-09-29T02:37:12",
    "url": "https://reddit.com/r/statistics/comments/1ntdmdq/question_biostatistics_books/",
    "score": 12,
    "num_comments": 0,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nt4y7m",
    "title": "[Q] Textbook on statistical tests and simple models as GLMMs",
    "content": "I saw a slide from a presentation some time ago where they showed a picture depicting the t-test as a special case of ANOVA as a special case of a linear model as a special case of GLM / GMM as a special case of a GLMM.\n\nThe point of the slide was basically that if you intuitively understand the most general model, then you can simply understand all these other tests and simpler models as just special cases of the general model. \n\nI really like this idea and want to understand this intuitively for myself. Can you recommend good texts (or specific chapters from texts) on this? Preferably focusing on intuition and conceptual understanding over mathematical rigor. \n\nThere are some other online resources that try to get at this idea, like: https://lindeloev.github.io/tests-as-linear/\n\nBut I think I want to read a little bit more formalized approach.\n\nThank you",
    "author": "padakpatek",
    "timestamp": "2025-09-28T18:05:56",
    "url": "https://reddit.com/r/statistics/comments/1nt4y7m/q_textbook_on_statistical_tests_and_simple_models/",
    "score": 24,
    "num_comments": 3,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nsx6hy",
    "title": "[Discussion] Is a masters in Statistics worth &lt;$40k in student loans?",
    "content": "I am graduating with my BS in statistics, and am pretty thoroughly set on graduate school. I don‚Äôt think I will be applying to PhD programs because my end goal is working in industry, and 6-7 years is just too long of a time commitment for me. I have considered applying to PhD programs with the option to master out, since I have a couple years of research + authorship on some papers, but I‚Äôm worried about the ethics of going in to a PhD wanting to master out.  \n\nI‚Äôm looking at thesis based masters, with the goal of being a TA/RA or some position that would provide tuition waivers. If I can‚Äôt get one of these (very competitive/rare for a masters student), I‚Äôd have to work part time and take out loans. \n\nI‚Äôve crunched the numbers and could fully support my living expenses with summer work + a part time job during the academic year. But I would have to cover tuition mostly or fully with loans ($40k total for a two year program). \n\nI‚Äôm finishing undergrad with no student debt, which is why I am open to a max of $40k in graduate loans. To me, it seems reasonable and financially worth it in the long run because a masters degree provides much higher starting salaries. I believe I could pay off these loans in one or two years if I paid them off aggressively. I‚Äôm just wondering how flawed my expectations or plans are. \n\nEdit: these are MS/MA programs in the University of California system. ",
    "author": "Voldemort57",
    "timestamp": "2025-09-28T12:26:10",
    "url": "https://reddit.com/r/statistics/comments/1nsx6hy/discussion_is_a_masters_in_statistics_worth_40k/",
    "score": 44,
    "num_comments": 42,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nt5ol6",
    "title": "[Discussion] should I major In math and minor in stats or should it be the other way around?",
    "content": "Hay guys I saw a conversations on this sub about before and it made me want to lean more so I made this post.",
    "author": "deesnuts78",
    "timestamp": "2025-09-28T18:42:11",
    "url": "https://reddit.com/r/statistics/comments/1nt5ol6/discussion_should_i_major_in_math_and_minor_in/",
    "score": 8,
    "num_comments": 9,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nsvi25",
    "title": "[Discussion] Choosing topics for Statober",
    "content": "During this October, I would like to repeat various statistical methods with my small statistical community. One day = one topic. I came up with the list of tests and distributions but I am not completely sure about the whole thing. Right now, I am going to just share some materials on the topic.\n\nWhat can I do to make it more entertaining/rewarding?\n\nPerhaps I could ask people to come up with interesting examples?\n\nAlso, what do you think about the topics? I am not really sure about including the distributions.\n\nList of the topics:\n\n1. Normal distribution\n2. Z-test\n3. Student's t distribution\n4. Unpaired t test\n5. Binomial distribution\n6. Mann-Whitney test\n7. Hypergeometric distribution\n8. Fisher's test\n9. Chi-squared distribution\n10. Paired t test\n11. Poisson distribution\n12. Wilcoxon test\n13. McNemar's test\n14. Exponential distribution\n15. ANOVA\n16. Uniform distribution\n17. Kruskal-Wallis test\n18. Chi-square test\n19. Repeated-measures ANOVA\n20. Friedman test\n21. Cochran's Q test\n22. Pearson correlation\n23. Spearman correlation\n24. Cramer's V\n25. Linear regression\n26. Logistic regression\n27. F Test\n28. Kolmogorov‚ÄìSmirnov test\n29. Cohen's kappa\n30. Fleiss's kappa\n31. Shapiro‚ÄìWilk test",
    "author": "DenOnKnowledge",
    "timestamp": "2025-09-28T11:20:31",
    "url": "https://reddit.com/r/statistics/comments/1nsvi25/discussion_choosing_topics_for_statober/",
    "score": 7,
    "num_comments": 6,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nsjjsm",
    "title": "[S] Differentiable parametric curves for PyTorch",
    "content": "I‚Äôve released a small library for parametric curves for PyTorch that are differentiable: you can backprop to the curve‚Äôs inputs and to its parameters. At this stage, I have B-Spline curves (efficiently, exploiting sparsity!) and Legendre Polynomials. Everything is vectorized - over the mini-batch, and over several curves at once.\n\nLink:¬†[https://github.com/alexshtf/torchcurves](https://github.com/alexshtf/torchcurves)\n\nApplications include:\n\n* Continuous embeddings for embedding-based models (i.e. factorization machines, transformers, etc)\n* KANs. You don‚Äôt have to use B-Splines. You can, in fact, use any well-approximating basis for the learned activations.\n* Shape-restricted models, i.e. modeling the probability of winning an auction given auction features x and a bid b - predict increasing B-Spline coefficients c(x) using a neural network, apply to a B-Spline basis of b.\n\n  \nI wrote ad-hoc implementations for past projects, so I decided to turn it into a library.  \nI hope some of you will find it useful!",
    "author": "alexsht1",
    "timestamp": "2025-09-28T01:58:15",
    "url": "https://reddit.com/r/statistics/comments/1nsjjsm/s_differentiable_parametric_curves_for_pytorch/",
    "score": 29,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nsyvxs",
    "title": "[E] Probability Question",
    "content": "Hey guys. I have an embarrassing probability question which for which I was hoping to get a relatively simple explanation.\n\nYou walk past a shop selling scratch cards, with a finite number of these cards printed. The sign in front of the shop says ‚Äòthis week we had a million dollar winner from this shop‚Äô.\n\nThe presumption is that it‚Äôs the same brand of scratch card we‚Äôre talking about.\n\nWould it be less likely that someone bought a second winning scratch card from the same vendor during the run of these scratch cards?\n\nI‚Äôm thinking an extreme example of this would be the likelihood of ten people in a row getting a big winning card from the same vendor.\n\nI‚Äôve heard of conditional probability and gambler‚Äôs fallacy but I‚Äôm still not getting it in this particular scenario.",
    "author": "Complex-Main",
    "timestamp": "2025-09-28T13:34:13",
    "url": "https://reddit.com/r/statistics/comments/1nsyvxs/e_probability_question/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nspf6j",
    "title": "[Question] Retrait d'individus dans questionnaire",
    "content": "**Bonjour,**\n\nJ'ai un questionnaire en psychologie du travail avec 722 participants. Certains n'ont pas r√©pondu √† toutes les questions donc dans un premier temps j'ai enlev√© tous les participants n'ayant pas r√©pondu √† toutes les questions (avec des trous dans la matrice donc). Il me reste 482 sujets. Le probl√®me est que si chaque participant n'avait pas r√©pondu √† une seule question parmi les 18 je me serais retrouv√©, avec cette m√©thode, avec z√©ro participant exploitable donc mon √©tude √† la poubelle.\n\nExiste t'il une norme √† ce sujet, une norme qui permettrait de d√©cider si on garde ou non un participant en fonction du nombre de questions r√©pondues versus le nombre total de questions?\n\nMerci pour vos r√©ponses\n\n\n\n",
    "author": "Nuclear_Maxx",
    "timestamp": "2025-09-28T07:15:22",
    "url": "https://reddit.com/r/statistics/comments/1nspf6j/question_retrait_dindividus_dans_questionnaire/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nsnbrg",
    "title": "Probability/Statistics guidance needed for warrant trading with rollovers and no Stop-Loss [Discussion]",
    "content": "Hello,\n\nI‚Äôm a retail trader for 3 years, focused on index warrants, and I want to get serious about quantifying risk, drawdowns, and position sizing using probability and statistics.\n\nHere‚Äôs my setup:\n\n* \\~300 trades/year\n* I don‚Äôt use stop losses. Losing positions are held until reversal, historically \\~14 days on average. I roll over warrants with a 9‚Äì12 month expiration window\n* I trade both directions (calls and puts) \n* Occasionally, extreme trades happen: \\~2 per year were historically ‚Äúunrecoverable.‚Äù I either offset them gradually with profits, or if critical, cut them and move on.\n* I currently use fractional Kelly (\\~1/6) for position sizing.\n\nMy goals:\n\n1. Estimate the tail risk of ruin and portfolio survival over multiple years, accounting for different trade counts.\n2. Optimize position sizing / Kelly fraction considering the above risk calculations.\n\nI have intermediate Python skills. I‚Äôm looking for practical guidance on where to start and focus, which methods/theories are directly applied to this case.\n\nAppreciate any help/resource/2cent.\n\nThank you!",
    "author": "MarionberryTotal2657",
    "timestamp": "2025-09-28T05:39:46",
    "url": "https://reddit.com/r/statistics/comments/1nsnbrg/probabilitystatistics_guidance_needed_for_warrant/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ns2797",
    "title": "Resume Advice for a Recent Stats/CS Grad with 0 YoE [C]",
    "content": "I'm just not getting any interviews. I am looking mostly at data analyst roles... I like data visualization. I have been looking all over the US and I am willing to relocate but would prefer the greater Seattle region. Any feedback would be appreciated on my resume. Thank you.\n\nhttps://preview.redd.it/2agel7eo0rrf1.png?width=653&amp;format=png&amp;auto=webp&amp;s=c59b5c5a11809b1153317cbb554b6768a9d9a728\n\n",
    "author": "tripcup",
    "timestamp": "2025-09-27T11:14:53",
    "url": "https://reddit.com/r/statistics/comments/1ns2797/resume_advice_for_a_recent_statscs_grad_with_0/",
    "score": 5,
    "num_comments": 11,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nrsclu",
    "title": "Factor Analysis for Categorical Data [Q]",
    "content": "Hello everyone, I'm conducting a factor analysis to investigate a possible latent structure for 10 symptoms defined by only dichotomous variables (0 = absent, 1 = present). How can I manage an exploratory factor analysis with only categorical variables? Which correlation matrix is ‚Äã‚Äãbest to use?",
    "author": "Funny-Leading-7476",
    "timestamp": "2025-09-27T03:59:59",
    "url": "https://reddit.com/r/statistics/comments/1nrsclu/factor_analysis_for_categorical_data_q/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nrnpvw",
    "title": "[Q] What",
    "content": "&gt;Consistent estimators do **NOT** always exist, but they do for most well-behaved problems.\n\n&gt;In the **Neyman-Scott** problem, for instance, a consistent estimator for œÉ^(2) **does** exist. The estimator\n\n&gt;T‚Çô = (1/n) Œ£·µ¢‚Çå‚ÇÅ‚Åø \\[ ((X·µ¢‚ÇÅ ‚àí X·µ¢‚ÇÇ) / 2) ¬≤\\]\n\n&gt;is unbiased for œÉ^(2) and has a variance that goes to zero, making it consistent. The MLE fails, but other methods succeed. However, for some **pathological, theoretically constructed** distributions, it can be proven that no consistent estimator can be found.\n\nCan anyone pls throw some light on what are these \"pathological, theoretically constructed\" distributions?  \nAny other known example where MLE is not consistent?\n\n(Edit- Ignore the title, I forgot to complete it)",
    "author": "Cold-Gain-8448",
    "timestamp": "2025-09-26T23:06:06",
    "url": "https://reddit.com/r/statistics/comments/1nrnpvw/q_what/",
    "score": 7,
    "num_comments": 3,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nrluye",
    "title": "[Career] Recent Stats BA (No Co-op/Internship) Aiming for a productive Gap Year before Grad School -  What Entry-Level Roles Are Realistic?",
    "content": "Hey everyone,\n\nI just graduated with a BA in Statistics and a minor in Economics in Canada. My original plan was to take a year off before applying to a master's program to gain some real-world, hands-on experience and find a focus for grad school.\n\nThe Problem: Struggling to Land the First Job\n\nMy university didn't offer a co-op program, so I'm finishing school with strong academic coursework (regression, time series, stochastic processes, experimental design, linear algebra) and projects, but no formal internship experience.\n\nI've been applying to Jr Data Analyst, Business Analyst, Research Assistant roles but so far I've had no luck. I'm worried about this \"gap year\" turning into wasted time.\n\nIdeally, I'd love to work in finance or quantitative analysis to better inform my grad school specialization, but I'm open to anything that uses my skill set. I know about the actuarial path and am ready to start studying for the first two exams if I can't find an analysis job soon.\n\nI'm looking for advice from those who have hired stats grads or successfully navigated a similar gap year.\n\nSpecific Questions:\n\n* **Target Jobs:** What entry-level jobs should someone with a fresh Stats BA and **no co-op** realistically target? (Specific titles or industries would be amazing.)\n* **Alternative Focus:** Should I temporarily shift my focus entirely to **internships (even post-grad), short-term research gigs, or volunteer data projects** instead of formal full-time jobs?\n* **Gap Year Success:** For those who took time off before grad school, what made that year truly **worthwhile** and productive?\n\nI'm feeling a little stuck and just want to make this year count. Any tips, advice, or personal stories would be hugely appreciated!\n\nThanks in advance.",
    "author": "WannaGetGood",
    "timestamp": "2025-09-26T21:16:43",
    "url": "https://reddit.com/r/statistics/comments/1nrluye/career_recent_stats_ba_no_coopinternship_aiming/",
    "score": 3,
    "num_comments": 7,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nrexfq",
    "title": "[Q] Alternatives to forest plots for large meta-analyses",
    "content": "I‚Äôm planning a meta-analysis for a scientific study, but I expect to include so many studies that a traditional forest plot would become overcrowded and unreadable. What are some effective and neat ways to present the results when the number of studies is too large for a forest plot to be practical?",
    "author": "-Krois-",
    "timestamp": "2025-09-26T15:29:26",
    "url": "https://reddit.com/r/statistics/comments/1nrexfq/q_alternatives_to_forest_plots_for_large/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nr5169",
    "title": "[Q] Calculating error bars for a binomial distribution",
    "content": "Hello all, i am working on some data analysis for an experiment in which i was estimating success rates of different surface chemistry functionalizations. The outcomes are binomial as they either worked or did not work. My sample size is small as it is 10. I want to calculate error bars for this data. Ive seen a lot of different approaches (Wald method, Wilson, Clopper Pearson etc). I am also not super well versed in statistics. Any advice or sources to use on how to best navigate how to approach this calculation? ",
    "author": "Crow-1-million",
    "timestamp": "2025-09-26T08:54:11",
    "url": "https://reddit.com/r/statistics/comments/1nr5169/q_calculating_error_bars_for_a_binomial/",
    "score": 7,
    "num_comments": 8,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nrdeln",
    "title": "[E] [R] How to analyse dataset with missing values",
    "content": "I have a dataset with missing values. I would normally do Friedman but it won‚Äôt let you run that with missing values so the next best thing was the mixed model cos that can at least show the ANOVA results but it takes into account the missing values BUT it won‚Äôt let me click repeated measures for some reason (I really don‚Äôt know). So is it possible I can just remove the extra replicates so all the samples have the same amount of replicates and so I can run the Friedman? I would obviously mention in my results/discussion that the analysis was with a specific n value compared to how many replicates I actually recorded and is shown on the graph. ",
    "author": "iambored003",
    "timestamp": "2025-09-26T14:22:44",
    "url": "https://reddit.com/r/statistics/comments/1nrdeln/e_r_how_to_analyse_dataset_with_missing_values/",
    "score": 1,
    "num_comments": 20,
    "upvote_ratio": 0.54,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nqd6sv",
    "title": "[Q] How do you calculate prediction intervals in GLMs?",
    "content": "I'm working on a negative binomial model. Roughly of the form:\n\n    import numpy as np  \n    import statsmodels.api as sm  \n    from scipy import stats\n    \n    # Sample data  \n    X = np.random.randn(100, 3)  \n    y = np.random.negative_binomial(5, 0.3, 100)\n    \n    # Train  \n    X_with_const = sm.add_constant(X)  \n    model = sm.NegativeBinomial(y, X_with_const).fit()\n\n`statsmodels` has a `predict` method, where I can call things like...\n\n    X_new = np.random.randn(10, 3)  # New data\n    X_new_const = sm.add_constant(X_new)\n    \n    predictions = model.predict(X_new_const, which='mean')\n    variances = model.predict(X_new_const, which='var')\n\nBut I'm not 100% sure what to do with this information. Can someone point me in the right direction?\n\nEdit: thanks for the lively discussion! There doesn‚Äôt appear to be a way to do this that‚Äôs obvious, general, and already implemented in a popular package. It‚Äôll be easier to just do this in a fully bayesian way. ",
    "author": "jjelin",
    "timestamp": "2025-09-25T10:36:11",
    "url": "https://reddit.com/r/statistics/comments/1nqd6sv/q_how_do_you_calculate_prediction_intervals_in/",
    "score": 10,
    "num_comments": 28,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nqb45m",
    "title": "[Q] Causal inference: completeness of do-calculus",
    "content": "Do-calculus has three rules that allow you to manipulate and simplify causal queries: https://en.wikipedia.org/wiki/Do-calculus . The rules of do-calculus are proven to be complete, meaning that if there is no way to derive a purely observational query from a causal query using the rules, then the query is not identifiable.\n\nOK, cool. But here's my hangup: none of the rules completely get rid of all the interventions in the query. Whatever causal query you have, and whatever rule you apply, you're always left with some intervention after applying the rule. So how can the rules be used to get rid of all interventions to begin with..?\n\nI considered that maybe there's other simple rules that technically fall out of the do-calculus, but are still relevant (e.g., P(Y | do(X)) = P(Y) if X is not an ancestor of Y), but I'm not confident that seems relevant, really, and if that were the case I think it's misleading to say that do-calculus only includes those exact three rules.\n\nHelp, anybody?",
    "author": "BigBlindBais",
    "timestamp": "2025-09-25T09:17:20",
    "url": "https://reddit.com/r/statistics/comments/1nqb45m/q_causal_inference_completeness_of_docalculus/",
    "score": 11,
    "num_comments": 2,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nqta7n",
    "title": "[Q] Default plot does not change labels when using log argument?",
    "content": "Hi,  \nBelow is the code for a scatterplot between two variables 'Store spend' and 'Distance to store' in R\n\n*plot(cust.df$distance.to.store, cust.df$store.spend, main=\"store\")*\n\nThen I use log argument to make logarithmic conversion of both axes but I find that Y axis labels do no change in the 2nd plot.\n\n*plot(cust.df$distance.to.store, cust.df$store.spend+1, log=\"xy\", main=\"store, log\")*\n\nAre the axis labels themselves are not automatically updated to reflect the logarithmic scale in plot function?",
    "author": "Empty_Regret6345",
    "timestamp": "2025-09-25T22:41:51",
    "url": "https://reddit.com/r/statistics/comments/1nqta7n/q_default_plot_does_not_change_labels_when_using/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nq9hb1",
    "title": "Stats [Career] advice",
    "content": "Good Morning,\n\nI‚Äôm trying to provide advice / mentorship to a young man on online graduate stat degrees. I‚Äôm an epidemiologist and aware of introductory statistics (practice) but don‚Äôt know enough about what constitutes a good degree program, much less an online grad program. \n\nUS news last updated their ranking in ‚Äò22 for Stat depts and not sure that provides relevance. I have suggested to look at computer science rankings when looking at stat depts given how the two may interconnect. Any other suggestions? \n\nThe individual has the necessary background in calc and intro linear algebra (BS in data science) and is considering Purdue, Iowa State, and Oklahoma stat programs at this time. Any others worth looking into? He may consider others. Online programs necessary to accompany work schedule. Wants to work definitively in applied stats.Thanks to all in advance. ",
    "author": "Frequent_Argument_43",
    "timestamp": "2025-09-25T08:14:53",
    "url": "https://reddit.com/r/statistics/comments/1nq9hb1/stats_career_advice/",
    "score": 12,
    "num_comments": 3,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nqnkzv",
    "title": "[E] Sampling Distribution Help",
    "content": "I am teaching the Sampling Distribution and need some help for a class example. I need people to choose a random number between 1-100 from my website [https://samplingexplorer.org/](https://samplingexplorer.org/) so I can show how random samples approximate the true mean. If you could just pick a number from my sight, that would be amazing!",
    "author": "Squ3lchr",
    "timestamp": "2025-09-25T17:42:56",
    "url": "https://reddit.com/r/statistics/comments/1nqnkzv/e_sampling_distribution_help/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1npxn0n",
    "title": "Are the Cherian-Gibbs-Candes results not as amazing as they seem? [Discussion]",
    "content": "I'm thinking here of \"Conformal Prediction with Conditional Guarantees\" and subsequent work building on it. \n\nI'm still having trouble interpreting some of the more mysterious results, but intuitively it feels like they managed to achieve conditional coverage in the face of an impossibility result. \n\nReally, I'm trying to understand the limitations in practice. I was surprised, honestly, that having the full expressiveness of an RKHS to induce covariate shift (by tilting the input distribution) wouldn't effectively be equivalent to allowing any nonnegative measurable function.\n\nI'm also a little mystified how they pivoted to the objective that they did with the Lagrangian dual - how did they see that coming and make that leap?\n\n(Not a shill, in case it sounds like it. I am however trying to use these results in my work.)",
    "author": "RepresentativeBee600",
    "timestamp": "2025-09-24T21:37:06",
    "url": "https://reddit.com/r/statistics/comments/1npxn0n/are_the_cheriangibbscandes_results_not_as_amazing/",
    "score": 13,
    "num_comments": 6,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nq1f3z",
    "title": "How do you guys feel about the online MS in applied statistics at Purdue? [Discussion]",
    "content": "Admissions requirement:\n- An applicant‚Äôs prior education must include the following prerequisites: \n(1) one semester of Calculus \n\n- It is recommended that applicants show successful completion of the following undergraduate courses:\n(1) one semester of Statistics\nKnowledge of Computer Programming\n\n\nFoundational courses for the masters:\nSTAT 50600 | Statistical Programming and Data Management\nSTAT 51400 | Design of Experiments\nSTAT 51600 | Basic Probability and Applications\nSTAT 52500 | Intermediate Statistical Methodology\nSTAT 52600 | Advanced Statistical Methodology\nSTAT 52700 | Introduction to Computing for Statistics\nSTAT 58200 | Statistical Consulting and Collaboration ",
    "author": "Usual_Command3562",
    "timestamp": "2025-09-25T01:38:03",
    "url": "https://reddit.com/r/statistics/comments/1nq1f3z/how_do_you_guys_feel_about_the_online_ms_in/",
    "score": 7,
    "num_comments": 11,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1npydzh",
    "title": "[Q] Aggregate score from a collection of dummy variables?",
    "content": "TL;DR: Could I turn a collection of binary variables into an aggregate score instead of having a bunch of dummy variables in my regression model?\n\nHowdy,\n\nFor context, I am a senior undergrad in the honors program for economics and statistics. I'm looking into this for a class and, if all goes well, may carry it forward into an honors capstone paper next semester.\n\nI'm early in the stages of a¬†**regression model looking at the adoption of Buy Now, Pay Later (BNPL) products (Klarna, etc.) and financial constraints among borrowers**. I have data from the Survey of Household Economics and Decisionmaking with a subset of respondents who took the survey 3 years in a row, with the aim to use their responses from 2022, 2023, and 2024 to do a time series analysis.\n\nIn a¬†[recent article](https://www.kansascityfed.org/research/economic-review/financial-constraints-among-buy-now-pay-later-users/), economists Fumiko Hayashi and Aditi Routh identified¬†**11 variables in the dataset that would signal \"financial constraints\" among respondents**. These are all dummy variables.\n\nI'm wondering if it's reasonable to¬†**aggregate these 11 variables into an overall measure of financial constraints.**¬†E.g., \"respondent 4 showed 6 of the 11 indicators\" becomes \"respondent 4 had a financial constraint 'score' of 6/11 = 0.545\" for use in an econometric model as opposed to 11 discrete binary variables.\n\nThe purpose is to¬†**see if worsening financial conditions are associated with an increased use of BNPL financial products**.\n\nIs this a valid technique? What are potential limitations or issues that could arise from doing so? Am I totally misguided? Your help is much appreciated.\n\nYour time and responses are sincerely appreciated.",
    "author": "WeirdAd1180",
    "timestamp": "2025-09-24T22:21:43",
    "url": "https://reddit.com/r/statistics/comments/1npydzh/q_aggregate_score_from_a_collection_of_dummy/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1npoupt",
    "title": "[Question] Correlation Coefficient: General Interpretation for 0 &lt; |rho| &lt; 1",
    "content": "Pearson's correlation coefficient is said to measure the strength of linear dependence (actually affine iirc, but whatever) between two random variables X and Y.\n\nHowever, lots of the intuition is derived from the bivariate normal case. In the general case, when X and Y are not bivariate normally distributed, what can be said about the meaning of a correlation coefficient if its value is, e.g. 0.9? Is there some, similar to the maximum norn in basic interpolation theory, inequality including the correlation coefficient that gives the distances to a linear relationship between X and Y?\n\nWhat is missing for the general case, as far as I know, is a relationship akin to the normal case between the conditional and unconditional variances (cond. variance = uncond. variance \\* (1-rho\\^2)).\n\nIs there something like this?  But even if there was, the variance is not an intuitive measure of dispersion, if general distributions, e.g. multimodal, are considered. Is there something beyond conditional variance?",
    "author": "Jaded-Data-9150",
    "timestamp": "2025-09-24T14:38:21",
    "url": "https://reddit.com/r/statistics/comments/1npoupt/question_correlation_coefficient_general/",
    "score": 2,
    "num_comments": 20,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1npfr11",
    "title": "[Question] Survival analysis on weather data but given time series data",
    "content": "Some context: I'm working on a project and I'm looking into applying survival analysis methods to some weather data to essentially extract some statistical information from the data, particularly about clouds, like given clear skies what's the time until we experience partly cloudy skies or mostly cloudy skies (those are the three states I'm working with).\n\nThe thing is, I only have time series data (from a particular region) to work with. The best I could do up to this point was encode a column for the three sky conditions based on another cloud cover column, and then another column with the duration of that sky condition up to that point.\n\nSo my question is: Does it make sense at all to try to fit survival models such as Weibull regression or Cox regression to get information like survival probability or cumulative hazard for these sky conditions?\n\nOr, is there a better way to try analyze and get some statistical information on the duration of clear skies, [partly] cloudy skies in a time-to-event fashion (beyond something like Markov or other stochastic models)?\n\nFeel free to ask for elaboration and feel free to be scathing in the comments bc I have a feeling that trying to do survival analysis on time series data might be nonsensical!\n\nEdit: There are covariates in data, hence why I had been looking into survival regression methods.",
    "author": "dasheisenberg",
    "timestamp": "2025-09-24T08:49:16",
    "url": "https://reddit.com/r/statistics/comments/1npfr11/question_survival_analysis_on_weather_data_but/",
    "score": 4,
    "num_comments": 12,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1npqhi6",
    "title": "How to standardize multiple experiments back to one reference dataset [Research] [Question]",
    "content": "First, I'm sorry if this is confusing..let me know if I can clarify.\n\nI have data that I'd like to normalize/standardize so that I can portray the data fairly realistically in the form of a cartoon (using means).\n\nI have one reference dataset (let's call this WT), and then I have a few experiments: each with one control and one test group (e.g. the control would be tbWT and the test group would be tbMUTANT). Therefore, I think I need to standardize each test group to its own control (use tbWT as tbMUTANT's standard), but in the final product, I would like to show only the reference (WT) alongside the test groups (i.e. WT, tbMUTANT, mdMUTANT, etc).\n\nHow would you go about this? First standardize each control dataset to the reference dataset, and then standardize each test dataset to its corresponding control dataset?\n\nThanks!",
    "author": "appleoorchard",
    "timestamp": "2025-09-24T15:46:52",
    "url": "https://reddit.com/r/statistics/comments/1npqhi6/how_to_standardize_multiple_experiments_back_to/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1np76ox",
    "title": "[Question] Sampling where I want to meet certain minimum criteria the population",
    "content": "Hi,\n\nI need to send a survey to 20% of our employee base. I have been given a breakdown of this 20% across grades, e.g. it will be 100% of the Executive Committee, 50% of the department heads, down to 12% of the rank and file employees. On top of this, I have been asked that the sample represents ethnic minorities and women at least as much as the overall population, ie my final sample has &gt;=46% women.\n\nOur senior grades are regrettably over represented by white and male (though it is only a couple of percentage points off), so if I were to randomly sample in line with the grade percentages my expected minority and gender representation would be under represented (as I am taking larger proportion from the skewed white and male population).\n\nI'm sure that there are more methods, but I am considering running the sample over and over until I get one that meets the sample, or adding a weighting to the female and minority employees to make them more likely to be selected (though the latter would only improve the expected ratios, I could still sample from the tail and get an under representation).\n\nI realise that regardless I will be adding bias, and an individual white male employee will be less likely to be picked, but we are ok with that. I can see that this sentence potentially takes this out of the realm of statistics, but would appreciate any opinions that anyone has.",
    "author": "Ok-Isopod4493",
    "timestamp": "2025-09-24T01:59:41",
    "url": "https://reddit.com/r/statistics/comments/1np76ox/question_sampling_where_i_want_to_meet_certain/",
    "score": 10,
    "num_comments": 6,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nowdvl",
    "title": "A Stats Textbook that is not Casella Berger, Anyone? [Q]",
    "content": "Can anyone recommend a stats textbook that does not suck the soul out of the \"learning\" bit. Casella and Berger (though an important textbook for stats professionals) is the *Dementor* for a budding social scientist. Some of us need to see the applications of a field and build intuition instead of just dry numericals on paper.\n\nNow this also does not mean that you start suggesting statistics books that would rather fall into the non-fiction side of the bookshelf (cough, Naked Statistics).\n\nCome on guys, a nice academic non-soul-sucking textbook.\n\n**EDIT**  \nWitnessed a lot of puritanism in the comments. And a lot of helpful comments (Thanks guys).\n\nBUT, This puritanism is why we have a bad-research crisis in the world right now. People want to work with new mathematical approaches to build more accurate estimators (and stuff), while not helping the folk who might use those estimators to get better predictions.\n\nWhat is even the point of Stats guys advancing the field when the 'Applied' guys are still working in the dark?\n\nSpread the illumination fellas!",
    "author": "Radiant-Rain2636",
    "timestamp": "2025-09-23T16:08:53",
    "url": "https://reddit.com/r/statistics/comments/1nowdvl/a_stats_textbook_that_is_not_casella_berger/",
    "score": 39,
    "num_comments": 147,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1npk0li",
    "title": "[e] what masters program is my realistic target univ.? Thank you so much for attention.",
    "content": "https://www.reddit.com/r/statistics/s/8SIj7lOZAA\n\nI apologize for re-posting a same context again. However, I need your input to know what really is my target school should be. My goal is Ph.d. At top universities after my masters. \n\nOG post as below:\n\n[E] How many MS programs should I apply to? Please review my list of Univ.?\n\n\\[EDUCATION\\] GPA 3.27 Undergrad: Small state school in WI (2013-2019) major: CS minor: mathematics\n\nI have lots of Bs in Mathematics and Statistics, just didn't really care about getting As at that time.  \n\\- Calc 1,2,3 , Differential Equation1, Linear Algebra, Statistical Methods with Applications (All Bs) AND Discrete Math (GRADE: C)\n\nPre-nursing(I was prepping nursing school since 2023)\n\n\\[Industry\\] Software Engineer at one of the largest Healthcare tech firm: working on developing platform (not too deeply involved in clinical side other than conducting multiple usability test)of a Radiation Oncology Treatment Planning System (linux, SQL, python, C, C++)\n\n* Intern (2018.01-2019.05)\n* Full Time (2019.05-2023.11)\n\nData Engineer at Florida DOT (Python, SQL, Big Data, Data visualization)\n\n* 2023.11 - 2025.01\n* Data Analysis for 3rd author published paper in Civil Engineering field (Impact Factor: 1.8 / 5-Year Impact Factor: 2.1)\n\nData Engineer at Industry (Python, SQL, Big Data, Data visualization)\n\n* 2025.02 - NOW\n\n\\[Question\\] 32 y/o male here. I would preferably get a teaching role in research institute in a future\n\nHowever, with my low GPA in a small state school, no academic letter of recommendation, and lack of research experience. I would like to get Masters in Statistics and get some research experiences first and bring up GPAs And later I would like to expose myself to Biostatistics for Ph.d.\n\nI have\n\nUGA (mid)\n\nGSU (low)\n\nFSU (top-mid)\n\nUCF (mid)\n\nUT-Dallas (mid)\n\nU of Iowa (Top-mid)\n\nUF (Top)\n\nUW-Madison (Top)\n\nIowa State. (Top)\n\nU of Kentucky (Maybe)\n\nCurrently working in Atlanta region so UGA and GSU is local.  \nBefore moving to ATL, I was in Gainesville, FL where I have lots of friends doing Ph.d at UF still.\n\nI also have good memory of Madison, WI where my first career job started :)\n\nPicked out where I thought is mid to low tier national universities where I might possibly can get TAs which is very important for me except for few I really want to go such as UW, Iowa and UF.\n\nPlease advice! Thank you so much for your help!! anything helps.",
    "author": "AutomationDev",
    "timestamp": "2025-09-24T11:30:13",
    "url": "https://reddit.com/r/statistics/comments/1npk0li/e_what_masters_program_is_my_realistic_target/",
    "score": 1,
    "num_comments": 8,
    "upvote_ratio": 0.53,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nplzik",
    "title": "[Question] What statistical tools should be used for this study?",
    "content": "For an experimental study about serial position and von restorff effect that is within-group that uses latin square for counterbalancing, are these the right steps for the analysis plan? For the primary test: 1. Repeated-measures ANOVA, 2. pairwise paried t-tests. For the distinctiveness (von restorff) test: 1. paired t-test.\n\nAre these the only statistics needed for this kind of experiment or is there a better way to do this?",
    "author": "lifecrawler",
    "timestamp": "2025-09-24T12:45:29",
    "url": "https://reddit.com/r/statistics/comments/1nplzik/question_what_statistical_tools_should_be_used/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nonfzh",
    "title": "Is Computational Statistics a good field to get into? [Q][R]",
    "content": "I have the chance to do my honours year thesis with my Statistics professor who's a Computational and nonparametric statistician. \n\nJust wondering, would computational stats and nonparametrics continue to be relevant and have big opportunities in the future? In academia and in industry (since im still unsure which i want to pursue)\n\n",
    "author": "gaytwink70",
    "timestamp": "2025-09-23T10:19:13",
    "url": "https://reddit.com/r/statistics/comments/1nonfzh/is_computational_statistics_a_good_field_to_get/",
    "score": 48,
    "num_comments": 7,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nozorz",
    "title": "[Q] Econ/Statistics Double Major or MA in Economics?",
    "content": "",
    "author": "Adventurous-Help9233",
    "timestamp": "2025-09-23T18:43:28",
    "url": "https://reddit.com/r/statistics/comments/1nozorz/q_econstatistics_double_major_or_ma_in_economics/",
    "score": 2,
    "num_comments": 9,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nopbqz",
    "title": "Not a statistician [Career]",
    "content": "I work in environmental as a geologist and am by no means a statistician. That being said i just had to create a statistically robust report to support and argument. Im comparing two non-normative datasets using the non-parametric K-S test the result supported my argument that the CDF of my Site lies below the CDF of the Subregion. I then created an ECDF chart to visually compare the difference. My question is does this chart actually support the result of the K-S test. To me it does not but again i barely have a grasp of what im doing. The chart is on my profile page. I realize this is not a handout subreddit but this report will be getting sent to the state and im really trying not to put my foot in my mouth here.",
    "author": "ZEBRAFIED",
    "timestamp": "2025-09-23T11:30:09",
    "url": "https://reddit.com/r/statistics/comments/1nopbqz/not_a_statistician_career/",
    "score": 4,
    "num_comments": 5,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nois84",
    "title": "[E] What stats electives should I prioritize taking for data science?",
    "content": "Hi everyone! I‚Äôm currently a junior CS major doing a Statistics minor as I have an interest in data science. I plan to do a master‚Äôs in statistics/related field as well, but not sure what electives would prepare me the best for the field. Would appreciate any advice on 2-3 recommended classes! \n\nEdit: I‚Äôve also already taken intro to probability and plan to take intro to stats theory as those are pre reqs for most of the other electives as well.\n\ncourse overview: https://catalog.ufl.edu/UGRD/colleges-schools/UGLAS/STA_UMN/\n\nSTA 3180\tStatistical Modelling\t\n\nSTA 4222\tSample Survey Design\t\n\nSTA 4241\tStatistical Learning in R\t\n\nSTA 4273\tStatistical Computing in R\t\n\nSTA 4321\tIntroduction to Probability\t\n\nSTA 4322\tIntroduction to Statistics Theory\t\n\nSTA 4502\tNonparametric Statistical Methods \n\nSTA 4504\tCategorical Data Analysis\t\n\nSTA 4702\tMultivariate Statistical Methods\t\n\nSTA 4712\tIntroduction to Survival Analysis\t\n\nSTA 4821\tStochastic Processes\t\n\nSTA 4853\tIntroduction to Time Series and Forecasting\t",
    "author": "itzjustbri",
    "timestamp": "2025-09-23T07:23:51",
    "url": "https://reddit.com/r/statistics/comments/1nois84/e_what_stats_electives_should_i_prioritize_taking/",
    "score": 4,
    "num_comments": 13,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nok39g",
    "title": "[Career] Statistics jobs in the film industry?",
    "content": "I was wondering if anyone had any insight into what statistic/analytics type jobs exist within the film space? Something like box office breakdowns, making predictions for what audiences may be interested in, VFX/Computer graphics? ",
    "author": "bgamer1026",
    "timestamp": "2025-09-23T08:13:48",
    "url": "https://reddit.com/r/statistics/comments/1nok39g/career_statistics_jobs_in_the_film_industry/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nokqji",
    "title": "Can Pearson Correlation Be Used to Measure Goal Alignment Between Manager and Direct Reports? [Q] [Question]",
    "content": "Hi everyone,\n\nI have some goal weight data for a manager and their direct reports, broken into categories with weights that sum to 100 for each person. I want to check if their goals are aligned using the Pearson correlation coefficient.\n\nSample data:\n\n|KRA|Manager (DT)|DR1 (CG)|DR2 (LG)|\n|:-|:-|:-|:-|\n|Culture|10|10|25|\n|Talent Acquisition|25|10|75|\n|Technology &amp; Analytics|20|5|0|\n|Talent Management|20|25|0|\n|MPC &amp; Budget|20|15|0|\n|Processes|5|5|0|\n|Stakeholder Management|0|25|0|\n|Retention|0|5|0|\n\nMy questions:\n\n1. Can Pearson correlation meaningfully measure strategic goal alignment here, given zeros and uneven distributions?\n2. What are common pitfalls when using it in this kind of HR/goal cascading context?\n\nWould appreciate any insights or alternative suggestions!\n\nThanks in advance!",
    "author": "fhstistiz",
    "timestamp": "2025-09-23T08:38:15",
    "url": "https://reddit.com/r/statistics/comments/1nokqji/can_pearson_correlation_be_used_to_measure_goal/",
    "score": 1,
    "num_comments": 8,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1noasll",
    "title": "[Q] Handling measurement error in GPS data from Android",
    "content": "Hello, \n\nI work as a digital forensics, and there is one thing that have always concerned me is how we handle GPS data from phone, as if it equals to the true position of the phone.  Android‚Äôs documentation includes the following statement about GPS accuracy:  \n  \n\"Returns the estimated horizontal accuracy radius in meters of this location at the 68th percentile confidence level. This means that there is a 68% chance that the true location of the device is within a distance of this uncertainty of the reported location. Another way of putting this is that if a circle with a radius equal to this accuracy is drawn around the reported location, there is a 68% chance that the true location falls within this circle. This accuracy value is only valid for horizontal positioning, and not vertical positioning.\"   \n  \nMy question is: **What is the best way to account for this measurement error in forensic analysis?**\n\n  \nFor context, the most common question we face is whether a phone was at a specific location during a given timeframe.\n\nWhen I search the internet it suggests using the Rayleigh distribution to calculate the standard deviation and from there use MCMC with two normal distribution, one for lat another for lon to generate a posterior distribution of the phone‚Äôs likelihood of being at the specified location.  While this approach seems logical to me, my limited statistical knowledge makes it hard to verify it the correct approach. ",
    "author": "Paradoxxs",
    "timestamp": "2025-09-23T00:15:15",
    "url": "https://reddit.com/r/statistics/comments/1noasll/q_handling_measurement_error_in_gps_data_from/",
    "score": 4,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nnk8j4",
    "title": "[E] Statistics Blog",
    "content": "Just wanted to share the statistics blog by Andrew Gelman,I saw somebody mentioning in a reply. You can find it here.\n\n[https://statmodeling.stat.columbia.edu/](https://statmodeling.stat.columbia.edu/)\n\nI'm finishing my stats degree and its a really nice place to read about statistics in a more laid-back way.I think you should all check it out.\n\nI hope you are all healthy and happy with whatever you're pursuing. \n\nŒöŒ±ŒªŒÆ œÉœÖŒΩŒ≠œáŒµŒπŒ±!",
    "author": "ArugulaImpossible134",
    "timestamp": "2025-09-22T04:46:16",
    "url": "https://reddit.com/r/statistics/comments/1nnk8j4/e_statistics_blog/",
    "score": 52,
    "num_comments": 6,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nnt3lm",
    "title": "[Q] pathway for transitioning from industry to PhD - is MS the only way?",
    "content": "My background:\n- BS in Computational Modeling &amp; Data Analytics in 2019. GPA: 3.56 or so\n- 6 years industry experience with a consulting firm as a data analyst -&gt; data scientist (at least in job title)\n- no education higher than undergrad and no research experience\n- 28 years old, female, in a solid relationship with no plans to start a family \n\n\nAfter 6 years working in corporate I have been doing some soul searching and have been considering the long pathway to achieving a statistics or biostatistics PhD. My research interest is in the application of computational modeling and statistical methods to epidemiology. Through googling I‚Äôve found several top schools doing this type of research - Carnegie, etc - but I understand my current background limits any chance I have of acceptance to those programs.\n\n\nIs my only real pathway to these types of programs a masters degree? 6 years removed from academia, it seems so. My current weak points for a PhD application are a weak undergrad GPA (which feels like ages ago‚Ä¶), zero research, and the concern that all my letters of recommendation would be professional, not academic. A masters would\n\n1. Provide me a refresh of mathematics and prime the pump for higher level statistics (I took calc I-III, linear algebra, prob&amp;stats, regression analysis, programming, and more back in undergrad - but 6 years is a long time)\n\n2. Give me an opportunity to increase my GPA for a more competitive application\n\n3. Open the door for research opportunities\n\n4. Offer networking opportunities for research and letters of recommendation\n\n5. Would be easier to back out of and return to industry, should I need to\n\n\nOf course, the downside of the masters is the cost and time commitment. Unfortunately my company cannot guarantee me any funding at this time. My question is:\n\n1. Do you all agree a masters is the best possible step?\n\n2. Do there exist any programs or advice you‚Äôd have for a transition from industry to PhD?\n\n3. Is there any chance I could simply get into a PhD program as-is? Certainly not a top program, but anything?\n\n Thank you in advance.\n\n\n\nDisclaimer: I have considered that my salary will be cut to 1/3 of what it is now in a PhD program. My partner (who has already completed a PhD and is working full time in industry now) and I are on board with the lifestyle adjustments it would take. I also have built up a decent nest egg for retirement and savings that makes the income cut easier to swallow. Just want to point out that I‚Äôm not going in blind here in this regard.\n\n\n",
    "author": "probably_not_an_ai",
    "timestamp": "2025-09-22T10:38:30",
    "url": "https://reddit.com/r/statistics/comments/1nnt3lm/q_pathway_for_transitioning_from_industry_to_phd/",
    "score": 12,
    "num_comments": 18,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nnyzgt",
    "title": "[Q][E] Good Regression Textbooks for Acccountants",
    "content": "Hi, I'm a studying accountant and I want to pick up some regression skills to boost my portfolio a lil bit, also to build a firm understanding for when I eventually pick up python and want to practice regression analysis there.\n\nIf i'm dumb and there's more than meets the eye, lmk too. all info is appreciated.\n\n  \nThanks in advance.",
    "author": "Appropriate_Ad_2874",
    "timestamp": "2025-09-22T14:21:03",
    "url": "https://reddit.com/r/statistics/comments/1nnyzgt/qe_good_regression_textbooks_for_acccountants/",
    "score": 3,
    "num_comments": 12,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1no4wp9",
    "title": "[E] How many MS programs should I apply to? Please review my list of Univ.?",
    "content": "\\[EDUCATION\\] GPA 3.27 Undergrad: Small state school in WI (2013-2019) major: CS minor: mathematics\n\nI have lots of Bs in Mathematics and Statistics, just didn't really care about getting As at that time.  \n\\- Calc 1,2,3 , Differential Equation1, Linear Algebra, Statistical Methods with Applications (All Bs) AND Discrete Math (GRADE: C)\n\nPre-nursing(I was prepping nursing school since 2023)\n\n\\[Industry\\] Software Engineer at one of the largest Healthcare tech firm: working on developing platform (not too deeply involved in clinical side other than conducting multiple usability test)of a Radiation Oncology Treatment Planning System (linux, SQL, python, C, C++)\n\n* Intern (2018.01-2019.05)\n* Full Time (2019.05-2023.11)\n\nData Engineer at Florida DOT (Python, SQL, Big Data, Data visualization)\n\n* 2023.11 - 2025.01\n* Data Analysis for 3rd author published paper in Civil Engineering field (Impact Factor: 1.8 / 5-Year Impact Factor: 2.1)\n\nData Engineer at Industry (Python, SQL, Big Data, Data visualization)\n\n* 2025.02 - NOW\n\n\\[Question\\] 32 y/o male here. I would preferably get a teaching role in research institute in a future\n\nHowever, with my low GPA in a small state school, no academic letter of recommendation, and lack of research experience. I would like to get Masters in Statistics and get some research experiences first and bring up GPAs And later I would like to expose myself to Biostatistics for Ph.d.\n\nI have\n\nUGA (mid)\n\nGSU (low)\n\nFSU (top-mid)\n\nUCF (mid)\n\nUT-Dallas (mid)\n\nU of Iowa (Top-mid)\n\nUF (Top)\n\nUW-Madison (Top)\n\nIowa State. (Top)\n\nU of Kentucky (Maybe)\n\nCurrently working in Atlanta region so UGA and GSU is local.  \nBefore moving to ATL, I was in Gainesville, FL where I have lots of friends doing Ph.d at UF still.\n\nI also have good memory of Madison, WI where my first career job started :)\n\nPicked out where I thought is mid to low tier national universities where I might possibly can get TAs which is very important for me except for few I really want to go such as UW, Iowa and UF.\n\nPlease advice! Thank you so much for your help!! anything helps.",
    "author": "AutomationDev",
    "timestamp": "2025-09-22T18:48:06",
    "url": "https://reddit.com/r/statistics/comments/1no4wp9/e_how_many_ms_programs_should_i_apply_to_please/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nnrqs0",
    "title": "[Discussion] Opinions on Openintro Statistics By David M Diez",
    "content": "I am a 2nd year student pursuing BS in data science. What are your opinions on the book and would you recommend me using it at this stage?",
    "author": "Weewoooowo",
    "timestamp": "2025-09-22T09:47:57",
    "url": "https://reddit.com/r/statistics/comments/1nnrqs0/discussion_opinions_on_openintro_statistics_by/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nnwpz5",
    "title": "[Q] Need help understanding A/B testing",
    "content": "Hi,\n\nI am interested in Product Management and learning about A/B testing. I took the Udacity course, and while overall informative, it left me with a lot of unanswered questions. Surprisingly, there is quite little information online about the analytical side of A/Bs. \n\nI want to understand how were the formulas created, what is the role of specific values in the formulas and so on. For example, I am using the evanmiller.org calculator. In the sample size calculator section, I do not really understand what are \"baseline conversion rate\", \"absolute\" and \"relative\" points. \n\nI've read that A/B tests are just rebranded T-tests. Is that true? By definition they do seem identical. Can I therefore dive deeper into T-tests to understand the formulas and apply that knowledge to A/B? I guess I'll find more info about T-tests, as they are a long established statistical concept. ",
    "author": "Targaryenation",
    "timestamp": "2025-09-22T12:54:08",
    "url": "https://reddit.com/r/statistics/comments/1nnwpz5/q_need_help_understanding_ab_testing/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nnntf9",
    "title": "[Q] Risk Correlation Help",
    "content": "Hi everyone - might be a basic statistic question, but I want to make sure I‚Äôm on the right track.\n\nI‚Äôm currently tasked with finding out what is causing rejected parts by comparing manufacturing data from the parts past. I have a sample of 100 rejects and 100 accepts and am looking at the past data (such as pressure measurements), comparing accept vs reject means, StDv, and looking at P-Values. \n\nAny advice on how to do this? There‚Äôs so much data and I feel like I‚Äôm not getting anywhere or I‚Äôm doing this incorrectly.\nAny resources too would be appreciated.\n\nThanks.",
    "author": "shadychicken",
    "timestamp": "2025-09-22T07:21:24",
    "url": "https://reddit.com/r/statistics/comments/1nnntf9/q_risk_correlation_help/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nnavkt",
    "title": "[Question] good resources for undergraduate mathematical statistics?",
    "content": "This semester I‚Äôm in introduction to probability, and I don‚Äôt find the content super intuitive, especially combinatorics. Does anyone know any good resources (books, YouTube, or otherwise) which could help?",
    "author": "Loud_Commission_5763",
    "timestamp": "2025-09-21T19:34:59",
    "url": "https://reddit.com/r/statistics/comments/1nnavkt/question_good_resources_for_undergraduate/",
    "score": 10,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nmipq6",
    "title": "[Question] When to Apply Bonferroni Corrections?",
    "content": "Hi, \nI‚Äôm super desperate to understand this for my thesis and would appreciate any response. \nIf I am doing multiple separate ANOVAs (&gt;7) and have applied Bonferroni corrections on GraphPad for multiple comparisons, do I still need to manually calculate a Bonferroni-corrected p-value to refer to for all the ANOVAs?? \nI am genuinely so lost even after trying to read more on this. Really hoping for any responses at all! ",
    "author": "beylat",
    "timestamp": "2025-09-20T21:32:59",
    "url": "https://reddit.com/r/statistics/comments/1nmipq6/question_when_to_apply_bonferroni_corrections/",
    "score": 26,
    "num_comments": 14,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nlyv8d",
    "title": "Is a PhD in Economics worse than a PhD in Statistics? [Q]",
    "content": "So I am currently studying econometrics, meaning in terms of specialisation i can pursue economic research (answering questions such as the effects of race on salary) or statistical research (deriving a new method for forecasting, modelling, etc.)\n\nIn terms of my interest, i am a bit torn as i am interested in both. So another thing im considering is the job prospects. I feel like a PhD in economics is less employable as I am restricted to a select few sectors (government, academia, policy, consultancy maybe) whereas statistics is used virtually everywhere. It also doesnt help that im a non PR, non citizen. \n\nI also feel like economics is less technical (and in the realm of STEM), which I feel may also make it less valuable.",
    "author": "gaytwink70",
    "timestamp": "2025-09-20T06:57:22",
    "url": "https://reddit.com/r/statistics/comments/1nlyv8d/is_a_phd_in_economics_worse_than_a_phd_in/",
    "score": 43,
    "num_comments": 49,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nm1rxm",
    "title": "I made a video about the intuition behind p-values and hypothesis testing, let me know what you think! [D]",
    "content": "https://youtu.be/qEE0rzytHls?si=jB2L-Z61qUVGZuGs\n\nMy entry into Grant Sanderson‚Äôs ‚ÄúSummer of Math Exposition‚Äù: A friendly introduction to hypothesis testing, with minimal math background required. Most p-value explanations that I've come across focus only on the mechanical process of calculation, without telling students why they're doing it or how to interpret the results. So this video is me attempting to motivate the concept of hypothesis testing from first principles. I had to cut things like error rates, test statistics, two-sided tests, and multiple testing correction for the next video, but Part 1 here should stand on its own.",
    "author": "MaxCooljazz",
    "timestamp": "2025-09-20T08:53:15",
    "url": "https://reddit.com/r/statistics/comments/1nm1rxm/i_made_a_video_about_the_intuition_behind_pvalues/",
    "score": 28,
    "num_comments": 7,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nm3kzx",
    "title": "[Question] Normality testing in &gt;100 samples",
    "content": "Hello, so I'm currently conducting a cross sectional correlation study. I'm using 2 validated questionnaires. My sample size is 130. I just want to ask if i still need to perform a normality test (Shapiro-Wilk or Kolmogorov-Smirnov?) to assess the distribution? Or should I automatically proceed to parametric tests since the sample size fulfills the Central Limit Theorem? \n\nIf ever i have to perform a normality test, should I use S-W or K-S? \nThanks üòä",
    "author": "honeyzyx9",
    "timestamp": "2025-09-20T10:04:16",
    "url": "https://reddit.com/r/statistics/comments/1nm3kzx/question_normality_testing_in_100_samples/",
    "score": 7,
    "num_comments": 11,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nm4dyy",
    "title": "Regression help [Q]",
    "content": "To start id like to say I am not an expert at statistics, hence I am here so don't be too confused if I do things in a non standard way.\n\nProblem : I have a table of Take off distances for an airplane which is controlled by density of the air so BOTH temp and altitude play a role. My goal is to find 1 equation which will give me distance with the input of both temp and altitude in a spreadsheet with an accuracy of no less than &gt;0.999 R\\^2. This value is required because the residuals may be no more than 5m due to certification requirements. So its a lot to ask...\n\nSolutions I have tried:\n\nI have been using Desmos to try and graph and regress the data points. However using polynomial and linear regressions I have been unable to achieve the accuracy requirements.\n\nMy intentions were to regress for a given altitude, get an equation and repeat this for the other altitudes. Then I would knit these together to account for changing altitude by regressing the coefficients again , which has previously worked but the error was too large this time.\n\nI have also tried more complicated regression models using SPSS but I am by no means an expert here.\n\nDoes anyone have a good idea on how to fulfil these requirements with a highly accurate regression using either Desmos or SPSS?\n\nI know this is an open question , but this is because I am sure there are multiple ways of doing this!\n\nMy data set : [70115e-r9-complete.pdf](http://support.diamond-air.at/fileadmin/uploads/files/after_sales_support/DA42_New_Generation/Airplane_Flight_Manual/Basic_Manual/70115e-r9-complete.pdf) on page 303\n\n",
    "author": "Any_Theory7289",
    "timestamp": "2025-09-20T10:36:11",
    "url": "https://reddit.com/r/statistics/comments/1nm4dyy/regression_help_q/",
    "score": 5,
    "num_comments": 7,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nmcg5z",
    "title": "[Education] Sufficient Maths for MSc/PhD Overseas?",
    "content": "Hi all, \n\nJust wondering if the amount of mathematics I've done at uni is sufficient for masters/PhD studies in the UK or Australia (open to other countries as well though these 2 are most convenient, not the US though). FYI I'm currently an honours student in Stats in New Zealand, here are the maths/mathematical statistics papers i've taken:\n\nFrom the maths dept i've done 2 courses on linear algebra and calculus - covered basic vector &amp; matrix operations, eigenvalues/vectors, vector spaces, sequences, series, single and multivariable calculus, optimisation and differential equations, among others.   \n  \nFor stats/probability theory I've done 2 courses in probability, 1 in financial mathematics and doing 1 in stochastic processes rn. I also plan to take a course in statistical inference/mathematics next semester. Unfortunately my university has cut a lot of statistical/probability theory courses recently. I've also done applied courses in bayesian inference, regression modelling, data science, etc.\n\nProbability courses covered sigma-algebra, L\\^p spaces, modes of convergence, generating functions and some stochastic models, distributions, among others.\n\nDo you think this background would be considered sufficient for graduate-level study overseas? Or would I likely need more (e.g. real analysis)? One worry atm is that some courses lacked rigour imo, only done 1 proof-heavy course atp. I'd be open to auditing or taking additional maths papers after my honours year.\n\nWould appreciate any advice, thanks!",
    "author": "Jealous_Agency_4673",
    "timestamp": "2025-09-20T16:11:33",
    "url": "https://reddit.com/r/statistics/comments/1nmcg5z/education_sufficient_maths_for_mscphd_overseas/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nlm9al",
    "title": "[Discussion] Update to the update: My professor was right and I am calling it done!",
    "content": "*(I made a really stupid mistake while typing this, so I am resubmitting it, with an addendum as well.)*\n\nThis is an update to [a post](https://www.reddit.com/r/statistics/comments/1nkkof5/discussion_pvalue_am_i_insane_or_does_my_genetics/) that got kind of spicy. I figured y'all deserved it!\n\nThose who said that there was some miscommunication or error in defining the null or alternative hypotheses were **correct**. That was the ticket.\n\nI went through all of your comments (which, frankly, got a little overwhelming!), visited with a tutor, had my professor re-explain, did more digging through the lab manual, and was still getting confused... but I must have been in a good headspace this evening because 2 words in the lab manual FINALLY clicked in my brain. *Expected* and *observed*. They're in the chi-squared table, but I wasn't fully grasping things. I was first comprehending the definition of H0 as \"Your results are due to chance alone,\" but it's ACTUALLY \"T*he difference between your expected and observed results* are due to chance alone.\" These are 100% opposite ideas. At least, as the lab manual tells it.\n\nLIGHTBULB.\n\nI should have been looking more closely at the lab manual, but we don't reference it as often, so I (wrongly) assumed it would not be a helpful resource. So that's a lesson for me.\n\nI want to thank everybody for their thoughtfulness and contributions. It's really cool how passionate y'all are, and how dedicated you are to accuracy. I know it got a bit divisive in there. But I really appreciate the time people spent trying to support me in my learning. My brain is now mush and I have dedicated more hours this week to this dang concept than my actual homework. But I wanted to truly understand this. And you helped. So, again, thank you.\n\n**ADDENDUM:**  \nSo, I have been told that I am still not getting this concept. I should note that this is for a genetics class, not a stats class. The thing I feel I DO have some authority to speak on is that, as a biology major, I've observed 100- and 200-level biology tends to dip a towel into other disciplines, wring out the towel, and then collect some of the drippings and re-present them. For example, when we first start learning about The Powerhouse Of The Cell(TM), textbooks say that energy is stored in chemical bonds, and when you break those bonds, energy is released. A chemistry professor told me this was absolute bunk as a general rule; if I recall, bonds are broken in this particular reaction, but energy is made by those resulting molecules making *new* bonds - so energy is being made as the bonds are broken, *technically*, but only because the broken bonds allow new bonds to form. Or something like that. If you are becoming an LPN and need a shortcut to understanding that adenosine triphosphate releases energy somehow, \"bonds are broken and energy is released\" will get you where you need to go. It ain't 100% chemistry. It's quasi-chemistry. Likewise, I think my genetics class is using quasi-statistics. It's not totally accurate, but it's what the lab manual says, and what my professor says, and I just gotta go with the flow for now.",
    "author": "SassyFinch",
    "timestamp": "2025-09-19T19:15:09",
    "url": "https://reddit.com/r/statistics/comments/1nlm9al/discussion_update_to_the_update_my_professor_was/",
    "score": 35,
    "num_comments": 11,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nldyz9",
    "title": "[E] Books to start working on functional data analysis",
    "content": "Hi all,\n\nSo my research has gone into using functional covariates and extracting information from them. I have not had any course offered in my degrees about the topic, so terms like kernel smoothing, density estimation, functional regression, smoothing splines all sound familiar but I trully do not understand them. I want to find a good book that could be considered a 'classic' or that is used in courses that focus on this topics so I can get a basic understanding. Any recomendations? \n\nMany thanks!",
    "author": "Chus717",
    "timestamp": "2025-09-19T13:05:51",
    "url": "https://reddit.com/r/statistics/comments/1nldyz9/e_books_to_start_working_on_functional_data/",
    "score": 10,
    "num_comments": 4,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nlhezh",
    "title": "[Q] Should I use robust SEs in Wald-test?",
    "content": "So, basically what the title says. Assume that my model suffers from hetero and I need to estimate robust SEs. But, is there any case when a Wald test should use the original SEs for some reason?\n\nAlso, should the robust SEs be used in the calculation of the SE of a coefficient that is a linear combination of other coefficients using the delta method?",
    "author": "AxterNats",
    "timestamp": "2025-09-19T15:28:26",
    "url": "https://reddit.com/r/statistics/comments/1nlhezh/q_should_i_use_robust_ses_in_waldtest/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nl577v",
    "title": "[Question] Do I understand confidence levels correctly?",
    "content": "I‚Äôve been struggling with this concept (all statistics concepts, honestly). Here‚Äôs an explanation I tried creating for myself on what this actually means:\n\nOk, so a confidence level is constructed using the sample mean and a margin of error. This comes from one singular sample mean. If we repeatedly took samples and built 95% confidence intervals from each sample, we are confident about 95% of those intervals will contain the true population mean. About 5% of them might not. We might use 95% because it provides more precision, though since its a smaller interval than, say, 99%, theres an increased chance that this 95% confidence interval from any given sample could miss the true mean. So, even if we construct a 95% confidence interval from one sample and it doesn‚Äôt include the true population mean (or the mean we are testing for), that doesn‚Äôt mean other samples wouldn‚Äôt produce intervals that do include it.\n\nAm i on the right track or am I way off? Any help is appreciated! I‚Äôm struggling with these concepts but i still find them super interesting.\n\n\n\n",
    "author": "manythrowsbana",
    "timestamp": "2025-09-19T07:32:54",
    "url": "https://reddit.com/r/statistics/comments/1nl577v/question_do_i_understand_confidence_levels/",
    "score": 14,
    "num_comments": 17,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nlf031",
    "title": "[E] Roof renewal - effect on attic temperature",
    "content": "Background: I replaced my shingles. Trying to see if the attic temperature is becoming more stable (i.e. the new roof offers better insulation).\n\nMethod: collecting temperature data via homeassistant and a couple of battery-operated thermometers connected via Bluetooth (\"outside\") or Zigbee (\"attic\"), before and after roof renewal (\"old\" vs \"new\"). Linear model in R via¬†`attic ~ outside * roof`.\n\nThe estimate for¬†`roofold`¬†is negative, showing a decrease in attic temperature from old to new. The graphs (not in this post) show a shallower slope of the line¬†`attic ~ outside`¬†for the new roof vs the old, although the lines cross at about 22 C: below 22 C the new roof becomes better at retaining heat in the attic.\n\n    &gt; summary(mod)\n    Call:\n    lm(formula = attic ~ outside * roof, data = temp %&gt;% drop_na())\n    \n    Residuals:\n        Min      1Q  Median      3Q     Max\n    -5.8915 -1.4008  0.1482  1.3432  7.1940\n    \n    Coefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)\n    (Intercept)       0.02274    0.51118   0.044    0.965\n    outside           1.14814    0.02368  48.481   &lt;2e-16 ***\n    roofold         -10.32104    0.74134 -13.922   &lt;2e-16 ***\n    outside:roofold   0.45975    0.03299  13.936   &lt;2e-16 ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n    \n    Residual standard error: 2.152 on 706 degrees of freedom\n    Multiple R-squared:  0.9139,    Adjusted R-squared:  0.9135\n    F-statistic:  2498 on 3 and 706 DF,  p-value: &lt; 2.2e-16",
    "author": "peperazzi74",
    "timestamp": "2025-09-19T13:46:41",
    "url": "https://reddit.com/r/statistics/comments/1nlf031/e_roof_renewal_effect_on_attic_temperature/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nlw5on",
    "title": "I don't know what to do?! Please, help. [Career]",
    "content": "",
    "author": "militar412",
    "timestamp": "2025-09-20T04:51:00",
    "url": "https://reddit.com/r/statistics/comments/1nlw5on/i_dont_know_what_to_do_please_help_career/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nlmnqw",
    "title": "[Question]",
    "content": "First inning run odds. If team A scores a run in the first inning 69% of the time and team B scores a run in the first inning 31% of the time, what is the percentage chance/odds that at least one of the 2 teams scores a run in the first inning?",
    "author": "ttownbigdog",
    "timestamp": "2025-09-19T19:35:45",
    "url": "https://reddit.com/r/statistics/comments/1nlmnqw/question/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nlhg3e",
    "title": "[Q] Discovering Statistics (IBM SPSS) by Andy Field Alternative?",
    "content": "I know a lot of people like this book but it‚Äôs not doing it for me, any alternative or resource I can pair it with to get through my course? His examples and jokes are a bit convoluted and I‚Äôd much rather get to the point. ",
    "author": "throwitlikeapoloroid",
    "timestamp": "2025-09-19T15:29:52",
    "url": "https://reddit.com/r/statistics/comments/1nlhg3e/q_discovering_statistics_ibm_spss_by_andy_field/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nle6lz",
    "title": "[Question] Rates of COVID-19 Cases or Deaths by Age Group and Vaccination Status Dataset - Question",
    "content": "",
    "author": "carpocapsae",
    "timestamp": "2025-09-19T13:14:15",
    "url": "https://reddit.com/r/statistics/comments/1nle6lz/question_rates_of_covid19_cases_or_deaths_by_age/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nl49yp",
    "title": "[Discussion] Question regarding Monty Hall",
    "content": "We all know how this problem goes. Let‚Äôs use the example with having 2 child and possibility of them are girls or boys.\n\nText book would tell us that we have 4 possibilities\n\nBB BG GB GG\n\nIf one is a boy (B) then GG is out and we have 3 remaining\n\nBB GB BG\n\nThus the chance of the other one is girl is 66%\n\nBUT i think since we assigned order to GB and BG to distinguish them into 2 pairs, BB should be separated too!\n\nPossibilities now become 5:\n\nB1B2 B2B1 G1B2 B1G2 G1G2\n\nAnd the possibility now for the original question is 50%!\n\nCan someone explain further on my train of though here?\n",
    "author": "xl129",
    "timestamp": "2025-09-19T06:56:34",
    "url": "https://reddit.com/r/statistics/comments/1nl49yp/discussion_question_regarding_monty_hall/",
    "score": 4,
    "num_comments": 21,
    "upvote_ratio": 0.58,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nlf8a2",
    "title": "[Q] Is an experiment allowed to \"fail\"?",
    "content": "Let's say we have an experiment E with sample space S and two random variables X, Y on S.\n\nIn probability we talk about E\\[X | Y=y\\], the expected value of X given that Y = y. Now, expected value is applied to a random variable, so \"X | Y = y\" must somehow be a random variable, which I'll denote by Z.\n\nBut a random variable is a function from the sample space of an experiment to the real numbers. So what's the experiment and the outcome space for Z?\n\nMy best guess is that the experiment for Z, which I'll denote by E', is as follows: perform experiment E. If Y = y, then the value of Z is the defined as the value of X. If Y is not y, then experiment E' failed, and there is no output for Z; try again. The outcome space for E' is defined as Y\\^(-1)(y).\n\nIs all of this correct? Am I wrong to say that just because we write down E\\[X | Y=y\\], it means there is a hidden random variable \"X | Y=y\"? Should I just think of E\\[X | Y=y\\] in terms of its formal definition as sum x\\*P(x|Y=y), and not try to relate it to the other definition of expected value, which is applied to a random variable?",
    "author": "LearningStudent221",
    "timestamp": "2025-09-19T13:55:56",
    "url": "https://reddit.com/r/statistics/comments/1nlf8a2/q_is_an_experiment_allowed_to_fail/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nlep14",
    "title": "[E] Survival analysis. Is a mixed approach valid?",
    "content": "Hello. I am working with a highly censored environmental dataset (&gt;70%) (left-censored). I subset it into different categories borne out of the combination of two variables (Site x Contaminant), so my dataset turned into several smaller datasets with varying degrees of censoring (ranging from 0 to 100) and different circumstances such as the highest value being a censored one, censored values being equal in number (say, 0.1 as concentration) as the non-censored values, amongst others that made it impossible to find an approach that would fit all of my smaller datasets. Therefore, I used a mixed approach of KM and MLE, and even then some datasets were constructed in such a way that I could not find an approach that would model them confidently.\n\nI don't have a background in statistics, and I have to present my results soon (this analysis is only the first step of a broader analysis), so my question is:  how defensible is what I did? I know both KM and MLE are reputable methods to handle censored datasets, but I cannot find a paper or report where they have both been used.\n\nThank you.\n\n  \nEDIT: If I was an idiot by doing so, I would greatly appreciate knowing it before presenting these results to my professor, lol.",
    "author": "ryomens",
    "timestamp": "2025-09-19T13:34:28",
    "url": "https://reddit.com/r/statistics/comments/1nlep14/e_survival_analysis_is_a_mixed_approach_valid/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nkkof5",
    "title": "[Discussion] p-value: Am I insane, or does my genetics professor have p-values backwards?",
    "content": "My homework is graded and done. So I hope this flies. Sorry if it doesn't.\n\nGenetics class. My understanding (grinding through like 5 sources) is that p-value x 100 = the % chance your results would be obtained by random chance alone, no correlation , whatever (null hypothesis). So a p-value below 0.05 would be a &lt;5% chance those results would occur. Therefore, null hypothesis is less likely? I got a p-value on my Mendel plant observation of ~0.1, so I said I needed to reject *my* hypothesis about inheritance, (being that there would be a certain ratio of plant colors).\n\nYes??\n\nI wrote in the margins to clarify, because I was struggling:\n\"0.1 = Mendel was less correct \n0.05 = OK\n0.025 = Mendel was more correct\"\n\n(I know it's not worded in the most accurate scientific wording, but go with me.)\n\nProf put large X's over my \"less correct\" and \"more correct,\" and by my insecure notation of \"Did I get this right?\" they wrote \"No.\" They also wrote that *my* plant count hypothesis was *supported* with a ~0.1 p-value. (10%?) I said \"My p-value was greater than 0.05\" and they circled that and wrote next to it, \"= support.\"\n\nAfter handing back our homework, they announced to the class that a lot of people got the p-values backwards and doubled down on what they wrote on my paper. That a big p-value was \"better,\" if you'll forgive the term.\n\nAm I nuts?!\n\nI don't want to be a dick. But I think they are the one who has it backwards?",
    "author": "SassyFinch",
    "timestamp": "2025-09-18T14:17:48",
    "url": "https://reddit.com/r/statistics/comments/1nkkof5/discussion_pvalue_am_i_insane_or_does_my_genetics/",
    "score": 46,
    "num_comments": 127,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nl3vg4",
    "title": "[Question] How to make AME's comparable across models?",
    "content": "I am currently working on a Seminar research project (social sciences). I use four different models predicting class consciousness (binary DV) in different societal classes (one for each class). I use Average Marginal Effects (AME) and now I am looking for a way (if such exists) to make the AME's comparable across the models.   \nThe models all use different n and as far as I know without the same n a cross model comparison is not possible.\n\nI've read different papers, such as Mize, Doan, Long (2019) where they recommend SUEST an STATA approach, that is not available for R (?). They also mention Bootstrapping but I can't really find anything regarding AME and Bootstraps.  \nIn this sub, I've found [this ](https://www.reddit.com/r/statistics/comments/l112rp/question_about_comparing_coefficients_between/)post but I am not sure if the problems are comparable.\n\nSo is there even a way to make the models comparable? And if so can you recommend any literature on it?  \nThank you all!\n\nMize, T. D., Doan, L., &amp; Long, J. S. (2019). A General Framework for Comparing Predictions and Marginal Effects across Models. *Sociological Methodology*, *49*(1), 152-189. [https://doi.org/10.1177/0081175019852763](https://doi.org/10.1177/0081175019852763) (Original work published 2019)",
    "author": "Knorke_forke",
    "timestamp": "2025-09-19T06:40:01",
    "url": "https://reddit.com/r/statistics/comments/1nl3vg4/question_how_to_make_ames_comparable_across_models/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nkhw4k",
    "title": "Quarto help -- I'm desperate!! [software]",
    "content": "hey everyone, I need to use quarto in R for class, except .qmd files will not render!\n\nYes I have tried uninstalling everything (R, Rstudio) and reinstalling with defaults only multiple times with no improvement. I've tried editing paths. Not sure what else I can do\n\nMy professor has said maybe I need to get a new laptop but obviously don't want to do that.\n\nAnyone else run into this error? Were you able to fix it  \n\n\nUPDATE:\n\nFor those that have the same problem as me, it seems like the problem was that my new laptop has a Snapdragon X processor which is ARM-based, not intel like the version of R I had downloaded.  (shoutout u/[COOLSerdash](https://www.reddit.com/user/COOLSerdash/))\n\nUnfortunately, it seems like most applications built for ARM are for an Ubuntu environment which I am unfamiliar with. But I set up Windows Subsystem for Linux (WSL) and got Ubuntu downloaded so I could run Linux ARM64 R + Quarto. Make sure you have the R packages you need in WSL. I can access the .qmd files I make in rstudio windows and just render them in WSL. \n\nFor now I will still make my files in Rstudio in windows with the intel version of R and then go to WSL to render, but hopefully I will get more comfortable in the linux environment as time goes on.\n\nAlso if anyone has any recs / tips for a better set up please let me know!\n\n\n\nthe error is:\n\n    Execution halted\n    Problem with running R found at C:\\Program Files (x86)\\R\\R-4.5.1\\bin\\x64\\Rscript.exe to check environment configurations.\n    Please check your installation of R.",
    "author": "GODZILLAateyou",
    "timestamp": "2025-09-18T12:31:15",
    "url": "https://reddit.com/r/statistics/comments/1nkhw4k/quarto_help_im_desperate_software/",
    "score": 1,
    "num_comments": 10,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nk5fc5",
    "title": "[Q] Bonferroni correction - too conservative for this scenario?",
    "content": "I'm analysing repeated measures data (n=8 datasets) comparing a nodes response probabilities across different neighbour counts (1, 2, 3, etc. a). Example, if 1 neighbour of a node responds what is the likelyhood the target node will respond. If two nodes respond.... etc.   \n  \nSame datasets contribute values for each condition, so it's clearly paired/repeated measures.  \nThe issue I am having is that 1 datatset is lower in the 3 neighbours (the other 7 are up).\n\nPost-hoc pairwise comparisons (paired t-tests with Bonferroni correction):\n\n*   1 vs 2: t=-3.306, p\\_raw=0.013, p\\_corrected=0.039\n*   1 vs 3: t=-2.785, p\\_raw=0.027, p\\_corrected=0.081\n*   2 vs 3: t=-2.434, p\\_raw=0.045, p\\_corrected=0.135\n\nBut if were to just do is 2 or 3 significantly different from 1 neighbour then 1 v 3 would be significant. This just seems crazy to me. or if I were to just compare 2 v 3 on its own again it would be significant. \n\nShould I use the Bonferroni correction in this instance?\n\nP.S. Each dataset value is the mean probability across all nodes in that dataset (i.e., what is the mean value of nodes with 1 neighbour, nodes with 2 neighbours... etc). Should I be comparing these dataset **means** (current approach) or treating all individual nodes as separate observations and doing an unpaired approach (unpaired)?\n\n",
    "author": "Matrim_Cauthon_91",
    "timestamp": "2025-09-18T04:14:23",
    "url": "https://reddit.com/r/statistics/comments/1nk5fc5/q_bonferroni_correction_too_conservative_for_this/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nk2u07",
    "title": "[Q] Why would an explanatory variable have more variance explained in a marginal RDA than a single RDA? Shouldn't the reverse generally be true?",
    "content": "If collinear explanatory variables are removed, wouldn't a larger percentage of variance explained from a marginal RDA vs. a single RDA imply collinearity or confounding effects of the explanatory variables?\n\nWhat could cause something like this?\n\n  \nEdit: Asked this question like an idiot.\n\n  \nMeant the marginal EFFECT in an RDA when using anova.cca() on an RDA object vs. running an RDA using only a single explanatory variable. I ran both simple and partial RDAs on single variables, then looked at marginal effect in simple and partial RDAs and the marginal effect are larger than the single effects, which seems counterintuitive.",
    "author": "Loves_His_Bong",
    "timestamp": "2025-09-18T01:34:50",
    "url": "https://reddit.com/r/statistics/comments/1nk2u07/q_why_would_an_explanatory_variable_have_more/",
    "score": 6,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nk5h3x",
    "title": "[Q] Why do the degrees of freedom of SSR are k?",
    "content": "I just can't understand it. I read a really good explanation about what is a degree of freedom in regards to the sum of residuals which is this one:\n\nhttps://www.reddit.com/r/statistics/s/WO5aM15CQc\n\nBut when you calculate F which is SSR/(k) / SSE/(n-k-1)\nWhy the degrees of freedom of SSR are k? I can not insert that idea inside my mind.\n\nWhat I can understand is that the degrees of freedom are the set of values that can \"vary freely\" once you fix a couple values. When you have a set of data and you want to set a line, you have 2 points to be fixed -and those two points gives you the slope and y-intercept-, and then if you have more than 2 then you can estimate the error (of course this is just for a simple linear regression)\n\nBut what about the SSR? Why \"k\" variables can vary freely? Like, if the definition of SSR is sum((estimated(y) - mean(y))¬≤) why would you be able to vary things that are fixed? (Parameters, as far as I can understand)\n\nIf you can give me an explanation for dumbs, or at lest very detailed about why I'm not understanding this or what are my mistakes, I will be completely greatful. Thank you so much in advance.\n\nPd: I don't use the matricial form of regression, at least not yet",
    "author": "LaissezFaireee",
    "timestamp": "2025-09-18T04:17:07",
    "url": "https://reddit.com/r/statistics/comments/1nk5h3x/q_why_do_the_degrees_of_freedom_of_ssr_are_k/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nkdyb3",
    "title": "[Q] Any recommendations for hiring statistician consultants?",
    "content": "I'm finishing a dissertation and need some hand holding with my quant work. Regression/moderation in SPSS. There are lots of consulting companies when you google search, but it's hard to know who is trustworthy and won't charge an outrageous amount. I'd like to pay hourly versus a flat fee. Any recommendations about this process?",
    "author": "ididntmakeitsugar",
    "timestamp": "2025-09-18T10:03:18",
    "url": "https://reddit.com/r/statistics/comments/1nkdyb3/q_any_recommendations_for_hiring_statistician/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.54,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nkig0y",
    "title": "[Question] What model should I use to determine the probability of something happening in the future?",
    "content": "Hello everyone, first time posting here.\n\nI want to start this off with saying that I have no background in statistics, just my own research with Google and YouTube videos. If you could explain you're reasonings to me like I'm 5.\n\nI am getting into the world of trading financial instruments like stocks, options, futures, currencies. I have an idea for a personal project where, based on variables that happened in the past, how likely an outcome is to happen in the future. The inputs would be the timeframe of price (1 second, 5mins, 1 hour, etc) and the different technical, fundamental, and economic indicators (could be singular or multiple). The output and what I would like to get the probability for is the % price change with an average hold time on the trade.\n\nEx. Inputs would be Timeframe: 5 mins, Technical variable: hammer candle stick. Output: probability of price =1%, &lt;=2%, &lt;=3% with the average Hold time respectively.\n\nWhat would be the best model to achieve this with? ",
    "author": "Tanknspankn",
    "timestamp": "2025-09-18T12:52:03",
    "url": "https://reddit.com/r/statistics/comments/1nkig0y/question_what_model_should_i_use_to_determine_the/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1njy57u",
    "title": "[Q] application of Doug Hubbard‚Äôs rule of 5‚Äôs concept",
    "content": "Back info: https://nsfconsulting.com.au/rule-of-five-reduce-uncertainty/\n\nI had an assignment that referenced a statistical concept to eliminate uncertainty while using a small sample size. It‚Äôs called the rule of 5‚Äôs in simple terms it‚Äôs been statistically validated that the median of a large population has a 93.75% chance of being correctly represented in a randomly selected sample of 5 participants. \nThe assignment asked if this concept would be useful in a situation where an office could select from 12 different restaurants for a holiday party. \n\nI said no because the restaurants are distinct choices and don‚Äôt have a numerical value. In my opinion to make this application work they would have to have people select restaurants based on a quality value (rating of 5 attributed to the restaurant), wait time (ex how long a customer will wait for food in minutes), cost (average price per person), etc but just a restaurant name leaves us with nothing but frequency of selection for mathematical manipulation. \n\nMy professor deducted points with the comment that the rule of 5‚Äôs states that there is a 93.75 chance that the actual mean will fall within the low and high outcome of any random sample of 5. \n\nI don‚Äôt think that feedback makes any sense. What‚Äôs your take? Did I over think this? Did I miss the point? I‚Äôve listed the assignment question word for word and my response below. \n\nQ: A manager intends to use ‚Äúthe rule of five‚Äù to determine which of a dozen restaurants to hold the company holiday party in. Why won‚Äôt this approach work? \n\nA: The ‚Äúrule of 5‚Äù is intended to get a general idea of a population‚Äôs opinion on a single characteristic. It‚Äôs not designed to compare different distinct choices. There are too many variables in what makes a restaurant the best choice and not a numerical value that can be manipulated.\n \n ",
    "author": "Safe_Ingenuity_911",
    "timestamp": "2025-09-17T20:50:15",
    "url": "https://reddit.com/r/statistics/comments/1njy57u/q_application_of_doug_hubbards_rule_of_5s_concept/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1njmkol",
    "title": "[Discussion] Any book recommendations?",
    "content": "I am a psychobiology student with a great interest in statistics.\n\n\nThese are the courses I took:\nStatistics A, Statistics B, Calculus 1, Linear Algebra 1, Variance Analysis and Computer Applications, Intro to R, Python for biology.\nAny recommendations that would be appropriate for my level on theoretical and applied stats &amp; ML?\n\n\n\nI just want to expand my knowledge!\nThank you :)",
    "author": "rosh_anak",
    "timestamp": "2025-09-17T12:20:07",
    "url": "https://reddit.com/r/statistics/comments/1njmkol/discussion_any_book_recommendations/",
    "score": 4,
    "num_comments": 11,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1njo9hn",
    "title": "[Q] Can something be \"more\" stochastic?",
    "content": "I'm building a model where one part of the model uses a stochastic process. I have two different versions of this process: one where the output can vary pretty widely (it uses a Poisson distribution), and one where the output can only vary within an interval of one. I'm presenting my model in a lab meeting, and I was wondering if it would be correct to describe the first version as \"more\" stochastic than the second one? If not, what's the best way to describe it?",
    "author": "Master_of_beef",
    "timestamp": "2025-09-17T13:25:30",
    "url": "https://reddit.com/r/statistics/comments/1njo9hn/q_can_something_be_more_stochastic/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1njke96",
    "title": "[Q] Golf ball testing: variables are controlled, but can differences still be not statistically significant?",
    "content": "Hi,\n\nMyGolfSpy did golf ball testing, here is the whole article, includes the methodology: https://mygolfspy.com/buyers-guides/golf-balls/2025-golf-ball-test/\n\nI know that the methodology looks robust: every variables are controlled using robots and other factors, even including a control ball to try and limit random effects. They also removed outliers.\n\nThey showed this golf ball ranking based on total distance, ranging from 275 yards to 289 yards.\n\nSome balls have only a few yards in difference. My first thought was:  we would still need to know standard deviation and n to be able to test if those differences are statistically significant, specifically if I want to compare two balls in the rankings. Am I wrong?  Or is this unnecessary because of the methodology and we can just compare values directly?\n\nWhat am I missing?\nThank you",
    "author": "Round-Collar-1117",
    "timestamp": "2025-09-17T10:59:19",
    "url": "https://reddit.com/r/statistics/comments/1njke96/q_golf_ball_testing_variables_are_controlled_but/",
    "score": 5,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nj9qz9",
    "title": "[E] Which courses should I really follow?",
    "content": "\nHi! For my exchange semester, coming from a more economics bachelor, I want to chose some Maths and CS courses in order to maximize my knowledge and chances to continue with a Statistics/applied math MSc :). Therefore, within:\n\n\n- computer vision (I don‚Äôt have the background yet so it scares me a bit, but so interesting and my thesis is on dimensionality reduction so maaaaybe a bit related to it I think)\n- optimal decision making (linear optimization, discrete optimization, nonlinear optimization)\n- information theory (again probably too advanced for me)\n- MC simulations with R\n\n\nWhich ones do you think I shouldn‚Äôt skip?\nOf course I also chose an advanced econometrics course, a big data analytics course with R, a brief Python programming course, and an interesting introduction on ML and DL that involves Python as well! \n\n\n\n",
    "author": "Ecstatic-Traffic-118",
    "timestamp": "2025-09-17T03:47:43",
    "url": "https://reddit.com/r/statistics/comments/1nj9qz9/e_which_courses_should_i_really_follow/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1njff44",
    "title": "[Question] Oaxaca Decomposition",
    "content": "Usually when people use the Oaxaca decomposition, they first do a group specific regression model, where they test the effects of the independent variables for each group separately. Could I just do a hierarchical OLS regression and use the groups as independent variable instead? I can‚Äôt figure out if the group specific model is necessary for me to use the Oaxaca decomp after. I thought the decomposition does group specific regression models anyway.",
    "author": "mmeIsniffglue",
    "timestamp": "2025-09-17T07:56:36",
    "url": "https://reddit.com/r/statistics/comments/1njff44/question_oaxaca_decomposition/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nj8v3n",
    "title": "[Question] Interpretation of moderation analysis",
    "content": "Basically, I am doing moderation analysis. I have an independent variable X, dependent variable Y, and Moderator M. Simple linear regressions gave me a significant relationship between X and Y as well as X and M. But M could not significantly predict Y. However, the moderation analysis showed me that M could moderate the relationship between X and Y. How do I interpret this? Is it correct to say the M may not have a direct effect on Y but it could moderate the relationship between X and Y significantly?",
    "author": "Kei_ai",
    "timestamp": "2025-09-17T02:56:22",
    "url": "https://reddit.com/r/statistics/comments/1nj8v3n/question_interpretation_of_moderation_analysis/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nj3w92",
    "title": "[D] for my fellow economist, how would friedman and lucas react to the credibility revolution/causal inference and big data/data science?",
    "content": "For my fellow economist, how would friedman and lucas react to the credibility revolution/causal inference and big data/data science?",
    "author": "Abject-Expert-8164",
    "timestamp": "2025-09-16T21:45:58",
    "url": "https://reddit.com/r/statistics/comments/1nj3w92/d_for_my_fellow_economist_how_would_friedman_and/",
    "score": 6,
    "num_comments": 2,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ninbe6",
    "title": "[Question] What are some great books/resources that you really enjoyed when learning statistics?",
    "content": "I am curious to know what books, articles, or videos people found the most helpful or made them fall in love with statistics or what they consider is absolutely essential reading for all statisticians. \n\nBasically looking for people to share something that made them a better statistician and will likely help a lot of people in this sub! \n\n\nFor books or articles, it can be a leisure read, textbook, or primary research articles! ",
    "author": "Upstairs_Inflation49",
    "timestamp": "2025-09-16T10:07:17",
    "url": "https://reddit.com/r/statistics/comments/1ninbe6/question_what_are_some_great_booksresources_that/",
    "score": 49,
    "num_comments": 42,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1njc51y",
    "title": "[Question]How to calculate power in causal observational studies?",
    "content": "Hey everyone, we are running some campaigns and then looking back retrospectively to see if they worked. How do you determine the correct sample size? Does a normal power size calculator work in this scenario?",
    "author": "Beautiful_Fuel5252",
    "timestamp": "2025-09-17T05:45:05",
    "url": "https://reddit.com/r/statistics/comments/1njc51y/questionhow_to_calculate_power_in_causal/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nj7npe",
    "title": "[Q] Sports Win Probability: Bowling",
    "content": "TL;DR - Is there any way to make a formula to calculate win probability in a one-on-one bowling match, with no historical data?\n\nHi all! Collegiate bowler here, in the recent season, the PBA (Prof. Bowlers Association) switched over to CBS for broadcasting. On the new channel, I noticed a new stat that appeared periodically during the match: Win Probability. I was extremely curious where they were getting the data for this; the PBA notoriously does not have an archive, at least a digital one, and this change only came with the swap from FOX to CBS. It‚Äôs very likely that they‚Äôre pulling numbers out of their‚Ä¶ backside.\n\nBut it made me wonder if it was even possible? I know for baseball and football, Win Probability is usually calculated by comparing the current state of the game to historical precedents, but there‚Äôs probably not a way to do that for bowling. The easiest numbers at our disposal would be the bowlers‚Äô averages throughout the tournament before matchplay began, first ball percentage as well as strike percentage. \n\nI‚Äôm not experienced in making up new statistical formulas wholecloth, is there any way to make a formula that would update after each shot/frame to show a bowler‚Äôs chance of winning the game? Or at the very least, can anyone point me in a direction to better figure out how to make one? Any help would be appreciated!",
    "author": "MrMindblown2005",
    "timestamp": "2025-09-17T01:39:06",
    "url": "https://reddit.com/r/statistics/comments/1nj7npe/q_sports_win_probability_bowling/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nj7msq",
    "title": "Platforms for sharing/selling large datasets (like Kaggle, but paid)? :[Discussion]",
    "content": "I was wondering if there are platforms that allow you to share very large datasets (even terabytes of data), not just for free like on Kaggle but also with the possibility to sell them or monetize them (for example through revenue-sharing or by taking a percentage on sales).\nAre there marketplaces where researchers or companies can upload proprietary datasets (satellite imagery, geospatial data, domain-specific collections, etc.) and make them available on the cloud instead of through physical hard drives?\n\nHow does the business model usually work: do you pay for hosting, or does the platform take a cut of the sales?\n\nDoes it make sense to think about a market for very specific datasets (e.g. biodiversity, endangered species, anonymized medical data, etc.), or will big tech companies (Google, OpenAI, etc.) mostly keep relying on web scraping and free sources?\n\nIn other words: is there room for a ‚Äúpaid Kaggle‚Äù focused on large, domain-specific datasets, or is this already a saturated/nonexistent market?",
    "author": "panspective",
    "timestamp": "2025-09-17T01:37:25",
    "url": "https://reddit.com/r/statistics/comments/1nj7msq/platforms_for_sharingselling_large_datasets_like/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nj2ywp",
    "title": "[Q] Having to use Jamovi and gotten myself confused on reporting the means/SDs (factorial ANOVA)",
    "content": "Sorry if I'm overthinking a factorial ANOVA. I need to report my means and SDs for each group (2x2). \n\nDo I take the M and SD from the descriptives? Or do I pull it from the estimated marginal means from the ANOVA? ",
    "author": "Think_Sleep2616",
    "timestamp": "2025-09-16T20:56:42",
    "url": "https://reddit.com/r/statistics/comments/1nj2ywp/q_having_to_use_jamovi_and_gotten_myself_confused/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nicr1h",
    "title": "[Software] Fast weighted selection using digit-bin-index",
    "content": "**What my project does:**  \nThis is slightly niche, but if you need to do weighted selection and can treat probabilities as fixed precision, I built a high-performing package called digit-bin-index with Rust under the hood. It uses a novel algorithm to achieve best in class performance.\n\n**Target audience:**  \nThis package is particularly suitable for iterative weighted selection from an evolving population, such as a simulation. One example is repeated churn and acquisition of customers with a simulation to determine the customer base evolution over time.\n\n**Comparison:**  \nThere are naive algorithms, often O(N) or worse. State of the art algorithms like Walker's alias method can do O(1) selection, but require an O(N) setup and is not suitable for evolving populations. Fenwick trees are also often used, with O(log N) complexity for selection and addition.¬†`DigitBinIndex`¬†is O(P) for both, where P is the fixed precision.\n\nHere's an excerpt from a test run on a MacBook Pro with M1 CPU:\n\n`--- Benchmarking with 1,000,000 items ---`  \n`This may take some time...`  \n`Time to add 1,000,000 items: 0.219317 seconds`  \n`Estimated memory for index: 145.39 MB`  \n`100,000 single selections: 0.088418 seconds`  \n`1,000 multi-selections of 100: 0.025603 seconds`\n\nThe package is available at:¬†[https://pypi.org/project/digit-bin-index/](https://pypi.org/project/digit-bin-index/)  \nThe source code is available on:¬†[https://github.com/Roenbaeck/digit-bin-index](https://github.com/Roenbaeck/digit-bin-index)",
    "author": "Roenbaeck",
    "timestamp": "2025-09-16T02:28:34",
    "url": "https://reddit.com/r/statistics/comments/1nicr1h/software_fast_weighted_selection_using/",
    "score": 6,
    "num_comments": 1,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nhtskq",
    "title": "Is the R score fundamentally flawed? [Question]",
    "content": "Is the R score fundamentally flawed?\n\nI have recently been doing some research on the R-score. To summarize, the R-score is a tool used in Quebec CEGEPS to assess a student's performance. It does this using a kind of modified Z-score. Essentially, it takes the Z-score of a student in his class (using the grades in that class), multiplies it by a dispersion factor (calculated using the grades of a class from High School) and adds it to a strength factor (also calculated using the grades of a class from High School). If you're curious I'll add extra details below, but otherwise they're less relevant.\n\nMy concern is the use of Z-scores in a class setting. Z-scores  seem like a useful tool to assess how far a data point is, but the issue with using it for grades is that grades have a limited interval. 100% is the best anyone can get, yet it isn't clearly shown in a Z-score. 100% can yield a Z-score of 1, or maybe 2.5, it depends on the group and how strict the teacher is. What makes it worse is that the R-score tries to balance out groups (using the strength factor) and so students in weaker groups must be even more above average to have similar R-scores than those in stronger groups, further amplifying the hard limit of 100%.\n\nI think another sign that the R-score is fundamentally flawed is the corrected version. Exceptionally, if getting 100% in a class does not yield an R-score above 35 (considered great, but still below average for competitive University programs like medicine), then a corrected equation is applied to the entire class that guarantees exactly 35 if a student has 100%. The fact that this is needed is a sign of the problem, especially for those who might even need more than an R-score of 35.\n\n\nI would like to know what you guys think, I don't know too much statistics and I know Z-scores on a very basic level, so I'm curious if anyone has any more information on how appropriate of an idea it is to use a Z-score on grades.\n\n\n\n(for the extra details:  The province of Quebec takes in the average grade of every High School student from their High School Ministry exams, and with all of these grades it finds the average and standard deviation. From there, every student who graduated High School is attributed a provincial Z-score. From there, the rest is simple and use the proprieties of Z-scores:\n\nIndicator of group dispersion (IGDZ): Standard deviation of every student's provincial Z-score in a group. If they're more dispersed than average, then the result will be above 1. Otherwise, it will be below 1. \n\nIndicator of group strength (IGSZ): Mean of every student's provincial Z-score in a group. If theyre stronger than average, this will be positive. Otherwise, it will be negative.\n\nR score =  (IGDZ x Z Score) + IGSZ ) x 5 + 25\n\nGeneral idea of R-score values:\n20-25: Below average\n25: Average\n25-30: Above average\n30-35: Great\n35+: Competitive\n~36: Average successful med student applicant's R-score",
    "author": "TheStrongestLemon",
    "timestamp": "2025-09-15T11:24:47",
    "url": "https://reddit.com/r/statistics/comments/1nhtskq/is_the_r_score_fundamentally_flawed_question/",
    "score": 16,
    "num_comments": 14,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nhyrrc",
    "title": "[Q] Probability Model for sum(x)&gt;=n, where sum(x) is the result of rolling 2+N d6 and dropping the N highest/lowest?",
    "content": "I recently got into a new wargame and I wanted to build a probabilities table for all the different modifiers and conditions involved with the dice rolling. Unfortunately, my statistical knowledge is very limited, and my goal is to create a formula that can easily go into an Excel spreadsheet.\n\nModifiers in the game are expressed as \"+N Dice\" and \"-N Dice.\"  \nFor +N Dice, roll 2+N 6-sided dice, and drop the N lowest results.  \nFor -N Dice, roll 2+N 6-sided dice, and drop the N highest results.\n\nIs there a formula I can use for any number of N&gt;0 for either +ND or -ND?  \nThe different target sums I'm looking for (sum(x)&gt;=n) are 7 &amp; 9, where sum(x) is the total result of rolling with the given modifier.\n\nThank you in advance, wise and intelligent statisticians",
    "author": "Jac000bi",
    "timestamp": "2025-09-15T14:30:03",
    "url": "https://reddit.com/r/statistics/comments/1nhyrrc/q_probability_model_for_sumxn_where_sumx_is_the/",
    "score": 5,
    "num_comments": 13,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nh4fb4",
    "title": "How to tell author post hoc data manipulation is NOT ok [question]",
    "content": "I‚Äôm a clinical/forensic psychologist with a PhD and some research experience,  and often get asked to be an ad hoc reviewer for a journal.\n\nI recently recommended rejecting an article that had a lot of problems, including small, unequal  n and a large number of dependent variables. There are two groups (n=16 and n=21), neither which is randomly selected. There are 31 dependent variables, two of which were significant. My review mentioned that the unequal, small sample sizes violated the recommendations for their use of MANOVA. I also suggested Bonferroni correction, and calculated that their ‚Äúsignificant‚Äù results were no longer significant if applied.\n\nI thought that was the end of it. Yesterday, I received an updated version of the paper. In order to deal with the pairwise error problem, they combined many of the variables together, and argued that should address the MANOVA criticism, and reduce any Bonferroni correction. To top it off, they removed 6 of the subjects from the analysis (now n=16 and n=12), not because they are outliers, but due to an unrelated historical factor. Of course, they later  ‚Äúunpacked‚Äù the combined variables, to find their original significant mean differences.\n\nI want to explain to them that removing data points and creating new variables after they know the results is absolutely not acceptable in inferential statistics, but can‚Äôt find a source that‚Äôs on point. This seems to be getting close to unethical data manipulation, but they obviously don‚Äôt think so or they wouldn‚Äôt have told me.",
    "author": "IllustriousSinger966",
    "timestamp": "2025-09-14T15:07:41",
    "url": "https://reddit.com/r/statistics/comments/1nh4fb4/how_to_tell_author_post_hoc_data_manipulation_is/",
    "score": 118,
    "num_comments": 24,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ngpklo",
    "title": "[E] The University of Nebraska at Lincoln is proposing to completely eliminate their Department of Statistics",
    "content": "One of 6 programs on the chopping block.  It is baffling to me that the University could consider such a cut, especially for a department with multiple American Statistical Association fellows and continued success with obtaining research funding.\n\nNews article here: https://www.klkntv.com/unl-puts-six-academic-programs-on-the-chopping-block-amid-27-million-budget-shortfall/",
    "author": "mathguymike",
    "timestamp": "2025-09-14T05:05:11",
    "url": "https://reddit.com/r/statistics/comments/1ngpklo/e_the_university_of_nebraska_at_lincoln_is/",
    "score": 520,
    "num_comments": 74,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nhocsk",
    "title": "[Q] Is an explicit \"treatment\" variable a necessary condition for instrumental variable analysis?",
    "content": "Hi everyone, I'm trying to model the causal impact of our marketing efforts on our ads business, and I'm considering an Instrumental Variable (IV) framework. I'd appreciate a sanity check on my approach and any advice you might have.\n\n**My Goal**: Quantify how much our marketing spend contributes to advertiser acquisition and overall ad revenue.\n\n**The Challenge**: I don't believe there's a direct causal link. My hypothesis is a two-stage process:\n\n* Stage 1: Marketing spend -&gt; Increases user acquisition and retention -&gt; Leads to higher Monthly Active Users (MAUs).\n* Stage 2: Higher MAUs -&gt; Makes our platform more attractive to advertisers -&gt; Leads to more advertisers and higher ad revenue.\n\nThe problem is that the variable in the middle (MAUs) is endogenous. A simple regression of Ad Revenue \\~ MAUs would be biased because unobserved factors (e.g., seasonality, product improvements, economic trends) likely influence both user activity and advertiser spend simultaneously.\n\n**Proposed IV Setup**:\n\n* **Outcome Variable**¬†(Y): Advertiser Revenue.\n* **Endogenous Explanatory Variable**¬†(\"Treatment\") (X): MAUs (or another user volume/engagement metric).\n* **Instrumental Variable**¬†(Z): This is where I'm stuck. I need a variable that influences MAUs but does not directly affect advertiser revenue, which I believe should be marketing spend.\n\nMy Questions:\n\n* Is this the right way to conceptualize the problem? Is IV the correct tool for this kind of mediated relationship where the mediator (user volume) is endogenous? Is there a different tool that I could use?\n* This brings me to a more fundamental question: Does this setup require a formal \"experiment\"? Or can I apply this IV design to historical, observational time-series data to untangle these effects?\n\nThanks for any insights!\n\n",
    "author": "Money-Commission9304",
    "timestamp": "2025-09-15T08:05:15",
    "url": "https://reddit.com/r/statistics/comments/1nhocsk/q_is_an_explicit_treatment_variable_a_necessary/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nhrysw",
    "title": "[Question] Standardized beta coefficient in regression vs. r value in meta analysis",
    "content": "I have found a meta analysis of a predictor that I also used in my regression. the meta analysis indicated r= 0.37. My standardized beta coefficient is 0.30. I want to make a claim that it is similar to the meta analysis. I know the B is a bit different than r. Can I do it? Is there something I should note when I say that? ",
    "author": "Loose_Molasses8884",
    "timestamp": "2025-09-15T10:18:09",
    "url": "https://reddit.com/r/statistics/comments/1nhrysw/question_standardized_beta_coefficient_in/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nhlr13",
    "title": "[Q] Is there way to mathematical way to implement direction to PCA?",
    "content": "I need a mathematical way to get a direction, a vector for the PC1 axis. The axis only gives me a line, but I need a vector that points to the ‚Äúpointier‚Äù side of the data. By ‚Äúpointier‚Äù I mean: on one side of the data, there is more variance but it stays closer to the mean point, and on the other side there is less variance but the points extend farther. Think of a diamond shape. I want a vector that shows the pointier side of it. How can I describe this?",
    "author": "Aggravating-Bed7550",
    "timestamp": "2025-09-15T06:23:56",
    "url": "https://reddit.com/r/statistics/comments/1nhlr13/q_is_there_way_to_mathematical_way_to_implement/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nh4m1b",
    "title": "[Q] Help please: I developed a game and the statistics that I rand, and Gemini, have not match the results of game play.",
    "content": "I'm designing a simple grid-based game and I'm trying to calculate the probability of a specific outcome. My own playtesting results seem very different from what I'd expect, and I'd love to get a sanity check from you all.\n\nHere is the setup:\n\n* **The Board:** The game is played on a 4x4 grid (16 total squares).\n* **The Characters:** On every game board, there are exactly 8 of a specific character, let's call them \"Character A.\" The other 8 squares are filled with other characters.\n* **The Placement Rule (This is the important part):** The 8 \"Character A\"s are not placed randomly. They are always arranged in **two full lines** (either two rows or two columns).\n* **The Player's Turn:** A player makes **7 random selections** (reveals) from the 16 squares without replacement.\n\n**The Question:**\n\nWhat is the probability that a player's 7 selections will consist of **exactly 7 \"Character A\"s**?\n\nAn AI simulation I ran gave me a result of \\~0.3%, I have limited skills in statistics and got 1.3%. For some reason AI says if you find 3 in a row you have a 96.5% chance of finding the fourth, but this would be 100%. \n\nIn my own playtesting, this \"perfect hand\" seems to happen much more frequently, maybe closer to 20% of the time. Am I missing something, or did I just not do enough playtesting?\n\nAny help on how to approach this calculation would be hugely appreciated!\n\nThanks!\n\nEdit: apologies for not being more clear, they can intersect, could be two rows, two columns, or one of each, and random wasn‚Äôt the word, because yes they know the strategy. I referenced this with the 4th move example but should‚Äôve been clearer. Thank you everyone for your thoughts on this!",
    "author": "BernCo4",
    "timestamp": "2025-09-14T15:15:42",
    "url": "https://reddit.com/r/statistics/comments/1nh4m1b/q_help_please_i_developed_a_game_and_the/",
    "score": 0,
    "num_comments": 22,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ngcdid",
    "title": "[E] \"Isn't the p-value just the probability that H‚ÇÄ is true?\"",
    "content": "",
    "author": "Inside-Machine2327",
    "timestamp": "2025-09-13T16:47:20",
    "url": "https://reddit.com/r/statistics/comments/1ngcdid/e_isnt_the_pvalue_just_the_probability_that_h‚ÇÄ_is/",
    "score": 53,
    "num_comments": 21,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ngt51b",
    "title": "[S] AM Dataset",
    "content": "Hi all, I'm looking for a copy of the abandoned AM Statistical Software or for how to convert an .am data file to a modern format. I have been completely unable to find a copy in software archives.",
    "author": "Lights-and-Sound",
    "timestamp": "2025-09-14T07:46:13",
    "url": "https://reddit.com/r/statistics/comments/1ngt51b/s_am_dataset/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ngyosp",
    "title": "[Education] Any free courses online thats similar to Stat 123/170 from harvard?",
    "content": "im looking at mit open courseware 18.s096 and 15.401 not sure if there is others. thanks for your help!\n\n",
    "author": "ase1ix",
    "timestamp": "2025-09-14T11:21:44",
    "url": "https://reddit.com/r/statistics/comments/1ngyosp/education_any_free_courses_online_thats_similar/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ng7ebo",
    "title": "[Q] What's the point of non-informative priors?",
    "content": "There was a similar thread, but because of the wording in the title most people answered \"why Bayesian\" instead of \"why use non-informative priors\".\n\nTo make my question crystal clear: What are the benefits in working in the Bayesian framework over the frequentist one, when you are forced to pick a non-informative prior?",
    "author": "Optimal_Surprise_470",
    "timestamp": "2025-09-13T13:07:53",
    "url": "https://reddit.com/r/statistics/comments/1ng7ebo/q_whats_the_point_of_noninformative_priors/",
    "score": 31,
    "num_comments": 17,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ngakuz",
    "title": "[Question] All R-Squared Values are &gt; 0.99. What Does This Mean?",
    "content": "Apologies in advance if I get any terminology wrong, I'm not very well-versed in statistics lingo.\n\nAnyway, a part of my lab for a physics class I'm taking requires me to use R-squared values to determine the strength of a line of best fit with five functions (linear, inverse, power, exp. growth, exp. decay). I was able to determine the line of best fit, but one thing made me curious, and I wasn't sure where to ask it but here.\n\nFor all five of the functions, the R-squared value was above 0.99. In high school, I was told that, generally, strong relationships have an R-squared value that's more than 0.9. That made me confused as to why all of mine were so high. How could all five of these very different equations give me such high R-squared values?\n\nI guess my bigger question is what does R-squared *really* mean? I know the closer to 1, the stronger relationship, but not much else. (I was using Mathematica for my calculations, if that means anything)",
    "author": "Sudden-Garden-2837",
    "timestamp": "2025-09-13T15:25:15",
    "url": "https://reddit.com/r/statistics/comments/1ngakuz/question_all_rsquared_values_are_099_what_does/",
    "score": 15,
    "num_comments": 24,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ngcpak",
    "title": "[Q] If I‚Äôm testing for sample ratio mismatch for an A/B test with a very high sample size (N&gt; 5,000,000), is a chi-squared test still appropriate?",
    "content": "Should I still be using a chi-squared test to find out if there is SRM, or would the high sample size mess with p-values enough that I‚Äôm rejecting deviations that are small enough where it won‚Äôt affect the rest of my analysis?\n\nAny help would be greatly appreciated.",
    "author": "im-a-mf-boner",
    "timestamp": "2025-09-13T17:02:38",
    "url": "https://reddit.com/r/statistics/comments/1ngcpak/q_if_im_testing_for_sample_ratio_mismatch_for_an/",
    "score": 3,
    "num_comments": 11,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nere5t",
    "title": "[Education]/[Question] Prospective Statistics Graduate Student In Canada Questions Regarding Education and Future Careers/Salary",
    "content": "Hi all!\n\nI'm planning on applying to Master's and PhD Statistics programs this year in Canada, and one of my top choices is UofT. Of course, I'm applying for all other Stats Master's/PhD programs in the country that match my interests, but I wanted to ask recent (last few years) Master's/PhD Statistics program graduates from Canada if you would be able to share some insight into the following general and specific questions? I would also welcome any advice from less recent graduates/well-established professionals. I just wanted to know the current climate for new graduates!\n\n***General Questions For Both Master's/PhD Graduates:***\n\n1. What you're doing now (work/career-wise)?\n\n2. How much do you earn/are projected to earn?\n\n3. In your opinion, was doing your post-grad in stats worthwhile? Would you have picked a different career path/post-grad degree looking back? If so, what would it be?\n\n4. Where are you living now (if you're staying in Canada or found good jobs elsewhere)? How is the statistics/stats-related job market in Canada actually, from personal experience? And\n\n5. What is the lifestyle you're able to live/afford, given your career choice and the current economic environment?\n\n***Master's Student Graduate Specific Questions:***\n\nI understand that for a Master's, there are course-based and thesis-based programs. I was wondering if people who've taken either would be able to share your job/career prospects out of the degree, how you find they differ, and what your opinions on it are? Additionally, for those who've taken a course-based master's, has that hindered you from getting a PhD if that's something you wanted/want to do? Has doing a course-based master's/ a thesis-based master's (not a PhD) prevented you from getting high-paying jobs (especially in recent times)?\n\n***PhD Student Graduate Specific Questions:***\n\n1. For PhD students, would you say it was worth it (time, money, etc...), especially if you want to work in the industry afterwards, or would a Master's have been better? Additionally, how were funding/expenses? Were you able to graduate without too much/any/manageable enough debt?\n\n2. I have also seen on other posts in the Statistics sphere that school prestige matters when considering a PhD for jobs, and most people try to go to the States because of that. I'm a little hesitant when applying there for political/funding reasons (I'll be applying as a Canadian international student, so my main concern is that they would send me back before fully completing my degree), so I wanted to hear your thoughts about that, and finding well-paying jobs (120k plus) in various stats-related fields as a Canadian graduate.\n\nThank you so much for taking the time to reply to me, I appreciate any help/advice you can offer and all that you're comfortable sharing!",
    "author": "Class-A_Star100",
    "timestamp": "2025-09-11T19:18:45",
    "url": "https://reddit.com/r/statistics/comments/1nere5t/educationquestion_prospective_statistics_graduate/",
    "score": 6,
    "num_comments": 13,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ness53",
    "title": "[Question] Help with understanding non-normal distribution, transformation, and interpretation for Multinomial logistic regression analysis",
    "content": "Hey everyone. I've been conducting some research and unfortunately my supervisor has been unable to assist me with this question. I am hoping that someone can provide some guidance.\n\nI am predicting membership in one of three categories (may be reduced to two). My predictor variables are all continuous. For analysis I am using multinomial logistic regression to predict membership based on these predictor variables. For one of the predictors which uses values 1-20, there is a large ceiling effect and the distribution is negatively skewed (quite a few people scored 20). Currently, with the raw values I have no significant effect, and I wonder if this is because the distribution is so skewed. In total I have around 100 participants.\n\nI was reading and saw that you can perform a log transformation on the data if you reflect the scores first. I used this formula **log10(20 (participant score + 1) - participant score)**, which seems to have helped the distribution normality a lot (although overall, the distribution does not pass the Shapiro-Wilks test \\[p =.03\\]). When I split the distributions by category group though, all of the distributions pass the Shapiro-Wilks test.\n\nAfter this transformation though, I can detect significant effects when fitting a multinomial logistic regression model, but I am not sure if I can \"trust it\". It also looks like the effect direction is backwards (I think because of the reflected log transformation?). In this case, should I interpret the direction backwards too? I started with three predictor variables, but the most parsimonious model and significant model only involves two predictor variables.\n\nI am a bit confused about the assumptions of logistic regression in general, with the difference between the assumptions of a normal overall distribution and residual distribution.\n\nLastly, is there a way to calculate power/sensitivity/sample size post-hoc for a multinomial logistic regression? I feel that my study may have been underpowered. Looking at some rules of thumb, it seems like 50 participants per predictor is acceptable? It seems like the effect I can see is between two category groups. Would moving to a binomial logistic regression have greater power?\n\nSorry for all of the questions‚ÄîI am new to a lot of statistics.\n\nI'd really appreciate any advice. (edit: less dramatic).",
    "author": "snackddy",
    "timestamp": "2025-09-11T20:29:15",
    "url": "https://reddit.com/r/statistics/comments/1ness53/question_help_with_understanding_nonnormal/",
    "score": 3,
    "num_comments": 8,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1neqw6q",
    "title": "[Q] Linear regression",
    "content": "I think I am being stupid. \n\nI am using stata to try to calculate the power of a linear regression. \n\nI'm a little confused. When I am calculating/predicting the effect size when comparing 2 discrete populations, an increased standard deviation will increase the effect size - **I need a bigger N to detect the same difference I did with a smaller standard deviation, with my power set to 80%.**\n\nWhen I am predicting the power of a linear regression using power one slope, increasing my predicted standard deviation DECREASES the sample size I need to hit in order to attain a power of 80%. Decreasing the standard deviation INCREASES the sample size. How can this be? ??? ",
    "author": "kerfluxxed",
    "timestamp": "2025-09-11T18:53:41",
    "url": "https://reddit.com/r/statistics/comments/1neqw6q/q_linear_regression/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1neitpc",
    "title": "[Q] conditional mean and median approximation",
    "content": "If the distriibution of residuals from ols regression is approximately normal, would the conditional mean of y approximate the conditional median of y? ",
    "author": "NullDistribution",
    "timestamp": "2025-09-11T12:56:17",
    "url": "https://reddit.com/r/statistics/comments/1neitpc/q_conditional_mean_and_median_approximation/",
    "score": 8,
    "num_comments": 13,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nef1q4",
    "title": "Need help deciding on time as a fixed or random effect [Question]",
    "content": "I‚Äôm running a mixed model on PM2.5 (an air pollutant) where treatment and gradient are my predictors of interest, and I include date and region as random effects. Sampling also happened at different hours of the day, and I know PM2.5 naturally goes up and down with time of day, but I‚Äôm not really interested in that effect ‚Äî I just want to account for it. Should the sampling hour be modeled as a fixed effect (each hour gets its own coefficient) or as a random effect (variation by hour is absorbed but not directly estimated)?",
    "author": "Bbandit25",
    "timestamp": "2025-09-11T10:31:33",
    "url": "https://reddit.com/r/statistics/comments/1nef1q4/need_help_deciding_on_time_as_a_fixed_or_random/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ne2jjx",
    "title": "[Q] Are there any ISO-type regulations for the implementation of statistical models?",
    "content": "Is there something like the ISO 9001 or ISO 31000 standard, but focused on the implementation of statistical models such as regression, logistics, among others?",
    "author": "NombreDeUsuario0038",
    "timestamp": "2025-09-11T00:34:54",
    "url": "https://reddit.com/r/statistics/comments/1ne2jjx/q_are_there_any_isotype_regulations_for_the/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nendt4",
    "title": "[R] Gambling",
    "content": "if you lose 100 dollars in blackjack, then you bet 100 on the next hand, lose that, bet 200 (keep going) how could you lose ur money if you have per say a few thousand dollars. What‚Äôs the chance you just keep losing hands like that? Do casinos have rules against this type of behavior?",
    "author": "FarAd8913",
    "timestamp": "2025-09-11T16:04:26",
    "url": "https://reddit.com/r/statistics/comments/1nendt4/r_gambling/",
    "score": 0,
    "num_comments": 23,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nduuhy",
    "title": "[Q] Polynomial Contrasts on Logistic Regression?",
    "content": "Hi all, I am performing an analysis with a binary dependent variable and an ordinal independent variable (no covariates). I was asked to investigate whether there is a \\*decreasing\\* trend in the binary dependent variable as a independent variable increases. I had a few thoughts on this:\n\n1. Perform a [Cochran-Armitage Test](https://search.r-project.org/CRAN/refmans/DescTools/html/CochranArmitageTest.html)  \n2. Throw this into a logistic regression with one independent variable with polynomial contrasts (see section 4 [here](https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/#ORTHOGONAL)) and examine in particular the linear contrast\n\nThese two methods returned significantly different p-values (think .10 vs .94) which makes me feel I am not thinking of these tests correctly, as I imagined they would return a similar results. Can someone help me reconcile this logically?",
    "author": "turd_ziggurat",
    "timestamp": "2025-09-10T17:27:55",
    "url": "https://reddit.com/r/statistics/comments/1nduuhy/q_polynomial_contrasts_on_logistic_regression/",
    "score": 6,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ndlpxs",
    "title": "[Question] Stats Help!",
    "content": "Hi everyone, I'm a PhD student in Music Education and I could use some help. I'm primarily self taught in a lot of stats since music school doesn't really teach you much statistics (go figure). Unfortunately, I feel like I've reached the point where my professors in the college of music aren't able to help me much because they don't have experience in this and they would be learning it alongside me. So I find myself here asking for help.\n\nOne of the projects I'm working on is trying to model the relationship between music student enrollment decisions and school characteristics (funding, demographic composition, staffing characteristics). \n\nUsing state administrative data I have access to students schedules, academics, demographic etc. The students then being clustered in schools. \n\nMy plan has been to fit a hierarchical model. I've used fixed effects before but not random effects. I've read chapters in books and watched YouTube videos but it's just not clicking for me. My understanding is that HLM's are kind of centered around random effects because you are allowing variance within the cluster whereas fixed effects would remove that. This results in being able to model both within and between school variation. Because of this I feel as if random effects are more appropriate than fixed effects unless I were to include a fixed effect for time invariant effects (right?). \n\nSo I guess my questions come down to\n\n1) Am I understanding this correctly?  \n2) Should I use random or fixed effects?   \n3) If using random effects how can I partition the between and within school variance. Initially I thought of using a fixed effect for year only to capture between school variation and then in a subsequent model introducing a fixed effect for school to look at within school variation. Is that a possibility too? But if I go that route its not really a HLM anymore is it?  \n4) My other thought is mixed effects using a random effect for schools but fixed effect for year.",
    "author": "Avante_Omnos",
    "timestamp": "2025-09-10T11:07:43",
    "url": "https://reddit.com/r/statistics/comments/1ndlpxs/question_stats_help/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ndjdex",
    "title": "[Q] Imputation Overloaded",
    "content": "I have question-level missing data and I'm trying to use imputation, but the model keeps getting overloaded. How do I decide which questions to un-include when they're all relevant to the overall model? Thanks in advance!",
    "author": "ididntmakeitsugar",
    "timestamp": "2025-09-10T09:42:04",
    "url": "https://reddit.com/r/statistics/comments/1ndjdex/q_imputation_overloaded/",
    "score": 2,
    "num_comments": 10,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nd4j61",
    "title": "[Question] Confused about distribution of p-values under a null hypothesis",
    "content": "Hi everyone! I'm trying to wrap my head around the idea that p values are equally distributed under a null hypothesis. Am I correct in saying that if the null hypothesis is true, then all p-values, including those &lt;.05, are equally likely? Am I also correct in saying that if the null hypothesis is false, then most p-values will be smaller than .05? \n\nI get confused when it comes to the null hypothesis being false. If the null hypothesis is false, will the distribution of p values right skewed?\n\nThanks so much!",
    "author": "cmadison_",
    "timestamp": "2025-09-09T21:14:27",
    "url": "https://reddit.com/r/statistics/comments/1nd4j61/question_confused_about_distribution_of_pvalues/",
    "score": 14,
    "num_comments": 18,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ndicjg",
    "title": "[Education] what statistically relevant elective courses should I take as a biotechnology student?",
    "content": "Hi there, I'm a biology student who wants to specialise in plant biotechnology. I'm currently thinking about what elective courses to take in my last year, and I want at least one or two statistically oriented courses to fully prepare myself my master's thesis and subsequently a career in industry or academia. I've already had a couple of biostat courses, but they mostly focused on univariate data analysis and a little bit of multivariate.\n\n  \nQuestion is, what are the most useful statistical skills for a plant biotechnologist these days? Should I choose a course in multivariate data analysis, genomics, experimental design or even in something else?",
    "author": "Cornered_plant",
    "timestamp": "2025-09-10T09:05:22",
    "url": "https://reddit.com/r/statistics/comments/1ndicjg/education_what_statistically_relevant_elective/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ndgu7z",
    "title": "[Q] is it possible to normalize different data types to show on 1 graph?",
    "content": "Apologies if I can't post here. I dont know where the proper subreddit is. \n\nI dont really know how to do math or stats besides the bare basics and even that is a struggle. Im hoping to look at the following 3 data sets in a single view, if possible: \nCall hold time in minutes (ranges from 3-12 minutes)\nPercent of calls answered\nNumber of disconnected calls (this number can be in the thousands). \n\nI am just hoping so show trends, not actual values, but i dont want to forfeit accuracy to do so.\n\nFor more context, I want to see how the data changes month to month and how updates to the phone system affects these metrics. I want it in 1 view because this if is part of a large visual mapping of a project and there isn't really room for 3 graphs.",
    "author": "867530Niiieeeiiine",
    "timestamp": "2025-09-10T08:10:19",
    "url": "https://reddit.com/r/statistics/comments/1ndgu7z/q_is_it_possible_to_normalize_different_data/",
    "score": 1,
    "num_comments": 10,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nbpvq7",
    "title": "What is the point of Bayesian statistics? [Q]",
    "content": "I am currently studying bayesian statistics and there seems to be a great emphasis on having priors as uninformative as possible as to not bias your results\n\nIn that case, why not just abandon the idea of a prior completely and just use the data?",
    "author": "gaytwink70",
    "timestamp": "2025-09-08T07:53:22",
    "url": "https://reddit.com/r/statistics/comments/1nbpvq7/what_is_the_point_of_bayesian_statistics_q/",
    "score": 198,
    "num_comments": 96,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nc3bxo",
    "title": "Is a stats degree useless if I don't go to grad school? [Career]",
    "content": "I'm thinking of majoring in Statistics and Data Science and then immediately go into the job market, but it seems many don't think this is the best path? Is there room for somebody with only an undergrad?",
    "author": "NightlyOverseer",
    "timestamp": "2025-09-08T16:28:51",
    "url": "https://reddit.com/r/statistics/comments/1nc3bxo/is_a_stats_degree_useless_if_i_dont_go_to_grad/",
    "score": 29,
    "num_comments": 32,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nbz01v",
    "title": "[Discussion] Bayesian framework - why is it rarely used?",
    "content": "Hello everyone, \n\nI am an orthopedic resident with an affinity for research. By sheer accident, I started reading about Bayesian frameworks for statistics and research. We didn't learn this in university at all, so at first I was highly skeptical. However, after reading methodological papers and papers on arXiv for the past six months, this framework makes much more sense than the frequentist one that is used 99% of the time.\n\nI can tell you that I saw zero research that actually used Bayesian methods in Ortho. Now, at this point, I get it. You need priors, it is more challenging to design than the frequentist method. However, on the other hand, it feels more cohesive, and it allows me to hypothesize many more clinically relevant questions.\n\nI initially thought that the issue was that this framework is experimental and unproven; however, I saw recommendations from both the FDA and Cochrane. \n\nWhat am I missing here?",
    "author": "vosegus91",
    "timestamp": "2025-09-08T13:32:25",
    "url": "https://reddit.com/r/statistics/comments/1nbz01v/discussion_bayesian_framework_why_is_it_rarely/",
    "score": 53,
    "num_comments": 55,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ncq2oa",
    "title": "[Education] Can I switch to Biophysics later from Statistics?",
    "content": "Hi! I am a high school graduate from South Asia. I have applied to one university for bachelors. However, it is very competitive to get into that university. Around 100 thousand students apply but there are only 1200 places.\nYou have to sit for an university entrance exam, then based on your score on that exam and your high school grade you will get a rank among the 100 thousand people. People who are ranked higher than you will get to choose their preferred majors first, and if the spots for that major fill up, you may not be able to get into it. This is how it works.\n\nNow you will also have to fill up a major choice list where you have to rank the majors according to your preference. My top choices are: (1)Physics, (2)Applied Mathematics, (3)Mathematics, (4)Chemistry, (5)Statistics, Biostatistics and Informatics (it's listed as one major), (6)Applied Statistics (more focused on data handling, programming languages like R, python, SQL and machine learning)\n\nThen you have other majors like Zoology, Botany, Geography, Soil Science, Psychology.\n\nNow I don‚Äôt have much chance to get my top 4 major choice, because my rank is not high enough. So my question is, if I get Statistics, Biostatistics and Informatics, will I be able to switch to Biophysics research later in my master's and phd?",
    "author": "Fit_Individual_55",
    "timestamp": "2025-09-09T10:59:53",
    "url": "https://reddit.com/r/statistics/comments/1ncq2oa/education_can_i_switch_to_biophysics_later_from/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nc638x",
    "title": "Why does my dice game result in what looks like a rotated bell curve? [Q]",
    "content": "In my dice game, two players roll 2d6, and then the winner adds the difference to their roll for a total score.\n\nI'm a programmer, not a statistician, and the pseudocode looks like this:\n\nresult_a = 2d6()\n\nresult_b = 2d6()\n\nscore = max(result_a, result_b) + abs(result_a - result_b)\n\nI brute force calculated a curve by taking all possible rolls and summing up the score, and it resulted in a curve that looks almost like a normal distribution rotated a little counterclockwise. Here's the CSV:\n4:2,5:6,6:15,7:28,8:49,9:64,10:68,11:68,12:62,13:54,14:45,15:36,16:28,17:20,18:14,19:8,20:5,21:2,22:1\n\nI was wondering what kind of transformation is happening here? It's a mechanically useful distribution because results tend to be around 10 or 11, but lucky matchups can be very impactful in gameplay.\n\nThank you for your help!",
    "author": "skunksies",
    "timestamp": "2025-09-08T18:34:13",
    "url": "https://reddit.com/r/statistics/comments/1nc638x/why_does_my_dice_game_result_in_what_looks_like_a/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nbr2oe",
    "title": "[C] What could be some of the questions asked at an interview for entry level biostatistician?",
    "content": "I am going to interview for the position the day after tomorrow. JD is very vague in terms of requirements, with requirements being a master's in stats, basic knowledge of R and SAS (which I don't have any experience with, given the pricing) and just generally decent communication skills. However, the responsibilities of course is in great detail, covering technicalities that I obviously don't know yet. \n\nI was told that the interview will cover topics I have mentioned within my resume, alongside additional 'statistical' stuff. So I wanted to come here and ask:\n\n1. What are the questions you might be asked as an entry level biostatistician?\n\n2. Should I spend time trying to learn the basics of SAS or just explain why I havent had experience with it?\n\nANY input is greatly appreciated, would love to know professionals' thoughts. Thanks!",
    "author": "Mayeeah",
    "timestamp": "2025-09-08T08:38:19",
    "url": "https://reddit.com/r/statistics/comments/1nbr2oe/c_what_could_be_some_of_the_questions_asked_at_an/",
    "score": 9,
    "num_comments": 3,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nbs0cd",
    "title": "[Career] How is actuary career as a senior undergraduate student in statistics?",
    "content": "I have been accepted to do my long term intern at an insurance company. I literally dont have anything about actuary before they accepted me. I know they need to pass some exams, they have good salaries, they are crucial for insurance industry and so on. However, Im curious about what should I know for this position as a senior statistics student. I do not want to be looked at as if I dont know anything. Im open to source suggestions to learn more.\n\nSo, Im also wondering your opinion... Would you choose that field for your career? If it is yes/no, I need you guys to elaborate it. ",
    "author": "Karagioz-19",
    "timestamp": "2025-09-08T09:13:04",
    "url": "https://reddit.com/r/statistics/comments/1nbs0cd/career_how_is_actuary_career_as_a_senior/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nbhnn4",
    "title": "[C] what the heck do I do",
    "content": "Hello, I'm gonna get straight to the point. Just graduated in spring 2025 with a B.S. in statistics. Getting through college was a battle in itself, and I only switched to stats late in my junior year. Because of how fast things went I wasn't able to grab an internship. My GPA isn't the best either.\n\nI've been trying to break into DA and despite academically being weak I'd say I know my way around R and python (tidyverse, matplotlib, shiny, the works) and can use SQL in conjunction with both. That said, I realize that DA is saturated so I may be very limited in opportunities.\n\nI am considering taking actuary P and FM exams in the fall to make some kind of headway, but I'm not really sure if I want to pigeonhole myself into the actuary path just yet.\n\n I was wondering if anyone has any advice as to where else I can go with a stat degree, and if there's somewhere that isn't as screwed as DA/DS right now. Not really considering a masters, immensely burnt out on school right now. To be clear, school sucked, but I don't necessarily have any disdain for the field of statistics itself. \n\nEven if it's something I can go into for the short term future, I'd just appreciate some perspectives.",
    "author": "mango350",
    "timestamp": "2025-09-08T00:52:36",
    "url": "https://reddit.com/r/statistics/comments/1nbhnn4/c_what_the_heck_do_i_do/",
    "score": 16,
    "num_comments": 9,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nbet3i",
    "title": "[Q] Time series forecasting papers for industrial purposes?",
    "content": "Looking for papers that can enhance forecasting skills in industry, any field for that matter. ",
    "author": "mbrtlchouia",
    "timestamp": "2025-09-07T21:56:44",
    "url": "https://reddit.com/r/statistics/comments/1nbet3i/q_time_series_forecasting_papers_for_industrial/",
    "score": 10,
    "num_comments": 8,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1naynzw",
    "title": "Time series forecasting [Career]",
    "content": "Hello everyone, i hope you are all doing well.. i am a 2nd year Msc student un financial mathematics and after learning supervised and unsupervised learning to a coding level i started contemplating the idea of specializing in time series forecasting... as i found myself drawn into it more than any other type of data science especially with the new ml tools and libraries implemented in the topic to make it even more interesting.. \nMy question is, is it worth pursuing as a specialization or should i keep a general knowledge of it instead..\nFor some background knowledge: i live and study in a developing country that mainly relies on the energy and gas sector... i also am fairly comfortable with R, SQL and power BI... \nAny advice would be massively appreciated in my beginner journey ",
    "author": "Dillon_37",
    "timestamp": "2025-09-07T10:05:48",
    "url": "https://reddit.com/r/statistics/comments/1naynzw/time_series_forecasting_career/",
    "score": 43,
    "num_comments": 26,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nb3tns",
    "title": "[Discussion] Causal Inference - How is it really done?",
    "content": "I am learning Causal Inference from the book All of Statistics.\nIs it quite fascinating and I read here that is a core pillar in modern Statistics, especially in companies: If we change X, what effect we have on Y?\n\nFirst question is: how much is active the research on Causal Inference ? is it a lively topic or is it a niche sector of Statistics?\n\nSecond question: how is it really implemented in real life? When you, as statistician, want to answer a causal question, what do you do exactly?\n\nFeom what I have studied up to now, I tried to answer a simple causal question from a dataset of Incidences in the service area of my companies.\nThe question was: ‚ÄúIs our Preventive Maintenance procedure effective in reducing the failures in a year of our fleet of instruments?‚Äù\n\nOf course I run through ChatGPT the ideas, but while it is useful to have insightful observations, when you go really deep i to the topic it kind of feeld it is just rolling words for sake of writing (well, LLM being LLM I guess‚Ä¶).\n\nSo here I ask you not so much about the details (this is just an excercise Ininvented myself), I want to see more if my reasoning process is what is actually done or if I am way off.\n\nSo I tried to structure the problem as follows:\n1) first define the question: I want the PM effect across all fleet (ATE) or across a specific type of instrument more representative of the normality (e.g. medium useage, &gt;5 years, Upgraded, Customer type Tier2) \n, i.e. CATE.\n\nI decided to get the ATE as it will tell menif the PM procedure is effective across all my install base included in the study.\n\nI also had challenge to define PM=0 and PM=1. \nAt first I wanted PM=1 to be all instruments that had a PM within the dataset and I will look for the number of cases in the following 365 days.\nThen PM=0 should be at least comparable, so I selected all instruments that had a PM in their lifetime, but not in the year previous to the last 365 days. (here I assume the PM effect fades after 365 days).\n\nSo then I compare the 365 days following the PM for the PM=1 case, with the entire 2024 for the PM=0 case. The idea is to compare them in two separate 365 days windows otherwise will be impractical. Hiwever this assumes that the different windows are comparable, which is reasonable in my case.\n\nI honestly do not like this approach, so I decided to try this way:\n\nConsider PM=1 as all instruments exposed to PM regime in 2023 and 2024.\nConsider PM=0 all instruments that had issues (so they are in use) but had no PM since 2023.\n\nThis approach I like more as is more clean. Although is answering the question: is a PM done regularly effective? Instead of the question: ‚Äúwhat is the effect of a signle PM?‚Äù. which is fine by me.\n\n2) I defined the ATE=E(Y|PM=1, Z)-E(Y|PM=0,Z), where Z is my confounder, Y is the number of cases in a year, PM is the Preventive Maintenance flag.\n\n3) I drafted the DAG according to my domain knowledge. I will need to test the implied independencies to see if my DAG is coherent with my data. If not (i.e. Useage and PM are correlated while in my DAG not), I will need to think about latent confounders or if I inadvertently adjusted for a collider when filtering instruments in the dataset.\n\n4) Then I write the python code to calculate the ATE:\nStratify by my confounder in my DAG (in my case only Customer Type (i.e. policy) is causing PM, no other covariates causes a customer to have a PM).\nThen calculate all cases in 2024 for PM=1, divide by number of cases, then do the same for for PM=0 and subtract. This is my ATE.\n\n5) curiosly, I found all models have an ATE between 0.5and 1.5.\nso PM actually increade the cases on average by one per year.\n\n6) this is where the fun begins:\nBefore drawing conclusions, I plan to answer the below questions:\ndid I miss some latent confounder?\ndid I adjusted for a collider?\nis my domain knowledge flawed? (so maybe my data are screaming at me that indeed useage IS causing PM).\nCould there be other explanations: like a PM generally results in an open incidence due to discovered issues (so will need to filter out all incidences open within 7 days of a PM, but this will bias the conclusion as it will exclude early failure caused by PM: errors, quality issues, bad luck etc‚Ä¶).\n\nHonestly, at first it looks very daunting.\neven a simple question like the one I had above (which by the way I already know that the effect of PM is low for certain type of instruments), seems very very complex to answer analytically from a dataset using causal inference.\nAnd mind I am using the very basics and firsts steps of causal inference. I fear what feedback mechanism, undirected graph etc‚Ä¶ are involving.\n\nAnyway, thanks for reading. Any input on real life causal inference is appreciated ",
    "author": "samgrep",
    "timestamp": "2025-09-07T13:24:53",
    "url": "https://reddit.com/r/statistics/comments/1nb3tns/discussion_causal_inference_how_is_it_really_done/",
    "score": 11,
    "num_comments": 12,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nb6x9k",
    "title": "[E] Introduction to Probability (Advice on Learning)",
    "content": "",
    "author": "Vegetable-Piece-4434",
    "timestamp": "2025-09-07T15:30:58",
    "url": "https://reddit.com/r/statistics/comments/1nb6x9k/e_introduction_to_probability_advice_on_learning/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1naw3yo",
    "title": "[Q] Why is there no median household income index for all countries?",
    "content": "It seems like such a fundamental country index, but I can't find it anywhere. The closest I've found is median equivalised household disposable income, but it only has data for OECD countries. \n\nIs there a similar index out there that has data at least for most UN member states?",
    "author": "BrumaQuieta",
    "timestamp": "2025-09-07T08:26:55",
    "url": "https://reddit.com/r/statistics/comments/1naw3yo/q_why_is_there_no_median_household_income_index/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1nb0zuq",
    "title": "[Q] Back transforming a ln(cost) model, need to adjust the constant?",
    "content": "I've run a multivariate regression analysis in R and got an equation out, which broadly is:\n\nln(cost) = 2.96 + 0.422\\*ln(x1) + 0.696\\*ln(x2) +......\n\nAs I need to back transform to get from ln(cost) to just cost, I believe there's some adjustment I need to do to the constant? I.e. the 2.96 needs to be adjusted to account for the fact it's a log model?",
    "author": "Desperate-Art-3048",
    "timestamp": "2025-09-07T11:35:02",
    "url": "https://reddit.com/r/statistics/comments/1nb0zuq/q_back_transforming_a_lncost_model_need_to_adjust/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1na0vuw",
    "title": "[E] Frequentist vs Bayesian Thinking",
    "content": "Hi there,\n\nI've created a video¬†[here](https://youtu.be/zIyMz5YUdcY)¬†where I explain the difference between Frequentist and Bayesian statistics using a simple coin flip.\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)",
    "author": "Personal-Trainer-541",
    "timestamp": "2025-09-06T07:25:46",
    "url": "https://reddit.com/r/statistics/comments/1na0vuw/e_frequentist_vs_bayesian_thinking/",
    "score": 30,
    "num_comments": 5,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1narqtr",
    "title": "[Education] How to get started with R Programming - Beginners Roadmap",
    "content": "Hey everyone!\n\nI know a lot of people come here who are learning R for the first time, so I thought I‚Äôd share a quick roadmap. When I first started, I was totally lost with all the packages and weird syntax, but once things clicked, R became one of my favorite tools for statistics.\n\n1. Get Set Up\n\t‚Ä¢\tInstall R and RStudio (most popular IDE).\n\t‚Ä¢\tLearn the basics: variables, data types, vectors, data frames, and functions.\n\t‚Ä¢\tGreat free book: R for Data Science\n\t‚Ä¢\tAlso check out DataDucky.com ‚Äì super beginner-friendly and interactive. \n\n‚∏ª\n\n2. Work With Real Data\n\t‚Ä¢\tImport CSVs, Excel files, etc.\n\t‚Ä¢\tLearn data wrangling with tidyverse (especially dplyr and tidyr).\n\t‚Ä¢\tPractice using free datasets from Kaggle.\n\n‚∏ª\n\n3. Visualize Your Data\n\t‚Ä¢\tggplot2 is a must ‚Äì start with bar charts and scatter plots.\n\t‚Ä¢\tSeeing your data come to life makes learning way more fun.\n\n‚∏ª\n\n4. Build Small Projects\n\t‚Ä¢\tAnalyze data you care about ‚Äì sports, games, whatever keeps you interested.\n\t‚Ä¢\tShare your work to stay motivated and get feedback.\n\n‚∏ª\n\nLearning R can feel overwhelming at first, but once you get past the basics, it‚Äôs incredibly rewarding. Stick with it, and don‚Äôt be afraid to ask questions here ‚Äì this community is awesome.",
    "author": "JDD17",
    "timestamp": "2025-09-07T05:20:32",
    "url": "https://reddit.com/r/statistics/comments/1narqtr/education_how_to_get_started_with_r_programming/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.23,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1na8qlb",
    "title": "[E] What courses are more useful for graduate applications?",
    "content": "I'm in my senior year before grad applications and have the choice between taking Data Structures and Algorithms (CS) and a PhD level topics course in statistics for neuroscience, which would look more compelling for a graduate (master's) application in Stats/Data Science? \n\nI've taken a few applied statistics courses (Bayesian, Categorical, etc), the requested math courses (linear algebra, multivariate calc), and am taking Probability theory.",
    "author": "Lucky_Fish_9451",
    "timestamp": "2025-09-06T12:40:19",
    "url": "https://reddit.com/r/statistics/comments/1na8qlb/e_what_courses_are_more_useful_for_graduate/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n9sdw8",
    "title": "Questions on Linear vs Nonlinear Regression Models [Discussion]",
    "content": "I understand this question has probably been asked many times on this sub, and I have gone through most of them. But they don't seem to be answering my query satisfactorily, and neither did ChatGPT (it confused me even more).\n\nI would like to build up my question based on this post (and its comments):  \n[https://www.reddit.com/r/statistics/comments/7bo2ig/linear\\_versus\\_nonlinear\\_regression\\_linear/](https://www.reddit.com/r/statistics/comments/7bo2ig/linear_versus_nonlinear_regression_linear/)\n\nAs an Econ student, I was taught in Econometrics that a Linear Regression model, or a Linear Model in general, is anything that is¬†**linear in its parameters**. Variables can be x, x^(2), ln(x), but the parameters have to be like - Œ≤, and not Œ≤^(2)¬†or sqrt(Œ≤).\n\nBased on all this, I have the following queries:\n\n**1)**¬†I go to Google and type nonlinear regression, I see the following images -¬†[image link](https://www.google.com/search?sca_esv=b860543b1a2d43da&amp;udm=2&amp;fbs=AIIjpHxU7SXXniUZfeShr2fp4giZ1Y6MJ25_tmWITc7uy4KIeoJTKjrFjVxydQWqI2NcOhZVmrJB8DQUK5IzxA2fZbQFdefm-R5bofa1WSrxtBzW8q6xnlhbYSM4DVHaH8nJ1Z2MXDajHuHd42lTtpbv09Wfuki1HmQNbarV7HfbRjDBxYZOBPEbqoFgQRDbwPXIGUQblpwlD_2Y8b4t7V2TLncG-WpXKQ&amp;q=nonlinear+regression&amp;sa=X&amp;ved=2ahUKEwjg9aCKu8OPAxV0UvUHHaEGN-8QtKgLegQIFBAB&amp;biw=1280&amp;bih=585&amp;dpr=1.5). But we were told in class (and also can be seen from the logistic regression model) that linear models need not be a straight line. That is fine, but going back to the definition, and comparing with the graphs in the link, we see they don't really match.\n\nI mean, searching for nonlinear regression gives these graphs, some of which are polynomial regression (and other examples, can't recall) too. But polynomial regression is also linear in parameters, right? Some websites say linear regression, including curved fitting lines, essentially refer to a hyperplane in the broad sense, that is, the internal link function, which is linear in parameters. Then comes¬†**Generalized Linear Models (GLM)**, which further confused me. They all seem the same to me, but, according to GPT and some websites, they are different.\n\n**2)**¬†Let's take the Exponential Regression Model -&gt;¬†**y = a \\* b\\^x**. According to Google, this is a nonlinear regression, which is visible according to the definition as well, that it is nonlinear in parameter(s).\n\nBut if I take the natural log on both sides,¬†**ln(y) = ln(a) + x ln(b)**, which further can be written as¬†**ln(y) = c + mx**, where the constants ln(a) and ln(b) were written as some other constants. This is now a linear model, right? So can we say that some (not all) nonlinear models can be represented linearly? I understand functions like¬†**y = ax/(b + cx)** are completely nonlienar and can't be reduced to any other form.\n\nIn the post shared, the first comment gave an example that¬†**y = abX**¬†is nonlinear, as the parameters interacting with each other violate Linear Regression properties, but the fact that they are constants means that we can rewrite it as¬†**y = cx**.\n\nI understand my post is long and kind of confusing, but all these things are sort of thinning the boundary between linear and nonlinear models for me (with generalized linear models adding to the complexity). Someone please help me get these clarified, thanks!",
    "author": "dwaynebeckham27",
    "timestamp": "2025-09-05T23:25:35",
    "url": "https://reddit.com/r/statistics/comments/1n9sdw8/questions_on_linear_vs_nonlinear_regression/",
    "score": 19,
    "num_comments": 17,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1na3rdv",
    "title": "[Question] Can IQR be larger than SD?",
    "content": "Hello everyone, I'm relatively new to statistics, and I'm having difficulty figuring out the logic behind this question. I've asked ChatGPT, but I still don't really understand.\n\nCan anyone break this down? Or give me steps on how I can better visualise/think through something like this?",
    "author": "DefsNotYourGurl",
    "timestamp": "2025-09-06T09:22:33",
    "url": "https://reddit.com/r/statistics/comments/1na3rdv/question_can_iqr_be_larger_than_sd/",
    "score": 0,
    "num_comments": 14,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n9ap6n",
    "title": "[Q] New starter on my team needs a stats test",
    "content": "I've been asked to create a short stats test for a new starter on my team. All the CV's look really good so if they're being honest there's no question they know what they're doing. So the test isn't meant to be overly complicated, just to check the candidates do know some basic stats. So far I've got 5 questions, the first 2 two are industry specific (construction) so I won't list here, but I've got two questions as shown below that I could do with feedback on.\n\nI don't really want questions with calculations in as I don't want to ask them to use a laptop, or do something in R etc, it's more about showing they know basic stats and also can they explain concepts to other (non-stats) people. Two of the questions are:\n\nWhen undertaking a multiple linear regression analysis:\n\ni) describe two checks you would perform on the data before the analysis and explain why these are important.\n\nii) describe two checks you would perform on the model outputs and explain why these are important.\n\n2) How would you explain the following statistical terms to a non-technical person (think of an intelligent 12-year old)\n\ni) The null hypothesis\n\nii) p-values\n\nAs I say, none of this is supposed to be overly difficult, it's just a test of basic knowledge, and the last question is about if they can explain stats concepts to non-stats people. Also the whole test is supposed to take about 20mins, with the first two questions I didn't list taking approx. 12mins between them. So the questions above should be answerable in about 4mins each (or two mins for each sub-part). Do people think this is enough time or not enough, or too much?\n\nThere could be better questions though so if anyone has any suggestions then feel free! :-)",
    "author": "Desperate-Art-3048",
    "timestamp": "2025-09-05T09:58:58",
    "url": "https://reddit.com/r/statistics/comments/1n9ap6n/q_new_starter_on_my_team_needs_a_stats_test/",
    "score": 8,
    "num_comments": 26,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n9a1fe",
    "title": "[Q] FAMD on large mixed dataset: low explained variance, still worth using?",
    "content": "Hi,\n\nI'm working with a large tabular dataset (~1.2 million rows) that includes 7 qualitative features and 3 quantitative ones. For dimensionality reduction, I'm using FAMD (Factor Analysis for Mixed Data), which combines PCA and MCA to handle mixed types.\n\nI've tried several encoding strategies and grouped categories to reduce sparsity, but the best I can get is 4.5% variance explained by the first component, and 2.5% by the second. This is for my dissertation, so I want to make sure I'm not going down a dead-end.\n\nMy main goal is to use the 2D representation for distance-based analysis (e.g., clustering, similarity), though it would be great if it could also support some modeling.\n\nHas anyone here used FAMD in a similar context? Is it normal to get such low explained variance with mixed data? Would you still proceed with it, or consider other approaches?\n\nThanks!",
    "author": "dsilva_Viz",
    "timestamp": "2025-09-05T09:33:41",
    "url": "https://reddit.com/r/statistics/comments/1n9a1fe/q_famd_on_large_mixed_dataset_low_explained/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n8u023",
    "title": "[Q] Do you think risk management jobs have good work life balance with decent pay ?",
    "content": "",
    "author": "[deleted]",
    "timestamp": "2025-09-04T19:55:05",
    "url": "https://reddit.com/r/statistics/comments/1n8u023/q_do_you_think_risk_management_jobs_have_good/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n89azt",
    "title": "[Q] seeking good learning materials for bayesian stats",
    "content": "Hi! I'm self taught in the topic of statistics. I utilize tools when analyzing climate data. Generally straightforward and I feel with constant revision and my favorite texts I understand it well enough to discuss it well academically. The only topic I find conceptually challenging is Bayesian statistics. I'm sure I utilize it and have come across it, but whenever I see it mentioned I struggle to understand what the theory is and why it's important in data analysis. Is there any good textbook or lecture series online that anyone would recommend to improve my understanding? Anything with environmental data or discussion in the context of applying it to data would be preferable! I've already read \"statistics for geography and environmental science\" and really love that textbook! Tyia!",
    "author": "[deleted]",
    "timestamp": "2025-09-04T05:52:47",
    "url": "https://reddit.com/r/statistics/comments/1n89azt/q_seeking_good_learning_materials_for_bayesian/",
    "score": 21,
    "num_comments": 30,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n83vr4",
    "title": "[Q] Roles in statistics?",
    "content": "I am a masters in stats, recent grad. Throughout my master's program, I learnt a bunch of theory and my applied stuff was in NLP/deep learning. Recently been looking into corporate jobs in data science and data analytics, either of which might require big data technologies, cloud, SQL etc and advanced knowledge of them all. I feel out of place. I don't know anything about anything, just a bunch about statistics and their applications. I'm also a vibe coder and not someone who knows a lot about algorithms. Struggling to understand where I fit in into the corporate world. Thoughts?",
    "author": "Mayeeah",
    "timestamp": "2025-09-04T00:43:51",
    "url": "https://reddit.com/r/statistics/comments/1n83vr4/q_roles_in_statistics/",
    "score": 27,
    "num_comments": 5,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n8bu6q",
    "title": "[Research] Is a paired t-test appropriate for comparing positive vs. negative questionnaire scores from the same participants?",
    "content": "Hi everyone,\n\nI‚Äôm analyzing data from a study where the same participants completed two different scales in one questionnaires: one focused on the positive aspects of substance use, and the other focused on the negative aspects.\n\nMy goal is to see whether the overall positive ratings are significantly higher than the negative ratings within the same individuals.\n\nSince the data come from the same participants (each person provides both a positive and a negative score), I was thinking of using a paired samples t-test to compare the two sets of scores.\n\nDoes this sound like the correct approach? Or would you recommend another test (e.g., Wilcoxon signed-rank) if assumptions aren‚Äôt met?\n\nThanks in advance for your help!",
    "author": "NovelDue6123",
    "timestamp": "2025-09-04T07:33:03",
    "url": "https://reddit.com/r/statistics/comments/1n8bu6q/research_is_a_paired_ttest_appropriate_for/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n8fbsi",
    "title": "[Education] continuing education for environmental data science work.",
    "content": "What would be the best avenue to take if I wanted to primarily do work focused on environmental data science in the future? I have a Master of Science degree in Geology and 14 years environmental consulting experience working on projects including contamination assessment, natural attenuation groundwater monitoring, Phase I &amp; II ESAs, and background studies. \n\nFor these projects I have experience conducting two-sample hypothesis testing, computing confidence intervals, ANOVA, hot spot/outlier analysis with ArcGIS Pro, Mann-Kendall trend analysis, and simple linear regression. I have experience using EPA ProUCL, Surfer, ArcGIS, and R. \n\nOver the past 6 years I have self-taught myself statistics, calculus, R programming, in addition to various environmental specific topics. \n\nMy long term goal is to continue building professional experience as a geologist in the application of statistics and data science. In the event that I hit a wall and need to look elsewhere for my professional interests, would a graduate statistics certificate provide any substantial boost to my resume? Is there a substantial difference between a program from a university (e.g. Penn State applied statistics certificate, CSU Regression models) or a professional certificate (e.g. MITx statistics and data science micro masters)? ",
    "author": "Geologist2010",
    "timestamp": "2025-09-04T09:43:19",
    "url": "https://reddit.com/r/statistics/comments/1n8fbsi/education_continuing_education_for_environmental/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n8ellc",
    "title": "Grad program with my background? [Education]",
    "content": "I am currently an undergrad, studying Business Analytics with a minor in Statistics. Currently, I have a 3.76 GPA. \n\nI have taken Business Calculus, Calculus 2, Calculus 3, where I've received a B+, B, and a B-. I got an A in my Introductory Statistics course, and will take Linear Algebra with a few extra statistics courses. \n\nI have some coding experience in Python and SQL as well. Would I be qualified for a masters program coming from a business degree background, and if so are there any funded programs?",
    "author": "No-Revolution-9581",
    "timestamp": "2025-09-04T09:16:06",
    "url": "https://reddit.com/r/statistics/comments/1n8ellc/grad_program_with_my_background_education/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n8dvr3",
    "title": "[Q] Using mutual information in differential network analysis",
    "content": "I'm currently attempting to use changes in mutual information in a differential analysis to detect edge-level changes in component interactions. I am still trying to get some bearings in this area and want to make sure my methodological approach is sound. I can bootstrap sampling within treatment groups to establish distributions of MI estimates within groups for each edge, then use a non-parametric test like Mann-Whitney U to derive statistical significance in these changes? If I am missing something or vulnerable to some sort of unsupported assumption I'd super appreciate the help.",
    "author": "FicklePlatform6743",
    "timestamp": "2025-09-04T08:49:34",
    "url": "https://reddit.com/r/statistics/comments/1n8dvr3/q_using_mutual_information_in_differential/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n8ipa6",
    "title": "[D][E] What are some must have features in a statistics software?",
    "content": "Hey everyone,   \nI am currently developing a website that allows you to run some pretty simple statistical models on your data without having to know how to code.   \n  \nI was just wondering what are some features that would be lifesavers when doing statistics? Or some features that are needed when making such a website? Its mostly simple linear regressions right now.\n\nfyi this is not a plug or anything i will not be sharing the websites name or anything just interested in seeing what i could add :)))))",
    "author": "Green_borrito",
    "timestamp": "2025-09-04T11:50:53",
    "url": "https://reddit.com/r/statistics/comments/1n8ipa6/de_what_are_some_must_have_features_in_a/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n7nvsz",
    "title": "[E] Kernel Density Estimation (KDE) - Explained",
    "content": "Hi there,\n\nI've created a video¬†[here](https://youtu.be/6sGOMbC5xdE)¬†where I explain how Kernel Density Estimation (KDE) works, which is a statistical technique for estimating the probability density function of a dataset without assuming an underlying distribution.\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)",
    "author": "Personal-Trainer-541",
    "timestamp": "2025-09-03T12:08:19",
    "url": "https://reddit.com/r/statistics/comments/1n7nvsz/e_kernel_density_estimation_kde_explained/",
    "score": 22,
    "num_comments": 1,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n7xb2a",
    "title": "[Career] What do I even look for at career fairs?",
    "content": "I‚Äôm in college and I want to start searching for internships. I‚Äôm a stats major and I have a decent idea of the kind of math I‚Äôll be doing after college. But in terms of companies people reach out to or what I‚Äôm doing the math for (more so I don‚Äôt want to use my talents for unethical things)‚Äîthat‚Äôs where I‚Äôm kind of lost. How do I even begin my job search?\n\nI‚Äôm sorry if this is a dumb question I AM a little stressed to be thinking completely straight to put my questions into words. Anyway, what do I even look for at career fairs to know that it‚Äôll relate with my major? ",
    "author": "Unalina",
    "timestamp": "2025-09-03T18:39:57",
    "url": "https://reddit.com/r/statistics/comments/1n7xb2a/career_what_do_i_even_look_for_at_career_fairs/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n7ma5d",
    "title": "[Career] Advice for recent grad?",
    "content": "Hi all, I graduated with my master's in Applied Statistics back in May and am currently extremely burnt out on job applications having sent 200+ applications with only 5 or so interviews. I will take any sort of data/analytics role, but I am most interested in finance and data science. At this point I am considering a few options:\n\n* Go back to college for my PhD\n\n* Study for actuarial exams\n\n* Study for CFA certification\n\n* Continue sending out job applications\n\nI graduated from a small midwest state university with a 3.8 graduate and 3.2 undergraduate gpa (B.S. Statistics)\n\nIf I did go back to college, what degree do you guys think would fit my background? I feel like Statistics, Data Science, or Econ would be my best options, but I haven't done a ton of research yet. Further, I worry I won't be accepted for a PhD program due to my low undergrad gpa and low prestige university.\n\nAny advice would be awesome. Thanks!",
    "author": "diamondiscrash",
    "timestamp": "2025-09-03T11:08:43",
    "url": "https://reddit.com/r/statistics/comments/1n7ma5d/career_advice_for_recent_grad/",
    "score": 14,
    "num_comments": 9,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n7licn",
    "title": "[R] Open-source guide + Python code for designing geographic randomized controlled trials",
    "content": "I‚Äôd like to share a resource we recently published that might be useful here.\n\nIt‚Äôs an open-source methodology for **geographic randomized controlled trials (geo-RCTs)**, with applications in business/marketing measurement but relevant to any cluster-based experimentation. The repo includes:\n\n* A 50-page ungated whitepaper explaining the statistical design principles\n* 12+ Python code examples for power analysis, cluster randomization, and Monte Carlo simulation\n* Frameworks for multi-arm, stepped-wedge designs at large scale\n\nRepo link: [https://github.com/rickcentralcontrolcom/geo-rct-methodology](https://github.com/rickcentralcontrolcom/geo-rct-methodology?utm_source=chatgpt.com)\n\nOur aim is to encourage more transparent and replicable approaches to causal inference. I‚Äôd welcome feedback from statisticians here, especially around design trade-offs, covariate adjustment, or alternative approaches to cluster randomization.",
    "author": "Inner_Vacation7734",
    "timestamp": "2025-09-03T10:40:25",
    "url": "https://reddit.com/r/statistics/comments/1n7licn/r_opensource_guide_python_code_for_designing/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n74upr",
    "title": "[Career] Question for those who made career changes",
    "content": "I am work a non-STEM job and have a non-STEM undergrad, but am looking for a career change.\n\nI really like math and statistics so I am currently enrolled in an online Statistics Master‚Äôs program. It‚Äôs a well accredited online program (based on the math requirements and general consensus I find online) which I am currently about 1/3 through.\n\nTwo questions for those who made similar career changes (or still may have valuable insight).\n\nHow difficult was it to find a job after graduating without very relevant experience? I am thinking that it could be worth getting some sort of internship first.\n\nSecond, at which point would I be able to make the career switch? Do I need to wait to complete the program, or would I already have sufficient skills say 2/3 through the program?\n\nThanks!",
    "author": "Clemenprez",
    "timestamp": "2025-09-02T21:00:05",
    "url": "https://reddit.com/r/statistics/comments/1n74upr/career_question_for_those_who_made_career_changes/",
    "score": 8,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n75y14",
    "title": "[Education] Intro to statistics for beginner?",
    "content": "Hi all,\n\nI got bachelor's degree 5+ years ago in political science and I am now also doing similar major for grad school. One of the core classes is basic statistics. The professor said we will be using one book, which is Introduction to Business Statistics by Ronald M. Weiers. \n\nReading the book really briefly and it already made me nervous, mainly because I have never done any statistics class before. I left my math class back in high school fully expecting not ever going to meet them again, never had to use it for work, so please understand why I am lowkey freaking out right now. In addition, unfortunately I don't think my professor will be much of a help for me understanding the materials considering the size of the class.\n\nSo I was wondering whether anyone here could help me what can I do to prepare myself for the class, any video or short course I could do to help me prepare for my class? What can I expect and anything I should be aware of, that I might struggle with? I am pretty good at remembering formulas and stuff but I wasn't that good in math back in high school.",
    "author": "krusherlover",
    "timestamp": "2025-09-02T22:00:19",
    "url": "https://reddit.com/r/statistics/comments/1n75y14/education_intro_to_statistics_for_beginner/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n6ydah",
    "title": "[Q] masters joint program",
    "content": "Just learned that Johns Hopkins offers their MS in applied math and stats as a joint degree to another program. Is it worth it to pair this with another degree? If so, what program would be a good pair?",
    "author": "chicanatifa",
    "timestamp": "2025-09-02T15:57:53",
    "url": "https://reddit.com/r/statistics/comments/1n6ydah/q_masters_joint_program/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n709qj",
    "title": "[Discussion] PhStat Issues",
    "content": "I am hoping someone may be able to help me. I am working in PhStat for the first time for my college program. I am attempting to do confidence intervals; however, when I go to select 'Sample Statistics Unknown', I get a run-time error. It does not appear to have a box to select the cells next to the sample cell range. I have attempted to do the interval with the statistics known, but it does not return the correct data in the example I am following. I have checked files, configured Excel settings to unrestrict the add-in, and I am just not sure what to do anymore to get it to work. Any help is greatly appreciated. TIA!",
    "author": "OGTempx",
    "timestamp": "2025-09-02T17:20:44",
    "url": "https://reddit.com/r/statistics/comments/1n709qj/discussion_phstat_issues/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n672y9",
    "title": "Should I switch from CS to Stats? [Career]",
    "content": "I‚Äôm a CS student in 3rd year. Realized i don‚Äôt enjoy coding as much and don‚Äôt wanna grind projects and leetcode just to get a job. \n\nI was looking into switching to stats because there‚Äôs quite a bit of overlap with CS so i won‚Äôt be put too far behind.\n\nI was wondering if Stats is a good degree with just an undergrad alone. How is the job market, pay, etc? \n\nothers options i was considering:\n\n- staying CS and  doubling with econ\n- graduating CS then getting a macc and maybe cpa? \n- switching to comp eng or electrical eng for hardware roles (hardest) \n\nideally i just want a degree to get me a stable and good paying job without too much effort outside of school. But also a backup if i decide to pursue entrepreneurial endeavours. \n\nthoughts? ",
    "author": "Lameness33",
    "timestamp": "2025-09-01T18:50:28",
    "url": "https://reddit.com/r/statistics/comments/1n672y9/should_i_switch_from_cs_to_stats_career/",
    "score": 27,
    "num_comments": 33,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n5y4cc",
    "title": "What should I do for the second half of high school? [Career]",
    "content": "I am a high school senior and am currently applying to colleges. I will most likely end up at a mediocre state school.\n\nWhat are some things I should do for the second half of senior year that will help me get an internship this summer and also help me in college? I know most people say that you should enjoy your second half of senior year; however, I would like to do something productive as well so I can be best prepared.\n\nFor reference, I plan on majoring in stats + finance and am looking at career paths such as actuarial science and data science. Should I work on GitHub projects, or try and publish a research paper? I would appreciate any advice.",
    "author": "StockFishyAnand",
    "timestamp": "2025-09-01T12:23:37",
    "url": "https://reddit.com/r/statistics/comments/1n5y4cc/what_should_i_do_for_the_second_half_of_high/",
    "score": 2,
    "num_comments": 15,
    "upvote_ratio": 0.58,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n5lzld",
    "title": "[QUESTION] is there a way to describe the distribution transition?",
    "content": "I have a random variable P(s) that approaches 1 as the sample size M is increased. P(s) itself is a probability, so it is bound in \\[0,1\\]\n\nWhen M=1, the distribution of P(s) is Gaussian, and the expectation value &lt;P(s)&gt; is the same as the median over many trials (in my case 10\\^5)  \nAs M increases, the distribution is no longer Gaussian. First, there is a dominant contribution in the P(s)=1-domain, whereas the rest seems to remain Gaussian. For M&gt;200, it looks like a Gamma or Exponential distribution.\n\nI made a little [animation](https://ibb.co/4wJZhBD0) that shows the transition. in the upper plot, you can the the histogram over many P(s)-trials, in the lower plot you can see the mean (dashed line) and the median (solid line) over increasing sample size M. The animation shows two different data sets (red/blue). the deviation of the median from the mean already hints that most trials have converged to 1, but some are taking much more time, hence skewing the mean value\n\nTo give a bit of context, I am trying to find a analytical bound for Q factor of some transmission process, and therefore am interested in precicesly the transition from Gaussian to Gamma/Exp",
    "author": "PiotrSanctuvich",
    "timestamp": "2025-09-01T04:17:23",
    "url": "https://reddit.com/r/statistics/comments/1n5lzld/question_is_there_a_way_to_describe_the/",
    "score": 5,
    "num_comments": 8,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n5nudd",
    "title": "Kurtosis update on Wikipedia page[Research]",
    "content": "The Wikipedia page (English version) now displays descriptions and graphs for (1) a low kurtosis distribution that is infinitely peaked, and (2) a high kurtosis distribution that is low and appears flat-topped.",
    "author": "CarelessParty1377",
    "timestamp": "2025-09-01T05:51:06",
    "url": "https://reddit.com/r/statistics/comments/1n5nudd/kurtosis_update_on_wikipedia_pageresearch/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.61,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n5bg1z",
    "title": "Determining skewness of distribution using mean [Q]",
    "content": "Hey guys, I was thinking the other day, Im aware we use the 3rd moment to determine the skewness of a distribution, however can we not evaluate the cumulative distribution of that distribution at its expected value and gauge  the skewness based on the probability given? ",
    "author": "Srt63",
    "timestamp": "2025-08-31T18:11:53",
    "url": "https://reddit.com/r/statistics/comments/1n5bg1z/determining_skewness_of_distribution_using_mean_q/",
    "score": 8,
    "num_comments": 4,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n4htq1",
    "title": "[D] Why the need for probabilistic programming languages ?",
    "content": "What's the additional value of languages such as Stan versus general purpose languages like Python or R ?",
    "author": "al3arabcoreleone",
    "timestamp": "2025-08-30T18:11:03",
    "url": "https://reddit.com/r/statistics/comments/1n4htq1/d_why_the_need_for_probabilistic_programming/",
    "score": 21,
    "num_comments": 31,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n4iud5",
    "title": "[Discussion] Should I take Statistics for Social Sciences or Introductory Statistics? (College)",
    "content": "I have to fulfill one of the two courses listed above. I'm at a lower division level college right now but for my major (that isn't math oriented) I have to take at least one of them. Which one would you suggest for someone who doesn't like too much math. Which one would be more complicated?",
    "author": "stokedchris",
    "timestamp": "2025-08-30T19:02:43",
    "url": "https://reddit.com/r/statistics/comments/1n4iud5/discussion_should_i_take_statistics_for_social/",
    "score": 3,
    "num_comments": 10,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n3xi5h",
    "title": "Is it worth it to take a databases course if I want to work as a statistician in academia? [Q][R]",
    "content": "As the question asks, is SQL, databases, etc. useful knowledge for a statistician/data scientist in academia? \n\nIf I had to choose between this course or discrete mathematics, which would be more useful?\n\nI have taught myself a bit of SQL already.",
    "author": "gaytwink70",
    "timestamp": "2025-08-30T02:40:33",
    "url": "https://reddit.com/r/statistics/comments/1n3xi5h/is_it_worth_it_to_take_a_databases_course_if_i/",
    "score": 12,
    "num_comments": 14,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n3v6tx",
    "title": "I have a simple and complex answer to a simple question [Discussion]",
    "content": "",
    "author": "lightningthief873",
    "timestamp": "2025-08-30T00:07:58",
    "url": "https://reddit.com/r/statistics/comments/1n3v6tx/i_have_a_simple_and_complex_answer_to_a_simple/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n32uzb",
    "title": "[Q] Best way to learn Statistics for Econometrics?",
    "content": "Hello everyone. \n\nI want to learn Econometrics as much as possible in 1 month, but I heard you need to be comfortable with statistics and probability for that. I wonder what are the best resources for studying statistics quickly and for total beginners, could you recommend some youtube channels maybe? Also, do I need to be comfortable with Bayesian statistics and probability as well? \n\nI have seen several full courses on youtube named ‚ÄúStatistics for Data Science‚Äù which are 8-hour long. However, I am not sure if they cover at least 1-semester material AND if they would suit me, since I am not a data science major. \n\nI also want to say that I am looking for the best econometrics full course now. Unfortunately, videos of Ben Lambert were quite difficult for me to understand, maybe it is because of the accent as well, idk ü•≤\n\nP.S. I am soon starting my Master‚Äôs in Management and I plan to take finance courses, that is why I want to prepare beforehand, as I was told that some courses are math-heavy and require a good understanding of econ knowledge. ",
    "author": "Senetto",
    "timestamp": "2025-08-29T02:35:48",
    "url": "https://reddit.com/r/statistics/comments/1n32uzb/q_best_way_to_learn_statistics_for_econometrics/",
    "score": 4,
    "num_comments": 12,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n2oh8u",
    "title": "[E] Master's in Statistics",
    "content": "Hey everyone! I‚Äôm about to start my senior year of undergrad and I have been advised by my department to consider graduate school. I‚Äôm seriously thinking about doing a Master‚Äôs in Statistics or Data Science. However, I would like to know just how competitive my profile is and/or what programs would suit me best. As of now, my inclination is to work in the industry rather than in academia.\n\n\n\nI‚Äôm an Applied Math major with a Statistics minor. My current GPA is 3.95 with a major GPA of 3.94 (lowest grade was a B+ in real analysis, then two A-s in Calc 2 and DiffEqs; everything else is As). My program is a mix of a lot of things, including theory of probability and stochastic processes, mathematical statistics, algorithm design and optimization, and mathematical analysis.¬†\n\n\n\nMy GRE scores are 170Q/168V/4.5AW. I have been working as a research assistant for several months, although I don‚Äôt think I‚Äôll have anything published by graduation. Regarding letters of recommendation, I can get one from my program‚Äôs director (who I work as an RA for) and another from a Math/Stats professor (or a CS professor I TA'd for). I also completed a year-long internship as a data analyst, so I can get a third LOR from my supervisor. If it‚Äôs relevant at all, I have received scholarships for all semesters/terms I was elegible for.\n\n\n\nIs there anything that could make my profile more complete or improve my chances? What programs should I consider with this profile? Thank you for reading. I would really appreciate your feedback/help!",
    "author": "BearAt39",
    "timestamp": "2025-08-28T14:13:44",
    "url": "https://reddit.com/r/statistics/comments/1n2oh8u/e_masters_in_statistics/",
    "score": 22,
    "num_comments": 26,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n2mvqd",
    "title": "[QUESTION] How should I report very small Œ≤ coefficients and CIs in tables?",
    "content": "Hi everyone,\n\nI‚Äôm running a mediation analysis and my Œ≤ coefficients and confidence intervals are extremely small ‚Äî for example, around 0.0001.\n\nIf I round to 3 decimals, these become 0.000. But here‚Äôs the issue:\n\nSome are negative (e.g., -0.0001) ‚Üí should I report them as -0.000 just to signal the direction?\n\nI also have one value that is exactly 0.0000 ‚Üí how do I distinguish this from ‚Äúnearly zero‚Äù values like 0.0001?\n\nI‚Äôm not sure what the best reporting convention is here. Should I increase the number of decimal places or just stick to 3 decimals and accept the rounding issue?\n\nI want to follow good practice and make the results interpretable without being misleading. Any advice on how journals or researchers usually handle this?\n",
    "author": "makislog",
    "timestamp": "2025-08-28T13:11:25",
    "url": "https://reddit.com/r/statistics/comments/1n2mvqd/question_how_should_i_report_very_small_Œ≤/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n2b2c6",
    "title": "[E] How to explore subjects before applying to a master's degree",
    "content": "Context: I am a recently graduated statistician looking for a Master's program, ideally outside of my country. I have decent grades and some research in stochastic processes, with an article to be published and 2 in progress.\n\n\n\nWhen talking to people about graduate programs, I've encountered a paradox: \n\n\nMasters (especially in the first year) should give you the freedom to explore multiple subjects before picking what you'll specialize in, however everyone says that your chances of getting accepted are much higher if you contact a professor directly saying that you'd like to do research with them, which requires you to know what research you want to do.\n\n\nI have about 4-6 months before my first applications, how can I explore different subjects in statistics to decide what I like, given I don't have access to any classes anymore? Stuff like youtube videos seems a bit too shallow.\n\nI liked my research but it was far too theoretical and abstract for me, and there are so many subjects that I didn't get a chance to study properly during my degree, like non-parametric, robust, machine learning, proper bayesian inference, the list goes on",
    "author": "hipotese_alternativa",
    "timestamp": "2025-08-28T05:36:31",
    "url": "https://reddit.com/r/statistics/comments/1n2b2c6/e_how_to_explore_subjects_before_applying_to_a/",
    "score": 9,
    "num_comments": 6,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n2drlb",
    "title": "[Education] Basic analyses of biological data for research undergraduates",
    "content": "Hi folks. Many thanks in advance. also cross-posted to r/AskStatistics \n\nI am trying to develop a training program for data analysis by undergraduate researchers in my laboratory. I am primarily an empirical researcher in the biological sciences and model proportions and count data over time. I hold in-person sessions at the start of every semester but find students vary immensely in their background and understanding.\n\nSo I thought it might to good to have them revisit basic statistics such as measures of central tendency and variation, and graph analysis before my session. Can you recommend some short written material and for those who prefer, video tutorials, that would give them some context before my session?",
    "author": "traditional_genius",
    "timestamp": "2025-08-28T07:29:02",
    "url": "https://reddit.com/r/statistics/comments/1n2drlb/education_basic_analyses_of_biological_data_for/",
    "score": 6,
    "num_comments": 6,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n2219t",
    "title": "[Education] Asking for assistantships",
    "content": "Hi,\n\nI am looking to apply for grad schools. Do I have to reach out to professors and ask if there's a position available or is it usually written on the university's website? What's the best way to look for assistantships for masters?",
    "author": "leena2123",
    "timestamp": "2025-08-27T20:45:32",
    "url": "https://reddit.com/r/statistics/comments/1n2219t/education_asking_for_assistantships/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n1rle3",
    "title": "[Question] concerning the transformation of the relative effect statistic of the Brunner-Munzel test.",
    "content": "Hello everyone! \nFor a paper i plan to use the Brunner-Munzel test. The relative effect statistic pÃÇ tells me the probability of a random measurement from sample 2 being higher than a random measurement from sample 1. This value may range from 0 to 1 with .5 indicating no relationship between belonging to a group and having a certain score. Now the question: is there any sense in transforming the pÃÇ value so it takes on a form between -1 and 1 like a correlation coefficient? Someone told me that this would make it easier for people to interpret, because it will take on a form similar to something everybody knows - the correlation coefficient. Of course a description would have to be added what -1 and what 1 means in that case. \n\nThanks in advance!",
    "author": "tytanxxl",
    "timestamp": "2025-08-27T13:05:39",
    "url": "https://reddit.com/r/statistics/comments/1n1rle3/question_concerning_the_transformation_of_the/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n18qil",
    "title": "[D][E] Aligning non-linear features with your data distribution",
    "content": "",
    "author": "alexsht1",
    "timestamp": "2025-08-26T22:35:21",
    "url": "https://reddit.com/r/statistics/comments/1n18qil/de_aligning_nonlinear_features_with_your_data/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n0i37w",
    "title": "The Incalculable Costs of Corrupt Statistics [Education]",
    "content": "Reliable statistics are the foundation of sound governance, which is why US President Donald Trump‚Äôs attacks on the Bureau of Labor Statistics have alarmed economists. While tampering with economic figures may yield short-term political benefits, in many recent cases, the long-term consequences have been catastrophic. [https://www.project-syndicate.org/commentary/trump-war-on-data-could-have-profound-consequences-by-diane-coyle-2025-08](https://www.project-syndicate.org/commentary/trump-war-on-data-could-have-profound-consequences-by-diane-coyle-2025-08)",
    "author": "Gloomy_Register_2341",
    "timestamp": "2025-08-26T03:25:00",
    "url": "https://reddit.com/r/statistics/comments/1n0i37w/the_incalculable_costs_of_corrupt_statistics/",
    "score": 61,
    "num_comments": 9,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n0vtpg",
    "title": "[Q] What kinds of inferences can you make from the random intercepts/slopes in a mixed effects model?",
    "content": "I do psycholinguistic research. I am typically predicting responses to words (e.g., how quickly someone can classify a word) with some predictor variables (e.g., length, frequency). \n\nI usually have random subject and item variables, to allow me to analyse the data at the trial level.\n\nBut I typically don't do much with the random effect estimates themselves. How can I make more of them? What kind of inferences can I make based on the sd of a given random effect?",
    "author": "MikeSidvid",
    "timestamp": "2025-08-26T12:46:52",
    "url": "https://reddit.com/r/statistics/comments/1n0vtpg/q_what_kinds_of_inferences_can_you_make_from_the/",
    "score": 9,
    "num_comments": 7,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n0zhhy",
    "title": "Question for Multilevel analysis diary study output [Question]",
    "content": "Question with Multilevel model output for diary study\n\nI am doing data analysis for a daily diary study and ran fixed and random slopes for my hypotheses. Problem is, the estimate, standard error and p- value numbers differed and I'm not sure which one to report for my apa style table. \n\nShould they differ? Or should they stay the same? Which one should be used?\n\nHappy to put more details or answer questions to make it clearer! ",
    "author": "oowooowoooo",
    "timestamp": "2025-08-26T15:08:47",
    "url": "https://reddit.com/r/statistics/comments/1n0zhhy/question_for_multilevel_analysis_diary_study/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n0zfkq",
    "title": "[Q] Reporting on time varying covariates in cox regression",
    "content": "I'm currently working on a model with a time varying covariate. I understand that the \"best\" route might be to include both the time invariant variable and a time varying one (via a function of time), where the overall B = B_invariant + B_variant * f(t). \n\n1) if I wanted to report one B, has anyone seen reporting B at let's say the median event time?\n\n2) if I wanted to report CI for overall B at that time, would it simply be ll = ll_invariant + ll_variant and ul = ul_invariant + ul_variant?\n\n3) For simplicity, I've also considered just modelling the time varying covariate component but am not confidence in that approach. Anyone have thoughts on that?\n\nThanks in advance! I really need guidance on this.",
    "author": "NullDistribution",
    "timestamp": "2025-08-26T15:06:34",
    "url": "https://reddit.com/r/statistics/comments/1n0zfkq/q_reporting_on_time_varying_covariates_in_cox/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1n0ae8l",
    "title": "[Question] Need help choosing a statistical test for biological research",
    "content": "I have a set of biological data with two categorial independent variables (Location and Zone), one quantitative independent variable (Count of People), and one quantitative dependent variable (Count of Birds). The study's purpose is to look at human disturbance affecting bird count in an area. There are two locations (let's say Loc A and Loc B) and three zones (High, Moderate, Low) that represent the typical amount of people that visit each zone in a day - so the High Zone has a high mean of visitors, Low Zone has very few visitors, and Moderate Zone is somewhere in between. Both Loc A and Loc B have all three of these zones. Each zone per location has \\~20 rows of data - each row with a count of people at the zone and count of birds - so about 120 rows in total.\n\n\n\nI ran some ANOVAs and made a couple linear models, and noticed the count of birds was very similar between the Moderate and Low zones of a location, and this was present at both locations. These results can't speak on their own, though, since it's possible there's a huge difference in # of visitors between the Moderate and Low zones at Loc A, for example, but a minor difference in # of visitors for the same zones at Loc B. This would indicate different factors in play, I assume. I have no idea what sort of test can do this. I don't know if it's enough to compare the means of the zones at each location, as in Moderate at Loc A vs Moderate at Loc B, or if I want to combine data for Moderate &amp; Low zones at each location and compare the ranges of # of visitors. What do you think?\n\n\n\nAny help is greatly appreciated, thank you!\n\n\n\n\\- An undergraduate bio major &amp; data science minor",
    "author": "Familiar_Ad_8375",
    "timestamp": "2025-08-25T19:41:07",
    "url": "https://reddit.com/r/statistics/comments/1n0ae8l/question_need_help_choosing_a_statistical_test/",
    "score": 8,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mzkjlg",
    "title": "[E] Dirichlet Distribution - Explained",
    "content": "Hi there,\n\nI've created a video¬†[here](https://youtu.be/ntHnrEV22zQ)¬†where I explain the Dirichlet distribution, which is a powerful tool in Bayesian statistics for modeling probabilities across multiple categories, extending the Beta distribution to more than two outcomes.\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)",
    "author": "Personal-Trainer-541",
    "timestamp": "2025-08-25T01:04:19",
    "url": "https://reddit.com/r/statistics/comments/1mzkjlg/e_dirichlet_distribution_explained/",
    "score": 37,
    "num_comments": 0,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mzhxzz",
    "title": "[Question] Algorithm to update variance calculation data point by data point?",
    "content": "I'm currently trying to collect data inside of a program that is not set up to keep track of an arbitrary number of variables, but I still want to analyze the probability distribution of a series of observations within the program. Calculating the mean of the observations is easy; I set up one variable to track the most recent observation, and one variable to track the sum of observations so far, and one variable to track the number of observations so far; when observations stop coming in, I can then just divide the sum by n. But calculating the variance is trickier. I can set up a variable to keep track of the first observation, and another for second observation, and another for the third observation, but then if a fourth observation comes in when I was expecting three observations, I don't have a way of accounting for it. Is there some way that I can do something like calculate the variance initially when there four or five observations, then update it to account new information when a new data point comes in, without having to keep track of every individual data point that came before?",
    "author": "Dull-Song2470",
    "timestamp": "2025-08-24T22:19:44",
    "url": "https://reddit.com/r/statistics/comments/1mzhxzz/question_algorithm_to_update_variance_calculation/",
    "score": 3,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mz3way",
    "title": "[Education] Should I learn statistics in the workplace or in academia?",
    "content": "I work for a pharmaceutical research company. I am having a hard time trusting the statistics being done being done here. I‚Äôm relatively new to stats so can‚Äôt comment on the suitability of the methods being applied but my partner who is doing a PhD in statistics raised concerns. My main concern is that there aren‚Äôt many barriers to protect against bad stats. The most senior seems to be very knowledgeable and very much based in theory but the other most senior member appear to be self thought as they didn‚Äôt have formal/extensive training in statistics. I work in the stats department and is composed of graduates who studied maths and their stats training mainly came from the training the senior members of the team provided. They seem to have been promoted rather quickly too. The training is rather disorganised at times and everyone says something different. I want to do good stats and don‚Äôt want to pick up bad habits so early on. I‚Äôm interested in pursuing a PhD later down the line ones i have a bit more experience but I‚Äôm not sure if I should fast forward this to learn in an institution (academia) that is held more accountable for the quality of statistics. Is it advisable that I stay and learn here? ",
    "author": "[deleted]",
    "timestamp": "2025-08-24T11:47:51",
    "url": "https://reddit.com/r/statistics/comments/1mz3way/education_should_i_learn_statistics_in_the/",
    "score": 9,
    "num_comments": 6,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mz8qha",
    "title": "[Question] What statistical method should I use for my situation?",
    "content": "I originally posted on askstatistics, but was told that my question might be too complex, so I thought I'd ask here instead.\n\nI am collecting behavioral data over a period of time, where an instance is recorded every time a behavior occurs. An instance can occur at any time, with some instances happening quickly after one another, and some with gaps in between.\n\nWhat I want to do is to find clusters of instances that are close enough to one another to be considered separate from the others. Clusters can be of any size, with some clusters containing 20 instances, and some containing only 3.\n\nI have read about cluster analysis, but am unsure how to make it fit my situation. The examples I find involve 2 variables, where my situation only involves counting a single behavior on a timeline. The examples I find also require me to specify my cluster size, but I want my analysis to help determine this for me and involve clusters of different sizes.\n\nThe reason why is because, in behavioral analysis, it's important to look at the antecedents and consequences of a behavior to determine its function, and for high frequency behaviors, it is better to look at the antecedent and consequences for an entire cluster of the behavior.\n\nedit:\n\nI was asked to provide more information about my specific problem. Let's say I've been asked to help a patient who engages in trichotillomania (hair pulling disorder, a type of repetitive self-harm behavior). The patient does not know why they do it. It started a few years ago, and they have been unable to stop it. An \"instance\" is defined as moving their hand to their head and applying enough force to remove at least 1 strand of hair. They do know that there are periods where the behavior occurs less than others (with maybe 1-3 minute gaps between instances), and periods where they do it almost constantly (with 1 second gaps between instances). So we know that these \"episodes\" are different somehow, but I am unsure how to define what constitutes an \"episode\".\n\nTo help them with this, I decide to do a home/community observation of them for a period of 5 hours, in order to determine the antecedents (triggers) to the episode and consequences (what occurs after the episode ends that explains why it has stopped) to an episode of hair pulling. This is essential to developing an intervention to help reduce or eliminate the behavior for the patient. We need to know when an episode \"starts\" and when it \"ends\".\n\nMy problem is, what constitutes an \"episode\"? How close together do a group of instances of the behavior have to be to be included in an episode? How much latency between instances does there need to be before I can confidently say that it is part of a new episode? This cannot be done using pure visual analysis. It's not as simple as 50 instances happen within the first hour, then an hour gap, then another 50 instances happen, where the demarkation between them would be trivial to determine. Instead, the behavior occurs to some degree at all times, making it difficult to determine when old episodes end and new episodes begin. It would be very unhelpful to view the entire 5 hour block as a single \"episode\". Clearly there are changes, but I don't know where to quantifiably determine it.\n\nIt's very important to be accurate here because if I determine the start point wrong, then I will identify the wrong trigger, and my intervention will target the wrong thing, and could potentially make the situation worse, which is very bad when the behavior is self-harm. The stakes are high enough to warrant a quantifiable approach here.",
    "author": "ToeRepresentative627",
    "timestamp": "2025-08-24T14:56:02",
    "url": "https://reddit.com/r/statistics/comments/1mz8qha/question_what_statistical_method_should_i_use_for/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1myy2yw",
    "title": "[Question] Linear Mixed-Effects Model: blocking with random factor with &lt; 5 levels?",
    "content": "Hello everyone!\n\nI am writing an academic article, and a part of it is: I am trying to determine if Species richness is driven by Disturbance (fire or clearcutting), Soil Type (Organic or mineral), or a large amount of chemical data from the samples taken from **four** different forests.\n\nThe literature I searched suggested I block/group the samples using forest names as a random factor to control the non-independence of the samples.\n\nOne test to do this is Linear Mixed-Effects Models; however, all the literature I have read says that blocking/creating a random factor with &lt; 5 levels is not appropriate.\n\nThus, can I please have some advice on how to progress?",
    "author": "MountainNegotiation",
    "timestamp": "2025-08-24T08:09:22",
    "url": "https://reddit.com/r/statistics/comments/1myy2yw/question_linear_mixedeffects_model_blocking_with/",
    "score": 7,
    "num_comments": 13,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1myv2nz",
    "title": "[Q] Recommendations for a novice",
    "content": "[Question] Hey guys, I‚Äôve just taken my first stats course as part of grad school, and I‚Äôm loving it. It‚Äôs primarily applied statistics and R studio, we don‚Äôt really delve too deep into derivations, and the course is focused on topics like AB testing,  regression (linear, non-linear, multiple) , time series, and so on.\n\nI would love to learn more and am seeking resources for the same! I‚Äôm looking at deeper knowledge of applied statistics (rusty on the calculus)",
    "author": "largehardoncollider7",
    "timestamp": "2025-08-24T06:05:37",
    "url": "https://reddit.com/r/statistics/comments/1myv2nz/q_recommendations_for_a_novice/",
    "score": 3,
    "num_comments": 11,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1myn47l",
    "title": "Statistics at Columbia University [E][Q]",
    "content": "Hey everyone, I'm interested in majoring in statistics and wanted to ask if anyone has insights on how the statistics undergraduate program is at Columbia University. I've seen some saying to avoid it from posts from many years ago so I'm wondering if that still might be the case. All thoughts are appreciated!",
    "author": "Friendly-Popper",
    "timestamp": "2025-08-23T22:19:02",
    "url": "https://reddit.com/r/statistics/comments/1myn47l/statistics_at_columbia_university_eq/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1myj8s2",
    "title": "[Q] multiple comparison problem in bivariate analysis in observational, exploratory studies.",
    "content": "",
    "author": "nmolanog",
    "timestamp": "2025-08-23T18:48:18",
    "url": "https://reddit.com/r/statistics/comments/1myj8s2/q_multiple_comparison_problem_in_bivariate/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1myj2zx",
    "title": "[Q] How do I test if the difference between two averages is significant / not up to chance?",
    "content": "For example if I‚Äôm looking at the location with the highest average sales, and the lowest average in the past 10 years, how can I statistically determine whether the difference between the two surprising/is not up to chance? Anova? T-test? ",
    "author": "Cluelessjoint",
    "timestamp": "2025-08-23T18:40:16",
    "url": "https://reddit.com/r/statistics/comments/1myj2zx/q_how_do_i_test_if_the_difference_between_two/",
    "score": 3,
    "num_comments": 9,
    "upvote_ratio": 0.61,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1my7mll",
    "title": "[Education] [E] Opinions on chosen Statistics modules",
    "content": "Hi everyone, I'm starting a MSc in Statistics at the University of St Andrews in a few weeks. I can pick all the modules I will study myself, and I wanted your opinion on my selection so far.\n\nSemester 1: Applied Statistical Modelling Using GLMS, Markov Chains and Processes, Applied Bayesian Statistics, Independent Study Module (thinking of exploring Digital Signal Processing).\n\nSemester 2: Multivariate Analysis, Advanced Data Analysis, Machine learning for Data Analysis, Statistical Machine Learning.",
    "author": "Suspicious_Cat119",
    "timestamp": "2025-08-23T10:32:46",
    "url": "https://reddit.com/r/statistics/comments/1my7mll/education_e_opinions_on_chosen_statistics_modules/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1my7asa",
    "title": "[Q]: JACC publication stats... Cardiomyopathy related to methamphetamine abuse",
    "content": "While reading a paper on Cardiomyopathy related to methamphetamines vs other etiologies, I came across the table. I do not see how there could possibly be a statistical difference between these two sets of values, but there sits p&lt;0.001 - Cardiomyopathy with meth on the left, without meth on the right. The distributions are the same to less than 0.1%. I don't know much about statistics - but I know enough to ask a statistician -  these numbers seem to be nearly identical. Is this an error? Link to paper below.\n\n|| || |Length of stay (d)|&lt;3 d|1,037,195 (40.34)|5,098,918.41 (40.39)|&lt;0.001| \n\n.\n\n|4-6 d|738,610 (28.73)|3,632,147.96 (28.77)|¬†| \n\n.\n\n|7-9 d|353,964 (13.77)|1,740,210.64 (13.79)|¬†| \n\n.\n\n|10-12 d|167,402 (6.51)|822,719.36 (6.52)|¬†| \n\n.\n\n|&gt;12 d|273,942 (10.65)|1,328,752.52 (10.53)|¬†|\n\n[https://www.jacc.org/doi/10.1016/j.jacadv.2024.100840](https://www.jacc.org/doi/10.1016/j.jacadv.2024.100840)\n\n\n\n",
    "author": "Dizzy_Restaurant3874",
    "timestamp": "2025-08-23T10:20:21",
    "url": "https://reddit.com/r/statistics/comments/1my7asa/q_jacc_publication_stats_cardiomyopathy_related/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mxk9t7",
    "title": "[Q] R¬≤ and Within R¬≤",
    "content": "Hey, I‚Äôm running a panel event study with unit and time fixed effects, and my output on Rstudio reports both overall R¬≤ and ‚ÄúWithin R¬≤.‚Äù I understand the intuition (variance explained after de-meaning by unit/time), but I need a citable source (textbook, methods paper, or official documentation) that formally defines and/or derives Within R¬≤.  \nAlso any notes on interpreting Within vs. Overall R¬≤ in TWFE event-study specs with leads and lags.\n\nIf you have a specific citation or recommendation, I‚Äôd really appreciate it.",
    "author": "tacosaremyreligion",
    "timestamp": "2025-08-22T15:19:51",
    "url": "https://reddit.com/r/statistics/comments/1mxk9t7/q_r¬≤_and_within_r¬≤/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mwnf29",
    "title": "[D] this is probably one of the most rigorous but straight to the point course on Linear Regression",
    "content": "[The Truth About Linear Regression](https://www.stat.cmu.edu/~cshalizi/TALR/) has all a student/teacher needs for a course on perhaps the most misunderstood and the most used model in statistics, I wish we had more precise and concise materials on different statistics topics as obviously there is a growing \"pseudo\" statistics textbooks which claims results that are more or less contentious.   ",
    "author": "al3arabcoreleone",
    "timestamp": "2025-08-21T14:12:21",
    "url": "https://reddit.com/r/statistics/comments/1mwnf29/d_this_is_probably_one_of_the_most_rigorous_but/",
    "score": 118,
    "num_comments": 17,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mxeejm",
    "title": "[Question] Lectures to couple with Hoel, Port and Stone?",
    "content": "I recently started working through introduction to probability theory by Hoel, Port and Stone. Ive taken several statistics/biostatistical courses in grad school (7 yrs ago) but they really only covered the formulas without diving much into theory. \n\nAnyway, was wondering if anyone recommends any particular lectures (ie MIT open courseware) that could work alongside this book. Then I can do the practice problems from the textbook. Thanks!",
    "author": "damn_i_missed",
    "timestamp": "2025-08-22T11:28:57",
    "url": "https://reddit.com/r/statistics/comments/1mxeejm/question_lectures_to_couple_with_hoel_port_and/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mxcjvo",
    "title": "[Question]Formula for probability of rolling all sides of a 12 sided die",
    "content": "Lets say I had a 12 sided die.  I wanted to roll EACH INDIVIDUAL side of the die at least once.  What would the formula be for the probability of having rolled all sides of the die at least once over total rolls.  To determine something like: after 30 rolls, I'd have an X chance of having rolled each side at least once, where I'm trying to find X.\n\nThank you for any help in this matter.",
    "author": "The_Orange_Cow",
    "timestamp": "2025-08-22T10:19:28",
    "url": "https://reddit.com/r/statistics/comments/1mxcjvo/questionformula_for_probability_of_rolling_all/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mx7xh9",
    "title": "[Q] Does it make sense for a multivariate R^2 to be higher than that of any individual variable?",
    "content": "I fit a harmonic regression model on a set of time series. I then calculated the R\\^2 for each individual time series, and also the overall R\\^2 by taking the observations and fitted values as matrices. Somehow, the overall R\\^2 is significantly higher than those of the individual time series. Does this make sense? Is there a flaw in my approach?",
    "author": "fuckosta",
    "timestamp": "2025-08-22T07:24:12",
    "url": "https://reddit.com/r/statistics/comments/1mx7xh9/q_does_it_make_sense_for_a_multivariate_r2_to_be/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mwtg0e",
    "title": "[Question] Regression Analysis Used Correctly?",
    "content": "I'm a non-statistician working on an analysis of project efficiency, mostly for people who know less about statistics than I do...but also a few that know a lot more about statistics than I do.\n\nI can see that there is a lot of variation in the number of services provided as compared to the number of staff providing services in different provinces and I want to use regression analysis to look at the relationship, with the number of staff in provinces as the x variable and the number of services as the y variable and express the results using R squared and a line plot. \n\nAI doesn't exactly answer if this is the best approach and I wanted to triangulate with some expert humans. Am I going in the right direction? \n\nThanks for any feedback or suggestions. ",
    "author": "menejike",
    "timestamp": "2025-08-21T18:33:42",
    "url": "https://reddit.com/r/statistics/comments/1mwtg0e/question_regression_analysis_used_correctly/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mwo31g",
    "title": "[Q] [E] whats a good GRE score for top programs",
    "content": "Essentially I took the GRE today and got a 167 Q and I'm wondering if it's too low. \nTons of people have perfect scores so mine's a bit lacking, only 76th percentile.\nMy V was pretty good for the stats field (164, 93rd percentile) but idk if that matters to anyone. Is it worth retaking for 168-169 Q score?\n\nThanks for any perspectives üôè ",
    "author": "DuragChamp420",
    "timestamp": "2025-08-21T14:38:23",
    "url": "https://reddit.com/r/statistics/comments/1mwo31g/q_e_whats_a_good_gre_score_for_top_programs/",
    "score": 4,
    "num_comments": 14,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mwoj27",
    "title": "[Question] Dynamic Linear Model: Classical vs Discount Approach",
    "content": "I'm working on a time series forecasting problem and trying to decide between the classical approach and discount approach for Dynamic Linear Models (DLMs). Anyone here has experience comparing these approaches?\n\nI have successfully implemented the discount approach in Python. There seems to be limited literature on the comparison of both models and I'm curious if anyone has practical experience or opinions.\n\n* Classical approach: Estimates fixed variance matrices (V, W) via maximum likelihood\n* Discount approach: Uses discount factors (Œ¥) to create adaptive evolution variance (West &amp; Harrison, 1997)\n\n**Follow-up question:**¬†I am using maximum likelihood to estimate the discount parameters - is this the correct?\n\nReference: West, M., &amp; Harrison, J. (1997). Bayesian Forecasting and Dynamic Models (2nd ed.). Springer-Verlag.",
    "author": "Jay31416",
    "timestamp": "2025-08-21T14:56:46",
    "url": "https://reddit.com/r/statistics/comments/1mwoj27/question_dynamic_linear_model_classical_vs/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mwmefy",
    "title": "[Q] Could someone please explain to me how a homogeneous sample causes us to underestimate a correlation?",
    "content": "Title. In a class of psychology and the prof talked about how homogeneous samples cause us to estimate a correlation to be lower than it actually is and that it needs to be potentially accounted for, but I just can't seem to wrap my head around it? Shouldn't it be the other way around? Could someone explain it to me like im 5, I feel really dumb.",
    "author": "Netgay",
    "timestamp": "2025-08-21T13:33:13",
    "url": "https://reddit.com/r/statistics/comments/1mwmefy/q_could_someone_please_explain_to_me_how_a/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mwpfa3",
    "title": "[question] Bayes conditional probability for 9 IID events",
    "content": "I feel dumb for not being able to work this out without drawing up a large tree, and quick google didn‚Äôt get me the exact calculator I am looking for but:\n\nI have 9 independent events, but they are condition in that if one fails, the test fails. I only have the probably of the test failing approx = 0.71  \n\nI want to know the probability of the individual events failing, what‚Äôs the smart way to do this ? ",
    "author": "Drowsy_Forest",
    "timestamp": "2025-08-21T15:33:21",
    "url": "https://reddit.com/r/statistics/comments/1mwpfa3/question_bayes_conditional_probability_for_9_iid/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mwfmmd",
    "title": "[Q] Qualified to apply to a masters?",
    "content": "Wondering if my background will meet the requisites for general stats programs. \n\n  \nI have an undergrad degree in economics, over 5 years of work experience and have taken calc I and an intro to stats course. \n\n  \nI am currently taking an intro to programming course and will take calc II, intro to linear algebra, and stats II this upcoming semester. \n\n  \nWhen I go through the prerequisites it seems like they are asking for a heavier amount of math which I won't be able to meet by the time applications are due. Do I have a chance at getting into a program next year or should I push it out? ",
    "author": "chicanatifa",
    "timestamp": "2025-08-21T09:21:09",
    "url": "https://reddit.com/r/statistics/comments/1mwfmmd/q_qualified_to_apply_to_a_masters/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mwcjz1",
    "title": "[Education] Applying/transferring to European PhD programs as a Brazilian",
    "content": "Hello guys, i'm currently a first year brazilian econ PhD Student at a top brazilian university specializing in econometrics (especifically pn semiparametric and nonparametric estimation and identification) looking to transfer/apply to a Stats PhD program in Europe.\n\nDue to the nature of econ PhDs i've spent the majority of this year grinding through coursework (Math Camp, Microeconomics, Macroeconomics, Econometrics) and haven't really had time to perform research at all with the exception of alignments with my doctoral advisor. Grading schema is a bit confusing (with three options: A &gt; B &gt; C) as basically all grades are normalized since people tend to do very bad (for example, i've got an A in Metrics II with an overall grade of 5.0) and a B in Micro II with an overall grade of (6.1).  \nMost of my grades are B, with a A in Metrics II and, unfortunately two Cs, however i am confident that i can scrape more As in the current bimesters (mainly focusing for As in Metrics III and IV).\n\nOriginally i opted for a econ PhD in Brazil as i had no intention of leaving Brazil for personal reasons, however my doctoral advisor (who is a statistician) has strongly recommend that i try to transfer to the econ Msc program and that i apply to Econ/Stat PhD programs at the US/Europe for career reasons. And that, even if i'm unable to transfer, that i should apply either way using the graduate courses + electives (i'm looking to take functionaly analysis and measure theory next year, as i'll need both for my research) grade and my research as a writing sample.\n\nTo that end, i'm currently negotiating with the econ dept bureaucracy for transfer, but if that doesn't work i'll be applying either way as my doctoral advisor has suggested. My current plan is to finish my current RA and core courses this year and dedicate the following year to electives + research and a RA that my advisor has lined up with a buddy of his from Wharton and apply sometime in 2027/2028 (i'd wish to apply later due to personal reasons).\n\nAs such, as these ideas are still in preliminary stages, i'd like more information about stats dept in Europe and some advice. How do Stats application works if i end up not managing to transfer to the Msc programme, is a master obligatory? Is there anyway to transfer from my current PhD to an european PhD (i think this is extremely unlikely), what is more relevant for application: my grades? research? rec letters?\n\nI can provide more information if it's deemed necessary, i'll be very grateful to anyone who can help :)",
    "author": "KronOliver",
    "timestamp": "2025-08-21T07:30:55",
    "url": "https://reddit.com/r/statistics/comments/1mwcjz1/education_applyingtransferring_to_european_phd/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mwd5om",
    "title": "[Q] Type 1 error rate higher than 0.05",
    "content": "Hi, I am designing a statistically relatively difficult physiological study for which I developed two statistical methods to detect an effect. I also coded a script which simulates 1000 data sets for different conditions (a condition with no effect, and a few varying conditions which have an effect).\n\nUnfortunately, on the simulated data where the effect I am looking for is not present, with a significance level of Œ±=0.05 one of my methods detects an effect at a rate of 0.073. The other method detects an effect at a rate of 0.063. \n\nIs this generally still considered within limits for type 1 error rates? Will reviewers typically let this pass or will I have to tweak my methods? Thank you in advance.\n\nEdit: Turns out the problem was actually in my fake data... I used a fixed seed for one of the random values so there was a bias in the overall dataset since one of the parameters that played into the data generation had the same \"random\" values in every single dataset",
    "author": "putinisretard",
    "timestamp": "2025-08-21T07:52:23",
    "url": "https://reddit.com/r/statistics/comments/1mwd5om/q_type_1_error_rate_higher_than_005/",
    "score": 4,
    "num_comments": 10,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mw09yz",
    "title": "[C] Guidance on higher-education trajectory, research interests?",
    "content": "I got my Bachelor's degree in mathematics with a statistics concentration in May 2024, and took a brief 2-year gap to work a completely not-math related job to save up money, and I'm now gearing up to apply to a master's degree program in applied statistics. My ultimate goal is to get my PhD in applied stats, and specifically I want to do research on methods or models used in humanitarian aid research, such as migration, refugee aid, etc. (Not applying directly to a PhD since I took a 2 year gap, and I did not have any research experience during my undergrad, though if you think I should try, just let me know) \n\nSince I only have my bachelor's I quite honestly don't really know what kinds of research I would be looking to do but I know it's in that category. From what I've been able to gather myself it seems like the usual \"buzzwords\" would pop up such as time series, spatial stats, Bayesian stats, etc. but I wouldn't know where to begin to niche down on the specifics. In the meantime I am trying to have Claude guide me through a mock research project on public migration data from the UNHCR and conflict data from ACLED but I'm largely treating it as a kind of review course for myself.\n\nAt some level I feel like the above isn't \"valid\" justification enough for me to want to go for these advanced degrees but quite honestly I just can't see myself doing anything else, and I've always enjoyed being a student, and I want to become a college professor some day. So in that sense I'm posting this to ask if this plan of mine makes sense, is the field of applied statistics the most appropriate for what I'm interested in, and if you all have any advice in terms of preparing, or learning more about what kind of research specifically I would be able to do? I'm the first in my immediate family to pursue anything past a bachelor's degree so I also am just trying to figure out how it all works with research and assistantships and grants and all that - any guidance would be much appreciated!",
    "author": "Firesnake64",
    "timestamp": "2025-08-20T20:38:44",
    "url": "https://reddit.com/r/statistics/comments/1mw09yz/c_guidance_on_highereducation_trajectory_research/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mvo3ap",
    "title": "[Question] Best online resources for a beginner to learn experiments?",
    "content": "I was moved into a new role at work that is more advanced than anything I have done before. I have experience as a data analyst, mostly dashboarding and running ad-hoc SQL queries. Now I am in an Advanced Analytics role and part of my job is to run statistical experiments.\n\nWe have some internal training, but it's not great. Are there any online courses that y'all would recommend to teach me the concepts of running experiments?  \n  \nIt's more difficult for me to absorb learning through reading a lot of text, like a textbook. Videos can be helpful, but I am more of an interactive learner. Something where I can do interactive tests and exercises would be ideal. Code Academy was great for learning SQL. They have a basic Data Science course, but I don't see anything specifically on experiments.  \n  \nI can pay for a course if it's not more than $200.",
    "author": "pedroyoyoma",
    "timestamp": "2025-08-20T12:06:34",
    "url": "https://reddit.com/r/statistics/comments/1mvo3ap/question_best_online_resources_for_a_beginner_to/",
    "score": 8,
    "num_comments": 6,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mv7nd0",
    "title": "[E] Markov Chain Monte Carlo - Explained",
    "content": "Hi there,\n\nI've created a video¬†[here](https://youtu.be/nndtTssgtZE)¬†where I explain Monte Carlo Markov Chains (MCMC), which are a powerful method in probability, statistics, and machine learning for sampling from complex distributions\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)",
    "author": "Personal-Trainer-541",
    "timestamp": "2025-08-19T23:59:19",
    "url": "https://reddit.com/r/statistics/comments/1mv7nd0/e_markov_chain_monte_carlo_explained/",
    "score": 50,
    "num_comments": 12,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mvsp38",
    "title": "[Question] What is the ‚Äúratio of variances‚Äù?",
    "content": "To provide more context, I am looking to perform a non-inferiority test, and in it I see a variable ‚ÄúR‚Äù which is defined as ‚Äúthe ratio of variances at which to determine power‚Äù.\n\nWhat exactly does that mean? I am struggling to find a clear answer.\n\nPlease let me know if you need more clarifications.\n\nEdit: I am comparing two analytical methods to each other (think two one-sided test, TOST, or OST). R is being used in a test statistic that uses counts from a 2x2 contingency table comparing positive and negative results from the two analytical methods. \n\nI have seen two options:  r=var1/var2, but this doesn‚Äôt seem right as the direction of the ratio would impact the outcome of the test. The other is F test related, but I lack some understanding there.",
    "author": "Fritos121",
    "timestamp": "2025-08-20T14:57:38",
    "url": "https://reddit.com/r/statistics/comments/1mvsp38/question_what_is_the_ratio_of_variances/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mv0aqb",
    "title": "[Q] what core concepts should i focus on for applied statistics master's degree?",
    "content": "",
    "author": "slapmenanami",
    "timestamp": "2025-08-19T17:38:43",
    "url": "https://reddit.com/r/statistics/comments/1mv0aqb/q_what_core_concepts_should_i_focus_on_for/",
    "score": 17,
    "num_comments": 13,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1muk07c",
    "title": "Is an applied statistics masters degree (Sweden) valuable? [E]",
    "content": "As the title says this is an applied statistics program. There is no measure-theoretic probability and all that fancy stuff. First sem has probability theory, inference theory, R programming and even basic math cause I guess they don't require a very heavy math background.\n\nThis program is in Sweden and from what i can see statistics is divided into 2 disciplines:\n\nMathematical statistics - usually housed in the department of mathematics and has significant math prerequisites to get in. \n\nStatistics - housed in the department of social sciences. This is the one im going for. Courses are more along the lines of experimental design, econometrics, GLM, with some machine and bayesian learning optional courses.\n\nIn terms of my background im completing my bachelors in econometrics and have taken some basic computer science and math courses and lots of data analytics stuff.\n\nI hope to pursue a PhD afterwards, but not sure what field I want to specialize in just yet.\n\nIs this a valuable degree to get? Or should I just do a master of AI and learn cool stuff?\n\n",
    "author": "gaytwink70",
    "timestamp": "2025-08-19T07:27:55",
    "url": "https://reddit.com/r/statistics/comments/1muk07c/is_an_applied_statistics_masters_degree_sweden/",
    "score": 29,
    "num_comments": 7,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mv6ciy",
    "title": "[Q] 23 events in 1000 cases - Multivariable Logistic Regression EPV sensitivity analysis",
    "content": "I am a medical doctor with Master of Biostatistics, though my hands-on statistical experience is limited, so pardon the potential basic nature of this question. \n\nI am working on a project where we aimed to identify independent predictor for a clinical outcome. All patients were recruited prospectively, potential risk factors (based on prior literature) were collected, and analysed with multivariable logistic regression. I will keep the details vague as this is still a work in progress but that shouldn't affect this discussion. \n\nThe outcome event rate was 23 out of 1000. \n\n||Adjusted OR|95% CI|p|\n|:-|:-|:-|:-|\n|Baseline|*0.010*|0.005 ‚Äì 0.019|&lt;0.001|\n|A|30.78|6.89 ‚Äì 137.5|&lt;0.001|\n|B|*5.77*|2.17 ‚Äì 15.35 |&lt;0.001 |\n|C|4.90|1.74 ‚Äì 13.80|0.003|\n|D|0.971|0.946 ‚Äì 0.996|0.026|\n\nI checked for multi-collinearity. I am aware of the conventional rule of thumb where event per variable should be ‚â•10. The factors above were selected using stepwise selection from univariate factors with p&lt;0.10, supported by biological plausibility. \n\nFactor A is obviously highly influential but is only derived with 3 event out of 11 cases. It is however a well established risk factor. B and C are 5 out of 87 and and 7 out of 92 respectively. D is a continuous variable (weight). \n\n**My questions are:**\n\n* With so few events this model is inevitably fragile, am I compelled to drop some predictors?\n* One of my sensitivity analysis is Firth's penalised logistic regression which only slightly altered the figures but retained the same finding largely. \n* Bootstrapping however gave me nonsensical estimates, probably because of the very few events especially for factor A where the model suggests insignificance. This seems illogical as A is a known strong predictor.\n* Do you have suggestions for addressing this conundrum? \n\n  \nThanks a lot. \n\n  \n",
    "author": "changyang1230",
    "timestamp": "2025-08-19T22:39:37",
    "url": "https://reddit.com/r/statistics/comments/1mv6ciy/q_23_events_in_1000_cases_multivariable_logistic/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mus6zf",
    "title": "[Q] Paired population analysis for different assaying methods.",
    "content": "First disclaimer not a statistician, so if this makes no sense sorry. Trying to figure out my best course of statistical analysis here.\n\nI have some analytical results from the assaying of a sample. The first analysis run was using a less sensitive analytical method. Say the detection limit (DL) for this one element, eg Potassium, is 0.5ppm using the less sensitive method. We decided to run a secondary analysis using the same sample pulps on a much more sensitive method where the detection limit is 0.01ppm for the exact same element (K) but using this different method.\n\nWhen the results were received it was noticed that anything between the DL and 10x DL for the first method the results were wildly varied between the two types of analysis. See table\n\n|Sample ID|Method 1 (0.5ppm DL)|Method 2 (0.01ppm DL)|Difference|\n|:-|:-|:-|:-|\n|1|0.8|0.6|0.2|\n|2|0.7|0.49|0.21|\n|3|0.6|0.43|0.17|\n|4|1.8|3.76|\\-1.96|\n|5|1.4|0.93|0.47|\n|6|0.6|0.4|0.2|\n|7|0.5|0.07|0.43|\n|8|0.5|0.48|0.02|\n|9|0.7|0.5|0.2|\n|10|0.5|0.14|0.36|\n|11|0.7|0.44|0.26|\n|12|0.5|0.09|0.41|\n|13|0.5|0.43|0.07|\n|14|0.9|0.88|0.02|\n|15|4.7|0.15|4.55|\n|16|0.9|0.81|0.09|\n|17|0.5|0.33|0.17|\n|18|1.2|0.99|0.21|\n|19|1|1|0|\n|20|1.3|0.91|0.39|\n|21|0.7|1.25|\\-0.55|\n\nThen continued to look at another element analyzed in the assay and noticed that the two method results were much more similar despite the sample parameters (results between the DL and 10x the DL). For this element, say Phosphorus, the DL is 0.05ppm for the more sensitive analysis and 0.5ppm for the less sensitive analysis.\n\n\n\n|Sample ID|Method 1 (0.5ppm DL)|Method 2 (0.05ppm DL)|Difference|\n|:-|:-|:-|:-|\n|1|1.5|1.49|\\-0.01|\n|2|1.4|1.44|0.04|\n|3|1.5|1.58|0.08|\n|4|1.7|1.76|0.06|\n|5|1.6|1.62|0.02|\n|6|0.5|0.47|\\-0.03|\n|7|0.5|0.53|0.03|\n|8|0.5|0.49|\\-0.01|\n|9|0.5|0.48|\\-0.02|\n|10|0.5|0.46|\\-0.04|\n|11|0.5|0.47|\\-0.03|\n|12|0.5|0.47|\\-0.03|\n|13|0.5|0.51|0.01|\n|14|0.5|0.53|0.03|\n|15|0.5|0.51|0.01|\n|16|1.5|1.48|\\-0.02|\n|17|1.8|1.86|0.06|\n|18|2|1.9|\\-0.1|\n|19|1.8|1.77|\\-0.03|\n|20|1.9|1.84|\\-0.06|\n|21|0.8|0.82|0.02|\n\n\n\nFor this element there is about 360 data points that are similar as the table but kept it brief for the sake of reddit.\n\nMy question, what is the best statistical analysis to proceed with here. I want to basically go through the results and highlight the elements where the difference between the two methods is negligible (see table 2) and where the difference is quite varied (table 1) to apply caution when using the analytical results for further analysis.\n\nNow some of this data is normally distributed but most of it is not. For the most part, most of the data (&gt;90%) runs at or near the detection limit with outlier high kicks (think heavy right skewed data).\n\nAny help to get me on the right path is appreciated.\n\nLet me know if some other information is needed\n\n¬†\n\nCheers\n\n|| || |||| ||||",
    "author": "dmarc031",
    "timestamp": "2025-08-19T12:20:48",
    "url": "https://reddit.com/r/statistics/comments/1mus6zf/q_paired_population_analysis_for_different/",
    "score": 5,
    "num_comments": 8,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mv4hub",
    "title": "How can I analyse data best for my dissertation? [R]",
    "content": "",
    "author": "bigtiddiemonster",
    "timestamp": "2025-08-19T20:57:40",
    "url": "https://reddit.com/r/statistics/comments/1mv4hub/how_can_i_analyse_data_best_for_my_dissertation_r/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mujwrr",
    "title": "[D] Estimating median treatment effect with observed data",
    "content": "I'm estimating treatment effects on healthcare cost data which is heavily skewed with outliers, so thought it'd be useful to find median treatment effects (MTE) or median treatment effects on the treated (MTT) as well as average treatment effects.\n\nIs this as simple as running a quantile regression rather than an OLS regression? This is easy and fast with the **MatchIt** and **quantreg** packages in R.\n\nWhen using propensity score matching followed by regression on the matched data, what's the best method for calculating valid confidence intervals for an MTE or MTT? Bootstrapping seems like the best approach with PSM or other methods like *g*\\-computation.\n\n",
    "author": "RobertWF_47",
    "timestamp": "2025-08-19T07:24:23",
    "url": "https://reddit.com/r/statistics/comments/1mujwrr/d_estimating_median_treatment_effect_with/",
    "score": 3,
    "num_comments": 11,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mun3lq",
    "title": "Littly biology student is not sure if his approach is correct, please have a look (spatial point patterns in 2D) [Question]",
    "content": "Hey I am writing my bachelor in biology, I do like math and am (relative to other bios) quite good at it but wanted to make sure my approach does make sense.\n\n I am working with spatial point patterns in 2D, and am using the quadrat test to check for complete spatial randomness (CSR). This test divides a big rectangle (sample) into little rectangeles, counts the amount of points per rectangle and does a chi squared test to compare if this follows a poisson distribution. Under CSR this should be the case. The amount and size of rectangles can be chosen manually and is (according to gemini) a big challenge and limitation to the test. \n\nSince its using chi squared test, gemini told me it makes sense to have at least 5 expected points per rectangle. I will probably go for 10 expected points per rectangle, this is completely arbitrary though. It feels like it makes sense to have it be twice of what the minimum should be but it still is arbitrary. \n\nR only allows me to change the amount of squares, not the size (I think). I am working in R studio with the spatstat package. \n\n  \nI have a big rectangle with sidelengths a (horizontal) and b (vertical) and Np (amount of points). \n\na\\*b gives me the Area (A), the number of rectangels (Nr) should be 1/10 of Np\n\n  \nNx is the amount of rectangles on the horizontal axis, Ny corresponds to the vertical axis\n\nNr should be equal to Nx\\*Ny.\n\nI think it makes sense to have the ratio of the little rectangels be the same as the big rectangle, otherwise I think the test would be more sensitive towards changes in one direction than the other. \n\nSo:     Nx \\* Ny = Nr   and  Ny = Nx \\* a/b\n\nNx \\* (Nx \\* a/b) =  Nr\n\nNx\\^2 \\* a/b = Nr\n\nNx = sqrt(Nr\\*b/a)\n\nNy = Nx \\* a/b\n\n  \nIm pretty confident that the calculation is correct (but please correct me if not), my question is more about the approach. Does it make sense to  have an expected amount of 10 points per rectangle, and does it make sense to not go for squares with sidelength sqrt(Nr) but to go for rectangels that have the same ratio as the big rectangel, the sample?\n\nAny feedback is appreciated\n\n",
    "author": "Radio__W",
    "timestamp": "2025-08-19T09:19:23",
    "url": "https://reddit.com/r/statistics/comments/1mun3lq/littly_biology_student_is_not_sure_if_his/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mtns3v",
    "title": "[Q] Batch correction for bounded variables (0-100)",
    "content": "I am working on drug response data from approximately 30 samples. For each sample, I also have clinical and genetic data and I'm interested in finding associations between drug response and clinical/genetic features. I would also like to perform a cluster analysis to see possible clustering. However, the samples have been tested with two batches of the compound plates (approximately half the patients for each batch), and I do notice statistically significant differences between the two batches for some of the compounds, although not all (Mann-Whitney U, p &lt; 0.01).\n\nEach sample was tested with about 50 compounds, with 5 concentrations, in duplicate; and my raw data is a fluorescence value related to how many cells survived, in a range of 0 to let's say 40k fluorescence units. I use these datapoints to fit a four-parameter log-logistic function, then from this interpolation I determine the area under the curve, and I express this as a percentage of the maximum theoretical area (with a few modifications, such as 100-x to express data as inhibition, but that's the gist of it). So I end up with a final AUC% value that's bound between the values of 0% AUC (no cells died even at the strongest concentration) and 100% AUC (all cells died at the weakest concentration). The data is not normally distributed, and certain weaker compounds never show values above 10% AUC.\n\nTo test for associations between drug response and genetic alterations, I opted to perform a stratified Wilcoxon-Mann-Whitney test, using the¬†[wilcox\\_test function from R's 'coin' package](https://cran.r-project.org/web/packages/coin/coin.pdf)¬†(formula:¬†`compound ~ alteration | batch`). For specific comparisons where one of the batches had 0 samples for one group, I dropped the batch and only used data from the other batch with both groups present. Is this a reasonable approach?\n\nI would also like, if possible, to actually harmonize the AUC values across the two batches, for example in order to perform cluster analysis. But I find it hard to wrap my head around options for this. Due to the range 0-100 I would think that methods such as ComBat might not be amenable. And I do know that clinical/genetic characteristics can be associated with the data, but I have a vast amount of these variables, most of them sparse, so... I could try to model the data, but I feel that I'm damned if I do include a selection of the less sparse clin/genetic variables and damned if I don't.\n\nAt the moment I'm performing clustering without batch harmonization - I first remove drugs with low biological activity (AUC%), then rescale the remaining ones to 0-100 of their max activity, and transform to a sample-wise Z-score. I do see interesting data, but I want to do the right thing here, also expecting possible questions from reviewers. I would appreciate any feedback.",
    "author": "bio_ruffo",
    "timestamp": "2025-08-18T07:34:37",
    "url": "https://reddit.com/r/statistics/comments/1mtns3v/q_batch_correction_for_bounded_variables_0100/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mt7tc3",
    "title": "[Q] Is MRP a better fix for low response rate election polls than weighting?",
    "content": "Hi all,\n\nI‚Äôve been reading about how bad response rates are for traditional election polls (&lt;5%), and it makes me wonder if weighting those tiny samples can really save them. From what I understand, the usual trick is to adjust for things like education or past vote, but at some point it feels like you‚Äôre just stretching a very small, weird sample way too far.\n\nI came across Multilevel Regression and Post-stratification (MRP) as an alternative. The idea seems to be:\n\n* fit a model on the small survey to learn relationships between demographics/behavior and vote choice,\n* combine that with census/voter file data to build a synthetic electorate,\n* then project the model back onto the full population to estimate results at the state/district level.\n\nApparently it‚Äôs been pretty accurate in past elections, but I‚Äôm not sure how robust it really is.\n\nSo my question is: for those of you who‚Äôve actually used MRP (in politics or elsewhere), is it really a game-changer compared to heavy weighting? Or does it just come with its own set of assumptions/problems (like model misspecification or bad population files)?\n\nThanks!",
    "author": "Ghost-Rider_117",
    "timestamp": "2025-08-17T17:50:57",
    "url": "https://reddit.com/r/statistics/comments/1mt7tc3/q_is_mrp_a_better_fix_for_low_response_rate/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1msxq7c",
    "title": "[Q] How do I stop my residuals from showing a trend over time?",
    "content": "Hey guys. I‚Äôve been looking into regression and analyzing residuals. I noticed when looking at my residual plots they are normally spread out when looking at them with the forecasted totals on the x axis and the residuals on the y axis. \n\nHowever, if I put time (month) on the x axis and residuals on the y axis the errors show a clear trend. How can I either transform my data or add dummy variables to prevent this from occurring? It‚Äôs leading to scenarios where the error of my regression line become uneven over time.\n\nFor reference my X variable is working hours and my Y variable is labor cost. Is the reason why this is happening because my data is inherently nonstationary? (The statistical properties of working hours changes based on inflation, wage increases every year, etc.)\n\nEDIT: Here is a photo of what the charts look like.\n\nhttps://imgur.com/a/O5ti3zn",
    "author": "dannydawiz",
    "timestamp": "2025-08-17T10:56:53",
    "url": "https://reddit.com/r/statistics/comments/1msxq7c/q_how_do_i_stop_my_residuals_from_showing_a_trend/",
    "score": 9,
    "num_comments": 15,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1msuxio",
    "title": "[Q] Any nice essays/books/articles that delve into the notion of \"noise\" ?",
    "content": "This concept is very critical for studying statistics nonetheless it's vaguely defined, I am looking for nice/concise readings about it please.",
    "author": "al3arabcoreleone",
    "timestamp": "2025-08-17T09:09:42",
    "url": "https://reddit.com/r/statistics/comments/1msuxio/q_any_nice_essaysbooksarticles_that_delve_into/",
    "score": 10,
    "num_comments": 13,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ms78mm",
    "title": "[Career] Statistics MS Internships",
    "content": "Hello,\n\n  \nI will be starting a MS in Statistical Data Science at Texas A&amp;M in about a week. I have some questions about priorities and internships.\n\n  \nSome background: I went to UT for my undergrad in chemical engineering and I worked at Texas Instruments as a process engineer for 3 years before starting the program. I interned at TI before working there so I know how valuable an internship can be. \n\n  \nI landed that internship in my junior year of undergrad where I had already taken some relevant classes. The master's program is only two years so I have only one summer to do an internship. What I did in my previous job is not really relevant to where I want to go after graduating (Data Science/ML/AI type roles) so I don't think my resume is very strong.\n\n  \nShould I still put my time into the internship hunt or is it better spent elsewhere?",
    "author": "aangaroo",
    "timestamp": "2025-08-16T13:46:42",
    "url": "https://reddit.com/r/statistics/comments/1ms78mm/career_statistics_ms_internships/",
    "score": 21,
    "num_comments": 10,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1msh4pl",
    "title": "[Q] GRE Quant Score for Statistics PhD Programs",
    "content": "I just took the GRE today and got a 168 score on the quant section. Obviously, this is less than ideal since the 90th percentile is a perfect score (170). I don't plan on sending this score to PhD programs that don't require the GRE, but is having less than a 170 going to disqualify me from consideration for programs that require it (e.g. Duke, Stanford, UPenn, etc.)? I realize those schools are long shots anyway though. :')",
    "author": "Conscious_Counter710",
    "timestamp": "2025-08-16T21:10:21",
    "url": "https://reddit.com/r/statistics/comments/1msh4pl/q_gre_quant_score_for_statistics_phd_programs/",
    "score": 4,
    "num_comments": 6,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mru1ds",
    "title": "[Q] Need help understanding p-values for my research data",
    "content": "Hi! Im working on a research project (not in math/finance, im in medicine), and im really struggling with data analysis. Specifically, I dont understand how to calculate a p-value or when to use it. I've watched a lot of YouTube videos, but most of them either go too deep into the math or explain it too vaguely. I need a practical explanation for beginners.\n What exactly does a p-value mean in simple terms? How do I know which test to use to get it? Is there a step-by-step example (preferably medical/health-related) of how to calculate it?\n\nIm not looking for someone to do my work, I just need a clear way to understand the concept so I can apply it myself.\n\nEdit: Your answers really cleared things up for me. I ended up using MedCalc: Fishers exact test for categorical stuff and logistic regression for continuous data. Looked at age, gender, and comorbidities (hypertension/diabetes) vs death. Ill still consult with a statistician, but this gave me a much better starting point.",
    "author": "pluhuryjesh",
    "timestamp": "2025-08-16T05:36:34",
    "url": "https://reddit.com/r/statistics/comments/1mru1ds/q_need_help_understanding_pvalues_for_my_research/",
    "score": 8,
    "num_comments": 15,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mslm2p",
    "title": "Is Statistics becoming less relevant with the rise of AI/ML? [Q]",
    "content": "In both research and industry, would you say traditional statistics and statistical analysis is becoming less relevant, as data science/AI/ML techniques perform much better, especially with big data?",
    "author": "gaytwink70",
    "timestamp": "2025-08-17T01:34:00",
    "url": "https://reddit.com/r/statistics/comments/1mslm2p/is_statistics_becoming_less_relevant_with_the/",
    "score": 0,
    "num_comments": 48,
    "upvote_ratio": 0.39,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mrk0g9",
    "title": "[Discussion] Philosophy of average, slope, extrapolation, using weighted averages?",
    "content": "There are at least a dozen different ways to calculate the average of a set of nasty real world data. But none, that I know of, is in accord with what we intuitively think of as \"average\".\n\nThe mean as a definition of \"average\" is too sensitive to outliers. For example consider the positive half of the Cauchi distribution (Witch of Agnesi). The mode is zero, median is 1 and the mean diverges logarithmically to infinity as the number of sample points increases.\n\nThe median as a definition of \"average\" is too sensitive to quantisation. For example the data 0,1,0,1,1,0,1,0,1 has mode 1, median 1 and mean 0.555...\n\nGiven than both mean and median can be expressed as weighted averages, I was wondering if there was a known \"ideal\" method for weighted averages that both minimises the effects of outliers and handles quantisation?\n\nI can define \"ideal\". The weighted average is sum(w_i x_i)/sum(w_i) for n &gt;= i &gt;= 1 Let x_0 be the pre-guessed mean. The x_i are sorted in ascending order. The weight w_i can be a function of either (i - n/2) or (x_i - x_0) or both.\n\nThe x_0 is allowed to be iterated. From a guessed weighted average we get a new weighted mean which is fed back in as the next x_0.\n\n*The \"ideal\" weighting is the definition of w_i where the scatter of average values decreases as rapidly as possible as n increases.*\n\nAs clunky examples of weighted averaging, the mean is defined by w_i = 1 for all i.\n\nThe median is defined as w_i = 1 for i = n/2, w_i = 1/2 for i = (n-1)/2 and i = (n+1)2, and w_i = 0 otherwise.\n\nOther clunky examples of weighted averaging are a mean over the central third of values (loses some accuracy when data is quantised). Or getting the weights from a normal distribution (how?). Or getting the weights from a norm other than the L_2 norm to reduce the influence of outliers (but still loses some accuracy with outliers).\n\nSimilar thinking for slope and extrapolation. Some weighted averaging that always works and gives a good answer (the cubic smoothing spline and the logistic curve come to mind for extrapolation).\n\nTo summarise, is there a best weighting strategy for \"weighted mean\"?",
    "author": "Turbulent-Name-8349",
    "timestamp": "2025-08-15T20:55:58",
    "url": "https://reddit.com/r/statistics/comments/1mrk0g9/discussion_philosophy_of_average_slope/",
    "score": 5,
    "num_comments": 5,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mrvi6m",
    "title": "[Discussion] Synthetic Control with Repeated Treatments and Multiple Treatment Units",
    "content": "",
    "author": "pvm_64",
    "timestamp": "2025-08-16T06:37:34",
    "url": "https://reddit.com/r/statistics/comments/1mrvi6m/discussion_synthetic_control_with_repeated/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mriadp",
    "title": "[E] Did you mainly aim for breadth or depth in your master‚Äôs program?",
    "content": "Did you use your master‚Äôs program to explore different topics/domains (finance, clinical trials, algorithms, etc.) or reinforce the foundations (probability, linear algebra, machine learning, etc.)? I think it‚Äôs expected to do a mix of both, but do you think one is more helpful than the other?\n\nI‚Äôm registered for master‚Äôs/PhD level of courses I‚Äôve taken, but I‚Äôm considering taking intro courses I haven‚Äôt had exposure to. I‚Äôm trying to leave the door open to apply to PhD programs in the future, but I also want to be equipped for different industries. Your opinions are much appreciated :-)",
    "author": "-ninn",
    "timestamp": "2025-08-15T19:34:56",
    "url": "https://reddit.com/r/statistics/comments/1mriadp/e_did_you_mainly_aim_for_breadth_or_depth_in_your/",
    "score": 7,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mrbt2y",
    "title": "[Q] Advanced book on risk analysis?",
    "content": "Are there books or fields that go deep into calculating risk? I've already read Casella and Berger, grad level stochastic analysis, convex optimization. the basic masters level books for the other major branches. or is this more a stats question? \n\nor am I asking the wrong question? is risk, uncertainty application based?",
    "author": "No-Ebb-5573",
    "timestamp": "2025-08-15T15:00:11",
    "url": "https://reddit.com/r/statistics/comments/1mrbt2y/q_advanced_book_on_risk_analysis/",
    "score": 9,
    "num_comments": 3,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mrclck",
    "title": "[Education] Need advice for Teaching Linear Regression to Non-Math Students (Accounting Focus)",
    "content": "Hi everyone! This semester, I‚Äôll be teaching linear regression analysis to accounting students. Since they‚Äôre not very familiar with advanced mathematical concepts, I initially planned to focus on practical applications rather than theory. However, I‚Äôm struggling to find real-world examples of regression analysis in accounting.\n\nDuring my own accounting classes in college, we mostly covered financial reporting (e.g., balance sheets, income statements). I‚Äôm not sure how regression fits into this field. Does anyone have ideas for relevant accounting applications of regression analysis? Any advice or examples would be greatly appreciated!",
    "author": "VanBloot",
    "timestamp": "2025-08-15T15:30:42",
    "url": "https://reddit.com/r/statistics/comments/1mrclck/education_need_advice_for_teaching_linear/",
    "score": 7,
    "num_comments": 6,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mrcjx1",
    "title": "[Q] Is it possible to do single-arm meta-analysis in revman5 or MetaXL?",
    "content": "I'm pretty novice at meta-analysis so Im struggling to figure how to go about my analysis. Im doing a study where there is no control group, just purely intervention and binary survival outcomes. I was trying to figure out how to perform meta-analysis on this. I have revman5 and metaXL (I just downloaded it), but I don't know how or if I even can do single arm analysis with these. Does anyone know what I can do? I've been beating my head in trying to figure it out.",
    "author": "SilentStaff",
    "timestamp": "2025-08-15T15:29:10",
    "url": "https://reddit.com/r/statistics/comments/1mrcjx1/q_is_it_possible_to_do_singlearm_metaanalysis_in/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mqt4r2",
    "title": "[Q] Repeated measures but only one outcome modelling strategy",
    "content": "Hi all,\n\nI have a dataset where longitudinal measurements have been taken daily over several months, and I want to look at the effect of this variable on a single outcome, that's measured at the end of the time period. I've been advised that a mixed effects model will account for within person correlations, but I'm having real trouble fitting the model to the real data and getting a simulation study to work correctly. The data looks like this:\n\n\n    id | x     | y\n    ----------------\n    1 | 10.5 | 31.1\n    1 | 14.6 | 31.1\n    ...\n    1 | 9.9  | 31.1\n    2 |15.4 | 25.5\n    2 |17.9 | 25.5\n    ...\n\n\nMy model is pretty simple, after scaling variables \n\n```\nlmer('y ~ x + (1|id)', data=df)\n```\n\nWhen I try to fit these models in general I get errors about the model failing to converge, or eigenvalues being large or negative. For a few sets of simulations I do get model convegence, but the simulation parameters are really sensitive. My concern is that there is no variance in y within group and that's causing the fit problems. Can this approach work or do I need to go back to the drawing board with my advisor?\n\nThanks!",
    "author": "joseph_fourier",
    "timestamp": "2025-08-15T02:48:42",
    "url": "https://reddit.com/r/statistics/comments/1mqt4r2/q_repeated_measures_but_only_one_outcome/",
    "score": 6,
    "num_comments": 11,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mr4axh",
    "title": "[Q] Interpreting SEM/SE",
    "content": "I have a (hopefully) quick question about interpreting SEM and SD in descriptive statistics. So I have a sample of 10 with 5 females and 5 males. I'm reporting my descriptive stats by the entire sample (n=10), and then the sexes separately. My question is, if the SEM and/or SD of the entire sample is higher than the SEM/SD of the separated female and/or male samples, does that mean that analysing the sexes separately is better? Some of my parameters have a higher SEM and/or SD than one of the sexes, but lower than the other (example with made-up values: entire sample = 3, female = 1, male = 2), so I'm a little confused about how to interpret that.",
    "author": "3catsinahumansuit",
    "timestamp": "2025-08-15T10:23:01",
    "url": "https://reddit.com/r/statistics/comments/1mr4axh/q_interpreting_semse/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mr3qge",
    "title": "[Q] Calculator",
    "content": "I am to soon start my freshman year as a statistics major and was wondering what calculator to purchase.  Would be much grateful for your advice.  Thanks!!!",
    "author": "84sebastian",
    "timestamp": "2025-08-15T10:02:40",
    "url": "https://reddit.com/r/statistics/comments/1mr3qge/q_calculator/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mqnrnt",
    "title": "[Question] Does Immortal Time Bias exist in this study design?",
    "content": "Hi all,\n\nI‚Äôm trying to understand if two survival comparison study designs I‚Äôm contemplating would be at risk of immortal time bias between the comparison groups. I understand the concept of ITB, but given it‚Äôs complexity I want to double check my reasoning:\n\n\nStudy 1:\n\nA cohort of cancer patients all receive the same therapy, treatment A after disease diagnosis. At various times prior to or during treatment, the patients receive genetic testing to determine whether they have mutation X or not. *Patients who die or for some reason don‚Äôt get testing to determine mutation status are removed from the study.* Assume no difference in the distribution of testing times in relation to treatment start time between those patients with and without the mutation. Presence or absence of mutation X does not impact patient treatment decisions (e.g, if a patient was known to have mutation X prior to treatment initiation, they would still receive treatment A). \n\n**If I were to compare the overall survival rates of patients on treatment A with and without mutation X (again, all treated with the same treatment A), with survival time starting at the initiation of treatment, would I be introducing ITB between the groups?**\n\n\nStudy 2:\n\nNow we have a cohort of cancer patients in which one group gets treatment A and one gets treatment B. Assume that for all patients, treatment starts at equivalent times after diagnosis. Like with study 1, at various times prior to or during treatment, the patients receive genetic testing to determine whether they have mutation X or not, and again patients that receive no testing are excluded from the study. Again, presence or absence of mutation X does not  impact patient treatment (treatment A/B is decided agnostic of any testing information).\n\n**If I were to compare overall survival between patients who received treatment A and those who received treatment B, restricted to just patients with mutation X, with survival time starting at the initiation of treatment, would I be introducing ITB between groups due to not limiting my cohort to those that received mutation testing _before_ treatment?**\n\n\nIn both cases, my interpretation is that ITB may be introduced, but NOT due to a non-standard testing time (e.g. patients might find out they are mutation X positive 5 days before treatment or 50 days after treatment begins). But I really appreciate any feedback anyone might have!",
    "author": "Apb58",
    "timestamp": "2025-08-14T21:44:38",
    "url": "https://reddit.com/r/statistics/comments/1mqnrnt/question_does_immortal_time_bias_exist_in_this/",
    "score": 6,
    "num_comments": 4,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mr8btq",
    "title": "[D] Should the mean - instead of median - almost never be used in descriptive statistics?",
    "content": "The only time I would prefer the mean to describe a distribution is when I cared about something over the long run, like if I were running a casino and wanted to know how much I expect to earn from each gambler. In that case though, I would be thinking of it as the expected value because long run convergence matters.\n\nIf we're talking about anything where you're not repeatedly sampling from the same distribution, it seems like the median is always better. My reasoning being, if you have a skewed distribution, the median will give you a value that is \"more typical\" of any possible value. If you have a symmetric distribution, the mean and the median are pretty much equal, so just use the median here too. \n\nIn any case, simply always using the median eliminates any uncertainty about if the distribution is too skewed or symmetric enough for the mean. ",
    "author": "ObeseMelon",
    "timestamp": "2025-08-15T12:50:21",
    "url": "https://reddit.com/r/statistics/comments/1mr8btq/d_should_the_mean_instead_of_median_almost_never/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mqki4p",
    "title": "[D] Statistics in the media: Opinion article in the UK's \"Financial Times\"",
    "content": "The author of¬†[Westminster forgets that inflation matters](https://www.ft.com/content/d21cb836-c8a6-4bd8-98c4-cfb5ededfd33)¬†writes:\n\n&gt;Elections are statistically noisy. And because they are often close-run things, we can‚Äôt draw clear conclusions. In the 21st century, just two US presidential elections ‚Äî the victories of Barack Obama ‚Äî were by large enough margins to be statistically significant.\n\nUmm, isn't statistical significance a tool used to detect whether findings from a representative group are generalisable to the population? So isn't that a nonsensical thing to say in the context of an election.\n\nIs this what happens when people who don't understand stats try to invoke stats or am I missing something.\n\nEdit - formatting",
    "author": "beachedwalker",
    "timestamp": "2025-08-14T19:05:58",
    "url": "https://reddit.com/r/statistics/comments/1mqki4p/d_statistics_in_the_media_opinion_article_in_the/",
    "score": 4,
    "num_comments": 10,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mq5h9p",
    "title": "[Q][E] Looking for recommendations for self-study or online programs, interest",
    "content": "I am looking for recommendations on plans or programs to follow to teach myself a solid undergraduate education in statistics out of interest. I am open to online degree programs or informal teaching plans.\n\nMy background is in Engineering and CS. I recently completed a course-based masters in AI out of interest and particularly enjoyed the courses on ML. However, I found my comprehension was limited by my minimal prior background in statistics. I want to get a more complete understanding of statistics, particularly for creating and analyzing experiments and data.",
    "author": "Potential-Music-5451",
    "timestamp": "2025-08-14T09:27:37",
    "url": "https://reddit.com/r/statistics/comments/1mq5h9p/qe_looking_for_recommendations_for_selfstudy_or/",
    "score": 6,
    "num_comments": 4,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mpowjl",
    "title": "[Q] Masters programs in 2026",
    "content": "Hi all, I know this question has been asked time and time again but considering the economy and labor market I thought it might be good to bring up. \n\nI'm considering a masters since projects, networking, and even internal movements are getting me nowhere. I work in tech but it is difficult to move out of product support even with a degree in economics. \n\nWould a masters help me transition to a more data analysis (any type really) role?  ",
    "author": "chicanatifa",
    "timestamp": "2025-08-13T19:59:47",
    "url": "https://reddit.com/r/statistics/comments/1mpowjl/q_masters_programs_in_2026/",
    "score": 12,
    "num_comments": 4,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mq54mr",
    "title": "[Question] [ Education] Biostatistics in France",
    "content": "Hello everyone,\n\nI‚Äôm French and I have always studied in France. This September I will begin a Master‚Äôs degree in Applied Mathematics and Statistics at the University of Lyon, France. I am particularly interested in specializing in biostatistics because I have always had a strong passion for biology. For example, I completed a BCPST preparatory program (equivalent to the first year of a biology degree) and, during my second year of a mathematics degree, I took an elective course on hereditary diseases.\n\nMy questions are: Is it a good idea to pursue biostatistics in France?\n\nWill biostatisticians be replaced by AI in the future?\n\nIs there a strong job market for junior professionals in this field, both abroad and especially in France? Also, coming from France, is it possible for me to work abroad, or is it rather difficult? If possible, which countries offer good opportunities?\n\nWhat is the typical salary for a junior biostatistician in France and internationally?\n\nThank you in advance!",
    "author": "Professional-Dot-132",
    "timestamp": "2025-08-14T09:14:45",
    "url": "https://reddit.com/r/statistics/comments/1mq54mr/question_education_biostatistics_in_france/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mp91s9",
    "title": "[Question] I‚Äôve never taken a statistics course but I have a strong background in calculus. Is it possible for me to be good at statistics? Are they completely different?",
    "content": "I‚Äôve never taken a statistics course. I‚Äôve taken multiple calculus level courses including differential equations and multivariable calculus. I‚Äôve done a lot of math and have a background in computer programming. \n\nRecently I‚Äôve been looking into data science, more specifically data analytics. Is it possible for me to get a grasp of statistics? Are these calculus courses completely different from statistics ? What‚Äôs the learning curve? Aside from taking a course in statistics what‚Äôs one way I can get a basic understanding of statistics.\n\nI apologize if this is a ‚Äúdumb question‚Äù !",
    "author": "establisher",
    "timestamp": "2025-08-13T09:23:57",
    "url": "https://reddit.com/r/statistics/comments/1mp91s9/question_ive_never_taken_a_statistics_course_but/",
    "score": 17,
    "num_comments": 21,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mp5sxy",
    "title": "New Gelman and Hill Multilevel/Hierarchical Modelling book? [Discussion]",
    "content": "Is anyone across the expected release date of the new version of the 2007 book? Will it use WinBugs or stan?",
    "author": "[deleted]",
    "timestamp": "2025-08-13T07:20:34",
    "url": "https://reddit.com/r/statistics/comments/1mp5sxy/new_gelman_and_hill_multilevelhierarchical/",
    "score": 13,
    "num_comments": 7,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mp6ety",
    "title": "[Education] Looking for a nice wall chart of statistics formulas (undergrad level)",
    "content": "I'm looking for a poster or wall chart of basic statistics formulas and concepts at roughly the undergraduate level. This is being weirdly hard to find.\n\nClosest thing I've found is [this chart](https://www.amazon.com/Statistics-Chart-Properties-Important-Theories-ebook/dp/B095WTV4X2#customerReviews) on Amazon, though it's a kindle download. I would rather find a poster I don't have to print myself (though I might text the whatsapp number in the bottom of the photo just to find out where it leads).\n\nI might also buy [this one](https://www.redbubble.com/i/poster/The-Statistics-Matrix-Knowledge-Map-Series-The-StatisticsMatrix-at-Redbubble-2020-by-markcstansberry/57512425.LVTDI), though I'd prefer something more comprehensive like the chart above. I'm curious if anyone on this sub has or knows of any other good posters before I pull the trigger.",
    "author": "runawayoldgirl",
    "timestamp": "2025-08-13T07:44:13",
    "url": "https://reddit.com/r/statistics/comments/1mp6ety/education_looking_for_a_nice_wall_chart_of/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1motq5x",
    "title": "[Research] From JASA: Fair Coins Tend to Land on the Same Side They Started: Evidence from 350,757 Flips (Open Access link inside)",
    "content": "Link: [https://www.tandfonline.com/doi/epdf/10.1080/01621459.2025.2516210?needAccess=true](https://www.tandfonline.com/doi/epdf/10.1080/01621459.2025.2516210?needAccess=true)\n\nABSTRACT  \nMany people have flipped coins but few have stopped to ponder the statistical and physical intricacies of  \nthe process. We collected 350,757 coin flips to test the counterintuitive prediction from a physics model  \nof human coin tossing developed by Diaconis, Holmes, and Montgomery (DHM; 2007). The model asserts  \nthat when people flip an ordinary coin, it tends to land on the same side it started. Our data support this  \nprediction: the coins landed on the same side more often than not, Pr(same side) = 0.508, 95% credible  \ninterval (CI) \\[0.506, 0.509\\], BFsame-side bias = 2359. Furthermore, the data revealed considerable between-  \npeople variation in the degree of this same-side bias. Our data also confirmed the generic prediction that  \nwhen people flip an ordinary coin‚Äîwith the initial side-up randomly determined‚Äîit is equally likely to  \nland heads or tails:Pr(heads) = 0.500, 95% CI \\[0.498, 0.502\\], BF heads-tails bias = 0.182. Additional analyses  \nrevealed that the within-people same-side bias decreased as more coins were flipped, an effect that is  \nconsistent with the possibility that practice makes people flip coins in a less wobbly fashion. Our data  \ntherefore provide strong evidence that when some (but not all) people flip a fair coin, it tends to land on the same side it started. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.\n\n  \n\\*My note: BF = Bayesian factor\\*",
    "author": "CommentSense",
    "timestamp": "2025-08-12T20:28:27",
    "url": "https://reddit.com/r/statistics/comments/1motq5x/research_from_jasa_fair_coins_tend_to_land_on_the/",
    "score": 21,
    "num_comments": 9,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mp10k8",
    "title": "[R][E] Advice on planning my future year of work/research internships?",
    "content": "Hi everyone, BSc in Economics and statistics, going to graduate next fall. Right now I have a summer research internship, that became a thesis project, with a Statistics Professor, but no other experience. I‚Äôd like to pivot to a more technical and math-rigorous MSc, like Statistics, therefore I‚Äôm trying to plan my next months.\n\n\n- During this last academic year, I hope I will independently focus on programming skills and gain a more rigorous math foundation, that my degree (and I) lacks. Oh, and maybe building at least one personal project since I don‚Äôt have any yet. \n- after finishing my Bachelor (so from around july 2026) I planned on taking a year, before starting a MSc, for gaining some experience and maybe continuing studying some math topics on the side. \n\n\nNow I‚Äôm a bit lost on how to efficiently plan this sort of experience-year and, since it‚Äôs already time to search for 2026 summer internships or regular internships, I would like to ask you for an advice: what if I try to find both a research internship and a company internship for different periods? Firstly I thought that I could try to find a summer internship, and then a longer one that starts hopefully right after, but I don‚Äôt know how I should properly plan this period, could you give me some advice? (For example, I don‚Äôt even know if it would be better to find more diverse internships that last few months, or to focus on finding at least 1 or 2 that last longer)",
    "author": "Ecstatic-Traffic-118",
    "timestamp": "2025-08-13T03:44:20",
    "url": "https://reddit.com/r/statistics/comments/1mp10k8/re_advice_on_planning_my_future_year_of/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mpc3ss",
    "title": "[Software] A statistical height calculator",
    "content": "[https://tallornah.com](https://tallornah.com/)\n\nSo this is pretty cool, it's a \"statistical height calculator\". It tells you how many people you're taller than and the tallest you will statistically ever be if you're still growing. I should mention that I've worked in the field of population-based statistics, and this calculator is rather impressive. It's an LMS calculator that uses a dataset that took 4 decade to compile.\n\nI got an 88.6% with a max height of 6'1\", if anyone was curious.",
    "author": "mojave22",
    "timestamp": "2025-08-13T11:15:36",
    "url": "https://reddit.com/r/statistics/comments/1mpc3ss/software_a_statistical_height_calculator/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mos7ho",
    "title": "[Q] Chapter 2, Question 22: A First Course in Probability, Ross",
    "content": "Hi all, could anyone help me solve this problem:\n\n**Each of 52 people are given a deck of cards, which they are asked to shuffle independent of each other. What is the probability that**\n\n**(a) the order of the cards in each shuffled deck is unique?**\n\n**(b) there is exactly one card that occupies the same position in the shuffled decks received from all 52 persons?**\n\n**(c) all cards occupy the same position in all the shuffled decks?**\n\n# Here is the way I solved it:\n\n**a)** P(all decks have unique positioning of cards) = # of ways 52 decks can be shuffled in in a unique order / # of ways orderings for 52 decks that are shuffled\n\nLets look at an example:\n\nSay we have a deck that includes the cards: A, B, C And we have 2 decks (or 2 people).\n\nThe first deck can be positioning in 3\\*2\\*1 ways which includes:\n\nABC, ACB, BAC, BCA, CAB, CBA. We choose one of these ways =&gt; 3\\*2\\*1 C 1 = 3\\*2\\*1\n\nThat leaves us with 3\\*2\\*1 - 1 choices for our second deck. So it also chooses one.\n\nSo this means that for every positioning that person 1 chooses for deck 1, we have 3\\*2\\*1 - 1 choices for our second deck. Meaning (3\\*2\\*1)(3\\*2\\*1 - 1) ways to get a unique ordering.\n\nExtending this to the problem at hand, we end up with (52!)(52! - 1)(52! - 2)... (52! - 51) = .\n\nThe total number of ways the 52 decks can be shuffled is (52!)\\^52. That is, every deck containing the 52 cards has 52 positions to fill so and the first position has 52 options, second position has 51 and so on. Leading to 52! positions for 1 deck. Since there is no constraint on the ordering of all the decks, all of them have the same positioning option.\n\nThis means:\n\nP(all decks have unique positioning of cards) = # of ways 52 decks can be shuffled in in a unique order / # of ways orderings for 52 decks that are shuffled = \\[(52!)(52! - 1)(52! - 2)... (52! - 51)\\] / (52!)\\^52 .\n\nBut the books answer is: (a) ‚àè·µ¢‚Çå‚ÇÅ‚Åµ¬π 1 / \\[52¬∑51¬∑...¬∑(52 ‚àí i + 1)\\], (b) (52 √ó 52 √ó 51! √ó ... √ó 2! √ó 1!) / (52!)‚Åµ¬≤, (c) 1 / (52!)‚Åµ¬π. I feel confident in my answer and do not know where I could have gone wrong. Can someone help me please?\n\nAlso for **(b)**, here is my work:\n\nWe can take a simple example. again imagine a deck containing 4 cards: A, B, C, D.\n\nAnd 4 people that have a deck of their own.\n\nEach person must have a card in the same position. Since there are 4 cards, we must first choose a card to maintain in a position. We have 4 options for this.\n\nNow we must now choose the position for that card. We have 4 options.\n\nTotal, we have 4 position \\* 4 distinct cards choices of a specific card in a specific location. This leaves us with 3 spaces to populate with the 3 other cards. Similar to what we did for (a), each the remaining positions will result in 3! ways to sort them. But this is the case only for the first deck since the others must have different positions other than the common card in the same position we picked above.\n\nSo like we did in (a), we choose one of these options (3! options) and the next deck will have 1 fewer options (3! - 1), ..., last deck having (3! - 2) options (since we have 4 decks aka 4 people).\n\nSo in total we have 4 \\* 4 \\* (3!) \\* (3! - 1) \\* (3! - 2).\n\nSimilarly for the problem above, we will have: 52 \\* 52 \\* (51!) \\* (51! - 1) \\* (51! - 2) \\* ... \\* (51! - 50)\n\nAs we know from (a), the total possibilities are (52!)‚Åµ¬≤.\n\nSo the P(there is exactly one card that occupies the same position in the shuffled decks received from all 52 persons) = \\[52 \\* 52 \\* (51!) \\* (51! - 1) \\* (51! - 2) \\* ... \\* (51! - 50)\\] / (52!)‚Åµ¬≤.\n\nBut the answer in the book is (52 √ó 52 √ó 51! √ó ... √ó 2! √ó 1!) / (52!)‚Åµ¬≤.\n\nFor **(c)**, I understand the result and got the same answer from the book. For (a) and (b), I think the book's answers are a mistake but I also looked on¬†[https://math.stackexchange.com/questions/3969570/rosss-probability-10th-edition-chapter-2-question-22](https://math.stackexchange.com/questions/3969570/rosss-probability-10th-edition-chapter-2-question-22)\n\nand it still doesn't make sense how people are getting the answer in the book and why my answer is incorrect.\n\nCan someone please help me out? Thank you so much in advance. I posted here once before and really appreciated the help especially as a newbie, thank you all.\n\n  \nEDIT: \n\nAlso, I'm not sure if the book's answer to (a) even makes sense. ‚àè·µ¢‚Çå‚ÇÅ‚Åµ¬π 1 / \\[52¬∑51¬∑...¬∑(52 ‚àí i + 1)\\] would be very small, essentially zero meaning that the this scenario is very unlikely. But that doesn't make sense.",
    "author": "AdImpressive9604",
    "timestamp": "2025-08-12T19:15:23",
    "url": "https://reddit.com/r/statistics/comments/1mos7ho/q_chapter_2_question_22_a_first_course_in/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1movc24",
    "title": "Struck by the sense that in many binomial experiments (and sample spaces in general), order doesn't matter the way people think it does [D]",
    "content": "",
    "author": "Will_Tomos_Edwards",
    "timestamp": "2025-08-12T21:54:36",
    "url": "https://reddit.com/r/statistics/comments/1movc24/struck_by_the_sense_that_in_many_binomial/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1moahow",
    "title": "Path‚ÄìKL Friction: A Gauged KL‚ÄìProjection Framework [Research] [Question]",
    "content": "What should I do with this paper I wrote?\n\nI'm very open to the answer to the question being \"kill it with fire\"\n\nThis was a learning exercise for me, and this represents my first paper of this type.\n\nAbstract: We prove existence/uniqueness for a gauge-anchored KL I-projection and give an order-free component split ŒîD\\_k = c\\_k ‚à´\\_0\\^1 Œª\\_k(t) dt along the path c(t)=tc. It reproduces the total D\\_KL(Q\\*||R0), avoids order bias, and matches a Shapley discrete alternative. Includes a reproducible reporting gauge and a SWIFT case study. Looking for methodological feedback and pointers.\n\n[https://archive.org/details/path-kl-friction](https://archive.org/details/path-kl-friction)\n\n1. Does the homotopy split read as the right canonical choice in stats methodology terms?\n2. Anything obvious I'm screwing up?\n3. If you publish on ArXiv in [stats.ME](http://stats.ME) and find this sound (or want to give me pointers), consider DMing me re: ArXiv endorsement, and what my steps to earning your endorsement would be.",
    "author": "RocketBombsReddit",
    "timestamp": "2025-08-12T07:37:45",
    "url": "https://reddit.com/r/statistics/comments/1moahow/pathkl_friction_a_gauged_klprojection_framework/",
    "score": 7,
    "num_comments": 6,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mnma86",
    "title": "Bachelors grad looking for advice on getting into a Stats Career [Career]",
    "content": "I earned my B.S. in pure Stats back in 2024. I opted not to go for a Masters outright because I wanted to earn money and pay off undergrad loans. Fortunately I‚Äôm on pace to do so relatively quickly.\n\nI landed a very low paying job fresh out of undergrad which has very little to do with my Stats skill set. To be honest, I accepted the low skill job because it was my only offer; the job market was (and currently is) rough and I needed some way to pay off my loans. I didn‚Äôt really have the luxury of being picky with my first gig, but currently I am more flexible.\n\nI‚Äôve been with this job less than a year and I really want to transition to a role more related to Statistics. I have a solid academic resume with internship and research experience with good grades. On my spare time I‚Äôve been doing R projects and brushing up on the ‚ÄúIntroduction to Statistical Learning- R‚Äù.\n\nI am going to start discreetly applying to other roles while I still have my current job. I‚Äôm mainly targeting entry level data analyst or business intelligence roles, and I‚Äôm very open to exploring other relevant roles too. I‚Äôm wondering if anyone here may be able to give advice on anything that may help me stand out to employers. Any advice would be greatly appreciated.",
    "author": "Rocket5700",
    "timestamp": "2025-08-11T12:16:26",
    "url": "https://reddit.com/r/statistics/comments/1mnma86/bachelors_grad_looking_for_advice_on_getting_into/",
    "score": 16,
    "num_comments": 8,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mnropb",
    "title": "Help. I need to prepare for grad school. [Education][Career]",
    "content": "I‚Äôm going back to school (economics) and will be taking a statistics for business course. I have always been intimidated by probability and statistics in general, so I am looking for an online course (or a book, or a website, or‚Ä¶ something) that will help me hit the ground running, or even be already advanced. I have been going through this Coursera one that I don‚Äôt find particularly helpful, even though it‚Äôs called Statistics for Business‚Äîit‚Äôs just too high-level. I would love a course that makes me understand the ideas well.\n\nWhat suggestions do you have?\n\nPlease, don‚Äôt say anything like, ‚Äúchoose another program.‚Äù",
    "author": "moureil",
    "timestamp": "2025-08-11T15:43:51",
    "url": "https://reddit.com/r/statistics/comments/1mnropb/help_i_need_to_prepare_for_grad_school/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mno26i",
    "title": "In Europe, if trades / unions pay more than i.e. Computer Science / Stats, isn't it self-torture to embrace academia? [Career] [Discussion]",
    "content": "For disclaimer, I'm a Master's student in Psychology / Statistics. Graduated from top universities in Asia / Netherlands. I forsee myself doing Data Analyst jobs in the future.\n\nThe joke? In Europe, it seems that trade jobs (electrician, plumber etc) pays more than a corporate job. Even menial jobs like construction, when backed by unions, have more job security and potential pay benefits.\n\nSo sometimes I feel like I'm torturing myself learning abstract stuff like Bayesian and R programming language - the countless hours put in, for such \"intellectual\" stuff, only to be met with lower pay, longer working hours, and less job security (rise of AI, outsourcing to cheap remote workers, oversaturation etc).\n\n1. Is my perspective fair? I mean, don't get me wrong, I enjoy the theory part of what I study in terms of subject, like the biological influence of hormones...but the hours put into stats / programming / coding...and the emotional pressure to get an A...it feels like the effort-reward ratio isn't making sense.\n\n2. Is it just me, or is it simply a pride thing? As in, people are conditioned to pursue academia and higher learning because society looks down on manual labour when they actually earn more, are subject to less stress, and have higher job security. For many of us, we were simply told that University is the default path in life. ",
    "author": "arcanehelix",
    "timestamp": "2025-08-11T13:22:57",
    "url": "https://reddit.com/r/statistics/comments/1mno26i/in_europe_if_trades_unions_pay_more_than_ie/",
    "score": 3,
    "num_comments": 15,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mndox0",
    "title": "[Question] Does anyone have any good strategies for knowing when to use Chi-square goodness of fit vs test of independence?",
    "content": "I‚Äôve taken 7 semesters worth of stats courses, been conducting my own research exclusively using archival data for 2 years; and yet for some reason when it comes to chi square I can never remember which test to use when. \n\nI know what they both are, like if you asked me to define either I could do it no problem. It‚Äôs when I have the data, I can even run the test and tell interpret the output; without being able to tell which chi-square I used. \n\nWhy won‚Äôt this click? Has anyone come across anything that helped make it click for you?",
    "author": "TrickFail4505",
    "timestamp": "2025-08-11T06:57:17",
    "url": "https://reddit.com/r/statistics/comments/1mndox0/question_does_anyone_have_any_good_strategies_for/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mn9win",
    "title": "[discussion] psych stats?",
    "content": "Hi!\n\nI'm a first years Psych student, and I'm TERRIBLE at statistics. I understand them, but it's not like i'm great at them so I don't do very well in stat exams, especially the multiple choice ones.\n\nIn this degree I don't have to do stats as a course anymore, but I'll still have to do stats in Psych units, so I was wondering if anyone has some insights to overcome this 'being bad at stats' issue?\n\nFor now, I think I struggle with the understanding of what everything means (slow processing), and the different symbols just feel foreign to me - need some keys to process better. And then there's application, and my uni just gives examples with very very real data without saying how exactly to calculate them, so I can't really understand much from that. This entire feeling is annoying, similar to someone giving you a 7 digit addition question after you learnt how to do 1+1.\n\nAny advice on this would be greatly appreciated. Thank you for reading :')\n\nedit: thank you all so so much for the advice - it is greatly appreciated üôè",
    "author": "YourBraincellOnline",
    "timestamp": "2025-08-11T04:01:35",
    "url": "https://reddit.com/r/statistics/comments/1mn9win/discussion_psych_stats/",
    "score": 9,
    "num_comments": 6,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mnm2zu",
    "title": "[Q] Extremely large OR/SE &amp; uninformative CI for logistic regression model",
    "content": "Bear with me, still learning regression -- open to any and all feedback. I'm trying to fit a generalized logistic regression model (in R, glmer(ftm \\~ ft \\* condition + (1|Subject))) and keep getting very large ORs/SEs (e.g., OR = 6317030, SE = 5944107, p &lt; .001) and infinite confidence intervals (95% CI \\[0.00, Inf\\]). The data I'm working with is from a mixed-methods design that has repeated measures (3 between X 5 within), so I've also added a random effects variable in the model (1|Subject) which I believe I should be doing(?). I've read a common reason for this instability is separation in cells, and while there isn't complete separation in any cell there is near-separation in some (when ft=0, very very few ftm=1 in all conditions). I would imagine this is likely what's causing these model summary results, however this near-separation in these cells was somewhat expected based on the experiment we ran. \n\n  \nAny suggestions on what I could/should do to make the summary table more interpretable? Thanks in advance to any help!",
    "author": "No-Fill1769",
    "timestamp": "2025-08-11T12:08:56",
    "url": "https://reddit.com/r/statistics/comments/1mnm2zu/q_extremely_large_orse_uninformative_ci_for/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mnd71h",
    "title": "Creating questionnaire index [Question]",
    "content": "Hello!\n\nI am using eurobarometer data to create an index on peoples support for issue X. I have 4 variables ranging from 1 (Totally disagree) to 4 (Totally agree). I tried to read how to go about and decided that the best method would be to sum up the score. Thus:  if person scores 4 in total (totally disagreed on 4 variables), in the index s/he will be labeled as totally opposed and if s/he gets 16 (score 4 on all 4 variables) s/he will be labeled as totally supporting. So my question is; can I really do this kind of interpretation? And how should I label all other scores? Can I label those scoring 8 as neutral? I would highly appreciate if anyone could link some more readings on the issue. As well as if you have antyhing more on pros/cons/ guidelines on using average/mean instead of sum to create a scale would be helpfull too.",
    "author": "pr64837",
    "timestamp": "2025-08-11T06:37:09",
    "url": "https://reddit.com/r/statistics/comments/1mnd71h/creating_questionnaire_index_question/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mltyis",
    "title": "[Q] Controlling for effects of other variables vs. collinearity issues",
    "content": "I came across a paper that said \"The crowding factors\r that we included in the models had a modest effect\r on waiting room time and boarding time after controlling \rfor time of day and day of week. This was expected given the colinearity between the crowding measures \rand the temporal factors.\" Wouldn't accounting for a confounder like temporal variables introduce multicollinearity into the model? If so, how is this handled in general? For reference, this paper was using quantile regression.",
    "author": "Conscious_Counter710",
    "timestamp": "2025-08-09T09:41:30",
    "url": "https://reddit.com/r/statistics/comments/1mltyis/q_controlling_for_effects_of_other_variables_vs/",
    "score": 5,
    "num_comments": 8,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ml8siy",
    "title": "[Q]: Statistics Masters with an Information Systems &amp; Analytics background",
    "content": "Hey everybody. \n\nI am a recent college graduate with a bachelor of science is Information Systems and Business Analytics. I work full time as a data analyst at a consulting firm. I am wondering if (1) getting a masters in stat is possible with my background and (2) if so, how I can best position myself for the degree. \n\nI have good programming skills from my job and undergrad degree (python, sql, r). Unfortunately, I am certainly lacking the math and statistical theory prerequisites for ideal candidacy. The most relevant coursework I have completed is Calc II and applied statistical modeling, both of which I thoroughly enjoyed. I am planning on taking multi variable calculus and linear algebra as a non degree student, but want to know if it's worth it/if it's possible to get into a graduate school with this less traditional path. \n\nAny advice would be appreciated!",
    "author": "willflanagan21",
    "timestamp": "2025-08-08T15:17:31",
    "url": "https://reddit.com/r/statistics/comments/1ml8siy/q_statistics_masters_with_an_information_systems/",
    "score": 14,
    "num_comments": 16,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mky25l",
    "title": "[Q] I just defended a dissertation that didn't have a single proof, no publications, and no conferences. How common is this?",
    "content": "On one hand, I feel like a failure. On the other hand, I know it doesn't matter since I want to get into industry. But back to the first hand, I can't get an industry job...",
    "author": "Maleficent-Seesaw412",
    "timestamp": "2025-08-08T08:20:26",
    "url": "https://reddit.com/r/statistics/comments/1mky25l/q_i_just_defended_a_dissertation_that_didnt_have/",
    "score": 21,
    "num_comments": 21,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1ml7tw7",
    "title": "[Q] any good library/module which is dedicated to applied stochastic processes ?",
    "content": "It doesn't matter which language, just that it is well documented and rich with methods/functions.",
    "author": "al3arabcoreleone",
    "timestamp": "2025-08-08T14:36:51",
    "url": "https://reddit.com/r/statistics/comments/1ml7tw7/q_any_good_librarymodule_which_is_dedicated_to/",
    "score": 4,
    "num_comments": 9,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mkhfd7",
    "title": "[Q] Intended Masters in Statistics, but undergrad in Applied Math or Statistics &amp; Probability?",
    "content": "Hello guys/gals!\n\nIf you don't mind, I am at a juncture in my undergraduate studies right now where I can pursue either Honors Applied Math or Honors Statistics and Probability.\n\nAfter looking both of them over at UCSD, I am leaning towards Honors Applied Math. However, I want to go for a masters in statistics, preferably at a top 10 in the field that also has strong industry connections (looking into Pharma/Biotech).\n\nNow, I've been purely chemical engineering so far and I would love to go through with applied math as it connects very well with my major here (more process engineering than chemical engineering here) and hopefully opens many doors.\n\nThe issue is, after scrolling through this subreddit and many other ones, I have received the impression that the best way to get into a statistics masters is to take multiple statistics courses. Honors Applied Math at UCSD might give me the chance to take a handful at UCSD given that it has electives, however, would it be better for me to enter Honors Statistics and Probability instead?\n\nAdditionally, how related do internships have to be to statistics for me to have a chance at a top 10 statistics in pharma-biotech school?\n\nThank you so much for any help you can provide!  \n  \n\\*\\*\\*Additional info: I am an international student in the US and my country is currently not in need of statisticians, but is in the period of growth where they generate a surplus of meaningful data that in the next 5 years, being a statistician with a heavy engineering background would be sought after.",
    "author": "Sudden_Quote_597",
    "timestamp": "2025-08-07T17:55:40",
    "url": "https://reddit.com/r/statistics/comments/1mkhfd7/q_intended_masters_in_statistics_but_undergrad_in/",
    "score": 11,
    "num_comments": 14,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mkmn7f",
    "title": "[Q] Any statistical approaches to analyzing movement across categorical 2D states over time?",
    "content": "Imagine a grid of categorical outcomes (e.g., N x N), and each subject is assigned a position each year. I want to analyze movement patterns across the grid over multiple time points.\n\nBeyond basic transition matrices, I‚Äôm wondering:\n\n- Are there Markov-style models for this kind of discrete 2D space?\n- Can sequence alignment or clustering apply to movement paths?\n- What statistical tools might capture directionality and variance in movement?\n\nAppreciate any references or techniques that handle structured movement between categorical states over time.",
    "author": "Proof_Wrap_2150",
    "timestamp": "2025-08-07T22:18:08",
    "url": "https://reddit.com/r/statistics/comments/1mkmn7f/q_any_statistical_approaches_to_analyzing/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mjxonz",
    "title": "[Discussion] Recommendation for a course on basic statistics",
    "content": "Hey everybody, I work at a company where we produce advertising videos to sell direct-to-consumer products. We are looking for a course on basic statistics that everybody in the company can watch so that we can increase our understanding of statistics and make better decisions. If anyone has any good recommendations, I would highly appreciate it. Thank you so much. ",
    "author": "planisking",
    "timestamp": "2025-08-07T04:37:11",
    "url": "https://reddit.com/r/statistics/comments/1mjxonz/discussion_recommendation_for_a_course_on_basic/",
    "score": 6,
    "num_comments": 4,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mkmgg2",
    "title": "[DISCUSSION]",
    "content": "I have 45 excel files to check for one of my team member and each excel file will take 30 mins to check.\n\nI want to do a spot check rather checking all of them.\n\nWith margin of error of 1% and confidence interval of 95%.  How much sample should I select?\n\n-What test name will it me? 1 proportion test? Z test or t test? And it somebody can share minitab process also?\n\nThanks",
    "author": "Conscious-Comb4001",
    "timestamp": "2025-08-07T22:07:29",
    "url": "https://reddit.com/r/statistics/comments/1mkmgg2/discussion/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mk4yc0",
    "title": "[Q] Questions about the different subfields of statistics/probability and what each one covers?",
    "content": "So I'm looking to learn statistics through online courses and textbooks but I'm a bit confused about what each textbook covers. If I take a book on statistics, will it cover probability too? Or are they different things? Do I need to take another book about probability as well?\n\nI was watching at statistics related courses on math college degrees and I saw they do several semesters worth of courses, and they study things like regressions and stuff like that outside the main statistics course later in the degree.\n\nIn case I finish the book, how can I know which topics hasn't it covered to expand with other resources?\n\nI was looking at the books Learning Statistics with R and Probability and Statistics for Engineers and Scientists. These two books cover many topics, how can I know which isn't covered? Does the fact that the first book doesn't mention probability mean that isn't covered?\n\nSorry for the messy post, I guess my main question is what are the different subtopics that I need to cover to make sure I didn't miss any major topic in this field? I'm scared I'll read a book about probability and it won't cover stuff like regressions because it's another topic.",
    "author": "Careless_Care8060",
    "timestamp": "2025-08-07T09:36:32",
    "url": "https://reddit.com/r/statistics/comments/1mk4yc0/q_questions_about_the_different_subfields_of/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mjuf67",
    "title": "[QUESTION] Help understanding Mann-Whitney positive/negative signs",
    "content": "I'm analyzing data in SPSS using the Mann-Whitney U test to compare two groups:\n\nFor DV1, Group 1 has lower mean rank, and the Z value is negative, which makes sense. But for DV2, Group 1 has a higher mean rank, yet the Z value is still negative. Both results are statistically significant.\n\nI thought a positive Z should indicate that Group 1 has higher ranks than Group 2.\n\nDoes SPSS reverse group codes internally or something? When reporting these results, should I keep the negative Z value in the table, even though it feels counterintuitive to the mean values?\n\nAny clarification would be appreciated!",
    "author": "makislog",
    "timestamp": "2025-08-07T01:21:28",
    "url": "https://reddit.com/r/statistics/comments/1mjuf67/question_help_understanding_mannwhitney/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mjwz1t",
    "title": "[Q] Analysis of dichotomous data",
    "content": "My professor force me to calculate mean and SD, and do ANOVA for dichotomous data. Am I mad or that is just wrong?",
    "author": "KamiyaHiraien",
    "timestamp": "2025-08-07T03:58:50",
    "url": "https://reddit.com/r/statistics/comments/1mjwz1t/q_analysis_of_dichotomous_data/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mjkbqh",
    "title": "[Question] Auxiliary variables related to missing data in Latent Profile Analysis",
    "content": "Hi there,\n\nI'm planning on conducting a Latent Profile Analysis (LPA) using items from three psychological measures. About 9% of my participants are missing an entire measure due to it being added later in the study. Because I'm planning to run this in Mplus, FIML is a convenient way to handle the missing data. Would adding a categorical yes/no auxiliary variable (e.g., measure\\_offered) that is conceptually related to this missingness improve the MAR assumption of FIML + be appropriate for an LPA? I believe in Mplus you can specify \"AUXILIARY = measure\\_offered(m);\" to ensure it acts only as an auxiliary variable for missing data and does not influence class formation. \n\nAppreciate any thoughts/advice/references!",
    "author": "pompaclour",
    "timestamp": "2025-08-06T16:32:37",
    "url": "https://reddit.com/r/statistics/comments/1mjkbqh/question_auxiliary_variables_related_to_missing/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mjn361",
    "title": "[Question] What if my weibull.dist column doesn't add up to 1 ?",
    "content": "Hey all, I watched a video by PSUwind, she plotted a weibull curve using a bin column and a weibull distribution column in Excel ( =weibull.dist(bin\\_element, shape, scale, false). She mentioned that after going through all bins the sum of weibull column elements must be around 1. In my case, I summed them up to 0.93, 0.95 96 97 but can't do 0.9935 like her. I found that the amount of bins will cause troubles like this. How can I choose my bin numbers (does it have to start at 0, how many bins do I need ?). Thank you\n\n",
    "author": "Perfect_Leave1895",
    "timestamp": "2025-08-06T18:36:33",
    "url": "https://reddit.com/r/statistics/comments/1mjn361/question_what_if_my_weibulldist_column_doesnt_add/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mjmqiy",
    "title": "[Discussion] How to determine sample size / power analysis",
    "content": "Given a normal data set with possibly more values than needed, a one sided spec limit, a needed confidence interval, and a needed reliability interval, how do I determine how many samples are needed to reach the specified power? ",
    "author": "Important-Yak-2787",
    "timestamp": "2025-08-06T18:20:10",
    "url": "https://reddit.com/r/statistics/comments/1mjmqiy/discussion_how_to_determine_sample_size_power/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mjlpww",
    "title": "[Software] Distribution of Sample Proportion with Statcrunch",
    "content": "So this isn't a homework question but it is class adjacent. Feel free to delete if you find it out of scope. Is there a way process distribution of sample proportion in Statcrunch? I have noticed that the naming conventions in statcrunch doesn't match whats in the book (or should I say statcrunch rejects the naming coventions in the book haha)\n\nI'm looking for automated ways to process œÉ subscript pÃÇ using statcrunch. ",
    "author": "Honest-Income1696",
    "timestamp": "2025-08-06T17:33:44",
    "url": "https://reddit.com/r/statistics/comments/1mjlpww/software_distribution_of_sample_proportion_with/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mj16zr",
    "title": "[Question] How to calculate a similarity distance between two sets of observations of two random variables",
    "content": "Suppose I have two random variables X and Y (in this example they represent the prices of a car part from different retailers). We have n observations of X: (x1, x2 ... xn) and m observations of Y : (y1, y2 .. ym). Suppose they follow the same family of distribution (for this case let's say they each follow a log normal law). How would you define a distance that shows how close X and Y are (the distributions they follow). Also, the distance should capture the uncertainty if there is low numbers of observations.   \nIf we are only interested in how close their central values are (mean, geometric mean), what if we just compute the estimators of the central values of X and Y based on the observations and calculate the distance between the two estimators. Is this distance good enough ? \n\nThe objective in this example would be to estimate the similarity between two car models, by comparing, part by part, the distributions of the prices using this distance.\n\nThank you very much in advance for your feedback !",
    "author": "showbrownies",
    "timestamp": "2025-08-06T03:40:30",
    "url": "https://reddit.com/r/statistics/comments/1mj16zr/question_how_to_calculate_a_similarity_distance/",
    "score": 7,
    "num_comments": 7,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mjtguy",
    "title": "[Q] Best AI for statistics",
    "content": "Hi. I‚Äôm currently only using the free version of Grok. Just wondering about other people‚Äôs experience with the best free version of an AI for statistics.\n\nI‚Äôm also interested in a modest paid version if it is worth the money. \n\nSpecifically, I‚Äôm wishing to upload CSV files to synthesise data and make forecasts.",
    "author": "trymorenmore",
    "timestamp": "2025-08-07T00:20:05",
    "url": "https://reddit.com/r/statistics/comments/1mjtguy/q_best_ai_for_statistics/",
    "score": 0,
    "num_comments": 31,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mjmiwm",
    "title": "[Question] How can I land an entry-level Business Analyst role before I graduate?",
    "content": "Hey everyone, I‚Äôm looking for some advice.\n\nI graduate this December with my bachelor‚Äôs in Business Administration and I‚Äôm really trying to land an entry-level business analyst, junior analyst, or project coordinator role before then, ideally within the next one to two months.\n\nI don‚Äôt have direct business analyst experience, but I‚Äôm a fast learner with a strong work ethic. I‚Äôm familiar with the basics of Excel and SQL, and I‚Äôve been applying through LinkedIn and Indeed, but I feel like I‚Äôm not standing out enough.\n\nFor those of you who‚Äôve broken into the field recently or have hired for these roles, what would you recommend I do right now to maximize my chances? Any specific certifications, skills, job boards, networking tips, resume tweaks, or outreach strategies?\n\nI‚Äôm based near Dallas if that helps. I‚Äôm open to any advice. I‚Äôm willing to put in the work, I just need to know what to focus on.\n\nThanks in advance!\n",
    "author": "PrestonCooper1024",
    "timestamp": "2025-08-06T18:10:38",
    "url": "https://reddit.com/r/statistics/comments/1mjmiwm/question_how_can_i_land_an_entrylevel_business/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mjg58h",
    "title": "[Q] how do we compare between multiple similarity measures (or distances) ?",
    "content": "suppose I have mixed attributes data set, and I want to choose the most relevant similarity measure, how shall one approach this problem ?",
    "author": "al3arabcoreleone",
    "timestamp": "2025-08-06T13:44:39",
    "url": "https://reddit.com/r/statistics/comments/1mjg58h/q_how_do_we_compare_between_multiple_similarity/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mjilr2",
    "title": "How to calculator chances of drawing a card when there is more than 100%? [Q]",
    "content": "My supermarket has a promotion with Disney cards. There are 40 cards in the set that I am collecting for my niece. I was trying to figure out how to calculate the odds I have of having a full set but can't figure it out.\n\nAssuming there is an even distribution of the cards what are the chances of having an individual card from a certain number of cards? If I have twenty cards it seems logical that I have a 50% chance of having an individual card. But once I have 40 cards then it can't be possible that there is 100% chance of having an individual card. How do I calculate the odds when there is more than 100%? If I have 120 cards what are the chances of having an individual card? It must be getting close to 100% but can't possibly be 100%\n\nI currently have 120 unopened cards and was hoping to have a full set of the 40 cards when my niece opens them.\n\nI read this article but disagree with the statement that the formula is simple, I don't understand the math.\n\n[https://www.grant-trebbin.com/2013/10/probability-of-collecting-full-set.html](https://www.grant-trebbin.com/2013/10/probability-of-collecting-full-set.html)",
    "author": "MrFartyBottom",
    "timestamp": "2025-08-06T15:20:25",
    "url": "https://reddit.com/r/statistics/comments/1mjilr2/how_to_calculator_chances_of_drawing_a_card_when/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mj4jq3",
    "title": "[Q] Interpreting bounds of CI in intraclass correlation coefficient",
    "content": "I've run ICC to test intra-rater reliability (specifically, testing intra-rater reliability when using a specific software for specimen analysis), and my values for all tested parameters were good/excellent except for two. The two poor values were the lower bounds of the 95% confidence interval for two parameters (the upper bounds and the intraclass correlation values were good/excellent for the two parameters). I assume the majority of good/excellent values means that the software can be reliably used, but I'm having trouble figuring out how the two low values in the lower bounds of the 95% confidence interval affect that finding. (This is my first time using ICC and stats really aren't my strong point.)",
    "author": "3catsinahumansuit",
    "timestamp": "2025-08-06T06:22:03",
    "url": "https://reddit.com/r/statistics/comments/1mj4jq3/q_interpreting_bounds_of_ci_in_intraclass/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1miop6e",
    "title": "Handling missing data in spatial statistics [Q][D]",
    "content": "Consider an areal-data spatial regression problem where some spatial units are missing responses and maybe predictors, due to the very small population sizes in those units (so the missingness is definitely not random). I'd like to run a standard spatial regression model on this data, but the missingness is a problem.\n\nAre there relatively simple approaches to deal with the missingness? The literature only seems to contain elaborate ad hoc imputation methods and complex hierarchical models that incorporate latent variables for the missing data. I'm looking for something practical and that doesn't involve a huge amount of computation.",
    "author": "sciflare",
    "timestamp": "2025-08-05T16:20:36",
    "url": "https://reddit.com/r/statistics/comments/1miop6e/handling_missing_data_in_spatial_statistics_qd/",
    "score": 7,
    "num_comments": 9,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mhj52s",
    "title": "Is the future looking more Bayesian or Frequentist? [Q] [R]",
    "content": "I understood modern AI technologies to be quite bayesian in nature, but it still remains less popular than frequentist.",
    "author": "gaytwink70",
    "timestamp": "2025-08-04T10:03:45",
    "url": "https://reddit.com/r/statistics/comments/1mhj52s/is_the_future_looking_more_bayesian_or/",
    "score": 153,
    "num_comments": 56,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mi7ebf",
    "title": "[Q] Best way to summarize Likert scale responses across actor groups in a perception study",
    "content": "Hi everyone! I'm a PhD student working on a chapter of my dissertation in which I investigate the perception of different social actors (4 groups).\n\nI used a 5-point Likert scale for about 50 questions, so my data is ordinal. The total sample size is 110, with each actor group contributing around 20‚Äì30 responses. I'm now working on the descriptive and analitical statistics and I'm unsure of the best way to summarize the central tendency and variation of the responses.\n\n* Should I use **means and standard deviations**?\n* Or should I report **medians and interquartile ranges**\n\nI‚Äôve seen both approaches used in the literature, but I'm having a hard time in decide what to use.\n\nAny insight would be really helpful - thanks in advance!",
    "author": "luizeco",
    "timestamp": "2025-08-05T05:08:16",
    "url": "https://reddit.com/r/statistics/comments/1mi7ebf/q_best_way_to_summarize_likert_scale_responses/",
    "score": 4,
    "num_comments": 9,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1micsr3",
    "title": "[Discussion] Looking for statistical analysis advice for my research",
    "content": "hello! i‚Äôm writing my own literature review regarding cnidarian venom and morphology. i have 3 hypotheses and i think i know what analysis i need but im also not sure and want to double check!! \n\nH1: LD50 (independent continuous) vs bioluminescence (dependent categorical)\nwhat i think: regression \n\nH2: LD50 (continuous dependent) vs colouration (independent categorical)\nwhat i think: chi-squared \n\nH3: LD50 (continuous dependent) vs translucency (independent categorical)\nwhat i think: chi-squared \n\ni am some what new to statistics and still getting the hang of what i need and things. do you think my deductions are correct? thanks! \n\n",
    "author": "Ill_Usual888",
    "timestamp": "2025-08-05T08:47:49",
    "url": "https://reddit.com/r/statistics/comments/1micsr3/discussion_looking_for_statistical_analysis/",
    "score": 2,
    "num_comments": 8,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1miif4l",
    "title": "[Question] Simple? Problem I would appreciate an answer for",
    "content": "This is a DNA question buts it‚Äôs simple (I think) statistics. If I have 100 balls and choose (without replacement) 50, and then I replace all chosen 50 balls and repeat the process choosing another set of 50 balls, on average, how many different/unique balls will I have chosen?  \n\nIt‚Äôs been forever since I had a stats class, and I appreciate the help. This will help me understand the percent of DNA of one parent that should show up when 2 of the parents children take DNA tests. Thanks in advance for the help!",
    "author": "BlueTribe42",
    "timestamp": "2025-08-05T12:14:09",
    "url": "https://reddit.com/r/statistics/comments/1miif4l/question_simple_problem_i_would_appreciate_an/",
    "score": 1,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mhndol",
    "title": "Bayesian optimization [E] [R]",
    "content": "Despite being a Bayesian method, Bayesian Optimization (BO) is largely dominated by computer scientists and optimization researchers, not statisticians. Most theoretical work centers on deriving¬†new acquisition strategies with no-regret¬†guarantees rather than improving the statistical modeling of the objective function. The Gaussian Process (GP) surrogate of the underlying objective is often treated as a fixed black box, with little attention paid to the implications of prior misspecification, posterior consistency, or model calibration.\n\nThis division might be due to a deeper epistemic difference between the communities. Nonetheless, the statistical structure of the surrogate model in BO is crucial to its performance, yet seems to be underexamined.\n\nThis seems to create an opportunity for statisticians to contribute. In theory, the convergence behavior of BO is governed by how quickly the GP posterior concentrates around the true function, which is controlled directly by the choice of kernel. Regret bounds such as those in the canonical GP-UCB framework (which assume the latent function are in the RKHS of the kernel -- i.e, no misspecification) are driven by something called the maximal information gain, which depends on the eigenvalue decay of the kernel‚Äôs integral operator but also the RKHS norm of the latent function. Faster eigenvalue decay and better kernel alignment with the true function class yield tighter bounds and better empirical performance.\n\nIn practice, however, most BO implementations use generic Matern or RBF kernels regardless of the structure of the objective; these impose strong and often inappropriate assumptions (e.g., stationarity, isotropy, homogeneity of smoothness). Domain knowledge is rarely incorporated into the kernel, though structural information can dramatically reduce the effective complexity of the hypothesis space and accelerate learning.\n\nMy question is, is there an opening for statistical expertise to improve both theory and practice?",
    "author": "EgregiousJellybean",
    "timestamp": "2025-08-04T12:36:19",
    "url": "https://reddit.com/r/statistics/comments/1mhndol/bayesian_optimization_e_r/",
    "score": 21,
    "num_comments": 2,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mi13a0",
    "title": "Seeking advice on choosing PhD topic/area [R] [Q] [D] [E]",
    "content": "Hello everyone,\n\nI'm currently enrolled in a master's program in statistics, and I want to pursue a PhD focusing on the theoretical foundations of machine learning/deep neural networks.\n\nI'm considering statistical learning theory (primary option) or optimization as my PhD research area, but I'm unsure whether statistical learning theory/optimization is the most appropriate area for my doctoral research given my goal.\n\nFurther context: I hope to do theoretical/foundational work on neural networks as a researcher at an AI research¬†lab in the¬†future.¬†\n\nQuestion:\n\n1)What area(s) of research would you recommend for someone interested in doing fundamental research in machine learning/DNNs?\n\n2)What are the popular/promising techniques and mathematical frameworks used by researchers working on the theoretical foundations of deep learning?\n\nThanks a lot for your help.",
    "author": "willingtoengage",
    "timestamp": "2025-08-04T22:49:09",
    "url": "https://reddit.com/r/statistics/comments/1mi13a0/seeking_advice_on_choosing_phd_topicarea_r_q_d_e/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mhzc9v",
    "title": "[Career] Jobs in systemic reviews and meta-analysis",
    "content": "I will be graduating with a bachelors in statistics next year, and I'm starting to think about masters programs and jobs.\n\nBoth in school and on two research teams I've worked with, I've really enjoyed what I've learned about conducting systemic reviews and meta-analysis.\n\nDoes anyone know if there are industries or jobs where statisticians get to perform these more often than in other places? I am especially interested in the work of organizations like Cochrane, or the Campbell Collaboration.",
    "author": "jejacobsen",
    "timestamp": "2025-08-04T21:10:20",
    "url": "https://reddit.com/r/statistics/comments/1mhzc9v/career_jobs_in_systemic_reviews_and_metaanalysis/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mhf9r5",
    "title": "[Question] How to know if my Weibull PDF is fit  (numerically / graphically )?",
    "content": "Hi all, I am trying to use Weibull distribution to predict the extreme worst cases I couldn't collect. I am using Python SciPy, weibull\\_min and got some results. However, in this algorithm it requires the first parameter, the shape, then it will use some formulas to obtain shift and scale automatically. Tuning a few shapes to get the bell shape I really don't know if the PDF it gave is fit or not. Is there a way for me to find out e.g. looking at it thinking it's correct or from my 1x15 data row I must do something to get the correct coefficients ?  There is another Weibull model that takes 2 instead of 1 but I really have  to know when is my data fit and correct. Thank you",
    "author": "Perfect_Leave1895",
    "timestamp": "2025-08-04T07:40:18",
    "url": "https://reddit.com/r/statistics/comments/1mhf9r5/question_how_to_know_if_my_weibull_pdf_is_fit/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mha0r1",
    "title": "[Question] Re-project non-Euclidean matrix into Euclidean space",
    "content": "I am working with approximate Gaussian Processes with Stan, but I have non-Euclidean distance matrices. These distance matrices come from theory-internal motivations, and there is really no way of changing that (for example the cophenetic distance of a tree). Now, approx GP algorithm takes the Euclidean distance between between observations in 2 dimensions. My question is: What is the least bad/best dimensionality reduction technique I should be using here?\n\nI have tried regular MDS, but when comparing the orignal distance matrix to the distance matrix that results from it, it seems quite weird. I also tried stacked auto encoders, but the model results make no sense.\n\nThanks!",
    "author": "cat-head",
    "timestamp": "2025-08-04T03:43:20",
    "url": "https://reddit.com/r/statistics/comments/1mha0r1/question_reproject_noneuclidean_matrix_into/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mhqqvg",
    "title": "Got a p-value of 0.000 when conducting a t-test? Can this be a normal result? [Discussion]",
    "content": "",
    "author": "[deleted]",
    "timestamp": "2025-08-04T14:43:12",
    "url": "https://reddit.com/r/statistics/comments/1mhqqvg/got_a_pvalue_of_0000_when_conducting_a_ttest_can/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mh4a8j",
    "title": "[Q] Pooling complex surveys with extreme PSU imbalance: how to ensure valid variance estimation?",
    "content": "I'm following a one-stage pooling approach using two complex surveys (Argentina's national drug use surveys from 2020 and 2022) to analyze Cannabis Use Disorder (CUD) by mode of cannabis consumption. Pooling is necessary due to low response counts in key variables, which makes it impossible to fit my model separately by year.\n\nThe issue is that the 2020 survey, affected by COVID, has only 10 PSUs, while 2022 has about 900 PSUs. Other than that, the surveys share structure and methodology.\n\nSo far, I‚Äôve:\n\n* Harmonized the datasets and divided the weights by 2 (number of years pooled).\n* Created combined strata using year and geographic area.\n* Assigned unique PSU IDs.\n* Used bootstrap replication for variance and confidence interval estimation.\n* Performed sensitivity analyses, comparing estimates and proportions between years ‚Äî trends remain consistent.\n\nStill, I'm concerned about the validity of variance estimation due to the extremely low number of PSUs in 2020.  \nIs there anything else I can do to address this problem more rigorously?\n\nLooking for guidance on best practices when pooling complex surveys with such extreme PSU imbalance.",
    "author": "ThrowRA_dianesita",
    "timestamp": "2025-08-03T21:44:28",
    "url": "https://reddit.com/r/statistics/comments/1mh4a8j/q_pooling_complex_surveys_with_extreme_psu/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mgxt5e",
    "title": "[E] Alternatives to PhD in statistics",
    "content": "Does anyone know if programs like machine learning, bio informatics, data science ect‚Ä¶ are less competitive to get into than statistics PhD programs? ",
    "author": "Necessary_Detail_868",
    "timestamp": "2025-08-03T16:24:24",
    "url": "https://reddit.com/r/statistics/comments/1mgxt5e/e_alternatives_to_phd_in_statistics/",
    "score": 9,
    "num_comments": 14,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mhgz51",
    "title": "[Question] If you were a thief statistician and you see a mail package that says \"There is nothing worth stealing in this box\", what would be the chances that there is something worth stealing in the box?",
    "content": "",
    "author": "maltliqueur",
    "timestamp": "2025-08-04T08:43:53",
    "url": "https://reddit.com/r/statistics/comments/1mhgz51/question_if_you_were_a_thief_statistician_and_you/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mgqczt",
    "title": "[Career] Please help me out! I am really confused",
    "content": "I‚Äôm starting university next month. I originally wanted to pursue a career in¬†Data Science, but I wasn‚Äôt able to get into that program. However, I did get admitted into¬†Statistics, and I plan to do my¬†Bachelor‚Äôs in Statistics, followed by a¬†Master‚Äôs in Data Science or Machine Learning.\n\n\n\nHere‚Äôs a list of the core and elective courses I‚Äôll be studying:\n\n\n\nüéì Core Courses:\n\n* STAT 101 ‚Äì Introduction to Statistics\n* STAT 102 ‚Äì Statistical Methods\n* STAT 201 ‚Äì Probability Theory\n* STAT 202 ‚Äì Statistical Inference\n* STAT 301 ‚Äì Regression Analysis\n* STAT 302 ‚Äì Multivariate Statistics\n* STAT 304 ‚Äì Experimental Design\n* STAT 305 ‚Äì Statistical Computing\n* STAT 403 ‚Äì Advanced Statistical Methods\n\n\n\nüß† Elective Courses:\n\n* STAT 103 ‚Äì Introduction to Data Science\n* STAT 303 ‚Äì Time Series Analysis\n* STAT 307 ‚Äì Applied Bayesian Statistics\n* STAT 308 ‚Äì Statistical Machine Learning\n* STAT 310 ‚Äì Statistical Data Mining\n\n\n\nMy Questions:\n\n1. Based on these courses, do you think this degree will help me become a Data Scientist?\n2. Are these courses useful?\n3. While I‚Äôm in university, what other¬†skills or areas¬†should I focus on to build a strong foundation for a career in Data Science? (e.g., programming, personal projects, internships, etc.)\n\nAny advice would be appreciated ‚Äî especially from those who took a similar path!\n\nThanks in advance!",
    "author": "Busy_Cherry8460",
    "timestamp": "2025-08-03T11:18:57",
    "url": "https://reddit.com/r/statistics/comments/1mgqczt/career_please_help_me_out_i_am_really_confused/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.45,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mgeu1h",
    "title": "[question] statistics in cross-sectional studies",
    "content": "Hi,\n\nI'm an immunology student doing a cross-sectional study. I have cell counts from 2 time points (pre-treatment and treatment) and I'm comparing the cell proportions in each treatment state (i.e. this type of cell is more prevalent in treated samples than pre-treated samples, could it be related to treatment?) \n\nI have a box plot with 3 boxes per cell type (pre treatment, treatment 1 and treatment 2) and I'm wondering if I can quantify their differences instead of merely comparing the medians on the box plots and saying \"this cell type is lower\". I understand that hypothesis testing like ANOVA and chi-square are used in inferential statistics and not appropriate for cross sectional studies. I read that epidemiologists use prevalence ratios in their cross sectional studies but I'm not sure if that applies in my case. What are your suggestions? ",
    "author": "Horror-Baker-2663",
    "timestamp": "2025-08-03T02:16:17",
    "url": "https://reddit.com/r/statistics/comments/1mgeu1h/question_statistics_in_crosssectional_studies/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mfrzjo",
    "title": "[Question] Are there any methods or algorithms to quantify randomness or to compared the degree of randomness between two games or events?",
    "content": "Ok so I've been wondering for a while, is there a way to know the degree of randomness of something, or a way to compare if one game or event is expected to be more random than one another?\n\nAllow me to give you a short example, if you roll a single dice one, you can expect 6 different results, 1 to 6, but if you roll the same dice twice, then you can except a value going from 1 to 12 with a total of 36 different combinations,  so the second game we played should be \"more random\" than the first, which is something we can easily judge intuitively without making any calculations.\n\nConsidering this, can we determine the randomness of more complex games? Are there any methods or algorithms to do this? Let's say something far more complex like Yugioh and MtG, or a board game like Risk vs Terraforming mars?\n\nIdk if this is even possible but I find this very interesting.",
    "author": "2pado",
    "timestamp": "2025-08-02T07:32:49",
    "url": "https://reddit.com/r/statistics/comments/1mfrzjo/question_are_there_any_methods_or_algorithms_to/",
    "score": 6,
    "num_comments": 16,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mfz3om",
    "title": "[Question] Looking for real datasets with significant quadratic effects in functional logistic regression (FDA)",
    "content": "Hi!\n\nI'm currently working on developing a functional logistic regression model that includes a quadratic term. While the model performs well in simulations, I'm trying to evaluate it on real datasets ‚Äî and that's where I'm facing a challenge.\n\nIn every real dataset I‚Äôve tried so far, the quadratic term doesn't seem to have a significant impact, and in some cases, the linear model actually performs better. üòû\n\nFor context, the Tecator dataset shows a notable improvement when incorporating a quadratic term compared to the linear version. This dataset contains the absorbance spectrum of meat samples measured with a spectrometer. For each sample, there is a 100-channel spectrum of absorbances, and the goal is typically to predict fat, protein, and moisture content. The absorbance is defined as the negative base-10 logarithm of the transmittance. The three contents ‚Äî measured in percent ‚Äî are determined via analytical chemistry.\n\nI'm wondering if you happen to know of any other real datasets similar to Tecator where the quadratic term might provide a meaningful improvement. Or maybe you have some intuition or guidance that could help me identify promising use cases.\n\nSo far, I‚Äôve tested several audio-related datasets (e.g., fake vs. real speech, female vs. male voices, emotion classification), thinking the quadratic term might highlight certain frequency interactions, but unfortunately, that hasn't worked out as expected.\n\nAny suggestions would be greatly appreciated!",
    "author": "ElRockNOmurio",
    "timestamp": "2025-08-02T12:30:55",
    "url": "https://reddit.com/r/statistics/comments/1mfz3om/question_looking_for_real_datasets_with/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mfqmi0",
    "title": "[Q] [E] Do I have enough prerequisites to apply for a Msc in Stats?",
    "content": "I will be finishing my business (yes, i know) degree next April and was looking at multiple Msc stats programs as I was looking toward Financial Engineering / more quantitatively based banking work. \n\nI have of course taken basic calculus, linear algebra and basic statistics pre-university. The possibly relevant courses I have taken during my university degree are: \n\nEconometrics\n\nLinear Optimisation\n\nApplied math 1&amp;2 (Non-linear dynamic optimization, dynamic systems, more advanced linear algebra)\n\nStochastic calculus 1&amp;2\n\nIntermediate statistics (Inference, anova, regression etc.)\n\nBasic &amp; advanced object-oriented C++ programming\n\nBasic &amp; advanced python programming\n\n\\+ multiple finance and applied econ courses, most of which are at least tangentially related to statistics\n\nI have also taken an online course on ODEs and am starting another one on PDEs.\n\nSo, do I have the required prerequisites, should I take some more courses on the side to improve my chances or am I totally out of my depth here?",
    "author": "TheDankBaguette",
    "timestamp": "2025-08-02T06:31:43",
    "url": "https://reddit.com/r/statistics/comments/1mfqmi0/q_e_do_i_have_enough_prerequisites_to_apply_for_a/",
    "score": 5,
    "num_comments": 10,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mftpzh",
    "title": "[Q] Need Help in calculating school admission statistics",
    "content": "Hi, I need help in assessing the admission statistics of a selective public school that has an admission policy based on test scores and catchment areas. \n\nThe school has defined **two catchment areas (namely A and B)**, where catchment A is a smaller area close to the school and catchment B is a much wider area, also **including A**. Catchment A is given a certain degree of preference in the admission process. Catchment A is a more expensive area to live in, so I am trying to gauge how much of an edge it gives.\n\nKey policy and past data are as follows:\n\n- Admission to Einstein Academy is solely based on performance in our admission tests. Candidates are ranked in order of their achieved mark.     \n- There are **2** assessment stages. Only successful stage 1 sitters will be invited to sit stage 2. The mark achieved in stage 2 will determine their fate.\n- There are 180 school places available.\n- Up to **60** places go to candidates whose mark is higher than the **350th** ranked mark of all stage 2 sitters and whose residence is in **Catchment A**.  \n- Remaining places go to candidates in Catchment B (which includes A) based on their stage 2 test scores.\n- Past 3year averages: 1500 stage 1 candidates, of which 280 from Catchment A; 480 stage 2 candidates, of which 100 from Catchment A\n\nMy logic:\n- assuming all candidates are equally able and all marks are randomly distributed; big assumption, just a start\n- 480/1500 move on to stage2, but catchment doesn't matter here       \n- in stage 2, catchment A candidates (100 of them) get a priority place (up to 60) by simply beating the 27th percentile (above 350th mark out of 480)\n- probability of having a mark above 350th mark is 73% (350/480), and there are 100 catchment A sitters, so 73 of them are expected eligible to fill up all the 60 priority places. With the remaining 40 moved to compete in the larger pool.    \n- *expectedly*, 420 (480 - 60) sitters (from both catchment A and B) compete for the remaining 120 places\n- P(admission | catchment A) = P(passing stage1) * [ P(above 350th mark)P(get one of the 60 priority places) + P(above 350th mark)P(not get a priority place)P(get a place in larger pool) + P(below 350th mark)P(get a place in larger pool)] = (480/1500) * [ (350/480)(60/100) + (350/480)(40/100)(120/420) + (130/480)(120/420) ] = 19%\n- P(admission | catchment B) = (480/1500) * (120/420) = 9%\n- Hence, the edge of being in catchment A over B is about 10%",
    "author": "donaldtrumpiscute",
    "timestamp": "2025-08-02T08:44:40",
    "url": "https://reddit.com/r/statistics/comments/1mftpzh/q_need_help_in_calculating_school_admission/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mg3sjh",
    "title": "[E] If I find my statistical course boring, is it the professor's fault? At what point does a student take responsibility over bad teaching?",
    "content": "Currently learning Bayesian at the Master's level.\n\nMy professor insists on a webcast based off his slides / notes.\n\nNo textbook to reference to.\n\nI find the terms he use boring and confusing. His voice monotonous. There's no personality to his presentations.\n\nI feel like I have ADHD or procrastination constantly.\n\nNo one seems to complain but me, but I have high standards for myself and have given my own fair share of presentations.\n\nI understand he is not here for my entertainment, but in your university years, how did you deal with statistical courses taught so poorly.\n\nI believe the value of a teacher is to teach - if I didn't absorb anything, or if I am confused, that means the teacher has done a poor job.\n\nIf I have to constantly ask ChatGPT for minor clarifications on terms, notations, and formulas, I think it was not I who failed as a student, but my teacher.\n\nA student fails when they plagiarize. Or cheat. Or refuses to study.\n\nBut I am TRYING to study, I just can't focus on this darn specific course.\n\nHow did you guys cope? Especially when the alternatives are so tempting...I could literally go on dates, go on parties, have a weekend trip to another city.",
    "author": "arcanehelix",
    "timestamp": "2025-08-02T15:58:17",
    "url": "https://reddit.com/r/statistics/comments/1mg3sjh/e_if_i_find_my_statistical_course_boring_is_it/",
    "score": 0,
    "num_comments": 19,
    "upvote_ratio": 0.46,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mfn696",
    "title": "[Question]: Hierarchical regression model choice",
    "content": "I ran a hierarchical multiple regression with three blocks:\n\n* **Block 1:** Demographic variables\n* **Block 2:** Empathy (single-factor)\n* **Block 3:** Reflective Functioning (RFQ), and this is where I‚Äôm unsure\n\n**Note about the RFQ scale:**  \nThe RFQ has 8 items. Each dimension is calculated using 6 items, with 4 items overlapping between them. These shared items are scored in opposite directions:\n\n* One dimension uses the original scores\n* The other uses reverse-scoring for the same items\n\nSo, while multicollinearity isn't severe (per VIF), there is structural dependency between the two dimensions, which likely contributes to the ‚Äì0.65 correlation and influences model behavior.\n\n**I tried two approaches for Block 3:**\n\n**Approach 1:** Both RFQ dimensions entered simultaneously\n\n* VIFs \\~2 (no serious multicollinearity)\n* Only one RFQ dimension is statistically significant, and only for one of the three DVs\n\n**Approach 2:** Each RFQ dimension entered separately (two models)\n\n* Both dimensions come out significant (in their respective models)\n* Significant effects for two out of the three DVs\n\n**My questions:**\n\n1. In the write-up, should I report the model where both RFQ dimensions are entered together (more comprehensive but fewer significant effects)?\n2. Or should I present the separate models (which yield more significant results)?\n3. Or should I include both and discuss the differences?\n\nThanks for reading!",
    "author": "makislog",
    "timestamp": "2025-08-02T03:22:16",
    "url": "https://reddit.com/r/statistics/comments/1mfn696/question_hierarchical_regression_model_choice/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mfo8x6",
    "title": "[Q] Difference-in-differences vs. regression (ANCOVA) vs. Propensity Score Matching",
    "content": "I'm working on a case where we launch a campaign for marketing and tried to estimate the impact. To simplify, we have Y1\\_pre, Y2\\_pre, Y1\\_post, Y2\\_post, and other covariates like location\\_id, gender ...\n\nWhat I think we can use:\n\n* DiD: Need to panelize the data so we can have model like: Y1 \\~ treatment\\*post or Y2 \\~ treatment\\*post. Those covariates like location and gender are fixed so it might not useful for DiD. However this assumes  parallel trend and it's pretty hard to validate. Some may also argue parallel trend among location is likely unmet due to different in geo. \n* ANCOVA: Simply put regression on Y1\\_post \\~ Y1\\_pre + Y2\\_pre + treatment + C(location, gender) or Y2\\_post \\~ Y1\\_pre + Y2\\_pre + treatment + C(location, gender). Yes, some might argue the interaction term among variables are not common for ANCOVA. But then this assumes the linear relationship among Y1\\_post vs Y1\\_pre, Y2\\_pre ... \n* Propensity Score matching (PSM): No regression, but tried to balance among groups. However, the balance might still has bias due to we can't guarantee all covariates are being matched. And it's hard to include everything too.\n\nGot a result quite different among 3 methods. PSM seems overestimating as it doesn't eliminate the bias while matching completely. The other model get results quite close (but still different).\n\nIn this case, should I trust DiD? Any chance to validate trend assumption? Or any more robust but interpretable approach?",
    "author": "RecognitionSignal425",
    "timestamp": "2025-08-02T04:29:01",
    "url": "https://reddit.com/r/statistics/comments/1mfo8x6/q_differenceindifferences_vs_regression_ancova_vs/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mfo1tc",
    "title": "[Question] Beginner to statistics, I can't figure out if I should use dharma for lmer model, please help",
    "content": "",
    "author": "PatternMysterious550",
    "timestamp": "2025-08-02T04:17:23",
    "url": "https://reddit.com/r/statistics/comments/1mfo1tc/question_beginner_to_statistics_i_cant_figure_out/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mfmi8p",
    "title": "[Question]: How do I analyse if one event leads to another? Football data",
    "content": "I have some data on football matches. I have a table with columns: match ID, league, home team, away team, home goals, away goals. I also have a detailed event table with columns match ID, minute the event occurred, type (either ‚Äòred card‚Äô or ‚Äògoal‚Äô), and team (home or away). I need to answer the question: ‚ÄòDo red cards seem to lead to more goals?‚Äô\n\nMy main thoughts are:\n1) analyse goal rate in matches with red cards both before and after the red cards, do some statistical test like a T-test if that‚Äôs appropriate to see if the goal rate has significantly increased.\n2) create a binary red card flag for each match, then either: attempt some propensity matching to see if I can establish some association between the red cards and total goals, or: fit some kind of regression/decision free model to see if the red cards flag has an effect on total goals.\n\nDoes this sound sensible, does anyone have any better ideas?",
    "author": "Bhhenjy",
    "timestamp": "2025-08-02T02:38:21",
    "url": "https://reddit.com/r/statistics/comments/1mfmi8p/question_how_do_i_analyse_if_one_event_leads_to/",
    "score": 1,
    "num_comments": 9,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mev40v",
    "title": "Statistics VS Data Science VS AI [R][Q]",
    "content": "What is the difference in terms of research among these 3 fields?\n\nHow different are the skills required and which one has the best/worst job prospects?\n\nI feel like statistics is a bit old-school and I would imagine most research funding is going towards data science/ML/AI stuff. What do you guys think?",
    "author": "gaytwink70",
    "timestamp": "2025-08-01T05:25:59",
    "url": "https://reddit.com/r/statistics/comments/1mev40v/statistics_vs_data_science_vs_ai_rq/",
    "score": 38,
    "num_comments": 26,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mfcztl",
    "title": "[Research] What are the probable research topics that a first year college student can tackle?",
    "content": "Hi! I am about to enter the world of stats in a few days and one of our seniors in college told us that despite being first-years, we do like mini theses in some major subjects such as Reasoning of Math. Any ideas or suggestions of what topics we could tackle that is under stats and what is feasible to do a mini thesis of? And any advice about statistics will be apprecuated, thank you!",
    "author": "Evelyn_Garden",
    "timestamp": "2025-08-01T17:35:36",
    "url": "https://reddit.com/r/statistics/comments/1mfcztl/research_what_are_the_probable_research_topics/",
    "score": 4,
    "num_comments": 6,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1memsws",
    "title": "[Q]  True Random Number List (Did I Notice a Pattern?)",
    "content": "Hi,\n\nI was reading an article about a true random number generator which generated random numbers based on the decay of a radioactive material (in this case, thorium from the lamp mantle).\n\nHere is their article: [https://partofthething.com/thoughts/making-true-random-numbers-with-radioactive-decay/](https://partofthething.com/thoughts/making-true-random-numbers-with-radioactive-decay/) for those interested. Also the data file (text file) is downloadable there so you can play around with it too).\n\nAt first, yes it appeared random to me, but I toyed with the numbers a bit by various sorts, playing with sets etc.. and I noticed something:\n\n1. Using the data that they posted on their site, I took a count of the frequency of appearances of a number (between 0 and 250). That came up with their graph, which makes sense..\n2. I sorted the frequencies then plotted the graph from the sorted freqiencies, which appears much like an x¬≥ graph of sorts (I took a screen grab of the graph I plotted in excel here: [https://i.imgur.com/aiUAAwx.png](https://i.imgur.com/aiUAAwx.png) )\n\nI would have assumed that given that due to the nature of it being a true random generation of numbers, that the frequency too would be random too or is there something that I'm missing in statistics or something else?\n\nI found this really interesting...",
    "author": "SpiffyCabbage",
    "timestamp": "2025-07-31T21:14:01",
    "url": "https://reddit.com/r/statistics/comments/1memsws/q_true_random_number_list_did_i_notice_a_pattern/",
    "score": 3,
    "num_comments": 11,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mec2d7",
    "title": "[Question] Resources for fundamentals of statistics in a rigorous way",
    "content": "straight to the topic, i did the basic stuff (variance, IQR, distributions etc) from [khan academy](https://www.khanacademy.org/math/statistics-probability/) but there's still something fundamental missing. Like why variance is still loved among statisticians (even tho it has different dimensions and doesn't represent actual deviations, being further exaggerated when the S.D. &gt; 1, and overly diminished when S.D. &lt; 1) and of its COOL PROPERTIES. Things like i.i.d, expectation etc in detail. Khan academy was helpful but i believe i should have some rigorous study material alongside it. I don't wanna get feed the same content over and over again by random youtube videos. So what would you suggest. Please suggest something that doesn't add more prerequisites to this list, i started from an AI course, its something like:\n\nCS50AI -&gt; neural netwoks -&gt; ISL (intro to statistical learning) -&gt;¬†[khan academy](https://www.khanacademy.org/math/statistics-probability/)¬†\\-&gt; the thing in question\n\n\n\nEDIT: by rigorous, i dont mean overly difficult/formal or designed for master's level such that it becomes incomprehensible, just detailed but still at introductory lvl\n\n\n\nThanks for your time :)\n\n\n",
    "author": "Hammadawan9255",
    "timestamp": "2025-07-31T13:10:37",
    "url": "https://reddit.com/r/statistics/comments/1mec2d7/question_resources_for_fundamentals_of_statistics/",
    "score": 8,
    "num_comments": 6,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mejea0",
    "title": "[Question] How do I introduce a deliberate bias into an average?",
    "content": "I have a data set of power rankings of Draft prospects for AFL (Australian Sport) That I am making. Whilst averaging out the rating of all the draft experts works fine for the top prospects, I'm not sure how to rank the bottom prospects. What should I do when one expert has a player ranked at, say, 29, but all other experts have them unranked (Implying they should fall below the 25-30 prospects that they ranked). I would also like to introduce a bias towards newer data that I add but is less of a priority. Advice appreciated. I am not a statistics expert and have only really studied normal distributions in school, though I have done calculus courses in university/college.",
    "author": "A4angus9",
    "timestamp": "2025-07-31T18:23:24",
    "url": "https://reddit.com/r/statistics/comments/1mejea0/question_how_do_i_introduce_a_deliberate_bias/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1med5uc",
    "title": "[Question] Two independent variables or one with 4 levels?",
    "content": "How can I tell if I have two independent variables or one independent variable with 4 levels? My experiment would measure ad effectiveness based on endorsing influencer's gender and whether it matches their content or not. So I would have 4 conditions (female congruent, female incongruent, male congruent, male incongruent), but I can't tell if I should use a one or two way anova?? maybe im stupid man idk\n\nidk if this counts as hw because i dont need answers i just cant remember which test to go with",
    "author": "Jellyfish-dot-org",
    "timestamp": "2025-07-31T13:52:57",
    "url": "https://reddit.com/r/statistics/comments/1med5uc/question_two_independent_variables_or_one_with_4/",
    "score": 5,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1me66k0",
    "title": "[Q] Any resources to learn basic statistics?",
    "content": "Hi everyone, I am a chemistry student and i need to learn about basic statistics. Instead of getting lessons, it's meant to be self study (austerities or smth idk). I get online exercises i need to complete, however i have no idea what they're actually talking about and we don't even have a textbook. I can memorize formula's just fine, but i have no idea what i am actually doing. \n\nI‚Äôm struggling a bit with understanding what the terms even mean, or what I‚Äôm actually doing when I calculate something like a p-value, standard deviation, or run a t-test and what the results actually mean. Most tutorials i find show the steps, but not the intuition or logic behind them.\n\nHopefully this question isn't too repetitive, but I‚Äôd really appreciate (preferable free) beginner-friendly materials (video's/books/websites) that explain:\n‚Äì What I‚Äôm doing\n‚Äì Why I‚Äôm doing it\n‚Äì And how it connects to real-world reasoning or decision-making.\n\nMy study materials include: normal probability distribution, CI, F-test, T-test, Critical area, sample parameters, P-value, Z-score, Type 1 and 2 mistakes, significance level, discernment and a T-value. They also expect me to see the connection between all of the terms.\n\nThanks alot üôè ",
    "author": "OkayStarfish",
    "timestamp": "2025-07-31T09:26:42",
    "url": "https://reddit.com/r/statistics/comments/1me66k0/q_any_resources_to_learn_basic_statistics/",
    "score": 6,
    "num_comments": 6,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1me0ogc",
    "title": "[Q] Test if one observation fits a historic collection",
    "content": "I have a small historic set of observations (n=15) and need to test if a new observation with one value and a measurement uncertainty can be assumed valid. \n\nWe currently test if the new observation is within +-2stdv of the historic set, but feel we can do better. Especially because we assume a measurement uncertainty exists.\n\nWhat kind of test can be used or do they all approach the same +-2stdv's approach?",
    "author": "bakjejebaksteen",
    "timestamp": "2025-07-31T05:49:18",
    "url": "https://reddit.com/r/statistics/comments/1me0ogc/q_test_if_one_observation_fits_a_historic/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1me3bte",
    "title": "[Q] Trying to find ratio between skaters/goalies and cats each account for in fantasy hockey",
    "content": "I am trying to use z-scores to determine value of players in my fantasy hockey league. In order to compare goalies and skaters against each other, I need to determine how each type of player affects the overall picture of my team. Each team has 11 skaters and 2 goalies, 13 total players. Skaters account for 12 categories and goalies account for 7 categories, 19 total categories. Each category is weighted evenly. Given that these numbers are not equal, simply taking the z-score flat and comparing them is not an accurate strategy so I need to create a multiplier to make these equal. Is it as simple as doing the following math?\n\nSkaters (12/19=.63157), (11/13=.84615) so .63157/.84615= .746411 factor\n\nGoalies (7/19-.36842), (2/13=.15384) so .36842/.15384 - 2.394737 factor\n\nThen take these factors and multiply each z-score by these factors to \"equal\" the stats among them and compare them against each other? It just doesn't seem right and I have been banging my head trying to figure out how to accomplish my goal.",
    "author": "jfigs9898",
    "timestamp": "2025-07-31T07:38:21",
    "url": "https://reddit.com/r/statistics/comments/1me3bte/q_trying_to_find_ratio_between_skatersgoalies_and/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mdnen0",
    "title": "[education] looking for help with understanding quantitative methods for social sciences",
    "content": "Hi everyone, I am hoping someone in this forum has some resources or advice for someone with degrees in sociology. I took a social stats course in undergrad and passed but didn‚Äôt retain much. I just finished my masters degree in Sociology (M.S) but i feel so unequipped for the research and data analysis aspect of this field and I really want to understand to help my job prospects.\n\nFor background, I took quantitative research methods but failed because I took an incomplete due to not understanding and not having the support via my professor.\n\nIn efforts for me to graduate, my advisor allowed me to substitute my quantitative methods requirement and I took a demographic methods course instead. I feel like this hindered me and confused me further on understanding social statistics, and I couldn‚Äôt do much about it because he just pushed me through the program to graduate in a timely manner.\n\nI am currently taking a research methods and statistics intro course on Udemy to hopefully learn the mechanisms of data analysis, but I am wanting a more hands on approach and instruction for this.\n\nAny recommendations on resources I can find to learn the art of quantitative stats for social sciences?",
    "author": "Parisianpurrsuasion",
    "timestamp": "2025-07-30T17:34:42",
    "url": "https://reddit.com/r/statistics/comments/1mdnen0/education_looking_for_help_with_understanding/",
    "score": 7,
    "num_comments": 7,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mdwzuk",
    "title": "[Q] about keno 7/7",
    "content": "I hit seven out of seven on Keno. Exactly 7 days later, playing the exact same numbers, I hit it again. Two different establishments. Is this as significant as I think it is?",
    "author": "DesertMonsoon777",
    "timestamp": "2025-07-31T02:30:22",
    "url": "https://reddit.com/r/statistics/comments/1mdwzuk/q_about_keno_77/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mddagr",
    "title": "[E] Looking for resources to improve stats skills/knowledge - healthcare",
    "content": "Hi all! I‚Äôm looking for resources (e.g textbooks) to support further learning in stats.\n\nI work in public health research where most of my projects are qualitative and descriptive stats focused. I have some experience with quantitative analysis (e.g. regression, t-tests) but as I‚Äôve not had to use it in practice, I feel that I may be rusty, so would like to brush up.\n\nI am also looking to advance in hierarchical regression, odds ratios &amp; log regression, Bayesian methods etc.\n\nIm comfortable with R but open to learning STATA (as I‚Äôve heard some in academia preferring the latter?).\n\nAny recommendations for where to start? I like reading about something and then have a data set at hand to apply my learnings. The goal is to move into epidemiology or at least have stronger transferable skills. \n\nThanks in advance :)",
    "author": "mingx24",
    "timestamp": "2025-07-30T10:46:30",
    "url": "https://reddit.com/r/statistics/comments/1mddagr/e_looking_for_resources_to_improve_stats/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mdfph7",
    "title": "[Q] Dumb question about correlations and ordinal values",
    "content": "Hey, people! I'm a Social Sciences student in Brazil, and I think I have what would be called a \"dumb question\" in parts for the lack of a good formation in statistics during my undergrad.\n\nSo... Let's say I have n = 131, and I have these two ordinal variables, and I'm testing linear correlation (Pearson) and monotonic relationship (Spearman) between them. Testing the null hypothesis, I get a p-value of 0.06 for Pearson and .07 for Spearman, what would indicate to discard the null hypothesis. I know that, if I test the positive hypothesis, those p-values will be the half (0.03 and 0.04, respectively), what is below the \"statistically significant\" value of 0.05. Should I, in my write, just say that the null hypothesis could not be discarded 'cause p-value is greater than 0.05 or, if I have some *a priori* reasons to believe the two variables are positively correlated, I could as well present the test for positive hypothesis (given the p-value, in this case, would be less than 0.05)?\n\nThank you all in advance!",
    "author": "mrdaltro",
    "timestamp": "2025-07-30T12:17:20",
    "url": "https://reddit.com/r/statistics/comments/1mdfph7/q_dumb_question_about_correlations_and_ordinal/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1md3ur8",
    "title": "[Question] High correlation but opposite estimate directions",
    "content": "Please bare with me on this, this is threatening to derail a project and it‚Äôs come down on me (even though this statistics is beyond me). Looking at effect of various metrics on emotional wellbeing. \n\nI‚Äôve ran a glmm with each emotional wellbeing metric separate as the outcome with various health metrics as the predictors. \nBut on predictor (age) is positively correlated with one emotional wellbeing measure and negatively correlated with another emotional wellbeing measure.  However, those two emotional wellbeing measures are highly correlated (according to excel correl). \n\nHow can they be highly correlated but then a predictor has opposite estimate direction from the glm? \nExplain it to me like I‚Äôm 5 because this has fallen to me to fix ",
    "author": "btredcup",
    "timestamp": "2025-07-30T04:17:15",
    "url": "https://reddit.com/r/statistics/comments/1md3ur8/question_high_correlation_but_opposite_estimate/",
    "score": 2,
    "num_comments": 21,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1md2ky2",
    "title": "[Education] Any resource where I can learn to differentiate between distributions?",
    "content": "I have been learning Business Statistics in my Master's Program, and I am not able to differentiate between distributions. For example, discrete and continuou,s then we have binomial, poisson and hypergrometric. Then comes the normal distributions and sample distributions. I am honestly confused in the lecture, so I would like to know any resource (video preferably) to help me understand.",
    "author": "leo_here86",
    "timestamp": "2025-07-30T03:03:50",
    "url": "https://reddit.com/r/statistics/comments/1md2ky2/education_any_resource_where_i_can_learn_to/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mcpdtd",
    "title": "Considering a Masters in Statistics... What are solid programs for me??? [Q]",
    "content": "Hi. I'm considering getting a Master's in Stat or Applied Stat, as the title says. Here's a bit more information. I have a BA in Economics with a minor in Statistics. I've been out of undergrad for 3 years, wherein I've been teaching middle school math while completing an MS in Secondary Math Education. I actually love teaching (I know... middle school AND math? Shocker!) and I want to continue with it as a career. That being said, I want to enter higher education. Before, I thought I'd do a PhD, but as someone nearing the end of my MS, I've realized I had no idea what I'd want to research at all. Now that I have savings and feel somewhat economically ok, I've realized I want to go back to graduate school and get a Master's in Statistics... or some kind of Data Analytics. I learned R in college, and took classes on Linear Regression, Categorical Data, Machine Learning, Econometrics, etc, for my minor, as well as Linear Algebra, Physics, and all the required math classes for Economics. I'm definitely rusty, but I really love statistics, primarily where it intersects with social sciences, research, and data analytics (I LOVE showing my kids how what they're learning aligns with what I learned. My middle schoolers have seen R very frequently.). I won't lie, I struggled with the classes in college (all B's, but I really had to fight for them), and I'm afraid of being behind or failing out. I want a Masters not just for the degree but to learn more about statistics, become a more qualified math educator, have a path to enter higher education to teach,  have options outside of education, better develop my logic and coding skills, and be more qualified and vocationally desirable (I guess). I've looked up programs for Statistics, but they vary everywhere. I love research and the intersection of statistics with social sciences. Machine Learning, I'm sorry to say, is not my thing. I'd love some advice or recommendations. I'm meeting with my undergrad career center soon. Thanks !!!",
    "author": "shesareallykeen",
    "timestamp": "2025-07-29T15:19:08",
    "url": "https://reddit.com/r/statistics/comments/1mcpdtd/considering_a_masters_in_statistics_what_are/",
    "score": 7,
    "num_comments": 12,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mcla6j",
    "title": "[Q] Why might OLS and WLS be giving the same results on Heteroscedastic Data?",
    "content": "Hi all! I am trying to handle the presence of heteroscedastiticy in a data set I'm working on. I am looking at volume over the last 12 months (indexed 0 to 11). For the dataset I am currently working on the slope, r^2, and p-valua are exactly the same for both OLS and WLS. I want to make sure I did it right. Is there an explanation for why these might be giving the exact same answers?\n\nCan I trust the results of the WLS?",
    "author": "idiosyncratic56",
    "timestamp": "2025-07-29T12:39:41",
    "url": "https://reddit.com/r/statistics/comments/1mcla6j/q_why_might_ols_and_wls_be_giving_the_same/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mcj00m",
    "title": "[Question] Are there cases where it is not appropriate to implement the use of SPC?",
    "content": "Hi guys! I‚Äôm a little unsure if this is the right sub to ask this question in, but here it goes. For anyone who has ever worked in supplier quality- are there situations where the implementation of statistical process control is not appropriate? Or can any supplier and industry benefit from SPC?",
    "author": "Few_Gas_8195",
    "timestamp": "2025-07-29T11:14:36",
    "url": "https://reddit.com/r/statistics/comments/1mcj00m/question_are_there_cases_where_it_is_not/",
    "score": 4,
    "num_comments": 3,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mcplqn",
    "title": "[Q] T-Tests between groups with uneven counts",
    "content": "I have three groups:  \nGroup 1 has n=261   \nGroup 2 has n=5545   \nGroup 3 has n=369  \n  \nI'm comparing Group 1 against Group 2, and Group 3 against Group 2 using simple Pairwise T-tests to determine significance. The distribution of the variable I'm measuring across all three groups is relatively similar:  \n\nGroup |                n |          mean |        median |    SD    \n1 |                     261 |       22.6 |          22 |           7.62  \n2 |                     5455 |    19.9 |          18 |           7.58  \n3 |                     369 |      18.2 |          18 |           7.21  \n   \nI could see weak significance between groups 1 and 2 maybe but I was returned a p-value of 3.0 x 10^-8, and for groups 2 and 3 (which are very similar), I was returned a p-value of 4 x 10^-5. It seems to me, using only basic knowledge of stats from college, that my unbalanced data set is amplifying any significance between might study groups. Is there any way I can account for this in my statistical testing? Thank you!",
    "author": "Strangeting",
    "timestamp": "2025-07-29T15:28:26",
    "url": "https://reddit.com/r/statistics/comments/1mcplqn/q_ttests_between_groups_with_uneven_counts/",
    "score": 1,
    "num_comments": 9,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mc5oz9",
    "title": "[Q] How to treat ordinal predictors in the context of multiple linear regression",
    "content": "Hi all, I have a question regarding an analysis I‚Äôm trying to do right now concerning data of 100 patients. I have a normally distrubuted continuous outcome Y. My predictor X is 13-scale ordinal predictor (disease severity score using multiple subdomains, minimum total score is 0 and maximum is 13). One thing to note is that the scores 0,1 and 13 do not occur in these patients. I want to do multiple linear regression analyses to analyse the association between Y and X (and some covariates such as sex, age and medication use etc), but the literature on how to handle ordinal predictors is a bit too overwhelming for me. Ordinal logistic regression (swithing X and Y) is not an option, since the research question and perspective changes too much in that way. A few questions regarding this topic:\n\n- Can I choose to treat this ordinal predictor as a continuous predictor? If so, what are some arguments generally in favor of doing so (quite a few categories for example)?\n\n- If I were to treat it as a continous predictor, how can I statistically test beforehand whether this is an‚Äò‚Äôokay‚Äô‚Äô thing to do (I work with Rstudio)? I‚Äôm reading about comparing AIC levels and such..\n\n- If that is not possible, which of the methods (of handeling ordinal predictors) is most used and accepted in clinical research?\n\nThank you in advance for your help and feedback!\n\nWith kind regards",
    "author": "PandahPowah",
    "timestamp": "2025-07-29T01:10:29",
    "url": "https://reddit.com/r/statistics/comments/1mc5oz9/q_how_to_treat_ordinal_predictors_in_the_context/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mcfpra",
    "title": "[Q] How to incorporate disruption period length as an explanatory variable in linear regression?",
    "content": "I have a time series dataset spanning 72 months with a clear disruption period from month 26 to month 44. I'm analyzing the data by fitting separate linear models for three distinct periods:\n\n* Pre-disruption (months 0-25)\n* During-disruption (months 26-44)\n* Post-disruption (months 45-71)\n\nFor the during-disruption model, I want to include the length of the disruption period as an additional explanatory variable alongside time. I'm analyzing the impact of lockdown measures on nighttime lights, and I want to test whether the duration of the lockdown itself is a significant contributor to the observed changes. In this case, the disruption period length is 19 months (from month 26 to 44), but I have other datasets with different lockdown durations, and I hypothesize that longer lockdowns may have different impacts than shorter ones.\n\nWhat's the appropriate way to incorporate known disruption duration into the analysis?\n\n  \nA little bit of context:\n\nThis is my approach for testing whether lockdown duration contributes to the magnitude of impact on nighttime lights (column ba in the shared df) during the lockdown period (knotsNum).\n\n  \nThat's how I fitted the linear model for the during period without adding the length of the disruption period:\n\n    pre_data &lt;- df[df$monthNum &lt; knotsNum[1], ]\n    during_data &lt;- df[df$monthNum &gt;= knotsNum[1] &amp; df$monthNum &lt;= knotsNum[2], ]\n    post_data &lt;- df[df$monthNum &gt; knotsNum[2], ]\n    \n    during_model &lt;- lm(ba ~ monthNum, data = during_data)\n    summary(during_model)\n\nHere is my dataset:\n\n    &gt; dput(df)\n    structure(list(ba = c(75.5743196350863, 74.6203366002096, 73.6663535653328, \n    72.8888364886628, 72.1113194119928, 71.4889580670178, 70.8665967220429, \n    70.4616902716411, 70.0567838212394, 70.8242795722238, 71.5917753232083, \n    73.2084886381771, 74.825201953146, 76.6378322273966, 78.4504625016473, \n    80.4339255221286, 82.4173885426098, 83.1250549660005, 83.8327213893912, \n    83.0952494240052, 82.3577774586193, 81.0798739040064, 79.8019703493935, \n    78.8698515342936, 77.9377327191937, 77.4299978963597, 76.9222630735257, \n    76.7886470146215, 76.6550309557173, 77.4315783782333, 78.2081258007492, \n    79.6378781206591, 81.0676304405689, 82.5088809638169, 83.950131487065, \n    85.237523842823, 86.5249161985809, 87.8695954274008, 89.2142746562206, \n    90.7251944966818, 92.236114337143, 92.9680912967979, 93.7000682564528, \n    93.2408108610688, 92.7815534656847, 91.942548368634, 91.1035432715832, \n    89.7131675379257, 88.3227918042682, 86.2483383318464, 84.1738848594247, \n    82.5152280388184, 80.8565712182122, 80.6045637522384, 80.3525562862646, \n    80.5263796870851, 80.7002030879055, 80.4014140664706, 80.1026250450357, \n    79.8140166545202, 79.5254082640047, 78.947577740372, 78.3697472167393, \n    76.2917760563349, 74.2138048959305, 72.0960610901764, 69.9783172844223, \n    67.8099702791755, 65.6416232739287, 63.4170169813438, 61.1924106887589, \n    58.9393579024253), monthNum = 0:71), class = \"data.frame\", row.names = c(NA, \n    -72L))\n\nThe disruption period:\n\n`knotsNum &lt;- c(26,44)`\n\nSession info:\n\n    &gt; sessionInfo()\n    R version 4.5.1 (2025-06-13 ucrt)\n    Platform: x86_64-w64-mingw32/x64\n    Running under: Windows 11 x64 (build 26100)\n    \n    Matrix products: default\n      LAPACK version 3.12.1\n    \n    locale:\n    [1] LC_COLLATE=English_United States.utf8  LC_CTYPE=English_United States.utf8    LC_MONETARY=English_United States.utf8\n    [4] LC_NUMERIC=C                           LC_TIME=English_United States.utf8    \n    \n    time zone:\n    tzcode source: internal\n    \n    attached base packages:\n    [1] stats     graphics  grDevices utils     datasets  methods   base     \n    \n    loaded via a namespace (and not attached):\n    [1] compiler_4.5.1    tools_4.5.1       rstudioapi_0.17.1",
    "author": "Nicholas_Geo",
    "timestamp": "2025-07-29T09:14:31",
    "url": "https://reddit.com/r/statistics/comments/1mcfpra/q_how_to_incorporate_disruption_period_length_as/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mbt03r",
    "title": "[C] Anything important one should know before majoring in statistics?",
    "content": "Not a lot of information, or atleast the kind of information I want, out there so I thought I would ask here. For people who majored in statistics and preferably have a masters/phd, what's something you feel is important for people that want to major in stats? \n\nVery vague and ambiguous question, I know, but that's the point of it. Am looking for something I couldn't find or would have a hard time finding on the internet.",
    "author": "[deleted]",
    "timestamp": "2025-07-28T14:29:59",
    "url": "https://reddit.com/r/statistics/comments/1mbt03r/c_anything_important_one_should_know_before/",
    "score": 19,
    "num_comments": 28,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mby926",
    "title": "[Q] GAMs in Ecology",
    "content": "Hi all, long shot.\n\nI have been working on my GAMs in R for the last 7 months, and I have pretty much self taught myself about them and how to run them. Every time I show my advisor the results, she doesn't like them and tells me to do something different. I am at my wits end and I was wondering if someone might be able to look over my coding and thought process as to what I have done? I am so tired of running and re-running them, but my confidence in them is now low since my advisor keeps telling me to try something else.",
    "author": "SnooBooks5390",
    "timestamp": "2025-07-28T18:17:15",
    "url": "https://reddit.com/r/statistics/comments/1mby926/q_gams_in_ecology/",
    "score": 4,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mbogkc",
    "title": "[E] PhD in Statistics vs Field of Application",
    "content": "Have a very similar issue as in [this previous post](https://www.reddit.com/r/statistics/comments/1kdfj9m/q_applying_to_phds_in_statistics_or_phd_in_domain/), but I wanted to expand on it a little bit. Essentially, I am deciding between a PhD in Statistics (or perhaps data science?) vs a PhD in a field of interest. For background, I am a computational science major and a statistics minor at a T10. I have thoroughly enjoyed all of my statistics and programming coursework thus far, and want to pursue graduate education in something related. I am most interested in spatial and geospatial data when applied to the sciences (think climate science, environmental research, even public health etc.).\n\n  \nMy main issue is that I don't want to do theoretical research. I'm good with learning the theory behind what I'm doing, but it's just not something I want to contribute to. In other words, I do not really want to  partake in any method development that is seen in most mathematics and statistics departments. My itch comes from wanting to apply statistics and machine learning to real-life, scientific problems.\n\n  \nHere are my pros of a statistics PhD:\n\n\\- I want to keep my options open after graduation. I'm scared that a PhD in a field of interest will limit job prospects, whereas a PhD in statistics confers a lot of opportunities.\n\n\\- I enjoy the idea of statistical consulting when applied to the natural sciences, and from what I've seen, you need a statistics PhD to do that\n\n\\- better salary prospects\n\n\\- I really want to take more statistics classes, and a PhD would grant me the level of mathematical rigor I am looking for\n\n  \nCons and other points:\n\n\\- I enjoy academia and publishing papers and would enjoy being a professor if I had the opportunity, but I would want to publish in the sciences.\n\n\\- I have the ability to pursue a 1-year Statistics masters through my school to potentially give me a better foundation before I pursue a PhD in something else.\n\n\\- I don't know how much real analysis I actually want to do, and since the subject is so central to statistics, I fear it won't be right for me\n\n  \nTLDR: how do I combine a love for both the natural sciences and applied statistics at the graduate level? what careers are available to me? do I have any other options I'm not considering?",
    "author": "redapplepi3141",
    "timestamp": "2025-07-28T11:38:44",
    "url": "https://reddit.com/r/statistics/comments/1mbogkc/e_phd_in_statistics_vs_field_of_application/",
    "score": 10,
    "num_comments": 7,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mbhm41",
    "title": "[Q] Recommendations for an online R course with a focus on ecology?",
    "content": "I'm looking for courses to upgrade my resume. \n\nI know the basics, can do simple analyses and plots in the tidyverse. And I can generally figure out how to do something if I google it enough. But, I'd like to stay in practice, and learn more complicated stuff. \n\nAny recommendations? Preferably not self-paced, I need the consistency of having an actual class time and instructor. Also, I graduated 2 years ago, I don't know if these skills are being phased out by AI? ",
    "author": "Some_Mortgage9604",
    "timestamp": "2025-07-28T07:25:57",
    "url": "https://reddit.com/r/statistics/comments/1mbhm41/q_recommendations_for_an_online_r_course_with_a/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mbjawr",
    "title": "[Career] Accounting -&gt; Stats",
    "content": "Has anyone transitioned from accounting to statistics and if so, can you share a little about your experience? I graduated with a Bachelor‚Äôs in economics last year and have been working in accounting for about a year now, but I‚Äôm not sure it‚Äôs something I want to do long term. I‚Äôm thinking that stats could be a field I would enjoy more, but it‚Äôs intimidating to think about trying to make a transition, especially with how tough the job market seems to be.\n\nIf anyone could provide me with some insight on how I could go about doing this, how realistic this is, etc, that would be much appreciated.",
    "author": "nick2658",
    "timestamp": "2025-07-28T08:29:53",
    "url": "https://reddit.com/r/statistics/comments/1mbjawr/career_accounting_stats/",
    "score": 1,
    "num_comments": 13,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mayq4x",
    "title": "[Discussion]What is the current state-of-the-art in time series forecasting models?",
    "content": "QI‚Äôve been exploring various models for time series prediction‚Äîfrom classical approaches like ARIMA and Exponential Smoothing to more recent deep learning-based methods like LSTMs, Transformers, and probabilistic models such as DeepAR.\n\nI‚Äôm curious to know what the community considers as the most effective or widely adopted state-of-the-art methods currently (as of 2025), especially in practical applications. Are hybrid models gaining traction? Are newer Transformer variants like Informer, Autoformer, or PatchTST proving better in real-world settings?\n\nWould love to hear your thoughts or any papers/resources you recommend.",
    "author": "PatternFew5437",
    "timestamp": "2025-07-27T14:54:13",
    "url": "https://reddit.com/r/statistics/comments/1mayq4x/discussionwhat_is_the_current_stateoftheart_in/",
    "score": 25,
    "num_comments": 3,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mb73a3",
    "title": "[Q] Help on a Problem 18 in chapter 2 of the \"First Course in Probability\"",
    "content": "Hello!\n\nCan someone please help me with this problem?\n\nProblem 18 in chapter 2 of the \"First Course in Probability\" by Sheldon Ross (10th edition):\n\nEach of 20 families selected to take part in a treasure hunt consist of a mother, father, son, and daughter. Assuming that they look for the treasure in pairs that are randomly chosen from the 80 participating individuals and that each pair has the same probability of finding the treasure, calculate the probability that the pair that finds the treasure includes a mother but not her daughter.\n\nThe books answer is 0.3734. I have searched online and I can't find a solution that concludes with this answer and that makes sense. Can someone please help me. I am also very new to probability (hence why I'm on chapter 2) so any tips on how you come to your answer would be much appreciated.\n\nI don't know if this is the place to ask for help about this. If it is not, please let me know.",
    "author": "AdImpressive9604",
    "timestamp": "2025-07-27T21:42:55",
    "url": "https://reddit.com/r/statistics/comments/1mb73a3/q_help_on_a_problem_18_in_chapter_2_of_the_first/",
    "score": 3,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mb59k9",
    "title": "[Q] is there a way to calculate how improbable this is",
    "content": "[Request] My wife father and my father both had the same first name (donald).  Additionally her maternal grandfather and my paternal grandfather had the same first name (Kenneth).  Is there a way to figure out how improbable this is?",
    "author": "beefymonkey",
    "timestamp": "2025-07-27T20:05:28",
    "url": "https://reddit.com/r/statistics/comments/1mb59k9/q_is_there_a_way_to_calculate_how_improbable_this/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1mainta",
    "title": "[Q] Thinking about Statistics PhD",
    "content": "Hello! I‚Äôve recently started thinking about applying for a PhD in Statistics, and would love some advice about how I could prepare myself. My academic interests have focused a lot more heavily on applied sciences (biology and machine learning). I‚Äôve never considered pursuing an PhD in theory, so I‚Äôm not sure how far of a shot I‚Äôm making. \n\nI am starting the third year of my undergraduate at MIT, and I am pursuing double majors in math and computer science. My current GPA is 5.0. \n\nI plan to complete both my bachelor‚Äôs and master‚Äôs in Spring 2027, so unless I decide to take more time, I‚Äôd likely start applying in ~1.5 year during Fall 2026. \n\nFor theory coursework, I‚Äôve taken a graduate course in discrete probability and stochastic processes. Otherwise, my coursework is at the undergraduate level: topology, real analysis, design and analysis of algorithms, statistics, linear algebra, differential equations, and multivariable calculus. For my computer science degree, I‚Äôve mostly just taken courses to fulfill my major requirements. In the coming year, I plan to take more graduate-level ML and theory courses!\n\nFor languages, I am familiar with Python, C, Assembly, TypeScript, Bluespec, and Verilog. I also have personal projects using the MERN stack, NextJS, Flask, and ThreeJS.\n\nI have some teaching (including UTA for real analysis) and service experience as well. \n\nOn the research side, I have two papers under review for NeurIPS 2025 (one as first author with two faculty members), but both are in applied machine learning. I have been reading Wainwright‚Äôs high dimensional statistics book and have some research ideas from papers I‚Äôve read in sparse coding, but I am not sure where to start with gaining theory research experience because I think I would need to take more graduate statistics courses first. However, by that time, I won‚Äôt have much time to work on research before the application cycle. I really regret not working on research this summer, but am willing to work throughout the school year and next summer.\n\nAs for letter of recs, I have two advisors I can ask. One of them is quite fond of me, but would be a new faculty in a BioE department. The other is more established in computer vision, but is still a younger faculty. Additionally, I have performed well in my courses (scoring in the top 10/200+ on theory exams), but have not interacted much with the teaching professors. Do people typically reach out for non-research letter of recs?\n\nIf you suggest I take another year to apply, are there post-bacc research programs for statistics that I could consider to make myself more competitive? Otherwise, I would really like to apply to top PhD programs in statistics!\n\nAny advice would be much appreciated! Thank you so much. :-)",
    "author": "YamBrioche",
    "timestamp": "2025-07-27T03:08:04",
    "url": "https://reddit.com/r/statistics/comments/1mainta/q_thinking_about_statistics_phd/",
    "score": 6,
    "num_comments": 15,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1map004",
    "title": "[Q] Applied Stats Masters as a Software Engineering undergrad?",
    "content": "I've recently decided to try and get a Master's in Applied Statistics to pivot into data science after a tough couple of internship searches in undergrad. I'm entering my final semester this fall in Sotware Engineering undergrad at a smaller D1 state school in Ohio, and will have taken courses in calc 1-3, linear algebra, computing with data (using R and Python with datasets) probabilities of stats, fundamentals of statistics, and intro to stats. \n\nI'll have a 3.9 GPA and two SE internships, and was looking at applying to Ohio State and Cincinnati. I was concerned my limited background would stop me from getting accepted since OSU's stats department is top 20, and out of state isn't viable financially. Do I have a chance?",
    "author": "[deleted]",
    "timestamp": "2025-07-27T08:23:05",
    "url": "https://reddit.com/r/statistics/comments/1map004/q_applied_stats_masters_as_a_software_engineering/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1maomzi",
    "title": "[Q] Newbie question about statistical testing (independece of observations etc.)",
    "content": "Hello! I don't have much expertise in statistics and I would appreciate some help.\n\nMy data is monthly means of groundwater table depths over two 20-year periods. The annual means (means taken over each year) are, on average, higher in one period, and I want to test if the difference is significant (I'm probably using the U-test).\n\nMy first thought was that I should be comparing two populations consisting of the annual means (n=20). But I was adviced to use populations that consist of the monthly means to avoid small sample size. But I feel like I shouldn't do that, mainly because there is clear seasonality in groudwater table depths and I don't think the monthly values are independent within the periods (deep groundwater table in June is probably often followed by deep groundwater table in July, as they depend on the weather conditions).\n\nIn other words: Is it valid in this case to use U-test for two populations consisting of monthly means and then to say \"On annual level, the mean groundwater table depths were lower in period A (p&lt;0.05)\"?\n\nI hope I was clear enough.",
    "author": "Beneficial-Type-8190",
    "timestamp": "2025-07-27T08:08:18",
    "url": "https://reddit.com/r/statistics/comments/1maomzi/q_newbie_question_about_statistical_testing/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m9ziwb",
    "title": "[Q][E] Math to self study, some guidance?",
    "content": "Hi everyone, background: 2year bachelor student in Economics in Europe, wanting to pursue a Statistics MSc and self-learn more math subjects (pure and applied) during these years.\n\nI'd like to make a **plan** of self study (since I procrastinate a lot) for my last year of BSc, where I'll try to combine some coding study (become more proficient with R and learn Python better) with **pure math subjects.** I ask here because there are a lot of topics so maybe I will give priority to the most needed ones in Statistics.\n\nCould you give me some guidance and maybe an order I should follow? Some courses I have taken by far are discrete structures, Calculus, Linear Algebra(should do it better by myself in a more rigorous way), Statistics (even though I think I'll still have to learn Probability in a more rigorous way than we did in my courses) and Intro to Econometrics.\n\nI am not sure which calculus courses I lack having done just one of them, and some of the most important subjects I've read here are like Real Analysis, Differential Equations, Measure Theory, but it is difficult for me to understand the right order one should follow\n\n  \n\n\n\n\n  \n",
    "author": "Ecstatic-Traffic-118",
    "timestamp": "2025-07-26T10:45:05",
    "url": "https://reddit.com/r/statistics/comments/1m9ziwb/qe_math_to_self_study_some_guidance/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m9v67z",
    "title": "[Q] Is there an alternative to t-test against a constant (threshold) for more than a group?",
    "content": "Hi! This is a little bit theoretical, I am looking for a type, model. I have a dataset with around 30 individual data points. I have to compare them against a threshold, but, I have to conduct this many times. Is there a better way to do that? Thanks in advance!",
    "author": "helloiambrain",
    "timestamp": "2025-07-26T07:48:16",
    "url": "https://reddit.com/r/statistics/comments/1m9v67z/q_is_there_an_alternative_to_ttest_against_a/",
    "score": 0,
    "num_comments": 18,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m8q8pe",
    "title": "[Q] Do non-math people tell you statistics is easy?",
    "content": "There‚Äôs been several times that I told a friend, acquaintance, relative, or even a random at a party that I‚Äôm getting an MS in statistics, and I‚Äôm met with the response ‚Äúisn‚Äôt statistics easy though?‚Äù\n\nI ask what they mean and it always goes something like: ‚ÄúWell I took AP stats in high school and it was pretty easy. I just thought it was boring.‚Äù\n\nYeah, no sh**. Anyone can crunch a z-score and reference the statistic table on the back of the textbook, and of course that gets boring after you do it 100 times.\n\nThe sad part is that they‚Äôre not even being facetious. They genuinely believe that stats, as a discipline, is simple.\n\nI don‚Äôt really have a reply to this. Like how am I supposed to explain how hard probability is to people who think it‚Äôs as simple as toy problems involving dice or cards or coins?\n\nDoes this happen to any of you? If so, what the hell do I say? How do I correct their claim without sounding like ‚ÄúAckshually, no ü§ì‚òùÔ∏è‚Äù?",
    "author": "jar-ryu",
    "timestamp": "2025-07-24T21:51:13",
    "url": "https://reddit.com/r/statistics/comments/1m8q8pe/q_do_nonmath_people_tell_you_statistics_is_easy/",
    "score": 139,
    "num_comments": 129,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m8uo1s",
    "title": "Fully Funded PhD Studentship Opportunity in Health Data Science / Medical Statistics [E][C]",
    "content": "Hope this kind of post is allowed. Apologies if not.\n\n  \nThis is an opportunity to come and work at Population Data Science at Swansea University developing ways to analyse time series data at a population scale. Funding is for students eligible for home student fees only. It would suit someone with a degree in maths, statistics, data science or another scientific discipline like physics. Let me know if you have any questions.\n\n  \n[https://www.swansea.ac.uk/postgraduate/scholarships/research/medical-mrc-nihr-phd--rs863.php](https://www.swansea.ac.uk/postgraduate/scholarships/research/medical-mrc-nihr-phd--rs863.php)",
    "author": "joseph_fourier",
    "timestamp": "2025-07-25T02:29:43",
    "url": "https://reddit.com/r/statistics/comments/1m8uo1s/fully_funded_phd_studentship_opportunity_in/",
    "score": 6,
    "num_comments": 2,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m8uipf",
    "title": "[QUESTION] reasonable visualization of skewed distribution around mean",
    "content": "Hi guys, I have a set of data that is roughly normal distributed if a certain parameter is sufficiently small, but the distribution becomes more and more skewed upon increasing that parameter. since the data consists of probability and they approach unity for sufficiently large choices of said parameter, at some point the distribution is so heavily skewed that the mean (and also median) are close to 1 and all the deviation left is ofc below 1. it resembles much more a gamma or exponential distribution in this realm.\n\nThe true nature of the data is hence much better captured by the median and a 50% percentil \"error\" than the usual mean plusminus standard deviation plot, as shown in the [picture](https://ibb.co/tMPym02X).\n\nI have found a formula for the moments of my desired quantity and therefore can analytically describe, say, the first and 2nd moment of the quantity, hence reproducing the plots solid line and light blue standard deviation area. Evaluating the higher moments, I could also gain information about the skewness of the quantity.\n\nNow I have two questions:\n\n* what is a way to determine wheter the data is more gamme or more exponential distributed?\n* How can I use the higher moments of my quantity to visualize not a symmentrical standard deviation as suggested by the second moment but rathar a skewd distribution as suggested by the data?\n\nI hope this makes sense and i have worded my wish properly",
    "author": "PiotrSanctuvich",
    "timestamp": "2025-07-25T02:19:50",
    "url": "https://reddit.com/r/statistics/comments/1m8uipf/question_reasonable_visualization_of_skewed/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m91oqm",
    "title": "[Question] Validation of LASSO-selected features",
    "content": "Hi everyone,\n\nAt work, I was asked to \"do logistic regression\" on a dataset, with the aim of finding significant predictors of a treatment being beneficial. It's roughly 115 features, with \\~500 observations. Not being a subject-matter expert, I didn't want to erroneously select features, so I performed LASSO regression to select features (dropping out features that had their coefficients dropped to 0).\n\nThen I performed binary logistic regression on the train data set, using only LASSO-selected features, and applied the model to my test data. However, only a 3 / 12 features selected were statistically significant.\n\nMy question is mainly: is the lack of significance among the LASSO-selected features worrisome? And is there a better way to perform feature selection than applying LASSO across the entire training dataset? I had expected, since LASSO did not drop these features out, that they would significantly contribute to one outcome or the other (may very well be a misunderstanding of the method).\n\nI saw some discussions on stackexchange about bootstrapping to help stabilize feature selection: [https://stats.stackexchange.com/questions/249283/top-variables-from-lasso-not-significant-in-regular-regression](https://stats.stackexchange.com/questions/249283/top-variables-from-lasso-not-significant-in-regular-regression)\n\nThank you!",
    "author": "RiceTaco12",
    "timestamp": "2025-07-25T08:08:29",
    "url": "https://reddit.com/r/statistics/comments/1m91oqm/question_validation_of_lassoselected_features/",
    "score": 0,
    "num_comments": 14,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m8i9f6",
    "title": "[Discussion] Getting opposite results for difference-in-differences vs. ANCOVA in healthcare observational studies",
    "content": "The standard procedure for the health insurance company I work for is difference-in-differences analyses to estimate treatment effects for their intervention programs. \n\nI've pointed out DiD should **not** be used because there's a causal relationship between pre-treatment outcome and treatment &amp; pre-treatment outcome with post-treatment outcome, but don't know if they'll listen. \n\nPart of the problem is many of their health intervention studies show fantastic cost reductions when you do DiD, but if you run an ANCOVA the significant results disappear. That's a lot of programs, costing many millions of dollars, that are no longer effective when you switch methodologies. \n\nI want to make sure I'm not wrong about this before I stake my reputation on doing ANCOVA. ",
    "author": "RobertWF_47",
    "timestamp": "2025-07-24T15:26:17",
    "url": "https://reddit.com/r/statistics/comments/1m8i9f6/discussion_getting_opposite_results_for/",
    "score": 8,
    "num_comments": 12,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m8qa84",
    "title": "[Discussion] Any statistics pdfs",
    "content": "Hello, as the title says, im an incoming statistics freshman, does anyone have any pdfs or wesbites i can use to self study/review before our semester starts? much appreciated. ",
    "author": "NKI69",
    "timestamp": "2025-07-24T21:53:32",
    "url": "https://reddit.com/r/statistics/comments/1m8qa84/discussion_any_statistics_pdfs/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.22,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m87q4n",
    "title": "[Q] Using \"complex surveys\" for a not-complex survey, in SPSS or R survey",
    "content": "Hi all, this is a follow-up to an earlier question that a bunch of you had very helpful input on.\n\nI have reasonable stats knowledge, but in my field convenience sampling is the norm. So, using survey weights is very new to me.\n\nI am preparing to collect a sample (\\~N = 3500) from Prolific, quota-matched to US census on age, race, sex. I will use raking to create a survey weight variable, to adjust to census-type data on factors such as sex, age, race/ethnicity, religious affiliation, etc.\n\nFrom there, my first analyses will be relatively simple, such as estimating prevalences of behaviors for different age groups and sex, and then a few simple associations, such as predicting recency of behaviors from a few health indices, etc.\n\nIn my previous question here, folks recommended a few resources, such as Lumley, and https://tidy-survey-r.github.io/site/. Plus I've learned that regular SPSS cannot handle these types of survey weights properly, and I need the complex samples module added.\n\nRegardless of whether I try to figure out my next steps using R survey or SPSS Complex Samples (where I've spent most of my recent time, due to years of SPSS experience, and limited R experience),  I find myself running up against  the fact that these complex survey packages are for survey data that are far more complicated than mine. Because I am recruiting from prolific, I do not have a probability sample, no strata nor clusters; I basically have a convenience sample with cases that I want to weight to better reflect population proportions on key variables (eg, sex, age, etc.).\n\nIn SPSS complex samples, I have successfully created a raked weight variable (only on test data, but still a big win for me). Am I right that in the Complex Surveys set up procedure, I should be indicating my weight variable, no strata nor clusters (because I have none, right?)?\n\nAnd for Stage 1: Estimation Method, I should indicate a sampling design of Equal WOR (equal probability sampling without replacement)? This seems to make most sense for my situation. The next window asks me to specify inclusion probabilities, but without strata/clusters, my hunch is to enter a fixed value for inclusion probability (chatGPT suggests the same and says this won't make a difference anyway?), does this make sense? And from there, I wonder if I'm good to go? Ie, load in the plan file when I'm ready to analyze?\n\nAside from SPSS, I'm open to exploring R survey, but the learning curve is steeper there. I have simply been overwhelmed trying to figure out SPSS. Is anyone familiar enough with R packages survey or srvyr to help me get started how I'd get started there? u/[Overall\\_Lynx4363](https://www.reddit.com/user/Overall_Lynx4363/) suggested the book Exploring Complex Survey Data Analysis, whcih I have, but I've just not gone there much. Quick view of the book suggests I can create a survey design object, simple random sample without replacement, aka an ‚ÄúIndependent Sampling design,‚Äù which has no clusters, and allows for my weight variable? From there, the relevant chapter moves into stratified and clustered designs, which is definitely irrelevant for my case?\n\nAny insights would be so much appreciated. Just trying to speed up my learning here! Thank you!",
    "author": "nc_bound",
    "timestamp": "2025-07-24T08:38:34",
    "url": "https://reddit.com/r/statistics/comments/1m87q4n/q_using_complex_surveys_for_a_notcomplex_survey/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m86owk",
    "title": "[Q] Which Test?",
    "content": "If I have two sample means and sample SD‚Äôs from two data sources (that are very similar) that always follow a Rayleigh Distribution (just slightly different scales), what test do I use to determine if the sources are significantly different or if they are within the margin of error of each other at this sample size? In other words which one is ‚Äúbetter‚Äù (lower mean is better), or do I need a larger sample to make that determination.\n\nIf the distributions were T or normal, I could use a Welch‚Äôs t-test, correct? But since my sample data is Rayleigh, I would like to know what is more appropriate.\n\nThanks!",
    "author": "chague94",
    "timestamp": "2025-07-24T07:59:38",
    "url": "https://reddit.com/r/statistics/comments/1m86owk/q_which_test/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m7fhl6",
    "title": "Advice for MS Stats student that has been out of school a while [E] [Q]",
    "content": "Hey all,\n\nI'm starting an MS in stats in a month and I've been out of school since 2018 working in Finance so I'm rusty af. I got good grades in all the pre-reqs Calc 1-3, linear algebra, mathematical probability. I work full time right now 50-60 hours a week so I don't really have unlimited time to review. Anyone able to give me some tips on something doable to get a good review in? I'm doing Calc 1-3 and linear algebra on Khan academy. Anything good I can casually read through while I'm at work? Honestly, any tips in generally would be greatly appreciated as I am very nervous to start. First course is a statistical inference course looks like going through Casella Berger text which I already bought and looks intimidating.",
    "author": "[deleted]",
    "timestamp": "2025-07-23T10:28:18",
    "url": "https://reddit.com/r/statistics/comments/1m7fhl6/advice_for_ms_stats_student_that_has_been_out_of/",
    "score": 11,
    "num_comments": 9,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m7iv9t",
    "title": "Need help regarding Monte Carlo Simulation [Discussion]",
    "content": "So there are random numbers used in calculation. \nIn practical life, what's the process? How those random numbers are decided?\n\nQuestion may sound silly, but yeah. It is what it is.",
    "author": "[deleted]",
    "timestamp": "2025-07-23T12:35:29",
    "url": "https://reddit.com/r/statistics/comments/1m7iv9t/need_help_regarding_monte_carlo_simulation/",
    "score": 6,
    "num_comments": 7,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m7ltiz",
    "title": "[Q] How can I test two curves?",
    "content": "Hi, how can I test the difference between two curves?  \nOn the Y-axis, I will have the mean Medication Possession Ratio, and on the X-axis, time in months over a two-year period. It is expected the mean MPR will decrease over time. \nThere will be two curves, stratified by sex (male and female).  \n\nHow can I assess whether these curves are statistically different?\n\n\nThe man MPR does not follow a Normal.",
    "author": "Lis_7_7",
    "timestamp": "2025-07-23T14:28:45",
    "url": "https://reddit.com/r/statistics/comments/1m7ltiz/q_how_can_i_test_two_curves/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m7ozlg",
    "title": "[Q] Distribution of dependent observations",
    "content": "I have collected 3 measures across a state in the US,  observations across all possible locations (full coverage across state). I only want to consider said state and so have the data for the entire target population.\n\nShould I fit a multivariate Gaussian or somehow a multivariate Gaussian Mixture? I know that neighboring locations are spatially correlated. But if I just want to know how these 3 measures are distributed in said state (in a nonspatial manner) + I have the data for the entire population, do I care about local spatial dependency? (my education tells me ignoring dependency amongst observations suppresses the true variance, but I literally have the entire data population)\n\n  \n**In short:** If I have the observed data   (of 3 measures) of all possible locations for the entire state, should I care about the the spatial dependency amongst the observations? And can I just fit a standard multivariate Gaussian or do I have to apply some spatial weighting to the covariance matrix? ",
    "author": "Other_Papaya_5344",
    "timestamp": "2025-07-23T16:40:42",
    "url": "https://reddit.com/r/statistics/comments/1m7ozlg/q_distribution_of_dependent_observations/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m77ovm",
    "title": "[Q] How do I deal with gaps in my time series data?",
    "content": "Hi, \n\nI have several data series i want to compare with each other. I have a few environmental variables over a ten year time frame, and one biological variable over the same time. I would like to see how the environmental variables affect the biological one. I do not care about future predictions, i really just want to test how my environmental variables, for example a certain temperature, affects the biological variable in a natural system. \n\nNow, as happens so often during long term monitoring, my data has gaps. Technically, the environmental variables should be measured on a work-daily basis, and the biological variable twice a week, but there are lots of missing values for both. gaps in the environmental variable always coincide with gaps in the biological one, but there are more gaps in the bio var then the environmental vars. \n\nI would still like to analyze this data, however lots of time series analysis seem to require the data measurements to be at least somewhat regular and without large gaps. I do not want to interpolate the missing data, as i am afraid that this would mask important information. \n\nIs there a way to still compare the data series? \n\n(I am not a statistician, so I would appreciate answers on a \"for dummies\" level, and any available online resources would be appreciated)",
    "author": "Frosty_Lawfulness_24",
    "timestamp": "2025-07-23T05:18:05",
    "url": "https://reddit.com/r/statistics/comments/1m77ovm/q_how_do_i_deal_with_gaps_in_my_time_series_data/",
    "score": 7,
    "num_comments": 9,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m7gvwl",
    "title": "[Q] What statistical test do I use?",
    "content": "I have some data points by zip code for my state (about 1500 zip codes). I have two variables I want to check for correlation. I can‚Äôt specify exactly what data I‚Äôm looking at because the data for one variable is from an academic partner and they haven‚Äôt published their methods yet and I don‚Äôt want to mention it before I publish.\n\nSo I‚Äôm going to give you some dummy variables that are similar. Let‚Äôs say for every zip code we have income categories ranked 1-5 and heart disease prevalence. What test do I use to determine if income category is correlated with heart disease prevalence by zip code? I used a t test but I‚Äôm still not confident that‚Äôs the best test to use.\n\nWhat if I also rank heart disease prevalence into categories of 1-5? So if I have ranked income and ranked heart disease prevalence by zip code, ranked 1-5?\n\nTIA!",
    "author": "lrlwhite2000",
    "timestamp": "2025-07-23T11:20:55",
    "url": "https://reddit.com/r/statistics/comments/1m7gvwl/q_what_statistical_test_do_i_use/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m69gx6",
    "title": "[Q] Why do we remove trends in time series analysis?",
    "content": "Hi, I am new to working with time series data. I dont fully understand why we need to de-trend the data before working further with it. Doesnt removing things like seasonality limit the range of my predictor and remove vital information?  I am working with temperature measurements in an environmental context as a predictor so seasonality is a strong factor.",
    "author": "Frosty_Lawfulness_24",
    "timestamp": "2025-07-22T02:25:36",
    "url": "https://reddit.com/r/statistics/comments/1m69gx6/q_why_do_we_remove_trends_in_time_series_analysis/",
    "score": 14,
    "num_comments": 15,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m6rmcd",
    "title": "[C] Help in Choosing a Path",
    "content": "Hello! I am an incoming BS Statistics senior in the Philippines and I need help deciding what masters program I should get into. I‚Äôm planning to do further studies in Sweden or anywhere in or near Scandinavia.\n\nSince high school, I‚Äôve been aiming to be a data scientist but the job prospects don‚Äôt seem too good anymore. I see in this site that the job market is just generally bad now so I am not very hopeful. \n\nBut I‚Äôd like to know what field I should get into or what kind of role I should pivot to to have even the tiniest hope of being competitive in the market. I‚Äôm currently doing a geospatial internship but I don‚Äôt know if GIS is in demand. My papers have been about the environment, energy, and sustainability. But these fields are said to be oversaturated now too.\n\nAny thoughts on what I should look into? Thank you!",
    "author": "onionsarecool",
    "timestamp": "2025-07-22T15:03:59",
    "url": "https://reddit.com/r/statistics/comments/1m6rmcd/c_help_in_choosing_a_path/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m6e94i",
    "title": "[Q] Kruskal-Wallis minimum amount of sample members in groups?",
    "content": "Hello everybody, I've been breaking my head about this and can't find any literature that gives a clear answer.\n\nI would like to know big my different sample groups should be for a Kruskal-Wallis test. I'm doing my masterthesis research about preferences in lgbt+bars (with Likert-scale) and my supervisor wanted me to divide respondents in groups based on their sexuality&amp;gender. However, based on the respondents I've got, this means that some groups would only have 3 members (example: bisexual men), while other groups would have around 30 members (example: homosexual men). This raises some alarm bells for me, but I don't have a statistics background so I'm not sure if that feeling is correct. Another thing is that this way of having many small groups makes it so that there would be a big number groups, so I fear the test will be less sensitive, especially for the \"post-hoc-test\" to see which of the groups differ, and that this would make some differences not statistically different in SPSS.\n\nOnline I've found the answer that a group should contain at least 5 members, one said at least 7, but others say it doesn't matter, as long as you have 2 members. I can't seem to find an academic article that's clear about this either. If I want to exclude the group of for example bisexual men as respondents I think I would need a clear justification for that, so that's why I'm asking here if anyone could help me figure this out.\n\nThanks in advance for your reply and let me know if I can clarify anything else.",
    "author": "Practical-Gear-7758",
    "timestamp": "2025-07-22T06:34:35",
    "url": "https://reddit.com/r/statistics/comments/1m6e94i/q_kruskalwallis_minimum_amount_of_sample_members/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m6pxq5",
    "title": "[Q] Small samples and examining temporal dynamics of change between multiple variables. What approach should I use?",
    "content": "Essentially, I am trying to run two separate analyses using longitudinal data: \n1. N=100, T=12 (spaced 1 week apart)\n2. N=100, T=5 (spaced 3 months apart)\n\nFor both, the aim is to examine bidirectional temporal dynamics in change between sleep (continuous variable) and 4 ptsd symptom clusters (each continuous). I think DSEM would be ideal given ability to parse within and between subjects effects, but based on what I‚Äôve read, N of 100 seems under-powered and it‚Äôs the same issue with traditional cross-lagged analysis. Am I better powered for a panel vector autoregression approach? Should I be reading more on network analysis approaches? Stumped on where to find more info about what methods I can use given the sample size limitation :/ \n\nThanks so much for any help!!",
    "author": "Complete-Nebula-6232",
    "timestamp": "2025-07-22T13:57:02",
    "url": "https://reddit.com/r/statistics/comments/1m6pxq5/q_small_samples_and_examining_temporal_dynamics/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m6p0zz",
    "title": "[Question] Is there a flowchart or sth. similar on what stats test to do when and how in academia?",
    "content": "Hey!\nTitle basically says it. I recently read discovering statistics using SPSS (and sex drugs and rockenroll) and it's great. However, what's missing for me, as a non maths academic, is a sort of flowchart of what test to do when, a step by step guide for those tests. I do understand more about these tests from the book now but that's a key takeaway I'm missing somehow.\n\nThanks very much. You're helping an academic who just wants to do stats right!\n\nBtw. Wasn't sure whether to tag this as question or Research, so I hope this fits.",
    "author": "Orovo",
    "timestamp": "2025-07-22T13:21:40",
    "url": "https://reddit.com/r/statistics/comments/1m6p0zz/question_is_there_a_flowchart_or_sth_similar_on/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.45,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m674uk",
    "title": "[DISCUSSION] Performing ANOVA with missing data (1 replication missing) in a Completely Randomized Design (CRD)",
    "content": "I'm working with a dataset under a Completely Randomized Design (CRD) setup and ran into a bit of a hiccup¬†**one replication is missing**¬†for one of my treatments. I know standard ANOVA assumes a balanced design, so I'm wondering how best to proceed when the data is¬†**unbalanced**¬†like this.",
    "author": "Emergency-Agency-373",
    "timestamp": "2025-07-21T23:53:38",
    "url": "https://reddit.com/r/statistics/comments/1m674uk/discussion_performing_anova_with_missing_data_1/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m57lxs",
    "title": "[E][Q] Should I be more realistic with the masters programs that I will be applying towards",
    "content": "Hello, everyone. This fall, I will be a senior studying data science at a large state school and applying to my master's program. My current GPA is 3.4. I am interning as a software engineer this summer in the marketing department of the company, which has given me some perspective into the areas of statistics I am interested in, specifically the design of experiments and time series. I have also been doing research in numerical analysis for the past seven months and astrophysics for a little over a year before that.\n\nThe first few semesters of my undergrad were rough for my math grade as I didn't know what I wanted to really do with my career, but my cs/ds courses were all A's and B's. Since then, almost all the upper division courses I've taken in math/stats/cs/ds have been A's and B's, except 2 of them. I have taken the standard courses: calc 1-3, linear algebra, intro to stats, probability, data structures and algorithms, etc. On top of those, I've done numerical methods, regression analysis, Bayesian stats, mathematical stats, predictive analytics, quantitative risk management, machine learning, etc, for some of my upper-level courses, and I have gotten A's and B's in these.\n\nI believe I can get some good letters of recommendation from 3 professors, and my mentor at my internship as well. But I am not sure if I am being unrealistic with the schools that I want to apply to. I have been looking through a good spread of programs and wanted to know if I am being too ambitious. Some of the schools are: UCSB,  UCSD, Purdue, Wake Forest, Penn State, University of Iowa, Iowa State, UIUC. I think that I should lower my ambitions and maybe apply to different programs. \n\nAny and all feedback is appreciated. Thank you in advance.",
    "author": "ChubbyFruit",
    "timestamp": "2025-07-20T19:50:17",
    "url": "https://reddit.com/r/statistics/comments/1m57lxs/eq_should_i_be_more_realistic_with_the_masters/",
    "score": 11,
    "num_comments": 9,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m5bvri",
    "title": "[R] I need help.",
    "content": "",
    "author": "Elegant-Ad9741",
    "timestamp": "2025-07-20T23:51:40",
    "url": "https://reddit.com/r/statistics/comments/1m5bvri/r_i_need_help/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m4qhl3",
    "title": "[Q] Bohling notes on Kriging, how does he get his data covariance matrix?",
    "content": "In Geoff Bohlings notes on Kriging, he has an example onnpage 32. There is a matrix of distances [km] between pairs of 6 data points:\n\n0000, 1897, 3130, 2441, 1400, 1265;\n1897, 0000, 1281, 1456, 1970, 2280;\n3130, 1281, 0000, 1523, 0000, 1970;\n2441, 1456, 1523, 0000, 1523, 1970;\n1400, 1970, 2800, 1523, 0000, 0447;\n1265, 2280, 3206, 1970, 0447, 0000;\n\n[I put 3 digits formatting here, e.g. 0000 = 0]\nThen he says the resultant data covariance matrix is:\n\n0.78, 0.28, 0.06, 0.17, 0.40, 0.43;\n0.28, 0.78, 0.43, 0.39, 0.27, 0.20;\n0.06, 0.43, 0.78, 0.37, 0.11, 0.06;\n0.17, 0.39, 0.37, 0.78, 0.37, 0.27;\n0.40, 0.27, 0.11, 0.37, 0.78, 0.65;\n0.43, 0.20, 0.06, 0.27, 0.65, 0.78;\n\nAny help on how he got that? interested in method as opposed to something from a program. TIA!\n",
    "author": "starvinggigolo",
    "timestamp": "2025-07-20T07:37:07",
    "url": "https://reddit.com/r/statistics/comments/1m4qhl3/q_bohling_notes_on_kriging_how_does_he_get_his/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m4nhkl",
    "title": "What is the best subfield of statistics for research? [R][Q]",
    "content": "I want to pursue statistics research at a university and they have several subdisciplines in their statistics department:\n\n1) Bayesian Statistics\n\n2) Official Statistics\n\n3) Design and analysis of experiments\n\n4) Statistical methods in the social sciences\n\n5) Time series analysis\n\n(note: mathematical statistics is excluded as that is offered by the department of mathematics instead).\n\nI'm curious, which of the above subdisciplines have the most lucrative future and biggest opportunities in research? I am finishing up my bachelors in econometrics and about to pursue a masters in statistics then a PhD in statistics at Stockholm University.\n\nI'm not sure which subdiscipline I am most interested in, I just know I want to research something in statistics with a healthy amount of mathematical rigour.\n\nAlso is it true time series analysis is a dying field?? I have been told this by multiple people. No new stuff is coming out supposedly.",
    "author": "gaytwink70",
    "timestamp": "2025-07-20T05:16:22",
    "url": "https://reddit.com/r/statistics/comments/1m4nhkl/what_is_the_best_subfield_of_statistics_for/",
    "score": 3,
    "num_comments": 25,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m4dq46",
    "title": "[Q] [C] career options for a stats degree?",
    "content": "First time posting here, so hopefully I got the flairs correct!\n\nI graduated with a bachelors in statistics\nand, after realizing many jobs seemed to necessitate a masters, jumped straight into grad school. I am now one year away from graduating with my masters, and am wondering if anything has improved? What are careers that a statistic degree could mesh well with? Just feeling unsure in my decisions and looking for some options! For context, my masters will be in data engineering &amp; analytics. ",
    "author": "xiening",
    "timestamp": "2025-07-19T19:25:10",
    "url": "https://reddit.com/r/statistics/comments/1m4dq46/q_c_career_options_for_a_stats_degree/",
    "score": 12,
    "num_comments": 11,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m495ar",
    "title": "Almudevar's Theory of Statistical Inference [Q]",
    "content": "Is anyone here familiar with [Anthony Almudevar‚Äôs Theory of Statistical Inference](https://www.routledge.com/Theory-of-Statistical-Inference/Almudevar/p/book/9780367502805?srsltid=AfmBOopzjPKhWFhlJD8u8tFTwYmScGuiDiZUwEtPJlvpaDZYZpTk1fcp)?  \n\nIt‚Äôs a relatively recent book ‚Äî not too long ‚Äîbut it manages to cover a wide range of statistical inference topics with solid mathematical rigor. It reminds me somewhat of Casella &amp; Berger, but the pace is quicker and it doesn't shy away from more advanced mathematical tools like measure theory, metric spaces, and even some group theory. At the same time, it's not as terse or dry as Keener‚Äôs book, which I found beautiful but hard to engage with.  \n\nFor context: I have a strong background in pure mathematics (functional analysis and operator theory), holding both a bachelor‚Äôs and a master‚Äôs degree, and some PhD level courses under my belt as well. I'm now teaching myself mathematical statistics with a view toward a career in data science and possibly a PhD in applied math or machine learning.  \n\nI'm currently working through Casella &amp; Berger (as well as more applied texts like ISLP and Practical Statistics for Data Scientists), but I find C&amp;B somewhat slow and bloated for self-study. My plan is to shift to Almudevar as a main reference and use C&amp;B as a complementary source.  \n\nHas anyone here studied Almudevar‚Äôs book or navigated similar resources? I‚Äôd greatly appreciate your insights ‚Äî especially on how it compares in practice to more traditional texts like C&amp;B.\n\nThanks in advance!",
    "author": "complexanalysisbr",
    "timestamp": "2025-07-19T15:40:00",
    "url": "https://reddit.com/r/statistics/comments/1m495ar/almudevars_theory_of_statistical_inference_q/",
    "score": 23,
    "num_comments": 3,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m4i2cs",
    "title": "Which statistical test should I use to compare the sensitivity of two screening tools in a single sample population? [Q]",
    "content": "Hi all,\n\nI hope it's alright to ask this kind of question on the subreddit, but I'm trying to work out the most appropriate statistical test to use for my data.\n\nI have one sample population and am comparing a screening test with a modified version of the screening test and want to assess for significance of the change in outcome (Yes/No). It's  a retrospective data set in which all participants are actually positive for the condition\n\nChatGPT suggested the McNemar test but from what I can see that uses matched case and controls. Would this be appropriate for my data?\n\nIf so, in this calculator ([McNemar Calculator](https://www.graphpad.com/quickcalcs/mcnemar1/)), if I had 100 participants and 30 were positive for the screening and 50 for the modified screening (the original 30+20 more), would I juat plumb in the numbers with the \"risk factor\" refering to having tested positive in each screening tool..?\n\nI'm sorry if this seems silly, I'm a bit out of my depth üò≠ Thank you!",
    "author": "baylo99",
    "timestamp": "2025-07-19T23:34:47",
    "url": "https://reddit.com/r/statistics/comments/1m4i2cs/which_statistical_test_should_i_use_to_compare/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m455n2",
    "title": "[Q] Figuring Out Pairs for Game Tournament",
    "content": "I am having a BBQ and game tournament tomorrow with 16 friends, but they are put into pairs, so 8 \"teams\". Each team needs to play all 5 games during 5 blocks of time, and will always be paired with another team at each game, so one game will be unplayed during each block. I have been messing with the pairings for a while, and cannot figure out how to make it so each team only plays each game once, and teams are never paired with the same oppenent team twice. Is this possible?",
    "author": "robswins",
    "timestamp": "2025-07-19T12:44:36",
    "url": "https://reddit.com/r/statistics/comments/1m455n2/q_figuring_out_pairs_for_game_tournament/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m491kf",
    "title": "[Discussion] Texas Hold 'em probability problem",
    "content": "I'm trying to figure out how to update probabilities of certain hands in Texas Hold 'em adjusted to the previous round. For example, if I draw mismatched cards, what are the odds that I have one pair after the flop? It seems to me that there are two scenarios: 3 unique cards with one matching rank with a card in the draw, or a pair with no cards in common rank with the draw, like this:\n\nDraw: a-b Flop: a-c-d or c-c-d\n\nMy current formula is \\[C(2 1)\\*C(4 2)\\*C(11 2)\\*C(4 1)\\*C(4 1) + C(11 1)\\*C(4 2)\\*C(10 1)\\*C(4 1)\\]/C(50 3)\n\nYou have one card matching rank with one of the two draw cards, (2 1), 3 possible suits (4 2), then two cards of unlike value (11 2) with 4 possible suits for each (4 1)\\*(4 1). Then, the second set would be 11 possible ranks (11 1) with 3 combinations of suits (4 2) for 2 cards with the third card being one of 10 possible ranks and 4 possible suits (10 1)(4 1). Then divide by the entire 3 cards chosen from 50 (50 3). I then get a 67% odds of improving to a pair on the flop from different rank cards in the hole.\n\nIf that does not happen and the cards read a-b-c-d-e, I then calculate the odds of improving to a pair on the turn as: C(5 1)\\*C(4 2)/C(47,1). To get a pair on the turn, you need to match rank with one of five cards, which is the (5 1) with three potential suits, (4 2), divided by 47 possible choices (47 1). This is then a 63% chance of improving to a pair on the turn.\n\nThen, if you have a-b-c-d-e-f, getting a pair on the river would be 6 possible ranks, (6 1), 3 suits, (4 2), divided by 46 possible events. C(6 1)\\*C(4 2)/C(46 1), with a 78% chance of improving to a pair on the river.\n\nThis result does not feel right, does anyone know where/if I'm going wrong with this? I haven't found a good source that explains how this works. If I recall from my statistics class a few years ago, each round of dealing would be an independent event.",
    "author": "kerbalcowboy",
    "timestamp": "2025-07-19T15:35:11",
    "url": "https://reddit.com/r/statistics/comments/1m491kf/discussion_texas_hold_em_probability_problem/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m3tnyt",
    "title": "[Q] Statistics nomenclature question for Slavic speaking statisticians",
    "content": "Hi,\n\nSorry if this belongs in r/linguistics and happy for Admin to delete if so.\n\nI‚Äôm curious why in Slavic languages we use ‚Äú**sredne/—Å—Ä–µ–¥–Ω–æ**-–∞—Ä–∏—Ç–º–µ—Ç–∏—á–Ω–æ‚Äù (literally \"middle arithmetical\") for the mean, but use a loanword for median (–º–µ–¥–∏–∞–Ω–∞).\n\nIt feels counterintuitive, since \"—Å—Ä–µ–¥–Ω–æ\" means \"in the middle\", and by that logic, it would make more sense to call the median \"—Å—Ä–µ–¥–Ω–∞ —Å—Ç–æ–π–Ω–æ—Å—Ç\" or something similar. Just like in Latin Median is derived from Middle.\n\nI often see this cause confusion, especially when stats are quoted in media without context. People assume \"—Å—Ä–µ–¥–Ω–æ\" means \"typical\" or \"middle\", but it‚Äôs actually the arithmetic mean.\n\nSo why did we end up with this naming? Was it a conscious decision or just a historical quirk?\n\nCouldn‚Äôt it have gone the other way - creating a word based on \"—Å—Ä–µ–¥–Ω–æ\" for median and borrowing a word for mean instead?\n\nWould love to hear if anyone knows the background.",
    "author": "Fancy-Persimmon9660",
    "timestamp": "2025-07-19T04:12:53",
    "url": "https://reddit.com/r/statistics/comments/1m3tnyt/q_statistics_nomenclature_question_for_slavic/",
    "score": 3,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m3d5bn",
    "title": "[C] Graduating next year without internship or projects. What can I do to secure a job out of college?",
    "content": "Hello! I am currently an undergraduate statistics student that will be graduating the following year (Spring 2026) and I am absolutely screwed. \n\nFor some context, I wasn‚Äôt rushed to find an internship until I realized that I will be graduating a year early with the number of credits I have. I tried to apply to many places using handshake but didn‚Äôt get a response back. And now it is almost the end of summer break before my senior year and I have nothing but four years of cashier experience. I focused on my academics and currently have a 3.9 GPA. But I have no personal project nor a strong background in coding. I found it so awkward to talk to my professors and I don‚Äôt have many friends either (so I lack the connections).\n\nMy question is; what can I do now to allow me to possibly get a job after graduation? I want to get into data analytics or another related field like finance. I realize that I am actually, extremely, ginormously, majorly done for. I don‚Äôt have anyone else to blame but myself. I don‚Äôt have a plan and I don‚Äôt know how anything works. (ie. Like what exactly is the end goal for a project or where to find the data?)\n\nAt the end of the day, I‚Äôm just panicking and I hope things eventually work out. Any advice on what to do moving forward would be helpful! Thank you! ",
    "author": "2ihui",
    "timestamp": "2025-07-18T13:31:39",
    "url": "https://reddit.com/r/statistics/comments/1m3d5bn/c_graduating_next_year_without_internship_or/",
    "score": 23,
    "num_comments": 12,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m3ih03",
    "title": "[Career] Has anyone interviewed at Jsm? How does it work?",
    "content": "Do you message the companies listed on the portal? Or do they message you? I messaged a few over the past few weeks and heard nothing back. The conference is in two weeks. Thanks!",
    "author": "Maleficent-Seesaw412",
    "timestamp": "2025-07-18T17:22:57",
    "url": "https://reddit.com/r/statistics/comments/1m3ih03/career_has_anyone_interviewed_at_jsm_how_does_it/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m2wyaz",
    "title": "[R] Can we use 2 sub-variables (X and Y) to measure a variable (Q), where X is measured through A and B while Y is measured through C? A is collected through secondary sources (population), while B and C are collected through a primary survey (sampling).",
    "content": "I am working on a study related to startups. Variable Q is our dependent variable, which is \"women-led startups\". It is measured through X and Y, which are Growth and performance, respectively. X (growth) is measured through A and B (employment and investment acquired), where A (employment) is collected through secondary sources and comprises the data of the entire population, while B (investment acquired) is collected through survey (primary data) of the sample (sampling). Similarly Y (performance) is measured through C (turn-over) which is also collected through primary method (sampling).\n\nI am not sure whether this is the correct approach or not? Can we collect the data from both primary and secondary to measure a variable. If then how do we need to process the data make it fit so as to be compatible with each other (primary and secondary).\n\nPS: If possible, please provide any refrence to support your opinion. That would be of immense help.   \nThank you!",
    "author": "cd_nikiki",
    "timestamp": "2025-07-18T01:35:51",
    "url": "https://reddit.com/r/statistics/comments/1m2wyaz/r_can_we_use_2_subvariables_x_and_y_to_measure_a/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m2xxlg",
    "title": "[Q] how to make a tournament with those conditions",
    "content": "Tournament triathlon:\n\nRules :\n- 21 players \n- 63 rounds in total ( 21 beer pong / 21 ping pong / 21 p√©tanque )\n- Each player plays 12 rounds in total ( 4 of each sport )\n- Each round is a 2v2 \n- Each round the teams of 2v2 are random and redraw from the poll of 21 players\n- Each round, the 3 sports are playing at the same time ( 12 players each round on the battlefield )\n\n\nPlease help me, I tried everything with friends, chatgpt, nobody can solve it and my tournament is tomorrow",
    "author": "Flushy_",
    "timestamp": "2025-07-18T02:39:57",
    "url": "https://reddit.com/r/statistics/comments/1m2xxlg/q_how_to_make_a_tournament_with_those_conditions/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m28y3t",
    "title": "[Q] I need help on how to design a mixed effect model with 5 fixed factors",
    "content": "I'm completely new to mixed-effects models and currently struggling to specify the equation for my lmer model.\n\nI'm analyzing how reconstruction method and resolution affect the volumes of various adult brain structures.\n\n# Study design:\n\n* Fixed effects:\n   * `method` (3 levels; *within-subject*)\n   * `resolution` (2 levels; *within-subject*)\n   * `diagnosis` (2 levels: healthy vs pathological; *between-subjects*)\n   * `structure` (7 brain structures; *within-subject*)\n   * `age` (continuous covariate)\n* Random effect:\n   * `subject` (100 individuals)\n\nAll fixed effects are essential to my research question, so I cannot exclude any of them.  \nHowever, I'm unsure how to build the model. As far as I know just multypling all of the factors creates too complex model.  \nOn the other hand, I am very interested in exploring the key interactions between these variables. Pls help &lt;3",
    "author": "PatternMysterious550",
    "timestamp": "2025-07-17T07:25:34",
    "url": "https://reddit.com/r/statistics/comments/1m28y3t/q_i_need_help_on_how_to_design_a_mixed_effect/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m1o4ce",
    "title": "[question] trying to determine if my data is univariate or multivariate",
    "content": "Hi everyone,\nApologies for such a basic question but, if \nI‚Äôm conducting statistical analysis on a stability study where the concentration of 1 analyte is measured at multiple time points for multiple batches, would this be considered univariate or multivariate?\n\nI‚Äôm struggling to categorise this because on one hand the only measured variable is concentration and the time points act as a factor, but on the other hand, I‚Äôm looking at the relationship between time points act and concentration so it may be bivariate/ multivariate?\n\n",
    "author": "Combustion77",
    "timestamp": "2025-07-16T13:48:25",
    "url": "https://reddit.com/r/statistics/comments/1m1o4ce/question_trying_to_determine_if_my_data_is/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m1f2th",
    "title": "[Question] Need some help with Bayesian analysis",
    "content": "I need help choosing priors for a Bayesian regression. I have around 3 predictors and a fairly small sample size (N = 27). I‚Äôm quite familiar with the literature on my topic, so I have a good idea of how the dependent variable typically responds to certain effects, based on previous research.\n\nGiven this context, how should a choose priors.? Would it be appropriate to use weakly informative priors? I‚Äôm feeling a bit lost and would appreciate some guidance.",
    "author": "Historical_Shame1643",
    "timestamp": "2025-07-16T08:08:26",
    "url": "https://reddit.com/r/statistics/comments/1m1f2th/question_need_some_help_with_bayesian_analysis/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m1cy4x",
    "title": "[Q] How do you decide on adding polynomial and interaction terms to fixed and random effects in linear mixed models?",
    "content": "I am using a LMM to try to detect a treatment effect in longitudinal data (so basically hypothesis testing). However, I ran into some issues that I am not sure how to solve. I started my model by adding treatment and treatment-time interaction as a fixed effect, and subject intercept as a random effect. However, based on how my data looks, and also theory, I know that the change over time is not linear (this is very very obvious if I plot all the individual points). Therefore, I started adding polynomial terms, and here my confusion begins. I thought adding polynomial time terms to my fixed effects until they are significant (p &lt; 0.05) would be fine, however, I realized that I can go up very high polynomial terms that make no sense biologically and are clearly overfitting but still get significant p values. So, I compromised on terms that are significant but make sense to me personally (up to cubic), however, I feel like I need better justification than ‚Äúthat made sense to me‚Äù. In addition, I added treatment-time interactions to both the fixed and random effects, up to the same degree, because they were all significant (I used likelihood ratio test to test the random effects, but just like the other p values, I do not fully trust this), but I have no idea if this is something I should do. My underlying though process is that if there is a cubic relationship between time and whatever I am measuring, it would make sense that the treatment-time interaction and the individual slopes could also follow these non-linear relationships. \n\nI also made a Q-Q plot of my residuals, and they were quite (and equally) bad regardless of including the higher polynomial terms. \n\nI have tried to search up the appropriate way to deal with this, however, I am running into conflicting information, with some saying just add them until they are no longer significant, and others saying that this is bad and will lead to overfitting. However, I did not find any protocol that tells me objectively when to include a term, and when to leave it out. It is mostly people saying to add them if ‚Äúit makes sense‚Äù or ‚Äúmakes the model better‚Äù but I have no idea what to make of that.\n\nI would very much appreciate if someone could advise me or guide me to some sources that explain clearly how to proceed in such situation. I unfortunately have very little background in statistics. \n\nAlso, I am not sure if it matters, but I have a small sample size (around 30 in total) but a large amount of data (100+ measurements from each subject).",
    "author": "Csicser",
    "timestamp": "2025-07-16T06:43:35",
    "url": "https://reddit.com/r/statistics/comments/1m1cy4x/q_how_do_you_decide_on_adding_polynomial_and/",
    "score": 7,
    "num_comments": 28,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m1fdb3",
    "title": "How does a link between outcomes constrains the correlation between their corresponding causal variants? [Question]",
    "content": "Assume the following diagram\n\n    X &lt;----&gt; Y\n    |        |\n    C        G\n\nWhere C-&gt;X (with correlation alpha), G-&gt;Y (with correlation gamma) and X and Y are directly linked (with correlation beta).\n\nCan I establish boundaries for the r(C, G) correlation? Using the fact that the correlation matrix is positive semi-definite?\n\n    [1,      phi,    alpha,         ?],\n    [phi,    1,          ?,     gamma],\n    [alpha,  ?,          1,      beta],\n    [?,      gamma,   beta,         1]\n\nperhaps assuming linearity?\n\n    [1,                     phi,        alpha, alpha * beta],\n    [phi,                     1, gamma * beta,        gamma],\n    [alpha,        gamma * beta,            1,         beta],\n    [alpha * beta,        gamma,         beta,            1] \n\nI think this is similar to [this](https://stats.stackexchange.com/questions/254282/completing-a-3-times-3-correlation-matrix-2-coefficients-of-the-3-given) question, but extended because now I don't have this diagram: C -&gt; X &lt;- G, but a slightly more complex one.",
    "author": "_quantum_girl_",
    "timestamp": "2025-07-16T08:19:30",
    "url": "https://reddit.com/r/statistics/comments/1m1fdb3/how_does_a_link_between_outcomes_constrains_the/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m1b3nb",
    "title": "[Q] auto-correlation in time series data",
    "content": "Hi! I have a time series dataset, measurement x and y in a specific location over a time frame. When analyzing this data, I have to (somehow) account for auto-correlation between the measurements. \n\nDoes this still apply when I am looking at the specific effect of x on y, completely disregarding the time variable?\n\n",
    "author": "Frosty_Lawfulness_24",
    "timestamp": "2025-07-16T05:19:44",
    "url": "https://reddit.com/r/statistics/comments/1m1b3nb/q_autocorrelation_in_time_series_data/",
    "score": 1,
    "num_comments": 12,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m0ywro",
    "title": "[Discussion] Help identifying a good journal for an MS thesis",
    "content": "Howdy, all! I'm a statistics graduate student, and I'm looking at submitting some research work from my thesis for publication. The subject is a new method using PCA and random survival forests, as applied to Alzheimer's data, and I was hoping to get any impressions that anyone might be willing to offer about any of these journals that my advisor recommended:\n\n1. Journal of Applied Statistics\n2. Statistical Methods in Medical Research\n3. Computational Statistics &amp; Data Analysis\n4. Journal of Statistical Computation and Simulation\n5. Journal of Alzheimer's Disease\n\n",
    "author": "JohnPaulDavyJones",
    "timestamp": "2025-07-15T17:45:36",
    "url": "https://reddit.com/r/statistics/comments/1m0ywro/discussion_help_identifying_a_good_journal_for_an/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m0m8z6",
    "title": "Can someone help me decipher these stats? My 2 year old son has had 2 brain CTs in his lifetime and I think this study is saying he has a 53% increased risk of cancer with just one CT, but I know I‚Äôm not reading this correctly. [discussion]",
    "content": "https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2832778\n",
    "author": "Ok_Bug_9921",
    "timestamp": "2025-07-15T09:22:44",
    "url": "https://reddit.com/r/statistics/comments/1m0m8z6/can_someone_help_me_decipher_these_stats_my_2/",
    "score": 18,
    "num_comments": 10,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m0ktzh",
    "title": "[Discussion] Looking for reference book recommendations",
    "content": "I'm looking for recommendations on books that comprehensively focus on details of various distributions. For context, I don't have access to the Internet at work, but I have access to textbooks. If I did have access to the internet, wikipedia pages such as [this](https://en.m.wikipedia.org/wiki/Dirichlet_distribution) would be the kind of detail I'd be looking for.\n\nSome examples of things I would be looking for\n- tables of distributions\n- relationships between distributions\n- integrals and derivatives of PDFs\n- properties of distributions\n- real world examples of where these distributions show up\n- related algorithms (maybe not all of the details, but perhaps mentions or trivial examples would be good)\n\nI have some solid books on probability theory and statistics. I think what is generally missing from those books is a solid reference for practitioners to go back and refresh on details.",
    "author": "Possibility_Antique",
    "timestamp": "2025-07-15T08:29:27",
    "url": "https://reddit.com/r/statistics/comments/1m0ktzh/discussion_looking_for_reference_book/",
    "score": 5,
    "num_comments": 9,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m08b33",
    "title": "what is the meaning of 8 percent in the p-value contest?[D][Q]",
    "content": "Two weeks ago, the interviewer asked me this question in an interview: and finally they rejected me, but I want to learn this. Here is the question:\n\n    suppose you want to test two hypotheses. The first is that the population mean is 100,\n    and the alternative hypothesis is that the population mean is greater\n    than 100. Let's say you sample some data, and you obtain a\n    p-value of 0.08. So now you need to go back to, \n    your cross-functional stakeholders and say, the p-value is %8, so\n    what is the meaning of 8% in this context?\n\nWhat they want to hear in this situation? also, english is not my first language and providing the well structured answer is so hard for me. Could you please help me to learn this? thank you",
    "author": "Designer_Grocery2732",
    "timestamp": "2025-07-14T21:23:45",
    "url": "https://reddit.com/r/statistics/comments/1m08b33/what_is_the_meaning_of_8_percent_in_the_pvalue/",
    "score": 6,
    "num_comments": 20,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m0ar6c",
    "title": "[Q]Need Explanation",
    "content": "Can anyone explain this to me, it's something we use in our reports:\n\nThe first image is an MS Excel Add-in, and the second image is how we report it.\n\n[https://imgur.com/a/VxKwm9t](https://imgur.com/a/VxKwm9t)\n\nShouldn't the margin of error and the confidence level, always total 100%?",
    "author": "Other_Candidate_5079",
    "timestamp": "2025-07-14T23:47:26",
    "url": "https://reddit.com/r/statistics/comments/1m0ar6c/qneed_explanation/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m02cjv",
    "title": "Probability Question [D]",
    "content": "Hi, I am trying to figure out the following: I am in a state that assigns vehicles tags that each have three letters and four numbers. I feel like I keep seeing four particular digits (7,8,6,and 4) very often. I‚Äôm sure I‚Äôm just now looking for them and so noticing them more often, like when you buy a car and then suddenly keep seeing that model. But it made me wonder how many combinations of those four digits are there between 0000 and 9999? I‚Äôm sure it‚Äôs easy to figure out but I was an English major lol.",
    "author": "rosie134134",
    "timestamp": "2025-07-14T16:38:50",
    "url": "https://reddit.com/r/statistics/comments/1m02cjv/probability_question_d/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lzqefh",
    "title": "[E] Central Limit Theorem - Explained",
    "content": "Hi there,\n\nI've created a video¬†[here](https://youtu.be/nAfjT-tWWzc)¬†where I explain the central limit theorem and why the normal distributions appear everywhere in nature, statistics, and data science\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)",
    "author": "Personal-Trainer-541",
    "timestamp": "2025-07-14T09:04:12",
    "url": "https://reddit.com/r/statistics/comments/1lzqefh/e_central_limit_theorem_explained/",
    "score": 8,
    "num_comments": 1,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lzuhdr",
    "title": "[Q] How to get marginal effects for ordered probit with survey design in R?",
    "content": "I'm working on an ordered probit regression that doest meet the proportional odds criteria using complex survey data. The outcome variable has three ordinal levels: no, mild, and severe. The problem is that packages like `margins` and `margineffects`don't support `svy_vgam`. Does anyone know of another package or approach that works with survey-weighted ordinal models?",
    "author": "ThrowRA_dianesita",
    "timestamp": "2025-07-14T11:34:01",
    "url": "https://reddit.com/r/statistics/comments/1lzuhdr/q_how_to_get_marginal_effects_for_ordered_probit/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1m0dfz6",
    "title": "Would econometrics and machine learning units count as equivalent to statistics for Statistics masters? [E]",
    "content": "As the question asks, my masters program requires a number of credits in \"statistics or equal\". Would econometrics, predictive modelling, data analytics, neural networks, survey sampling, etc. be counted as equal to statistics?\n\nWhat about pure math units (calculus, linear algebra, discrete math)? Would those be counted?\n\nThis university has another program in mathematical statistics that requires credits specifically in mathematical statistics. So they differentiate between mathematical statistics and statistics.\n\nThe program im applying for is more practical, with R programming, experimental design, etc. in the syllabus (of course with core courses in probability, inference theory, etc).\n\nThe program im applying for is in Sweden",
    "author": "gaytwink70",
    "timestamp": "2025-07-15T02:42:18",
    "url": "https://reddit.com/r/statistics/comments/1m0dfz6/would_econometrics_and_machine_learning_units/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.21,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lznaut",
    "title": "[Q] How do I best explore the relationships within a long term data series?",
    "content": "I have two long term data series which I want to compare. One is temperature and the other is a biological temperature dependent variable (Var1). Measurements span about ten years, with temperature being sampled on a work-daily schedule, and Var1 being measured twice a week. Now there are gaps in the data, as it is bound to happen with such long term biological measurements.\n\nThe relationship between Temp and Var1 looks quadratic, but I want to look at specific temperature events and how quick the effect is/ how long it lasts/ etc.\n\nDoes anyone have any idea what analysis would work best for this?",
    "author": "Frosty_Lawfulness_24",
    "timestamp": "2025-07-14T07:05:58",
    "url": "https://reddit.com/r/statistics/comments/1lznaut/q_how_do_i_best_explore_the_relationships_within/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lz0k6t",
    "title": "[Question] Do variable random sizes tend toward even?",
    "content": "I have a question/scenario. Let's say I'm running a small business, and I'm donating 20% of profit to either Charity A or Charity B, buyer's choice. Would it be acceptable for me to just tally the number of people choosing each option, or should I include the amount of the purchase? Meaning, if my daily sales are $1,000, and people chose Charity B over Charity A at a rate of 65-35, would it be close enough to donate $130 and $70, respectively, with the belief that the actual sales will even out over time? I believe that the answer is yes, as the products would have set prices. However, what if it is a \"pay what you want\" business? For instance, an artist collecting donations for their work, or a band collecting concert donations. Would unset donations also even out? (Ex. Patron X donates $80 and selects Charity A and Patron Y donates $5 and selects Charity B, but as we see, at the end of the day B is outpacing A 65-35.) Over enough days, would tallying the simple choice and splitting the total profits suffice? Thanks for any help.\n\nEdit: I made a damn typo in the title. Meant to say \"trend.\"",
    "author": "streetsofarklow",
    "timestamp": "2025-07-13T11:51:05",
    "url": "https://reddit.com/r/statistics/comments/1lz0k6t/question_do_variable_random_sizes_tend_toward_even/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lypftk",
    "title": "[R] Toto: A Foundation Time-Series Model Optimized for Observability Data",
    "content": "Datadog open-sourced¬†*Toto*¬†(Time Series Optimized Transformer for Observability), a model purpose-built for observability data.\n\nToto is currently the most extensively pretrained time-series foundation model: The pretraining corpus contains 2.36 trillion tokens, with¬†\\~70%¬†coming from Datadog‚Äôs private telemetry dataset.\n\nAlso, the model uses a composite Student-T mixture head to capture the heavy tails in observability time-series data. \n\nToto currently ranks 2nd in the GIFT-Eval Benchmark.\n\nYou can find an analysis of the model¬†[here](https://aihorizonforecast.substack.com/p/toto-a-foundation-time-series-model).",
    "author": "nkafr",
    "timestamp": "2025-07-13T03:19:43",
    "url": "https://reddit.com/r/statistics/comments/1lypftk/r_toto_a_foundation_timeseries_model_optimized/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lyecxj",
    "title": "[Q] Are (AR)I(MA) models used in practice ?",
    "content": "Why are ARIMA models considered \"classics\" ? did they show any useful applications or because their nice theoretical results ?",
    "author": "al3arabcoreleone",
    "timestamp": "2025-07-12T16:35:12",
    "url": "https://reddit.com/r/statistics/comments/1lyecxj/q_are_arima_models_used_in_practice/",
    "score": 14,
    "num_comments": 14,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lyfybt",
    "title": "Which course should I take? Multivariate Statistics vs. Modern Statistical Modeling? [Discussion]",
    "content": "",
    "author": "Novel_Arugula6548",
    "timestamp": "2025-07-12T17:54:30",
    "url": "https://reddit.com/r/statistics/comments/1lyfybt/which_course_should_i_take_multivariate/",
    "score": 6,
    "num_comments": 30,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lybz8f",
    "title": "[Q] Is this curriculum worthwhile?",
    "content": "I am interested in majoring in statistics and I think the data science side is pretty cool, but I‚Äôve seen a lot of people claim that data science degrees are not all that great. I was wondering if the University of Kentucky‚Äôs curriculum for this program is worthwhile. I don‚Äôt want to get stuck in the data science major trap and not come out with something valuable for my time invested.\n\nhttps://www.uky.edu/academics/bachelors/college-arts-sciences/statistics-and-data-science#:~:text=The%20Statistics%20and%20Data%20Science,all%20pre%2Dmajor%20courses).",
    "author": "Firm-Feedback-6648",
    "timestamp": "2025-07-12T14:44:18",
    "url": "https://reddit.com/r/statistics/comments/1lybz8f/q_is_this_curriculum_worthwhile/",
    "score": 3,
    "num_comments": 12,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lygcc8",
    "title": "[Q] How do I write a report in this situation? (Please check the description)",
    "content": "Suppose there are different polls:\n\n1. Which one of these apocalypses are likely to end the world?\n\n- options like zombies, flu, etc.\n- 958 respondants.\n\n2. How prepared are you for any apocalypse situation?\n\n- options like most prepared, normal, least prepared, etc.\n- 396 respondants.\n\nNow all respondants are from the same community, but they are anonymous. There's no way to know which ones are the same ones and which ones are different.\n\nNow I want both polls results to fit into one single data report, with some title that says \"People's views on apocalypse\" (for example). How do I make this happen? Is it fair to include both poll results from different respondants into one data report?",
    "author": "kev_world",
    "timestamp": "2025-07-12T18:14:00",
    "url": "https://reddit.com/r/statistics/comments/1lygcc8/q_how_do_i_write_a_report_in_this_situation/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lxkuq3",
    "title": "[Q] how exactly does time series linear regression with covariates work?",
    "content": "I haven't found any good resources explaining the basics of this concept, but in linear regressive models involving time series lags as covariates, how are the following assumptions theoretically met?\n\n1. The covariates (some) aren't completely independent since I might take more than one lagged covariates.\n\n2. As a result the error does not become iid distributed. \n\nSo how does one circumvent this problem?",
    "author": "[deleted]",
    "timestamp": "2025-07-11T15:51:17",
    "url": "https://reddit.com/r/statistics/comments/1lxkuq3/q_how_exactly_does_time_series_linear_regression/",
    "score": 9,
    "num_comments": 7,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lx5jme",
    "title": "[Q] How to better assess my Data Set given an objective.",
    "content": "I have this data set. I have a data on the number of project proposals each institutions has submitted from 2020-2024. The data looks like this\n\n|Institution|2020|2021|2022|2023|2024|2025|\n|:-|:-|:-|:-|:-|:-|:-|\n|A|0|0|1|5|3|1|\n|B|12|17|11|16|12|9|\n|C|0|2|2|0|1|0|\n|D|0|2|0|0|3|2|\n|E|3|0|0|1|2|5|\n|F|3|0|0|0|0|0|\n\nI've made an intervention on 2025 to help them increase their submissions. I have a target of 25% increase in submitted proposals due to the intervention.\n\n**What I tried:**  I've tried linear regression to determine the targeted output for 2025 of each institution. y=mx+b .... Then I calculated the percent deviation from the Actual submissions on 2025 to the expected output and checked if it exceeded 25%. However, I am having doubts with this method (as observed in the table data is inconsistent). Are there any approaches I should take? or will the linear progression be enough?\n\nThank you in advance.",
    "author": "Born_Confidence1786",
    "timestamp": "2025-07-11T05:25:40",
    "url": "https://reddit.com/r/statistics/comments/1lx5jme/q_how_to_better_assess_my_data_set_given_an/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lwjf17",
    "title": "[D] Grad school vs no grad school",
    "content": "Hi everyone, I am an incoming sophomore in college and after taking 2120: intro to statistical application, the intro stats class I loved it and decided I want to major in it, at my school how it works is there is both a BA and BS in stats, essentially, BA is applied stats BS is more theoretical stats (you take MV calc and linear algebra in addition to calc 1 and 2), BA is definitely the route I want. However, I‚Äôve noticed through this sub so many people are getting a masters or doctorates in Statistics, that isn‚Äôt really something I think I would like to do, nor if I could even survive that, but is it a path that is necessary in this field? I see myself working in data analyst roles interpreting data for a company and communicating to people what it means and how to change and adapt based on it. Any advice would be useful , thx",
    "author": "Global-Hat-1139",
    "timestamp": "2025-07-10T10:56:12",
    "url": "https://reddit.com/r/statistics/comments/1lwjf17/d_grad_school_vs_no_grad_school/",
    "score": 6,
    "num_comments": 6,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lwe8ag",
    "title": "[E] Degrees of Freedom - Explained",
    "content": "Hi there,\n\nI've created a video¬†[here](https://youtu.be/XKQvAM1N3vc)¬†where I break down the concept of degrees of freedom in statistics through a geometric lens, exploring how residuals and mean decomposition reveal the underlying mathematical structure.\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)",
    "author": "Personal-Trainer-541",
    "timestamp": "2025-07-10T07:31:55",
    "url": "https://reddit.com/r/statistics/comments/1lwe8ag/e_degrees_of_freedom_explained/",
    "score": 6,
    "num_comments": 2,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lwn7p9",
    "title": "[R] Theoretical (probabilistic) bounds on error for L1 and L2 regularization?",
    "content": "I'm wondering if there are any theoretical results giving probabilistic bounds the error when using L1 and/or L2 regularization on top of linear regression. Here's what I mean.\n\nLet's say we assume that we get tabular data with p explanatory variables (`x_1, ..., x_p`¬†)and one outcome variable (`y`) and we get n data points where each data point is drawn IID from some distribution¬†`D`¬†such that that for each data point,\n\n`y = c_1 x_1 + ... + c_p x_p + err`\n\nwhere the err are IID from some distribution¬†`E`.\n\nAre there any results showing that if¬†`D`,¬†`E`,¬†`p`, and¬†`n`¬†meet certain conditions (I'm not sure what they would be) and if we estimate the¬†`c_i`¬†using L1 or L2 regularization with linear regression, then with some high probability, the estimates of the¬†`c_i`¬†will not be too different from the real¬†`c_i`?",
    "author": "paradoxinmaking",
    "timestamp": "2025-07-10T13:25:14",
    "url": "https://reddit.com/r/statistics/comments/1lwn7p9/r_theoretical_probabilistic_bounds_on_error_for/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "statistics",
    "post_id": "1lwc8a1",
    "title": "[Question] Very Basic Statistics Question",
    "content": "\n\nI'm not sure this is the right sub for this, but I have searched and searched various textbooks, course data, and the internet and I feel like I'm still not coming to a solid conclusion even though this is very basic level statistics.\n\nI am working on an assignment that has us working through hypothesis testing for research questions.\n\nThe research question is whether older employees are more likely to report unsafe working conditions.\n\nThe null hypothesis is that there is no relationship between age and willingness to report unsafe work.\n\nThe research hypothesis is that there is a positive correlation between age and willingness to report unsafe work.\n\nThe independent variable is age, which is ratio level.\n\nThe dependent variable is willingness to report unsafe work (scale of 0-10 in equal increments of 1 with 0 being never and 10 being always willing).\n\nMy first question is whether this is interval or ordinal. My initial thought was ordinal because while it is ranked in equal increments with hard limits (always and never) the rankings are subjective and someone's \"sometimes\" is different than someone elses, and a sometimes at 5 is not necessarily half of an always at 10.\n\nI then ran into the issue of which hypothesis test to use.\n\nI cannot use a Chi-square because this question specifies age, not age groups and our prof has been specific on using the variable indicated.\n\nA pearson's r isn't appropriate unless both variables are continuous, but it would be the most appropriate test based on the question and what is being compared which made me think maybe I am misinterpreting the level of measure and it should be interval.\n\nAny assistance or clarification on points I may be misunderstanding would be appreciated.\n\nThanks!",
    "author": "PsychologicalBus3267",
    "timestamp": "2025-07-10T06:05:22",
    "url": "https://reddit.com/r/statistics/comments/1lwc8a1/question_very_basic_statistics_question/",
    "score": 5,
    "num_comments": 7,
    "upvote_ratio": 0.86,
    "is_original_content": false
  }
]