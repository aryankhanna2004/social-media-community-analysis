[
  {
    "subreddit": "DataEngineering",
    "post_id": "1ocgq86",
    "title": "Serverless Inference Providers Compared: Qwen Image + Nvidia H100",
    "content": "Hello Reddit,\n\nWe ran a series of simple benchmark using the most popular serverless inference/GPU providers and compared the basic but important aspects: cold start and true cost.\n\nYes, we are in the comparison, so there might be some bias. But for competing platforms we followed their best practices and the source code used is shared in a GitHub repository.\n\nShort conclusion: if you don't feel comfortable using us, use Modal.\n\nThanks for your time.",
    "author": "dat1-co",
    "timestamp": "2025-10-21T08:55:24",
    "url": "https://reddit.com/r/dataengineering/comments/1ocgq86/serverless_inference_providers_compared_qwen/",
    "score": 93,
    "num_comments": 2,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1occxjl",
    "title": "Developing with production data: who and how?",
    "content": "Classic story: you should not work directly in prod, but apply the best devops practices, develop data pipelines in a development environment, debug, deploy in pre-prod, test, then deploy in production.\n\nWhat about data analysts? data scientists? statisticians? ad-hoc reports?\n\nMost data books focus on the data engineering lifecycle, sometimes they talk about the \"Analytics sandbox\", but they rarely address heads-on the personas doing analytics work in production. Modern data platform allow the decoupling of compute and data, enabling workload isolation to allow users read-only access to production data without affecting production workloads. Other teams perform data replication from production to lower environments. There's also the \"blue-green development architecture\", with two systems with production data.\n\nHow are you dealing with users requesting production data?",
    "author": "aburkh",
    "timestamp": "2025-10-21T06:26:54",
    "url": "https://reddit.com/r/dataengineering/comments/1occxjl/developing_with_production_data_who_and_how/",
    "score": 17,
    "num_comments": 22,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oc1w56",
    "title": "How do you decide when to move from batch jobs to real-time pipelines?",
    "content": "Our team has been running nightly batch ETL for years and it works fine, but product leadership keeps asking if we should move “everything” to real-time. The argument is that fresher data could help dashboards and alerts, but honestly, I’m not sure most of those use cases need second-by-second updates.\n\nWe’ve done some early tests with Kafka and Debezium for CDC, but the overhead is real, more infrastructure, more monitoring, more cost. I’m trying to figure out what the actual decision criteria should be.\n\nFor those who’ve made the switch, what tipped the scale for you? Was it user demand, system design, or just scaling pain with batch jobs? And if you stayed with batch, how do you justify that choice when “real-time” sounds more exciting to leadership?",
    "author": "stephen8212438",
    "timestamp": "2025-10-20T20:02:52",
    "url": "https://reddit.com/r/dataengineering/comments/1oc1w56/how_do_you_decide_when_to_move_from_batch_jobs_to/",
    "score": 80,
    "num_comments": 35,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ocie74",
    "title": "Our 7 Snowflake query optimization tips and why they work",
    "content": "Hope y'all find it useful!",
    "author": "hornyforsavings",
    "timestamp": "2025-10-21T09:58:09",
    "url": "https://reddit.com/r/dataengineering/comments/1ocie74/our_7_snowflake_query_optimization_tips_and_why/",
    "score": 8,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ocksmo",
    "title": "Thoughts on Using Synthetic Tabular data for DE projects ?",
    "content": "Thoughts on using Synthetic Data for Projects ?\n\nI'm currently a DB Specialist with 3 YOE learning Spark, DBT, Python, Airflow and AWS to switch to DE roles.\n\nI’d love some feedback on a portfolio project I’m working on. It’s basically a modernized spin on the kind of work I do at my job, a Transaction Data Platform with a multi-step ETL pipeline.\n\nQuick overview of setup:\n\nDB structure:\n\nDimensions = Bank -&gt; Account -&gt; Routing\n\nFact = Transactions -&gt; Transaction\\_Steps\n\nHistory = Hist_Transactions -&gt; Hist_Transaction_Steps (identical to fact tables, just one extra column) \n\nI mocked up 3 regions -&gt; 3 banks per region -&gt; 3 accounts per bank -&gt; 702 unique directional routings.\n\nA Python script first assigns following parameters to each routing:\n\ntype (High Intensity/Frequency/Normal)\n\ncountry\\_code, region, cross\\_border\n\nbase\\_freq, base\\_amount, base\\_latency, base\\_success\n\nvolatility vars (freq/amount/latency/success)\n\nThen the synthesizer script uses above paramters to spit out 85k-135k records per day, and 5x times Transaction\\_Steps \n\nAnomaly engine randomly spikes volatility (50–250x) \\~5 times a week for a random routing, the aim is (hopefully) the pipeline will detect the anomalies.\n\nPipeline workflow:\n\nBatch runs daily (simulating off business hours migration).\n\nEvery day data older than 1 month in live table is moved to history tables (partitioned by day and OLTP compressed) \n\nThen the partitions older than a month in history tables are exported to Parquet (maybe I'll create a Data lake or something) cold storage and stored. \n\nThe current day's transactions are transformed through DBT, to generate 12 marts, helping in anomaly detection and system monitoring \n\nA Great Expectation + Python layer takes care of data quality and Anomaly detection\n\nFinally for visualization and ease of discussion I'm generating a streamlit dashboard from above 12 marts.\n\nMain concerns/questions:\n\n1. Since this is just inspired by my current work (I didn’t use real table names/logic, just the concept), should I be worried about IP/overlap ?\n2. I’ve done a barebones version of this in shell+SQL, so I personally know business and technical requirements and possible issues in this project, it feels really straightforward.  Do you think this is a solid enough project to showcase for DE roles at product-based-companies / fintechs (0–3 YOE range)?\n3. Thoughts on using synthetic data? I’ve tried to make it noisy and realistic, but since I’ll always have control, I feel like I'm missing something critical that only shows up in real-world messy data?\n\nWould love any outside perspective\n\nThis would ideally be the portfolio project, and there's one more planned using spark where I'm just cleaning and merging Spotify datasets from different types (CSV, json, sqlite, parquet etc) from Kaggle, it's just a practice project to showcase spark understanding. \n\n**TLDR:**  \nBuilt a synthetic transaction pipeline (750k+ txns, 3.75M steps, anomaly injection, DBT marts, cold storage). Looking for feedback on:\n\n* IP concerns (inspired by work but no copied code/keywords)\n* Whether it’s a strong enough DE project for Product Based Companies and Fintech. \n* Pros/cons of using synthetic vs real-world messy data",
    "author": "Markymark285",
    "timestamp": "2025-10-21T11:26:56",
    "url": "https://reddit.com/r/dataengineering/comments/1ocksmo/thoughts_on_using_synthetic_tabular_data_for_de/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ocst3a",
    "title": "Need advice choosing between Data engineer vs Sr Data analyst",
    "content": "Hey all I could really use some career advice from this community.\n\nI was fortunate to land 2 offers in this market, but now I’m struggling to make the right long term decision.\n\nI’m finishing my Master’s in Data Science next semester. I interned last summer at a big company and then started working in my first FT data role as a data analyst at a small company (I’m about 6 months in). My goal is to eventually move into Data Science/ML maybe ML engineer and end up in big tech.\n\nOption A: Data Engineer I\n* Industry: Finance. This one pays $15k more. I’ll be working with a smaller team and I’d be the main technical person on the team. So no strong mentorship and I’ll have the pressure to “figure it out” on my own. \n\nOption B: Senior Data Analyst\n* Industry: retail at a large org.\n\nI’m nervous about being the only engineer on a team this early in my career…But I’m also worried about not being technical enough as a data analyst and not being technical. \n\nWhat would you do in my shoes?\nGo hard into engineering now and level up fast even if it’s stressful without much support? Or take the analyst role at a big company, build brand and transition later?\n\nWould appreciate any advice from people who’ve been on either path.\n",
    "author": "Agile_Yak3819",
    "timestamp": "2025-10-21T16:41:05",
    "url": "https://reddit.com/r/dataengineering/comments/1ocst3a/need_advice_choosing_between_data_engineer_vs_sr/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ocqznw",
    "title": "Tools for automated migration away from Informatica",
    "content": "Has anyone ever had any success using tools like DataBricks Lakebridge or Snowflake's SnowConvert to migrate Informatica powercenter ETL pipelines to another platform? I assume at best they \"kind of work sometimes for some things\" but am curious to hear anyone's actual experience with them in the wild.",
    "author": "BadKafkaPartitioning",
    "timestamp": "2025-10-21T15:22:39",
    "url": "https://reddit.com/r/dataengineering/comments/1ocqznw/tools_for_automated_migration_away_from/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ocery5",
    "title": "Anyone else use AWS Redshift Zero-ETL in US-EAST-1?",
    "content": "This is a service that basically puts a read replica of an RDS streaming into your Redshift data warehouse. \n\nWe have this set up in our environment and it runs many critical systems. After the nightmares of yesterday I checked this morning after getting some complaints from unhappy users about stale data and our ZETL integrations appear to have disappeared entirely. I can see the data and it appears to have stopped updating coincident with yesterday's outage. Looks like I'll have to completely remake these. This is pretty irritating because I can't find any information anywhere from AWS about the outage having *deleted* this infrastructure.",
    "author": "bingbongbangchang",
    "timestamp": "2025-10-21T07:40:29",
    "url": "https://reddit.com/r/dataengineering/comments/1ocery5/anyone_else_use_aws_redshift_zeroetl_in_useast1/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oco9fs",
    "title": "Date time granularity",
    "content": "Hi all,\n\nI have some room booking data I need to do some time-related calculations with using Power Bi.\n\n1st table has room bookings data with room name, meeting start date time, meeting end date time, snapshot\\_date, etc.\n\nAs part of my ETL I am already building the snapshot\\_date rows based on the meeting start date time and meeting end date time.\n\n2nd table has room occupancy data which has room name, start date time, stop date time and usage which are in hour buckets.\n\nI have a dim date table connected to snapshot\\_date in the room bookings table and start date time in the room occupancy table.\n\nQuestion is do I need to have my room bookings data at the same time granularity (hourly) as the room occupancy data to make life easier with time calculations moving forward.\n\nCheers",
    "author": "ImFizzyGoodNice",
    "timestamp": "2025-10-21T13:35:28",
    "url": "https://reddit.com/r/dataengineering/comments/1oco9fs/date_time_granularity/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ocalca",
    "title": "Any good PR review tools for data stacks?",
    "content": "Has anyone tried using PR review tools like **CodeRabbit** or **Greptile** for data engineering workflows (dbt, Airflow, Snowflake, etc.)?\n\nIf anyone can share theier experience on if they can handle things like schema changes, query optimization, or data quality checks well, or if they’re more tuned for general code reviews (which I m mostly expecting).",
    "author": "Hot_Donkey9172",
    "timestamp": "2025-10-21T04:39:23",
    "url": "https://reddit.com/r/dataengineering/comments/1ocalca/any_good_pr_review_tools_for_data_stacks/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oc3vf9",
    "title": "IaC a prerequisite for DE?",
    "content": "Hi subreddit. \n\nI’ve been tipping my toes back in the job search; one thing I see this round I didn’t see 3 years ago is that Terraform/IaC is required by almost every job. \n\nThought I could get away without it - was invited for an interv for job, but then they cancelled due to lack of IaC experience. \n\nIs this really the common expectation now? I’ll spend some time learning it but really suprised by this outcome. ",
    "author": "ruben_vanwyk",
    "timestamp": "2025-10-20T21:47:22",
    "url": "https://reddit.com/r/dataengineering/comments/1oc3vf9/iac_a_prerequisite_for_de/",
    "score": 17,
    "num_comments": 14,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1occmld",
    "title": "DBT unit tests on Redshift",
    "content": "Hello!\n\nI'm trying to implement unit tests for the DBT models used by my team, so we have more trust on those models and their logic. However I'm getting stuck when the model contains a SUPER-typed column for JSON data.\n\nIf I write the JSON object inside a string in the YAML file of the test, then DBT expects unquoted JSON. If I remove the quotes around the JSOn object, then I get a syntax error on the YAML file. I also tried writing the JSON object as a YAML object with indent but it fails too. \n\nWhat should I do ?",
    "author": "Adrien0623",
    "timestamp": "2025-10-21T06:14:11",
    "url": "https://reddit.com/r/dataengineering/comments/1occmld/dbt_unit_tests_on_redshift/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oca4c7",
    "title": "Help a noob: CI/CD pipelines with medallion architecture",
    "content": "Hello,   \nI have worked for a few years as an analyst (self taught) and now I am trying to get into data engineering. I am trying to simply understand how to structure a DWH using **medallion architecture** (Bronze → Silver → Gold) across **multiple environments** (Dev / Test / Prod).\n\nNow, with the last company I worked with, they simply had two databases, staging, and production. Staging is basically the data lake and they transformed all the data to production. I understand this is not best practice. \n\nI thought if I wanted to have a proper structure in my DWH, I was thinking of this:\n\nDWH |\n\n\\-&gt; StagingDB -&gt; BronzeSchema, SilverSchema, GoldSchema\n\n\\-&gt; TestDB -&gt; BronzeSchema, SilverSchema, GoldSchema\n\n\\-&gt; ProdDB -&gt; BronzeSchema, SilverSchema, GoldSchema\n\nWould you even create a bronze layer on staging and test DBs or not really? I mean it is just the raw data no? ",
    "author": "Nomad_chh",
    "timestamp": "2025-10-21T04:13:49",
    "url": "https://reddit.com/r/dataengineering/comments/1oca4c7/help_a_noob_cicd_pipelines_with_medallion/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obzvv0",
    "title": "Quick dbt question, do you name your data marts schema 'marts'?",
    "content": "Or something like 'mrt\\_&lt;sql\\_file\\_name&gt;'?\n\nWhy don't you name it into, for example, 'recruitment' for marts for recruitment team?\n\n",
    "author": "ketopraktanjungduren",
    "timestamp": "2025-10-20T18:26:52",
    "url": "https://reddit.com/r/dataengineering/comments/1obzvv0/quick_dbt_question_do_you_name_your_data_marts/",
    "score": 12,
    "num_comments": 18,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oce5db",
    "title": "Schema Evolution in GCP",
    "content": "Hi, i have experience with ELT pipelines on GCP. One that loads data from Postgres tables data into BigQuery and another one that loads CSV files data from SFTP server to BigQuery. \n\n* Postgres to BQ:\n\nSource -&gt; BQ Bronze layer \nDataflow (custom template for multi table ingestion in single job)\n\nBQ Bronze-&gt; BQ Silver Layer -&gt; BQ Gold \n\n————————————————————\n\n* SFTP to BQ:\n\nSFTP -&gt; GCS\nAirflow (Composer) SFTP to GCS operator\n\nGCS -&gt; BQ Bronze\nGCS to BQ airflow operator\n\nBQ Bronze layer -&gt; BQ Silver Layer -&gt; BQ Gold \n\n—————————————————————\n\nBQ bronze to silver in both cases is handled using a stored procedure that handles upsert of Bronze data (cleaned data) to Silver. Dataform for Silver to Gold layer.\n\nNow, could you please explain how below could be handled in both cases?:\n\n1. Schema Evolution \n2. Handling both incremental or Full load as per requirement using same pipeline (if possible)\n3. Handling Large Volumes of data (TB) \n4. Frameworks/tools/methods for Data Quality checks and data validations\n\n\nThank you in advance! \n\n\n",
    "author": "FeeOk6875",
    "timestamp": "2025-10-21T07:15:38",
    "url": "https://reddit.com/r/dataengineering/comments/1oce5db/schema_evolution_in_gcp/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obsrfm",
    "title": "Small company head of dept or team lead at a dominant global company?",
    "content": "currently manage a small data team for a stable, growing and relaxed company. It’s somewhat cross functional but doesn’t have a clear growth path forward in terms of position or comp. Also, I am probably 75% hands on DE and remainder is a cross of business strategy, PM and misc. Dept growth may be stagnant since the it’s not a tech company.\n\nI have an offer from a non-FAANG, but top company in their industry for a team lead position. TC is ~50% more. Growth is more defined and I think could have a much higher comp ceiling.\n\nI’ve been running the small company route for a while and have never done DE at scale for a company with the resources/need to use the big tech. Can’t decide whether finally being thrown into an actual engineering env would be beneficial or unnecessary at this stage in my career.\n\nAnyone have any words of wisdom?",
    "author": "Cactuslover72",
    "timestamp": "2025-10-20T13:26:42",
    "url": "https://reddit.com/r/dataengineering/comments/1obsrfm/small_company_head_of_dept_or_team_lead_at_a/",
    "score": 17,
    "num_comments": 11,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ocaik3",
    "title": "Integrating sqlmesh models with Dagster",
    "content": "Hey folks,\n\nI’m currently using Dagster as the orchestrator in my team’s data stack and I’m considering incorporating sqlmesh as our transformation library. But, I can’t really figure out a way to integrate my sqlmesh models with Dagster so that they show up as individual assets. Has anyone had any luck in achieving this ? How did you go about doing it ?",
    "author": "YameteGPT",
    "timestamp": "2025-10-21T04:35:12",
    "url": "https://reddit.com/r/dataengineering/comments/1ocaik3/integrating_sqlmesh_models_with_dagster/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oc1c98",
    "title": "Azure Data Factory pipelines in Python",
    "content": "I am looking for ideas to leverage my Python programming knowledge while creating ADF pipelines to build a traditional DWH. Both source and target are Azure SQL. I am very new to ADF as this will be the first project in ADF. The project timeline is very tight. I want to avoid as much UI part (drag and drop) as possible during development and rely more on Python scripts. Any suggestion will be greatly appreciated. Thanks.",
    "author": "SignificantDig1174",
    "timestamp": "2025-10-20T19:36:09",
    "url": "https://reddit.com/r/dataengineering/comments/1oc1c98/azure_data_factory_pipelines_in_python/",
    "score": 6,
    "num_comments": 11,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oc6sq1",
    "title": "Cannot determine primary keys in raw data as no column is unique and concatenation of columns too don’t provide uniqueness",
    "content": "Hey guys, Cannot determine primary keys in raw data as no column is unique and concatenation of columns too don’t provide uniqueness even if I go by business logic and say these columns are pk I don’t get uniqueness, I get many duplicate rows, any idea on how to approach this? I can’t just remove those duplicates\n\nEDIT - I checked each column for uniqueness and concatenation of columns and checked their uniqueness by using distinct but nothing unique\nI got duplicates and then I hashed all the columns together and removed the duplicate hashed columns and now I'm only hashing ID columns as other columns can like time and date can be changed and got some unique combo of columns that can be pk, I hope this approach is good guys ",
    "author": "Pleasant-Insect136",
    "timestamp": "2025-10-21T00:46:44",
    "url": "https://reddit.com/r/dataengineering/comments/1oc6sq1/cannot_determine_primary_keys_in_raw_data_as_no/",
    "score": 0,
    "num_comments": 50,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obp5go",
    "title": "Anyone who uses DBT at large scale? looking for feedback",
    "content": "\\[CLOSED, got enough interest and i will postback\\]  \nHey everyone,  \nwe are a small team building a data orchestrator and we have a dbt use case we would like to demo. We would like to meet someone using DBT at large scale and understand how you use dbt/ usecase and would like to demo our product to get your feedback  \n",
    "author": "never_know29",
    "timestamp": "2025-10-20T11:07:42",
    "url": "https://reddit.com/r/dataengineering/comments/1obp5go/anyone_who_uses_dbt_at_large_scale_looking_for/",
    "score": 10,
    "num_comments": 20,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obij50",
    "title": "Flink Watermarks. WTF?",
    "content": "Yeah, so basically that. WTF. That was my first, second, and third reaction when I started trying to understand watermarks in Apache Flink. \n\nSo I got together with a couple of colleagues and built flink-watermarks.wtf. \n\nIt's a 'scrollytelling' explainer of what watermarks in Apache Flink are, why they matter, and how to use them.\n\nTry it out: [https://flink-watermarks.wtf/](https://flink-watermarks.wtf/)",
    "author": "rmoff",
    "timestamp": "2025-10-20T06:07:20",
    "url": "https://reddit.com/r/dataengineering/comments/1obij50/flink_watermarks_wtf/",
    "score": 22,
    "num_comments": 5,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obnj8i",
    "title": "Databases Without an OS? Meet QuinineHM and the New Generation of Data Software",
    "content": "",
    "author": "dataware-admin",
    "timestamp": "2025-10-20T09:54:43",
    "url": "https://reddit.com/r/dataengineering/comments/1obnj8i/databases_without_an_os_meet_quininehm_and_the/",
    "score": 8,
    "num_comments": 6,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obo6mx",
    "title": "Looking for open-source projects",
    "content": "I have worked on some decent project pipelines with the stack of airflow, apache kafka, pyspark and snowflake. Looking for open-source projects to build my profile more and to build my portfolio.",
    "author": "Successful_Today_220",
    "timestamp": "2025-10-20T10:25:05",
    "url": "https://reddit.com/r/dataengineering/comments/1obo6mx/looking_for_opensource_projects/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obmj9a",
    "title": "Umbrella word for datawarehouse, datalake and lakehouse?",
    "content": "Hi,\n\nI’m currently doing some research for my internship and one of my sub-questions is which of a data warehouse, data lake, or lakehouse fits in my use case. Instead of listing those three options every time, I’d like to use an umbrella term, but I haven’t found a widely used one across different sources. I tried a few suggested terms from chatgpt, but the results on Google weren’t consistent, so I’m not sure what the correct umbrella term is.\n\n",
    "author": "WhiteAza",
    "timestamp": "2025-10-20T09:05:57",
    "url": "https://reddit.com/r/dataengineering/comments/1obmj9a/umbrella_word_for_datawarehouse_datalake_and/",
    "score": 6,
    "num_comments": 23,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oboeyg",
    "title": "[Need Career Advice] Stuck In WITCH Trap with no Real learning. What Should I Do?",
    "content": "Hey everyone, I’ve been working at a WITCH company for about a year now, as a Data Engineer. I’ve been in the same project throughout, where I mainly handle support tasks— documentation, debugging, adding columns, updating views, ensuring data flow, and just ensuring everything runs smoothly. Essentially, I’m not involved in any real development work like building pipelines or writing scripts.\n\nI’ve seen a lot of posts about people in similar situations, like the one here [Need career advice – Am I a Data Engineer?](https://www.reddit.com/r/dataengineersindia/comments/1nf3h0u/need_career_advice_am_i_a_data_engineer_how_do_i/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) , and it feels like I’m in the same boat. The problem is, I’ve been stuck in this role for a year, and I honestly feel like if I stay for another year or two, I won’t learn anything new. I’ve been trying to switch jobs and have been applying for the last 3-4 months, but most of the time, I don’t even make it past the shortlisting stage. I don’t blame the companies, to be honest—I just don’t have anything unique to show in my experience section. I’ve worked on some personal projects, but that’s about it.\n\nAnother challenge is the 90-day notice period. It’s hard to move quickly when companies need immediate joiners, so I feel stuck in that regard too.\n\nI see two possible options right now:\n\nAsk to be released from my current project: But this is tricky. I’m not sure if they’ll even let me go (seniors have told that release is not provided easily), and if I do get released, I’m worried the next project might be even worse. Plus, I don’t know how long I’d be able to stay on the bench, and that’s also not ideal.\n\nResign and serve the notice period quietly: This comes with its own set of risks, mainly the uncertainty of not having a job while I keep applying. The idea of not having a stable income while job hunting scares me.\n\nSo, I guess I’m at a crossroads. Has anyone been in a similar situation? How did you navigate it? Any advice on how I should proceed from here?   \n  \nI don't want to be stuck here in this swamp in the early stages of my career as it will also affect my future career path. I have been making few more personal projects to add but I still don't think that will be enough.\n\nAppreciate any insights or suggestions!\n\nTL/DR : I’ve been working as a Data Engineer in a WITCH company for a year, mostly handling support tasks with no real development experience. I’ve been trying to switch jobs for 3-4 months but keep getting rejected. I’m stuck with a 90-day notice period and feel like I’m not learning anything new. Should I ask to be released from my project (with no guarantee of better work) or resign and serve the notice period while job hunting (despite the risk of unemployment)?",
    "author": "Mysterious-Sky5410",
    "timestamp": "2025-10-20T10:35:30",
    "url": "https://reddit.com/r/dataengineering/comments/1oboeyg/need_career_advice_stuck_in_witch_trap_with_no/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obtotd",
    "title": "Is there a website like MDN for data engineers?",
    "content": "MDN seems to be the gold standard for web devs for gaining knowledge. Are there any similar websites for Data Engineers?",
    "author": "Aurora-Optic",
    "timestamp": "2025-10-20T14:01:01",
    "url": "https://reddit.com/r/dataengineering/comments/1obtotd/is_there_a_website_like_mdn_for_data_engineers/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obmg2l",
    "title": "BSP-inspired bitsets: 46% smaller than Roaring (but probably not faster)",
    "content": "Roaring-like bitset indexes are used in most OLAP databases (Lucene, Spark, ClickHouse, etc).  \n  \nI explored plausibly fast-to-decode compression schemes and found a couple BSP-based approaches which can half Roaring's size. The decode complexity is quite high so these will probably match (rather than beat) Roaring throughput on bitwise ops once tuned, but their might be some value for memory-constrained and disk/network-bound contexts.  \n  \nWith an alternative simpler compression scheme I was able to reduce size by 23%, and expect the throughput will beat Roaring once the implementation is further along.",
    "author": "ashtonsix",
    "timestamp": "2025-10-20T09:02:02",
    "url": "https://reddit.com/r/dataengineering/comments/1obmg2l/bspinspired_bitsets_46_smaller_than_roaring_but/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obisxn",
    "title": "Help: Fine-grained Instructions on SparkSQL",
    "content": "Hey folks, I need to pick your brains to brainstorm a potential solution to my problem.\n\nCurrent stack: SparkSQL (Databricks SQL), storage in Delta, modeling in dbt.\n\nI have a pipeline that generally works like this:\n\n    WITH a AS (SELECT * FROM table)\n    SELECT a.*, 'one' AS type\n    FROM a\n    \n    UNION ALL\n    \n    SELECT a.*, 'two' AS type\n    FROM a\n    \n    UNION ALL\n    \n    SELECT a.*, 'three' AS type\n    FROM a\n\nThe source table is partitioned on a column, let's say column \\`date\\`, and the output is stored also with partition column \\`date\\` (both with Delta). The transformation in the pipeline is just as simple as select one huge table, do broadcast joins with a couple small tables (I have made sure all joins are done as \\`BroadcastHashJoin\\`), and then project the DataFrame into multiple output legs.\n\nI had a few assumptions that turns out to be plain wrong, and this mistake really f\\*\\*ks up the performance.\n\n**Assumption 1:** I thought Spark will scan the table once, and just read it from cache for each of the projections. Turns out, Spark compiles the CTE into inline query and read the table thrice.\n\n**Assumption 2**: Because Spark read the table three times, and because Delta doesn't support bucketization, Spark distributes the partition for each projection leg without guarantee that rows that share the same \\`date\\` will end up in the same worker. The consequence of this is a massive shuffling at the end before writing the output to Delta, and this shuffle really kills the performance.\n\nI have been thinking about alternative solutions that involve switching stack/tools, e.g. use pySpark for a fine-grained control, or switch to vanilla Parquet to leverage the bucketization feature, but those options are not practical. Do you guys have any idea to satisfy the above two requirements: (a) scan table once, and (b) ensure partitions are distributed consistently to avoid any shuffling.",
    "author": "ukmurmuk",
    "timestamp": "2025-10-20T06:19:52",
    "url": "https://reddit.com/r/dataengineering/comments/1obisxn/help_finegrained_instructions_on_sparksql/",
    "score": 3,
    "num_comments": 14,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obdp5t",
    "title": "Anyone experiencing issues with AWS right now",
    "content": "Hey all. Do you experience some issues with AWS as well? It seems it might be down.\n\nIf it is down, we will have a wonderful day for sure (\\\\s).",
    "author": "tiredITguy42",
    "timestamp": "2025-10-20T00:34:47",
    "url": "https://reddit.com/r/dataengineering/comments/1obdp5t/anyone_experiencing_issues_with_aws_right_now/",
    "score": 8,
    "num_comments": 6,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obf0uv",
    "title": "AWS US East DynamoDB and pretty much everything else down...",
    "content": "Entire AWS management console page down... that's a first...\n\nAnd of course it had to happen right before production deployment, congrats to all you people not on call I guess.\n\nhttps://preview.redd.it/nldday0gd8wf1.png?width=1504&amp;format=png&amp;auto=webp&amp;s=c899e84f1e203ce27ab09122a45e696fb093f107\n\nhttps://preview.redd.it/3o1kwblhd8wf1.png?width=1100&amp;format=png&amp;auto=webp&amp;s=b2bd02c5a78fc33d2ea3bbe9e76663a02b6883f7\n\n",
    "author": "Willy2721",
    "timestamp": "2025-10-20T01:55:40",
    "url": "https://reddit.com/r/dataengineering/comments/1obf0uv/aws_us_east_dynamodb_and_pretty_much_everything/",
    "score": 6,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ob3566",
    "title": "What tools do you prefer to use for simple interactive dashboards?",
    "content": "I have been trying Apache Superset for some time, and it does most of the job but also comes just short of what I need it to do. Things like:\n\n- Not straightforward to reuse the same dashboard with different source tables or views. \n- Supports cert auth for some DB connections but not others. Unless I am reading the docs wrong.\n\nWhat other alternatives are out there? I do not even need the fancy visualizations, just something that can do filtering and aggregation on the fly for display in tabular format.",
    "author": "IDoCodingStuffs",
    "timestamp": "2025-10-19T15:28:23",
    "url": "https://reddit.com/r/dataengineering/comments/1ob3566/what_tools_do_you_prefer_to_use_for_simple/",
    "score": 28,
    "num_comments": 15,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oblnpe",
    "title": "Advice on hiring a data architect?",
    "content": "So we've had a data architect for a while now who's been with the business a long time, and he's resigned so we're looking to replace him. I discovered that he has, for the most part, been copying and pasting data model designs from some confluence docs created in 2018 by someone else... so it's not a huge loss, but he does know the org's SAP implementation quite well.\n\n  \nI'm wondering... what am I looking for? What do I need? We don't need technical implementation help from a platform perspective, I think we just need someone mainly doing data modelling. I also want to steer clear of anyone wanting to create an up front enterprise data model.\n\n  \nWe're trying to design our data model iteratively, but carefully.",
    "author": "General-Parsnip3138",
    "timestamp": "2025-10-20T08:25:48",
    "url": "https://reddit.com/r/dataengineering/comments/1oblnpe/advice_on_hiring_a_data_architect/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ob6d1n",
    "title": "What to show during demo's?",
    "content": "Looking for broad advice on what should data engineering teams be showing during demos to customers or stakeholders (KPIs, dashboards, metrics, reports, other?). My team doesn't have anything super time sensitive coming up, just wondering what reports/dashboards people recommend we invest time into creating and maintaining to show progress in our data engineering. We just want to get better at showing continuous progress to customer/stakeholders.\n\nI feel this is harder than for data scientists or analysts since they are a lot closer to the work that directly relates to \"the core business\".\n\nI have been reading into DORA metrics from software engineering as well, but I don't know if those are things we could share to show progress to stakeholders.",
    "author": "data_5678",
    "timestamp": "2025-10-19T17:54:38",
    "url": "https://reddit.com/r/dataengineering/comments/1ob6d1n/what_to_show_during_demos/",
    "score": 10,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obdu67",
    "title": "How do you do a Dedup check in batch &amp; steam?",
    "content": "How would you design your pipelines for handling deduplicates before they move to your downstream?",
    "author": "NefariousnessSea5101",
    "timestamp": "2025-10-20T00:42:38",
    "url": "https://reddit.com/r/dataengineering/comments/1obdu67/how_do_you_do_a_dedup_check_in_batch_steam/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oatq7o",
    "title": "Beginner Confused About Airflow Setup",
    "content": "Hey guys,\n\nI'm total beginner learning tools used data engineering and just started diving into orchestration , but I'm honestly so confused about which direction to go\n\ni saw people mentioning Airflow, Dagster, Prefect\n\nI figured \"okay, Airflow seems to be the most popular, let me start there.\" But then I went to actually set it up and now I'm even MORE confused...\n\n* First option: run it in a Python environment (seems simple enough?)\n* BUT WAIT - they say it's recommend using a Docker image instead\n* BUT WAIT AGAIN - there's this big caution message in the documentation saying you should really be using Kubernetes\n* OH AND ALSO - you can use some \"Astro CLI\" too?\n\nLike... which one am I actually supposed to using? Should I just pick one setup method and roll with it, or does the \"right\" choice actually matter?\n\nAlso, if Airflow is this complicated to even get started with, should I be looking at Dagster or Prefect instead as a beginner?\n\nWould really appreciate any guidance because i'm so lost and thanks in advance",
    "author": "Amomn",
    "timestamp": "2025-10-19T09:16:55",
    "url": "https://reddit.com/r/dataengineering/comments/1oatq7o/beginner_confused_about_airflow_setup/",
    "score": 26,
    "num_comments": 17,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ob2shp",
    "title": "Confused by Offline Data Systems terminology",
    "content": "In this Meta data engineering [blog post](https://engineering.fb.com/2025/08/13/data-infrastructure/agentic-solution-for-warehouse-data-access/) it says, \"As part of its offline data systems, Meta operates a data warehouse that supports use cases across analytics, ML, and AI\".\n\n  \nI'm familiar with OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing) data systems. What makes Meta's offline data system different than the average OLAP data system. E.g what makes a data warehouse online vs offline?",
    "author": "CDCheerios",
    "timestamp": "2025-10-19T15:13:26",
    "url": "https://reddit.com/r/dataengineering/comments/1ob2shp/confused_by_offline_data_systems_terminology/",
    "score": 7,
    "num_comments": 4,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obdotn",
    "title": "Designing a hybrid batch stream pipeline for fintech data",
    "content": " We recently had to handle both batch and stream data for a fintec client. I set up Spark structured streaming on top of Delta Lake with Airflow scheduling. The tricky part was ensuring consistency between batch historical loads and realtime ingestion \n\nHad to tweak checkpointing and watermarks to avoid duplicates and late arrivals. Felt like juggling clocks and datasets at the same time. Anyone else run into weird late arrival issues with Spark streaming?\n\n",
    "author": "FitImportance606",
    "timestamp": "2025-10-20T00:34:17",
    "url": "https://reddit.com/r/dataengineering/comments/1obdotn/designing_a_hybrid_batch_stream_pipeline_for/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1obddb7",
    "title": "Data compliance &amp; governance in Salesforce Retail environments?",
    "content": "Hi everyone, I'm working in retail logistics. Obviously retail enterprises face unique challenges meeting data governance and compliance requirements, especially multi-channel sales and regional variations in regulations (we're based in US, btw). When your company goes through audits or compliance reviews, what processes or frameworks help you streamline governance, auditability, and consent management?\n\nIf there's a more relevant place to post this please let me know!",
    "author": "LogisticalNightmare7",
    "timestamp": "2025-10-20T00:14:35",
    "url": "https://reddit.com/r/dataengineering/comments/1obddb7/data_compliance_governance_in_salesforce_retail/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oajs0h",
    "title": "Struggling with separate Snowflake and Airflow environments for DEV/UAT/PROD - how do others handle this?",
    "content": "Hey all,\n\nThis might be a very dumb or ignorant question from me who know very little about DevOps or best practices in DE but would be great if I can stand on the shoulders of giants!\n\nFor the background context, I'm working as a quant engineer at a company with about 400 employees total (60\\~80 IT staff, separate from our quant/data team which consists of 4 people, incl myself). Our team's trying to build out our analytics infrastructure and our IT department has set up **completely separate environments** for DEV, UAT, and PROD including:\n\n* Separate Snowflake accounts for each environment\n* Separate managed Airflow deployments for each environment\n* GitHub monorepo with protected branches (dev/uat/prod) for code (In fact, this is what I asked for. IT dept tried to setup polyrepo for n different projects but I refused)\n\nThis setup is causing major challenges or at least I do not understand how to:\n\n* As far as I am aware, zero copy cloning doesn't work across Snowflake accounts, making it impossible to easily copy production data to DEV for testing\n* We don't have dedicated DevOps people so setting up CI/CD workflows feels complicated\n* Testing ML pipelines is extremely difficult without realistic data given we cannot easily copy data from prod to dev account in Snowflake\n\nI've been reading through blogs &amp; docs but I'm still confused about what's standard practice for this circumstance. I'd really appreciate some real-world insights from people who've been in similar situations.\n\n# This is my best attempt to distill the questions:\n\n* For a small team like ours (4 people handling all data work), is it common to have completely separate Snowflake accounts AND separate Airflow deployments for each environment? Or do most companies use a single Snowflake account with separate databases for DEV/UAT/PROD and a single Airflow instance with environment-specific configurations?\n* How do you handle testing with production-like data when you can't clone production data across accounts? For ML development especially, how do you validate models without using actual production data?\n* What's the practical workflow for promoting changes from DEV to UAT to PROD? We're using GitHub branches for each environment but I'm not sure how to structure the CI/CD process for both dbt models and Airflow DAGs without dedicated DevOps support\n* How do you handle environment-specific configurations in dbt and Airflow when they're completely separate deployments? Like, do you run Airflow &amp; dbt in DEV environment to  generate data for validation and do it again across UAT &amp; PROD? How does this work?\n\nAgain, I have tried my best to arcitulate the headaches that I am having and any practical advice would be super helpful. \n\nThanks in advance for any insights and enjoy your rest of Sunday!",
    "author": "Dependent_Lock5514",
    "timestamp": "2025-10-19T00:47:36",
    "url": "https://reddit.com/r/dataengineering/comments/1oajs0h/struggling_with_separate_snowflake_and_airflow/",
    "score": 43,
    "num_comments": 21,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oafuv2",
    "title": "Company is paying for my next DE cert. Which one to choose right now ?",
    "content": "Hey r/dataengineering,\n​My company (consulting in europe) is giving me some time and an open budget to grab my next certification. I need your honest opinions on what's worth the time and money in today's market.\n\n\n​My Profile:\n​Started as: Data Analyst 5years ago (Power BI, SQL, Python).\n​Now shifting into: Data Engineering (Fabric, dbt, Snowflake).\n​Goal: Go deeper into proper DE work, (while keeping analytics sttenghts).\n\n​Current Certs I've already passed:\n* ​PL-300 (Power BI)\n* ​DP-600 (fabric Analytics Engineer Associate)\n* ​Plus, the basic dbt and Databricks Foundations certs.\n\n\n​So, what's the next move?\n​What serious, paid certification is the actual game-changer right now for staying competitive? \nShould I double down on a specific cloud (AWS/GCP/Azure DE path)? Focus on something like Databricks/snowflake/dbt ?\n\n\nI know certif are sometimes bullshiy, but I can't resist free time and free voucher :)\n\n​Hit me with your best recommendations !\n\nEdit: formating",
    "author": "Fast-Mediocre",
    "timestamp": "2025-10-18T20:51:39",
    "url": "https://reddit.com/r/dataengineering/comments/1oafuv2/company_is_paying_for_my_next_de_cert_which_one/",
    "score": 43,
    "num_comments": 36,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oaq471",
    "title": "Questions about the hugging face inference API",
    "content": "Before we begin, I'm a 17-year-old high school student in South Korea. I'm currently researching AI APIs for a personal project. Grok and Open AI are expensive, so I tried using the Gemini API in Google AI Studio. However, I can't use it in Korea because the minimum age requirement is 18. Then, I found the Hugging Face Inference API, but I can't find any reviews or detailed information about it in Korea, so I'm posting my questions here.\n\n1: Is this API free?\n\n2: If it's free, how many free requests can I make per day or month?\n\nThat's all. (I'm still learning, so I might be wrong.)",
    "author": "Inner_Truck_9561",
    "timestamp": "2025-10-19T06:49:13",
    "url": "https://reddit.com/r/dataengineering/comments/1oaq471/questions_about_the_hugging_face_inference_api/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oaqz6g",
    "title": "OTel tuning",
    "content": "Hi everyone , I've some bulleted queries regarding Open Telemetry tuning in production for ETL.\n1. What parameters to capture\n2. Sampling rate",
    "author": "Meal_Last",
    "timestamp": "2025-10-19T07:26:10",
    "url": "https://reddit.com/r/dataengineering/comments/1oaqz6g/otel_tuning/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oa0bi3",
    "title": "Best approach to large joins.",
    "content": "Hi I’m looking at table that is fairly large 20 billion rows.  Trying to join it against table with about 10 million rows. It is aggregate join that an accumulates pretty much all the rows in the bigger table using all rows in smaller table.  End result not that big. Maybe 1000 rows. \n\nWhat is strategy for such joins in database.  We have been using just a dedicated program written in c++ that just holds all that data in memory.  Downside is that it involves custom coding, no sql, just is implemented using vectors and hash tables. Other downside is if this server goes down it takes some time to reload all the data.  Also machine needs lots of ram.  Upside is the query is very fast.\n\nI understand a type of aggregate materialized view could be used.  But this doesn’t seem to work if clauses added to where. Would work for a whole join though.\n\nWhat are best techniques for such joins or what end typically used ?\n\n",
    "author": "Nearing_retirement",
    "timestamp": "2025-10-18T09:30:13",
    "url": "https://reddit.com/r/dataengineering/comments/1oa0bi3/best_approach_to_large_joins/",
    "score": 67,
    "num_comments": 46,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oagoef",
    "title": "handling sensitive pii data in modern lakehouse built with AWS stack",
    "content": "currently i'm building data lakehouse using aws native services - glue, athena, lakeformation, etc.   \n\npreviously wihtin data lake, sensitive PII data was handling in redimentary way, wherein, static fields per datasets are maintained ,and regex based data masking/redaction in consumption layers. With new data flowing, handling newly ingested sensitive data is reactive.\n\nwith data lakehouse, as per my understanding PII handling would be done i a more elegant way as part of data governance strategy, and to some extent i've explored lakeformation , PII tagging, access control based on tags, etc.  however, i still have below gaps : \n\n* with medallian architecture, and incremental data flow, i'm i suppose to auto scan incremental data and tag them while data is moving from bronze to silver?\n* should the tagging be from silver layer onwards?\n* whats the best way to accurately scan/tag at scale - any llm/ml option\n* scanning incremental data given high volume, to be scalable, should it be separate to the actual data movement jobs?\n   * if kept separate , now should we still redact from silver and how to workout the sequence as tagging might happen layer to movement\n   * or should we rather go with dynamic masking , again whats the best technology for this\n\n  \nany suggestion/ideas are highly appreciated.\n\n",
    "author": "bnarshak",
    "timestamp": "2025-10-18T21:37:23",
    "url": "https://reddit.com/r/dataengineering/comments/1oagoef/handling_sensitive_pii_data_in_modern_lakehouse/",
    "score": 8,
    "num_comments": 4,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oabl3i",
    "title": "For anyone working with Salesforce data in BigQuery, how are you handling formula fields?",
    "content": "I am currently using big query salesforce data transfer service to ingest salesforce data - right now it is on a preview mode and only supports full refreshes\n\nGoogle is releasing incremental updates feature to the connector, which is the more efficient option\n\nProblem with salesforce data is the formula fields and how they’re calculated on the go instead of storing actual data on the object \n\nI have a transaction data object with 446 fields and 183 of those fields are calculated/formula fields\n\nSome fields , like customer_address_street, is a formula field that references the customer object\n\nIf the address on the customer record on the customer object gets updated, the corresponding row(s) referencing the same customer on the transaction object will not get updated as the transaction row is not explicitly updated, and thus the systemmodstamp field remains unchanged\n\nIncremental refreshes wont capture this change of data and the transaction row from the transaction object will show the old address of the customer.\n\nHow are you currently handling this behaviour? Especially for objects with 183 formula fields, and more being added within the salesforce database?\n\nIdeally i want my salesforce data to refresh every 2 hours in the warehouse\n\n*For reference, i develop BI dashboards and i have very little experience in data engineering ",
    "author": "tytds",
    "timestamp": "2025-10-18T17:10:11",
    "url": "https://reddit.com/r/dataengineering/comments/1oabl3i/for_anyone_working_with_salesforce_data_in/",
    "score": 8,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oai1xu",
    "title": "Important analytical models/metrics you have made for social media and web analyst",
    "content": "Hello!\n\nI am making some data models for marketing insights through social and web channels. Surely each of their APIs provide users with some useful default metrics\n\nBut I am curious tho if anyone here has the experience on building metrics that don't exists in the first place\n\nWhat important metrics have you built for social media and web analyses that are not provided by default?\n\nHow's that helping your analyst or scientist?",
    "author": "ketopraktanjungduren",
    "timestamp": "2025-10-18T22:59:41",
    "url": "https://reddit.com/r/dataengineering/comments/1oai1xu/important_analytical_modelsmetrics_you_have_made/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oaa9jv",
    "title": "Evaluating my proposed approach",
    "content": "Hey guys, looking for feedback on a potential setup. For context, we are a medium sized company and our data department consists of me, my boss and one other analyst. I'm the most technical one, the other two can connect to a database in Tableau and that's about it. I'm fairly comfortable writing Python scripts and SQL queries, but I am not a software engineer.\n\nWe currently have MS SQL Server on prem that was set up a decade ago and is reaching its end of life in terms of support. For ETL, we've been using Alteryx for about as long as that, and for reporting we have Tableau Server. We don't have that much data (550GB total), and we ingest about 50k rows an hour in batched CSV files that our vendors send us. This data is a little messy and needs to be cleaned up before a database can ingest it.\n\nWith the SQL Server getting old and our renewal conversations with Alteryx going extremely poorly, my boss has directed me to research options for replacing both, or scaling Alteryx down to just the last mile for Tableau Server. Our main purposes are 1) upgrade our data warehouse to something with as little maintenance as possible and 2) continue to serve our Tableau dashboards 3) make ad-hoc analysis in Tableau possible for my boss and the other analyst. Ideally, we'd keep our costs to under 70k a year.\n\nSo far I've played around with Databricks, Clickhouse, Prefect, Dagster, and have started doing the dbt fundementals courses to get a better idea of it. While I liked Databricks's unity catalog and time travel capabilities of delta tables, the price and computing power of spark seems like overkill for our purposes/size. It felt like I was spending a lot of time spinning up clusters and frivolously spending cash working out the syntax.\n\nClickhouse caught my eye since it promises fast query times, it is easy enough to set up and put together a sample pipeline together, and the cloud database offering seems cheaper than DBX. It's nice that dbt-core can be used with it as well, because just writing queries and views inside the cloud console there seems like it can get hairy and confusing really fast.\n\nSo far, I'm thinking that we can run local Python scripts for ingesting data into Clickhouse staging tables, then write views on top of those for the cleaner silver + gold tables and let Alteryx/analysts connect to those. The tricky part with CH is how it manages upserts/deletions behind the scenes, but I think with ReplacingMergeTrees and solid queries, we could get around those limitations. It's also less forgiving with schema drift and inferring data types.\n\nSo my questions are as follows:\n\n\n* Does my approach make sense? \n* Are there other products worth looking into for my use case?\n* How do you guys evaluate the feasibility of a setup when the tools are new to you?\n* Is Clickhouse in your experience a solid product that will be around for the next 5-10 years?\n\n\nThank you for your time.",
    "author": "SoloArtist91",
    "timestamp": "2025-10-18T16:08:47",
    "url": "https://reddit.com/r/dataengineering/comments/1oaa9jv/evaluating_my_proposed_approach/",
    "score": 4,
    "num_comments": 9,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o9xi5u",
    "title": "How to deploy airflow project on EC2 instance using Terraform.",
    "content": "I'm currently working on deploying an Apache Airflow project to AWS EC2 using Terraform, and I have a question about how to handle the deployment of the project files themselves. I understand how to use Terraform to provision the infrastructure, but I’m not sure about the best way to automatically upload my entire Airflow project to the EC2 instance that Terraform creates. How do people typically handle this step?\n\nAdditionally, I’d like to make the project more complete by adding a machine learning layer, but I’m still exploring ideas. Do you have any suggestions for some ML projects using Reddit data? \n\nThank you in advance for your attention.",
    "author": "FindingVinland",
    "timestamp": "2025-10-18T07:37:11",
    "url": "https://reddit.com/r/dataengineering/comments/1o9xi5u/how_to_deploy_airflow_project_on_ec2_instance/",
    "score": 5,
    "num_comments": 7,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o9x20y",
    "title": "thoughts on databricks genie",
    "content": "Hi everyone, what are your thoughts on Databricks genie? I am just worried about it hallucinating or my business team relying too much on gen AI. Do you guys use it and is it comparable to other products and platforms?? What would you recommend instead and what don’t you like about it??",
    "author": "Ambitious-Option5637",
    "timestamp": "2025-10-18T07:19:13",
    "url": "https://reddit.com/r/dataengineering/comments/1o9x20y/thoughts_on_databricks_genie/",
    "score": 5,
    "num_comments": 19,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o9q246",
    "title": "Data Factory extraction techniques",
    "content": "Hey looking for some direction on Data factory extraction design patterns. Im new to the Data Engineering world but i come from infrastructure with experience standing Data factories and some simple pipelines. Last month we implemented a Databricks DLT Meta framework that we just scrapped and pivoted to a similar design that doesn't rely on all those onboarding ddl etc files. Now its just dlt pipelines perfoming ingestion based on inputs defined in asset bundle when ingesting. On the data factory side our whole extraction design is dependent on a metadata table in a SQL Server database. This is where i feel like this is a bad design concept to totally depend on a unsecured non version controlled table in a sql server database. That table get deleted or anyone with access doing anything malicious with that table we can't extract data from our sources. Is this a industry standard way of extracting data from sources? This feels very outdated and non scalable to me to have your entire data factory extraction design based on a sql table. We only have 240 tables currently but we are about to scale in December to 2000 and im not confident in that scaling at all. My concerns fall on deaf ears due to my co workers having 15+ years in data but primary using Talend not Data Factory and not using Databricks at all. Can someone please give me some insights on modern techniques if my suspicions are correct? ",
    "author": "Upstairs_Drive_305",
    "timestamp": "2025-10-18T01:05:43",
    "url": "https://reddit.com/r/dataengineering/comments/1o9q246/data_factory_extraction_techniques/",
    "score": 13,
    "num_comments": 24,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o957aj",
    "title": "Data infrastructure so \"open\" that there's only 1 box that isn't Fivetran...",
    "content": "Am I crazy in thinking this doesn't represent \"open\" at all? ",
    "author": "alittletooraph3000",
    "timestamp": "2025-10-17T08:58:01",
    "url": "https://reddit.com/r/dataengineering/comments/1o957aj/data_infrastructure_so_open_that_theres_only_1/",
    "score": 244,
    "num_comments": 54,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oa26wu",
    "title": "What is the right tool for running adhoc scripts (with some visibility)",
    "content": "We have many adhoc scripts to run at our org like:\n\n1. postgres data insertions based on certain params\n\n2. s3 to postgres \n\n3. run certain data cleaning scripts\n\n  \nI am thinking to use dagster for this because I need to have some visibility into when the devs are running certain scripts, view logs, track them etc.\n\n  \nI am I in the right direction to think about using dagster ? or any other tool better suits this purpose ??\n\n  \n",
    "author": "srimanthudu6416",
    "timestamp": "2025-10-18T10:45:07",
    "url": "https://reddit.com/r/dataengineering/comments/1oa26wu/what_is_the_right_tool_for_running_adhoc_scripts/",
    "score": 2,
    "num_comments": 9,
    "upvote_ratio": 0.58,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o9uqkv",
    "title": "How to convince my boss that table is the way to go",
    "content": "Hi all,\n\nfollowing the discussion here:  \n[https://www.reddit.com/r/dataengineering/comments/1n7b1uw/steps\\_in\\_transforming\\_lake\\_swamp\\_to\\_lakehouse/](https://www.reddit.com/r/dataengineering/comments/1n7b1uw/steps_in_transforming_lake_swamp_to_lakehouse/)\n\nIve explained my boss that the solution is to create some kind of pipeline that:  \n1. model the data  \n2. transform it to tabular format (Iceberg)  \n3. save it as parquet with some metadata\n\nHe insist that its not correct - and there is much better and easy solution - which is to index all the data and create our own metadata files that will have the location of the files we are looking for (maybe like MongoDB)  \nanother aspect why he against the idea of table format is because all our testing pipeline is based on some kind of json format (we transform the raw json to our own msgpec model).\n\nhow can I deliver to him that we are getting all this indexing for free when we are using iceberg, and if we miss some indexing in his idea we will need to go over all the data again and again.\n\nThank (for his protection he has 0 background in DE) ",
    "author": "CompetitionMassive51",
    "timestamp": "2025-10-18T05:41:09",
    "url": "https://reddit.com/r/dataengineering/comments/1o9uqkv/how_to_convince_my_boss_that_table_is_the_way_to/",
    "score": 6,
    "num_comments": 14,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o97xkg",
    "title": "Anyone feel like too much is expected of DEs (at small companies)",
    "content": "For example, I’ve noticed that an Eng department will have dedicated teams per product area/feature, i.e. multiple front end developers who only work on one part of the code base. More concretely, there may be one front end developer for marketing/onboarding, another for the customer facing app and maybe another for internal tools.\n\n\nEdit: I’m just using the FE role as an example. In reality, it’s actually a complete team \n\n\nHowever, the expectation is that one DE is responsible for all of the areas; understanding the data model, owning telemetry/product analytics, ensuring data quality, maintaining data pipelines, building the dw and finally either building charts or partnering with analytics/reporting on the BI. The point being that if one of these teams drops the ball, the blame still falls on the DE. \n\nI’ve had this expectation everywhere I’ve been. Some places are better than others in terms of how big the Data team can be and perhaps placing more responsibility on the downstream and upstream teams, but it’s generally never a “you are only responsible for this area”\n\nI’m rambling a bit but hopefully you get the idea. Is it only my experience? Is it only a startup thing? I’m curious to hear from others. ",
    "author": "jawabdey",
    "timestamp": "2025-10-17T10:40:55",
    "url": "https://reddit.com/r/dataengineering/comments/1o97xkg/anyone_feel_like_too_much_is_expected_of_des_at/",
    "score": 93,
    "num_comments": 37,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o9gfu5",
    "title": "Late data arrival partitioning best practices",
    "content": "This is a problem I’ve been thinking about for quite some time, and I just can’t wrap my head around it.\nIt’s generally recommended to partition data by the time it lands in S3 (i.e., event processing time) so that your pipelines are easier to make idempotent and deterministic. That makes sense operationally — but it creates a disconnect because business users don’t care about processing time; they care about event time.\nTo complicate things further, it’s also recommended to keep your bronze layer append-only and handle deduplication downstream.\nSo, I have three main questions:\n1. How would you approach partitioning in the bronze layer under these constraints?\n2. How would you design an efficient deduplication view on top of the bronze layer, given that it can contain duplicates and the business only cares about the latest record?\n3. Given that there might be intermediary steps in between, like dbt transformations when going from bronze to gold. How do you partition data in each layer so that your pipeline can scale? \n\nIs achieving idempotentcy and deterministic behavior at scale a huge challenge? \n\nI would be grateful if there are any resources on it that you can point me towards too? \n\n",
    "author": "afnan_shahid92",
    "timestamp": "2025-10-17T16:24:09",
    "url": "https://reddit.com/r/dataengineering/comments/1o9gfu5/late_data_arrival_partitioning_best_practices/",
    "score": 20,
    "num_comments": 12,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o96rrn",
    "title": "If you could work as a DE anywhere, what company or industry would it be - and why?",
    "content": "Curious what everyone's \"dream job\" looks like as a DE",
    "author": "luminoumen",
    "timestamp": "2025-10-17T09:57:25",
    "url": "https://reddit.com/r/dataengineering/comments/1o96rrn/if_you_could_work_as_a_de_anywhere_what_company/",
    "score": 58,
    "num_comments": 81,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o9gtyi",
    "title": "Courses for dim and fact modelling",
    "content": "Any recommendations for a course which teaches advanced and basic dimensional and fact modelling (kimball one preferably) \n\nPlease provide the one you have used and learnt from.",
    "author": "No_Requirement_9200",
    "timestamp": "2025-10-17T16:42:19",
    "url": "https://reddit.com/r/dataengineering/comments/1o9gtyi/courses_for_dim_and_fact_modelling/",
    "score": 16,
    "num_comments": 20,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o9335d",
    "title": "Engineers modifying DB columns without informing others",
    "content": "Hi everyone, I'm the only DE at a small startup, and this is my first DE job.\n\nCurrently, as engineers build features on our application, they occasionally modify the database by adding new columns or changing column data types, without informing me. Thus, inevitably, data gets dropped or removed and a critical part of our application no longer works. This leaves me completely reactive to urgent bugs.\n\nWhen I bring it up with management and our CTO, they said I should put in tests in the DB to keep track as engineers may forget. Intuitively, this doesn't feel like the right solution, but I'm open to suggestions for either technical or process implementations. \n\nStack: Postgres DB + python scripting to clean and add data to the DB.",
    "author": "Prestigious_Trash132",
    "timestamp": "2025-10-17T07:38:34",
    "url": "https://reddit.com/r/dataengineering/comments/1o9335d/engineers_modifying_db_columns_without_informing/",
    "score": 62,
    "num_comments": 77,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1oa7fk5",
    "title": "Is the data engineering job market good?",
    "content": "I am completely new to this. I just switched from mathematics to data engineering and had my first job. I am wondering whether the job market of this particular profession is tough or not? The US and Europe are both of interest to me. ",
    "author": "Dry_Masterpiece_3828",
    "timestamp": "2025-10-18T14:07:54",
    "url": "https://reddit.com/r/dataengineering/comments/1oa7fk5/is_the_data_engineering_job_market_good/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.44,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o98a9f",
    "title": "Embracing data engineering as a hobby",
    "content": "Hello all,\n\nI've decided to swallow my dreams of data engineering as a profession and just enjoy it as a hobby. I'm disentangling my need for more work from my desire to work with more data.\n\nAnyone else out there in a different field that performs data engineering at home for the love of it? I have no shortage of project ideas that involve modeling, processing, verifying, and analyzing \"massive\" (relative to home lab - so not massive) amounts of data. At hyper laptop scale!\n\nTo kick off some discussion... What's your home data stack? How do you keep your costs down? What do you love about working with data that compels you to do it without being paid for it?\n\nI'm sporting pyspark (for initial processing), cuallee (for verification and quality control), and pandas (for actual analysis). I glue it together with Bash and Python scripts. Occasionally parts of the pipeline happen in Go or C when I need speed. For cloud, I know my way around AWS and GCP, but don't typically use them for home projects.\n\nTake care,  \nme (I swear).\n\nEdit: minor readability edit.",
    "author": "axolotl-logic",
    "timestamp": "2025-10-17T10:54:33",
    "url": "https://reddit.com/r/dataengineering/comments/1o98a9f/embracing_data_engineering_as_a_hobby/",
    "score": 24,
    "num_comments": 53,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o9h3xu",
    "title": "Dagster, dbt, and DataHub integration",
    "content": "Currently, I have the [acryl_datahub_dagster_plugin](https://docs.datahub.com/docs/lineage/dagster) working in my Dagster instance, so that all assets that Dagster materializes will automatically show up in my DataHub instance. And with any dbt models that materialize via Dagster, those too all show up in DataHub, including the table lineage of all of the models that were executed.\n\nBut has anyone else figured out how to automatically get the columns for each model to show up in DataHub? The above plugin doesn't seem to do that, but wasn't sure if anyone already figured out a trick to get Dagster to upload those models' columns for me?\n\nLooking at the [Important Capabilities](https://docs.datahub.com/docs/generated/ingestion/sources/dbt#important-capabilities) for dbt in DataHub, it states that Column-Level Lineage should be possible, but wasn't sure if there was an automated way of doing this via Dagster? Or would I have to get the `CLI based Ingestion` working instead, and then just run that each time I deploy my code?\n\n\n#NOTE: using `Dagster OSS` and `dbt core`",
    "author": "actually_offline",
    "timestamp": "2025-10-17T16:55:19",
    "url": "https://reddit.com/r/dataengineering/comments/1o9h3xu/dagster_dbt_and_datahub_integration/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8vlp6",
    "title": "What are some other underrated books in the field of data?",
    "content": "",
    "author": "tastuwa",
    "timestamp": "2025-10-17T01:16:20",
    "url": "https://reddit.com/r/dataengineering/comments/1o8vlp6/what_are_some_other_underrated_books_in_the_field/",
    "score": 73,
    "num_comments": 14,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o81ydc",
    "title": "Hard to swallow.....",
    "content": "",
    "author": "growth_man",
    "timestamp": "2025-10-16T02:52:18",
    "url": "https://reddit.com/r/dataengineering/comments/1o81ydc/hard_to_swallow/",
    "score": 4165,
    "num_comments": 122,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o9du1r",
    "title": "Open source verifiable synthetic data library",
    "content": "Hi everyone, I’ve kicked off this open source project and I’d love to have you all try it. Full disclosure, this is a personal solo project and I’m releasing it under the MIT license so this is not a marketing post. \n\nIt’s a python library that allows you to create unlimited synthetic tabular data for training AI models. It uses Gaussian Copula to learn from the seed data and produce realistic and believable copies. It’s not just randomized noise so you’re not going to have teens with high blood pressure in a medical dataset or toddlers with mortgages on a financial dataset.\n\nAdditionally, it generates a cryptographic proof with every synthesis using hashes and Merkle roots for auditing purposes.\n\nI’d love your feedback and PRs if you’re up for it!",
    "author": "VeriSynth",
    "timestamp": "2025-10-17T14:30:16",
    "url": "https://reddit.com/r/dataengineering/comments/1o9du1r/open_source_verifiable_synthetic_data_library/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8yjpe",
    "title": "How do you architect your boilerplate code over projects.",
    "content": "Hey everyone, I have this one question maybe vague, but hope its ok to ask.... As there is a lot of boilerplate code around open telemetry, retries, DLQ's, scaling and overall code structure. How do you manage it from projects to projects.",
    "author": "Meal_Last",
    "timestamp": "2025-10-17T04:17:48",
    "url": "https://reddit.com/r/dataengineering/comments/1o8yjpe/how_do_you_architect_your_boilerplate_code_over/",
    "score": 10,
    "num_comments": 4,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o96izv",
    "title": "Compare and update two different databases",
    "content": "Hi guys,\n\nI have a client db (mysql) with 3 tables of each 3M rows.\n\nThis tables are bloated with useless and incorrect data, and thus we need to clean it and remove some columns and then insert it in our db (postgres).\n\nRuns fine the first time on my colleague pc with 128GB of ram....\n\nI need to run this every night and can't use so much ram on the server since it's shared....\n\nI thought about comparing the 2 DBs and updating/inserting only the rows changed, but since the schema is not equal i can't to that directly.\n\nI even thought about hashing the records, but still schema not equal...\n\nThe only option i can think of, is to select only the common columns and create an hash on our 2nd DB  and then successively compare only the hash, but still need to calculate it on the fly ( can't modify client db).\n\nUsing the updated\\_at column is a no go since i saw it literally change every now and then on ALL the records.\n\nAny suggestion is appreciated.  \nThanks",
    "author": "nikitarex",
    "timestamp": "2025-10-17T09:48:16",
    "url": "https://reddit.com/r/dataengineering/comments/1o96izv/compare_and_update_two_different_databases/",
    "score": 3,
    "num_comments": 12,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8vwac",
    "title": "Iceberg support in Apache Fluss - first demo",
    "content": "Iceberg support is coming to Fluss in 0.8.0 - but I got my hands on the first demo (authored by Yuxia Luo and Mehul Batra) and recorded a video running it.\n\nWhat it means for Iceberg is that now we'll be able to use Fluss as a hot layer for sub-second latency of your Iceberg based Lakehouse and use Flink as the processing engine - and I'm hoping that more processing engines will integrate with Fluss eventually.\n\nFluss is a very young project, it was donated to Apache Software Foundation this summer, but there's already a first success story by Taobao.\n\nHave you head about the project? Does it look like something that might help in your environment?",
    "author": "JanSiekierski",
    "timestamp": "2025-10-17T01:35:38",
    "url": "https://reddit.com/r/dataengineering/comments/1o8vwac/iceberg_support_in_apache_fluss_first_demo/",
    "score": 9,
    "num_comments": 2,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o93iwl",
    "title": "Power BI + Azure Synapse to Fabric migration",
    "content": "Wondering if anybody has experienced this type of migration to Fabric. I have met with Microsoft numerous times and have not gotten a straight answer. \n\nFor a long time we have had the BI tool decoupled from the ETL/Warehouse and we are used to be able to refresh models and re-run ETL/Pipelines or scripts in the DB in parallel, the DW300c size warehouse is independent from the \"current\" Power BI capacity. we have a large number of users, and I'm really skeptical that a P1 (F64) capacity will suffice for all our data related activities.\n\nWhat has been your experience so far?  To me migrating the models/dashboards sounds straightforward but sticking everything in Fabric (all-in-one platform) sounds scary to me, I have not had the chance to POC it myself to discard the \"resource contention\" problem. We can scale up/down in Synapse without worrying if it's going to break any Power BI related activities.\n\nI decided to post it here because looking up online is just a bunch of consulting firms trying to sell the \"product\". I want the real thing  . Thanks for your time in advance!!!",
    "author": "Plastic_Ad_9302",
    "timestamp": "2025-10-17T07:55:19",
    "url": "https://reddit.com/r/dataengineering/comments/1o93iwl/power_bi_azure_synapse_to_fabric_migration/",
    "score": 2,
    "num_comments": 8,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8td28",
    "title": "ClickHouse Date and DateTime types",
    "content": "Hi, how do u deal with Date columns which have valid dates before 1900-01-01? I have a Date column as Decimal(8, 0) which i want to convert to Date column, but a lot of the values are valid dates before 1900-01-01, which CH cant support, what do u do with this? Why is this even behavior?",
    "author": "Hot_While_6471",
    "timestamp": "2025-10-16T22:53:10",
    "url": "https://reddit.com/r/dataengineering/comments/1o8td28/clickhouse_date_and_datetime_types/",
    "score": 8,
    "num_comments": 7,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o93vum",
    "title": "Lakehouse Catalog Feature Dream List",
    "content": "What features would you want in your Lakehouse catalog? What features you like in existing solutions?",
    "author": "AMDataLake",
    "timestamp": "2025-10-17T08:08:17",
    "url": "https://reddit.com/r/dataengineering/comments/1o93vum/lakehouse_catalog_feature_dream_list/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8yia8",
    "title": "Any experiences with Marks and Spencer UK Digital (Data Engineer role)?",
    "content": "Hey all, I wanted to check regarding a Data Engineer role in M&amp;S Digital UK.\nWould love to know from people who’ve been there in Data teams what’s the culture like, how’s the team, and what should I look forward to?",
    "author": "Fearless_Choice7051",
    "timestamp": "2025-10-17T04:15:39",
    "url": "https://reddit.com/r/dataengineering/comments/1o8yia8/any_experiences_with_marks_and_spencer_uk_digital/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8tn2c",
    "title": "Databricks Serverless on GCP",
    "content": "Hey\nI’ve written a full Databricks Serverless blueprint on GCP (europe-west1) and would really appreciate your technical feedback and real-world insights.\nThe architecture includes:\n• 1 single GCP project with 3 Databricks workspaces (dev / preprod / prod)\n• Unity Catalog for governance and environment isolation\n• GitHub Actions CI/CD (linting, testing, automated deploys, manual gate for prod)\n• Terraform for infra (buckets, workspaces, catalogs)\n• Databricks Workflows for serverless orchestration\n• A strong focus on security, governance, and FinOps (usage-based billing, auto-termination, tagging)\n\nDoes this setup look consistent with your Databricks/GCP best practices?\nAny real-world feedback on:\n\nrunning serverless compute in production,\n\nmanaging multi-environment governance with Unity Catalog,\n\nor building mature CI/CD with Databricks Asset Bundles?\n\nOpen to any critique or advice \nThanks",
    "author": "LearnTeachSomething",
    "timestamp": "2025-10-16T23:09:50",
    "url": "https://reddit.com/r/dataengineering/comments/1o8tn2c/databricks_serverless_on_gcp/",
    "score": 6,
    "num_comments": 1,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8yh1r",
    "title": "Poc on using duckdb to read iceberg tables, and facing a problem with that (help!)",
    "content": "Hi, so I am a fresher and I have been told to do a poc on reading iceberg tables using duckdb. Now I am using duckdb in python to read iceberg tables but so far my attempts have been unsuccessful as the code is not executing. I have tried using iceberg_scan method by creating a secret before that as I cannot provide my aws credentials like access_id_key, etc in my code (as it is a safety breach). I know there are other methods too like using the pyiceberg library in python but I was not able to understand how that works exactly.\nIf anyone has any suggestions or insights or any other methods that could work, please let me know, it would be a great help and I would really appreciate it.\nHope everyone’s doing good:)\n\nEDIT- I was able to execute the code using iceberg_scan successfully without facing any errors.\nNow my senior said to look into using glue catalog for the same thing, if anyone has any suggestions for that, please let me know, thanks :)",
    "author": "pastelandgoth",
    "timestamp": "2025-10-17T04:13:45",
    "url": "https://reddit.com/r/dataengineering/comments/1o8yh1r/poc_on_using_duckdb_to_read_iceberg_tables_and/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o99nzs",
    "title": "I built a tool- csv/parquet to API in 30 seconds?",
    "content": "Is this of any value to anyone? i would love some people to test it. \n\nUses postgres and duckdb on the backend with php/htmx/alpinejs and c# on the backend\n\n  \n[https://instantrows.com](https://instantrows.com)",
    "author": "adulion",
    "timestamp": "2025-10-17T11:47:20",
    "url": "https://reddit.com/r/dataengineering/comments/1o99nzs/i_built_a_tool_csvparquet_to_api_in_30_seconds/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8j9ct",
    "title": "Code‑first Postgres→ClickHouse CDC with Debezium + Redpanda + MooseStack (demo + write‑up)",
    "content": "We put together a demo + guide for a code‑first, local-first CDC pipeline to ClickHouse using Debezium, Redpanda, and MooseStack as the dx/glue layer.   \n  \nWhat the demo shows:\n\n* Spin up ClickHouse, Postgres, Debeizum, and Redpanda locally in a single command\n* Pull Debezium managed Redpanda topics directly into code\n* Add stateless streaming transformations on the CDC payloads via Kafka consumer\n* Define/manage ClickHouse tables in code and use them as the sink for the CDC stream\n\nBlog: [https://www.fiveonefour.com/blog/cdc-postgres-to-clickhouse-debezium-drizzle](https://www.fiveonefour.com/blog/cdc-postgres-to-clickhouse-debezium-drizzle) • Repo:[ https://github.com/514-labs/debezium-cdc](https://github.com/514-labs/debezium-cdc)\n\n*(Disclosure: we work on MooseStack. ClickPipes is great for managed—this is the code‑first path.)*    \n\n\nRight now the demo solely focuses on the local dev experience, looking for input from this community on best practices for running Debezium in production (operational patterns, scaling, schema evolution, failure recovery, etc.).",
    "author": "Ok_Mouse_235",
    "timestamp": "2025-10-16T14:45:16",
    "url": "https://reddit.com/r/dataengineering/comments/1o8j9ct/codefirst_postgresclickhouse_cdc_with_debezium/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o80e49",
    "title": "Accidentally Data Engineer",
    "content": "I'm the lead software engineer and architect at a very small startup, and have also thrown my hat into the ring to build business intelligence reports.\n\nThe platform is 100% AWS, so my  approach was AWS Glue to S3 and finally Quicksight. \n\nWe're at the point of scaling up, and I'm keen to understand where my current approach is going to fail.\n\nShould I continue on the current path or look into more specialized tools and workflows?\n\nCost is a factor, ao I can't just tell my boss I want to migrate the whole thing to Databricks.. I also don't have any specific data engineering experience, but have good SQL and general programming skills",
    "author": "CzackNorys",
    "timestamp": "2025-10-16T01:09:22",
    "url": "https://reddit.com/r/dataengineering/comments/1o80e49/accidentally_data_engineer/",
    "score": 85,
    "num_comments": 48,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8luf7",
    "title": "Using modal for transformation of a huge dataset",
    "content": "Hi!\n\nAssume I got a huge dataset of crawled internet webpages, and I'd like to transform them page by page doing some kind of filtration, pre-processing, tokenization etc. Let's say that original dataset is stored along some metainformation in form of parquet files in S3.\n\nComing from enterprises, I have some background in Apache ecosystem as well as some older Big Tech MapReduce-kinda data warehouses, so my first idea was to use Spark to define those transformations using some scala/python code and just deal with it in batch processing manner. But before doing it \"classic ETL-style\" way, I decided to check some more modern (trending?) data stacks that are out there.\n\nI learned about Modal. They seem to be claiming about revolutionizing data processing, but I am not sure how exactly the practical usecases of data processing are expressed in them. Therefore, a bunch of questions to the community:\n\n1. They don't provide a notion of \"dataframes\", nor know anything about my input datasets, thus I must be responsible for somehow partitioning of the input into chunks, right? Like, reading slices of parquet file if needed, or coalescing groups of parquet files together before running an actual distributed computation?\n\n2. What about fault-tolerance? Spark has implemented protocols for atomic output commit, how do you expose result of a distributed data processing atomically without producing garbage from restarted jobs when using Modal? Do I, again, implement this manually?\n\n3. Is there any kind of long-running data processing operation state snapshotting? (not saying about individual executors, but rather the application master) If I have a CPU intensive computation running for 24 hours and I close my laptop lid, or the initiator host dies some other way, am I automatically screwed?\n\n4. Are there tweaks like speculative execution, or at least a way how to control/abort individual function executions? It is always a pity to see how 99% of a job finished with high concurrency and last couple of tasks ended up on some faulty host and take eternity to finish. \n\n5. Since they are a cloud service - do you know about their actual scalability limits? I have a computation CPU cluster of \\~25k CPU cores in my company, do they have some comparable fleet? It would be quite stupid to hit into some limitation like \"no more than 1000 cpu cores per user unless you are an enterprise folk paying $20k/month just for a license\"... \n\n6. Also, them being non-opensource also makes it harder to understand what exactly happens under the hood, are there any open-source competitors to them? Or at least a way how to bring them on-premise to my company's fleet?\n\nAnd a more generic question – has any of you folks ever tried actually processing some huge datasets with them? Right now it looks more like a tool for smaller developer experiments, or for time-slicing GPUs for seconds, but not something that I would use to build a reliable data pipeline over. But maybe I am missing something. \n\nPS I was told Ray also became popular recently, and they seem to be open-source as well, so will check them later as well.",
    "author": "Remote_Impact_8173",
    "timestamp": "2025-10-16T16:35:08",
    "url": "https://reddit.com/r/dataengineering/comments/1o8luf7/using_modal_for_transformation_of_a_huge_dataset/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8tf72",
    "title": "Elusion v7.9.0 has additional DASHBOARD features",
    "content": "Elusion v7.9.0 has a few additional features for filtering Plots. This time I'm highlighting filtering categorical data.\n\nWhen you click on a Bar, Pie, or Donut chart, you'll get cross-filtering.\n\nTo learn more, check out the GitHub repository: [https://github.com/DataBora/elusion](https://github.com/DataBora/elusion)\n\nhttps://preview.redd.it/0cuocl8d3mvf1.png?width=1106&amp;format=png&amp;auto=webp&amp;s=0c064cd71955385c2e0970e84f66faa64e9244e2\n\n  \n",
    "author": "DataBora",
    "timestamp": "2025-10-16T22:56:41",
    "url": "https://reddit.com/r/dataengineering/comments/1o8tf72/elusion_v790_has_additional_dashboard_features/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8d1i4",
    "title": "What’s your motivation ?",
    "content": "Unless data is a top priority from your top management which means there will multiple teams having data folks - anayst, engineer, mle, data scientists etc\nOr, you are tech company which is truly data driven what’s the point of working in data in small teams and companies where it is not the focus?\nCoz no one’s looking at the dashboards being built, data pipelines optimized or even the business questions being answered using data. \nIt is my assumption but 80% of people working in data fall in the category where data is not a focus, it is a small team or some exec wanted to grow his team hence hired a data team.\nHow do you keep yourself motivated if no one uses what you build?\nI feel like a pivot to either SWE or a business role would make more sense as you are creating something that has utility in most companies.\n\nP.S : Frustrated small team DE",
    "author": "cyamnihc",
    "timestamp": "2025-10-16T10:48:31",
    "url": "https://reddit.com/r/dataengineering/comments/1o8d1i4/whats_your_motivation/",
    "score": 7,
    "num_comments": 5,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o87r49",
    "title": "Data Vendors Consolidation Speculation Thread",
    "content": "With Fivetrans getting dbt and Tobiko under it's belt, is there any other consolidation you'd guess is coming sooner or later?",
    "author": "AMDataLake",
    "timestamp": "2025-10-16T07:32:19",
    "url": "https://reddit.com/r/dataengineering/comments/1o87r49/data_vendors_consolidation_speculation_thread/",
    "score": 12,
    "num_comments": 12,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8qtsx",
    "title": "How am i supposed to set up an environment with Airflow and Spark?",
    "content": "I have been trying to set up Airflow and Spark with Docker. Apparently, the easiest way would usually be to use the Bitnami Spark image. However, this image is no longer freely available, and I can't find any information online on how to properly set up Spark using the regular Spark image. Anyone have any idea on how to make it work with Airflow?",
    "author": "goreshiet",
    "timestamp": "2025-10-16T20:32:53",
    "url": "https://reddit.com/r/dataengineering/comments/1o8qtsx/how_am_i_supposed_to_set_up_an_environment_with/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8whgl",
    "title": "Ai-based specsheet data extraction tool for products.",
    "content": "Hey everyone,\n\nI wanted to share a tool I’ve been working on that’s been a total game-changer for comparing product spec sheets.\n\nYou know the pain: downloading multiplePDFs from different vendors or manufacturers, opening each one, manually extracting specs, normalizing units, and then building a comparison table in Excel… takes hours (sometimes days).\n\nWell, I built something to solve exactly that problem:\n\n1.) Upload multiple PDFs at once.\n\n2.) Automatically extract key specs from each document.\n\n3.) Normalize units and field names across PDFs (so “Power”, “Wattage”, and “Rated Output” all align) \n\n4.)Generate a sortable, interactive comparison table\n\n5.)Export as CSV/Excel for easy sharing\n\nIt’s designed for engineers, procurement teams, product managers, and anyone who deals with technical PDFs regularly.\n\nI want anyone who is interested and faces these problems regularly to help me validate this tool and comment \"interested\" and leave your opinions and feedback. ",
    "author": "Acceptable-Hunt1823",
    "timestamp": "2025-10-17T02:13:48",
    "url": "https://reddit.com/r/dataengineering/comments/1o8whgl/aibased_specsheet_data_extraction_tool_for/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.45,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7vyxe",
    "title": "What is your favorite viz tool and why?",
    "content": "I know this isn't \"directly\" related to data engineering, but I find myself constantly looking to visualize my data while I transform it. Whether part of an EDA process, inspection process, or something else.\n\n  \nI can't stand any of the existing tools, but curious to hear about what your favorite tools are, and why?\n\n  \nAlso, if there is something you would love to see, but doesn't exist, share it here too.",
    "author": "Impressive_Run8512",
    "timestamp": "2025-10-15T20:39:13",
    "url": "https://reddit.com/r/dataengineering/comments/1o7vyxe/what_is_your_favorite_viz_tool_and_why/",
    "score": 42,
    "num_comments": 77,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o88ocz",
    "title": "How do you build anomaly alerts for real business metrics with lots of slices?",
    "content": "Hey folks! I’m curious how teams actually build anomaly alerting for business metrics when there are many slices (e.g., country \\* prime entity \\* device type \\* app version).\n\nWhat I’m exploring:  \nMAD/robust Z, STL/MSTL, Prophet/ETS, rolling windows, adaptive thresholds, alert grouping.\n\nOne thing I keep running into: the more “advanced” the detector, the more false positives I get in practice. Ironically, a simple 3-sigma rule often ends up the most stable for us. If you’ve been here too - what actually reduced noise without missing real incidents?",
    "author": "Civil-Bee-f",
    "timestamp": "2025-10-16T08:07:44",
    "url": "https://reddit.com/r/dataengineering/comments/1o88ocz/how_do_you_build_anomaly_alerts_for_real_business/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8jom8",
    "title": "Large memory consumption where it shouldn't be with delta-rs?",
    "content": "I know this isn't a sub specifically for technical questions, but I'm really at a loss here. Any guidance would be greatly appreciated.\n\nDisclaimer that this problem is with delta-rs (in Python), not Delta Lake with Databricks.\n\nThe project is simple: We have a Delta table, and we want to update some records.\n\nThe solution: use the merge functionality.\n\n    dt = DeltaTable(\"./table\")\n    updates_df = get_updates()\n    \n    dt.merge(\n        updates_df,\n        predicate=(\n            \"target.pk       = source.pk\"\n            \"AND target.salt = source.salt\"\n            \"AND target.foo  = source.foo\"\n            \"AND target.bar != source.bar\"\n        ),\n        source_alias=\"source\",\n        target_alias=\"target\",\n    ).when_matched_update(\n        updates={\"bar\": \"source.bar\"}\n    ).execute()\n\nThe above code is essentially a simplified version of what I have, but all the core pieces are there. It's quite simple in general. The delta table in `./table` is very very large, but it is partitioned nicely with around 1M records per partition (salted to get the partitions balanced). Overall there's \\~2B records in there, while `updates_df` has 2M.\n\nThe problem is that the merge operation balloons memory **massively** for some reason. I was under the impression that working with partitions would drastically decrease the memory consumption, but no. It eventually OOMs, exceeding 380G. This doesn't make sense. Doing a join on the same column between the two tables with duckdb, I find that there would be \\~120k updates across 120 partitions (there are a little over 500 partitions). For one, duckdb can handle the join just fine, and two, it's working with such a small amount of updates. How is it using so much? The partitioned columns are `pk` and `salt`, which I am using in the predicate, so I don't think it has anything to do with lack of pruning.\n\nIf anyone has any experience with this or the solution is glaringly obvious (never used Delta before), then I'd love to hear your thoughts. Oh and if you're wondering why I don't use a more conventional solution for this - that's not my decision. And even if it were, now I'm just curious at this point.",
    "author": "echanuda",
    "timestamp": "2025-10-16T15:02:34",
    "url": "https://reddit.com/r/dataengineering/comments/1o8jom8/large_memory_consumption_where_it_shouldnt_be/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7xl3r",
    "title": "Data Engineering Playbook for a leader.",
    "content": "I have been in software leadership positions - VP at Small to medium company, and Director at a large company for last few years and have managed mostly web/mobile related projects and have a very strong hands on experience with architecture and coding in the same. During the time, I have also led some analytics teams which had reporting frameworks and most recently GenAI related projects. Have a good understanding of GenAI LLM integrations. I have basic understanding of models and model architecture but have a good handle on with the recent LLM integration/workflow frameworks like Langchain, Langtrace etc. \n\nCurrently, while looking for a change, I am seeing much more demand in Data which makes total sense to me with the direction industry is heading. I am wondering how should i get myself more framed as a Data engineering leader than the generic engineering leader role. I have done some LinkedIn basic trainings but seems like i will need a little more indepth knowledge as my past hands on experience has been in Java, nodejs and cloud native architectures. \n\nDo you folks have any recommendation on how should i get up to speed, is there a databricks or snowflake level certification which i go for to understand the basic concepts. I don't care whether i clear the exam or not but learning is going to be a key to me.",
    "author": "logical-dreamer",
    "timestamp": "2025-10-15T22:09:12",
    "url": "https://reddit.com/r/dataengineering/comments/1o7xl3r/data_engineering_playbook_for_a_leader/",
    "score": 20,
    "num_comments": 6,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o857sq",
    "title": "How to implement text annotation and collaborative text editing at the same time?",
    "content": "General problem I'm been considering in the back of my head, when trying to figure out how to make some sort of interactive web UI for various language texts, and allow text annotation, and text editing (to progressively/slowly clean up the text mistakes over time, etc.). But in a way such a way that, if you or someone edits the text down the road, it won't mess up the annotations and stuff like that?\n\nI don't know much about linguistic annotation software (saw [this brief overview of some options](https://www.labellerr.com/blog/text-annotation-labeling-tools/#1-labellerr)), but what I've looked at so far are basically these:\n\n- [Perseus Greek Texts](https://www.perseus.tufts.edu/hopper/text?doc=Perseus%3atext%3a1999.01.0227) (click on individual words to lookup)\n- [Prodigy demo](https://demo.prodi.gy/?=null&amp;view_id=pos_manual) (on of the text annotation tools I could quickly try in basic mode for free)\n- [Logeion](https://logeion.uchicago.edu/articulus) (double click to visit terms anywhere in the text)\n\nBut the general problem I'm getting stuck on in my head is what I was saying, here is a brief example to clarify:\n\n- Say we are working with the Bible text (bunch of books, divided into chapters, divided into verses)\n- The data model I'm considering at this point is a tree of JSON basically, `text_section` can be arbitrarily nested (bible -&gt; book -&gt; chapter), and then at the end are `text_span` in the children (verses here).\n- Say the Bible unicode text is super messy, random artifacts here and there, extra whitespace and punctuation in various spots, overall the text is 90% good quality but could use months or years of fine-tuned polish to clean it up and make it perfect. (Sefaria texts, open-source Hebrew texts, are super-super messy, tons of textual artifacts that could use some love to clean up and stuff eventually over time... for example.).\n- But say you can also annotate the text at any point, creating probably \"selection_ranges\" of text within or across verses, etc.. Then you can label or do whatever to add metadata to those ranges.\n\nProblem is:\n\n- Text is being cleaned up over say a couple years, a few minor tweaks every day.\n- Annotations are being added every day too.\n\nEdge-case is basically this:\n\n- Annotation is added on some selected text\n- Text gets edited (maybe user is not even aware of or focused on the annotation UI at this point, but under the hood the metadata is still there).\n- Editor removes some extra whitespace, and adds a missing word (as they found say by looking at a real manuscript scan).\n- Say the editor added `Newton` to `Isaac`, so whereas before it said `foo    bar &lt;thing&gt;Isaac&lt;/thing&gt; ...   baz`, now it says `foo bar &lt;thing&gt;Isaac&lt;/thing&gt; Newton baz`.\n- Now the annotation sort of changes meaning, and needs to be redone (this is a terrible example, I tried thinking of what my mind's stumbling on, but can't quite pin it down totally yet).\n- Should say `foo bar &lt;thing&gt;Isaac Newton&lt;/thing&gt; baz` let's say (but the editor never sees anything annotation-wise...)\n\nBasically, trying to show that, the annotations can get messed up, and I don't see a systematic way to handle or resolve that if editing the text is also allowed.\n\nYou can imagine other cases where some annotation marks like a phrase or idiom, but then the editor comes and changes the idiom to be something totally different, or just partially different, whatever. Or splits the annotation somehow, etc..\n\nBasically, have apps or anyone figured out generally how to handle this general problem? How to not make it so when you edit, you have to just delete the annotations, but it somehow smart merges, or flags it for double-checking, etc.. Basically there is a lot to think through functionality-wise, and I'm not sure if it's already been done before. It's both a data-modeling problem, and a UI/UX problem. But mainly concerned about the technical data-modeling problem here.",
    "author": "lancejpollard",
    "timestamp": "2025-10-16T05:47:18",
    "url": "https://reddit.com/r/dataengineering/comments/1o857sq/how_to_implement_text_annotation_and/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7k8av",
    "title": "Final nail in the coffin of OSS dbt",
    "content": "https://www.reuters.com/business/a16z-backed-data-firms-fivetran-dbt-labs-merge-all-stock-deal-2025-10-13/\n\nFirst they split off Fusion as proprietary and put dbt-core in maintenance mode, now they merged with Fivetran (which has no history of open). Not to mention SQLMesh which will probably get killed off.\n\nIs this the death of OSS dbt? ",
    "author": "City-Popular455",
    "timestamp": "2025-10-15T12:12:38",
    "url": "https://reddit.com/r/dataengineering/comments/1o7k8av/final_nail_in_the_coffin_of_oss_dbt/",
    "score": 103,
    "num_comments": 86,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o88lxr",
    "title": "Is Azure blob storage slow as fuck?",
    "content": "Hello,\n\nI'm seeking help with a bad situation I have with Synapse + Azure storage (ADLS2).\n\nThe situation: I'm forced to use Synapse notebooks for certain data processing jobs; a couple of weeks ago I was asked to create a pipeline to download some financial data from a public repository and output it to Azure storage.\n\nSaid data is very small, a few Megabytes at most. So I first developed the script locally, used Polars for dataframe interface and once I verified everything worked, I put it online.\n\n# Edit\n\nApparently I failed to explain myself since nearly everyone who answered, implicitly thinks I'm an idiot, so while I'm not ruling that option out I'll just simplify:\n\n* I have some code that reads data from an online API and writes it somewhere.\n* The data is a few MBs.\n* I'm using Polars, not Pyspark\n* Locally it runs in one minute.\n* On Synapse it runs in 7 minutes.\n* Yes, I did account for pool spin up time, it takes 7 minutes after the pool is ready.\n* Synapse and storage account are in the same region.\n* I am FORCED to use Synapse notebooks by the organization I'm working for.\n* I don't have details about networking at the moment as I wasn't involved in the setup, I'd have to collect them.\n\nNow I understand that data transfer goes over the network, so it's gotta be slower than writing to disk, but what the fuck? 5 to 10 times slower is insane, for such a small amount of data.\n\nThis also makes me think that the Spark jobs that run in the same environment would be MUCH faster in a different setup.\n\nSo this said, the question is, is there anything I can do to speed up this shit?\n\n# Edit 2\n\nUnder suggestion of some of you I then profiled every component of the pipeline, which eventually confirmed the suspicion that the bottleneck is in the I/O part.\n\nHere's the relevant profiling results if anyone is interested:\n\n\n### local\n\n```\n_write_parquet:\n  Calls: 1713\n  Total: 52.5928s\n  Avg:   0.0307s\n  Min:   0.0003s\n  Max:   1.0037s\n\n_read_parquet (this is an extra step used for data quality check):\n  Calls: 1672\n  Total: 11.3558s\n  Avg:   0.0068s\n  Min:   0.0004s\n  Max:   0.1180s\n\ndownload_zip_data:\n  Calls: 22\n  Total: 44.7885s\n  Avg:   2.0358s\n  Min:   1.6840s\n  Max:   2.2794s\n\nunzip_data:\n  Calls: 22\n  Total: 1.7265s\n  Avg:   0.0785s\n  Min:   0.0577s\n  Max:   0.1197s\n\nread_csv:\n  Calls: 2074\n  Total: 17.9278s\n  Avg:   0.0086s\n  Min:   0.0004s\n  Max:   0.0410s\n\ntransform (includes read_csv time):\n  Calls: 846\n  Total: 20.2491s\n  Avg:   0.0239s\n  Min:   0.0012s\n  Max:   0.2056s\n```\n\n### synapse\n\n```\n_write_parquet:\n  Calls: 1713\n  Total: 848.2049s\n  Avg:   0.4952s\n  Min:   0.0428s\n  Max:   15.0655s\n\n_read_parquet:\n  Calls: 1672\n  Total: 346.1599s\n  Avg:   0.2070s\n  Min:   0.0649s\n  Max:   10.2942s\n\ndownload_zip_data:\n  Calls: 22\n  Total: 14.9234s\n  Avg:   0.6783s\n  Min:   0.6343s\n  Max:   0.7172s\n\nunzip_data:\n  Calls: 22\n  Total: 5.8338s\n  Avg:   0.2652s\n  Min:   0.2044s\n  Max:   0.3539s\n\nread_csv:\n  Calls: 2074\n  Total: 70.8785s\n  Avg:   0.0342s\n  Min:   0.0012s\n  Max:   0.2519s\n\ntransform (includes read_csv time):\n  Calls: 846\n  Total: 82.3287s\n  Avg:   0.0973s\n  Min:   0.0037s\n  Max:   1.0253s\n```\n\n### context:\n\n`_write_parquet`: writes to local storage or adls.\n\n`_read_parquet`: reads from local storage or adls.\n\n`download_zip_data`: downloads the data from the public source to a local `/tmp/data` directory. Same code for both environments.\n\n`unzip_data`: unpacks the content of downloaded zips under the same local directory. The content is a bunch of CSV files. Same code for both environments.\n\n`read_csv`: Reads the CSV data from local `/tmp/data`. Same code for both environments.\n\n`transform`: It calls `read_csv` several times so the actual wall time of just the transformation is its total minus the total time of read_csv. Same code for both environments.\n\n\\---\n\nold message:\n\n&gt;!~~The problem was in the run times. For the same exact code and data:~~!&lt;\n\n* &gt;!~~Locally, writing data to disk, took about 1 minute~~!&lt;\n* &gt;!~~On Synapse notebook, writing data to ADLS2 took about 7 minutes~~!&lt;\n\n&gt;!~~Later on I had to add some data quality checks to this code and the situation became even worse:~~!&lt;\n\n* &gt;!~~Locally only took 2 minutes.~~!&lt;\n* &gt;!~~On Synapse notebook, it took 25 minutes.~~!&lt;\n\n&gt;!~~Remember, we're talking about a FEW Megabytes of data. Under suggestion of my team lead I tried to change destination an used a blob storage of premium tier (this one in the red).~~!&lt;\n\n&gt;!~~It did have some improvements, but only went down to about 10 minutes run (vs again the 2 mins local).~~!&lt;",
    "author": "wtfzambo",
    "timestamp": "2025-10-16T08:05:09",
    "url": "https://reddit.com/r/dataengineering/comments/1o88lxr/is_azure_blob_storage_slow_as_fuck/",
    "score": 4,
    "num_comments": 69,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8ejxi",
    "title": "Practical Guide to Semantic Layers: Your MCP-Powered AI Analyst (Part 2)",
    "content": "",
    "author": "OkraCommercial3138",
    "timestamp": "2025-10-16T11:44:02",
    "url": "https://reddit.com/r/dataengineering/comments/1o8ejxi/practical_guide_to_semantic_layers_your/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7xer0",
    "title": "7 Best Free Data Engineering Courses",
    "content": "",
    "author": "SilverConsistent9222",
    "timestamp": "2025-10-15T21:59:04",
    "url": "https://reddit.com/r/dataengineering/comments/1o7xer0/7_best_free_data_engineering_courses/",
    "score": 10,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8apr1",
    "title": "How would you handle nwp data and customer data both time series with different frequencies for a data warehouse?",
    "content": "So the idea is that we get weather data with reference time and forecast time with a frequency of 6 hours and customer data with a frequency of 15 minutes. Consider also that there 5 weather data sources and many customers i.e. 100.\nThere are some options I have thought of:\n1. Storing as parquet files in gcs in a hive structure bucket/customer_id/source/year/month/day/hour. With duckDB on top to query these files.\n2. Postgres with a single table hash partiotioned by customer id with fields: reference time, forecast time, customer id, nwp source, features as JSON.\nHaving difficulties in wrapping up my head over the pros and cons of these options. Any suggestions would be helpful.",
    "author": "Ok_Garbage_2884",
    "timestamp": "2025-10-16T09:24:03",
    "url": "https://reddit.com/r/dataengineering/comments/1o8apr1/how_would_you_handle_nwp_data_and_customer_data/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8208j",
    "title": "Expanding a local dbt-core project to production — should I integrate with Airflow or rely on CI/CD + Pre-Prod?",
    "content": "Hi I'm Steve.\n\nOur organization running dbt-core locally and want to move it into production\n\nWe already use Airflow on Kubernetes and CI/CD vis GitHub Actions.\n\nCurious what others do - run dbt inside Airflow DAGs? or just let CI/CD handle it separately?\n\nAny pros/cons you've seen in production?\n\nAdditional.\n\nWe are using...\n\nApache Airflow 2.7.3 (running in Kubernetes)\n\ndbt-core 1.9.1 (Just test, run in local environment)\n\nAnd we have two repositories:\n\n* One for Apache Airflow DAGs\n* One for dbt-core\n\nWould you recommend we have to integrate them or keeping seperate?\n\nI wish you guys help us :)",
    "author": "Upstairs_Pack3280",
    "timestamp": "2025-10-16T02:55:34",
    "url": "https://reddit.com/r/dataengineering/comments/1o8208j/expanding_a_local_dbtcore_project_to_production/",
    "score": 4,
    "num_comments": 3,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o84cs6",
    "title": "data engineering test",
    "content": "hey guys! so, i have an assessment to do in the next 4 days regarding a job position, for a junior data engineer role. i’ve never had to do one so idk what is the best place to find material to study and train\ndo you guys recommend anything? any website or material? i believe the test will be focused on pyspark and sql\nps.: i’m more interested in websites with practical questions and answers than theory :)",
    "author": "ExampleInteresting11",
    "timestamp": "2025-10-16T05:06:32",
    "url": "https://reddit.com/r/dataengineering/comments/1o84cs6/data_engineering_test/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7eicq",
    "title": "Looking for Advice to Stay Relevant technically as a Senior Data Engineer",
    "content": "I have 15 years of experience as a Data Engineer, mostly in investment banking, working with ETL pipelines, Snowflake, SQL, Spark, Python, and Shell scripting.\n\nLately, my role has shifted more toward strategy and less hands-on engineering. While my firm is modernizing its data stack, I find that the type of work I’m doing no longer aligns with where I want to grow technically.\n\nI realize the job market is competitive, and I haven’t applied for any roles in the past five years, which feels daunting. I also worry that my hands-on skills are getting rusty, as I often rely on tools like Copilot to assist with development.\n\nQuestions:\n\n1. What emerging tools or skills should I focus on to stay relevant as a senior data engineer in 2025–26?\n\n\n2. How do you recommend practicing technical skills and market readiness after being out of the job market for a while?\n\n\nAny advice from fellow senior data engineers or those in banking/finance tech would be greatly appreciated!",
    "author": "Final-Mix-9106",
    "timestamp": "2025-10-15T08:41:36",
    "url": "https://reddit.com/r/dataengineering/comments/1o7eicq/looking_for_advice_to_stay_relevant_technically/",
    "score": 82,
    "num_comments": 24,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o88lg6",
    "title": "Suggestion needed with Medallion Architecture",
    "content": "Hi, I'm new to databricks (Please go easy) and  i'm trying to implement an ETL pipeline for data coming from different sources for end users in our company. Although new data comes in the Azure SQL Database daily basis (we anticipate 10 GB approximately of data on the weekly basis).\n\n  \nWe get also get Files in Landing Zone (ADLS Gen2) on weekly basis (Upto 50 GB).\n\nNow we need to process all of this data weekly.  Currently, i have come up with this medallion architecture:\n\n**Landing to Bronze:**\n\n**-&gt; data in azure sql source**\n\n\t\\-&gt; Using ADF to copy the files from azure sql (multiple database instances) to bronze. \n\n\t\\-&gt; We have a configuration file from which we know, what is the database, table, the load type (full load/incremental), datasource\n\n\t\\-&gt; We process the data accordingly and also have an audit table where the watermark for tables with incremental load is maintained\n\n\t\\-&gt; Creating delta tables on the bronze (the tables here contain the data source and timestamp columns as well)\n\n**-&gt; data in landing zone**\n\n\\-&gt; using autoloader to copy the files from landing zone to bronze\n\n\t\\-&gt; Landing zone uses a fairly nested structure (files arriving weekly).\t\n\n\\-&gt; Also fetching ICD Codes from athena and saving then to bronze.\n\n\\-&gt; We create delta tables in the bronze layer.\n\n**Silver:**\n\n\\-&gt; From bronze, we read the data into silver. This is incremental using MERGE UPSERT (Is there a better approach)\n\n\\-&gt; We apply Common Data Model in the Silver Layer and Type SCD 2 for dimension tables. Here \n\n\\-&gt; We do the quality checks as well. On failures we halt the pipeline as the data quality is critical to the end user.\n\n\\-&gt; We are also get the data dictionary so schema evolution is handled by using a custom schema registry and compare the current infered schema with the latest schema version we are maintaining. All of these come under the data quality checks. If anyone fail, we send email.\n\n\\-&gt; The schema is checked for the raw files we receive in the ADLS2 Landing Zone.\n\n**Gold:**\n\n\\-&gt; Data is loaded from silver to Gold Layer with predefined data model \n\n\n\nPlease tell me what changes i can make in this approach?",
    "author": "Time-Cardiologist-51",
    "timestamp": "2025-10-16T08:04:36",
    "url": "https://reddit.com/r/dataengineering/comments/1o88lg6/suggestion_needed_with_medallion_architecture/",
    "score": 1,
    "num_comments": 7,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o88isc",
    "title": "Every company is a data company, but most don't know where to start",
    "content": "",
    "author": "jorinvo",
    "timestamp": "2025-10-16T08:01:50",
    "url": "https://reddit.com/r/dataengineering/comments/1o88isc/every_company_is_a_data_company_but_most_dont/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o85f8n",
    "title": "Intelligent Applications: The Next Step in Data-Driven Decision-Making",
    "content": "",
    "author": "sadyetfly11",
    "timestamp": "2025-10-16T05:56:37",
    "url": "https://reddit.com/r/dataengineering/comments/1o85f8n/intelligent_applications_the_next_step_in/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7tcyz",
    "title": "Data Collecting",
    "content": "Hi everyone! I'm doing data collection for a class, and it would be amazing if you guys could fill this out for me! (it's anonymous). Thank you so much!!!\n\n[https://forms.gle/zjFdkprPyFWv5Utx6](https://forms.gle/zjFdkprPyFWv5Utx6)",
    "author": "According_Meal_387",
    "timestamp": "2025-10-15T18:30:27",
    "url": "https://reddit.com/r/dataengineering/comments/1o7tcyz/data_collecting/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o83xw9",
    "title": "Any free solution for integrating BI into React Website?",
    "content": "Spent two days creating a DBT medallion architecture pipeline for creating a dashboard from a Postgres DB containing 25+ tables which was highly normalised. Today they tell me the requirements have changed and they want the dashboard Integrated to website directly ( even iframe won't work).\n\nI was explaining the pipeline to some full stack devs and explained them why I created the gold layer and it is essential for adding BI services. They were highly dismissive of it saying that's not how things should be ( we were discussing how we can make the dashboard using chart.js which I know nothing about).\n\nDid they have a point( for building the dashboard directly in React using APIs? or should I ignore them?",
    "author": "Potential_Loss6978",
    "timestamp": "2025-10-16T04:45:42",
    "url": "https://reddit.com/r/dataengineering/comments/1o83xw9/any_free_solution_for_integrating_bi_into_react/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o83jwp",
    "title": "Best way to run a detailed global market model - Google Sheets?",
    "content": "I run a huge data product that gives information on the revenue and susbcriber numbers of most major video services in the world (e.g. Netflix) on a by-country basis.\n\ncurrently this is split across 14 siloed Google Sheets , that are largely not linked with eachother (except for some core demographic data which all points to another single sheet). There are 2 Google Sheets for each of the 7 global regions we cover, with a tab for every country in that region, as well as summary tabs for every data point.\n\nthis seems like a crazily inefficient way to run a model this size but I don't have a background in data and am unsure how I could improve the process. any ideas? could learning SQL (or anything else) help me? ",
    "author": "ThisEconomist4908",
    "timestamp": "2025-10-16T04:25:08",
    "url": "https://reddit.com/r/dataengineering/comments/1o83jwp/best_way_to_run_a_detailed_global_market_model/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7dp2i",
    "title": "\"Data Person\" in a small fintech - How do I shape my “flexible”role towards Data Engineering?",
    "content": "Sorry I’m posting from a new account as my main one indicates my full name.\n\nI'm a fairly new hire at a fintech company that deals with payment data from a bunch of different banks and clients. I was hired a few months ago as a Data Analyst but the role has become super flexible right now, and I'm basically the only person purely focused on data.\n\nI spent the first few months under the Operations team helping with reconciliation (since my manager, who is now gone, wasn't great at it), using Excel/Google Sheets and a few Python scripts to expedite that process. That messy part is thankfully over, and I'm free to do data stuff.\n\nThe problem is, I'm not experienced enough to lead a data team or even know the best place to start. I'm hoping you all can help me figure out how to shape my role, what to prioritize, and how to set myself up for growth.\n\nI’m comfortable with Python and SQL and have some exposure to Power BI, but not advanced. Our stack includes AWS, Metabase via PostgreSQL (for reporting to clients/partners or to expose our data to non technical colleagues e.g. customer support). No Snowflake or Spark that I'm aware of. Any data engineering tasks are currently handled by the software engineers.\n\nNote: A software engineer who left some time ago used dbt for a bit and I'm very interested in picking this up, if relevant. \n\nI was given a mix of BAU reporting tasks (client growth, churn rate, performance metrics, etc.) but the CTO gave me a 3-month task to investigate our current data practices, suggest improvements, and recommend new tools based on business needs (like Power BI).\n\nMy ideal plan is to slowly transition into a proper Data Engineering role. I want to take over those tasks from the developers, build a more robust and automated reporting pipeline, and get hands-on with ETL practices and more advanced coding/SQL. I want to add skills to my CV that I'll be proud of and are also in demand.\n\nI'd really appreciate any advice on two main areas:\n\n1. a. What are the most effective things I can do right now to improve my daily work and start shaping the data?\n\nb. How do I use the currently available tools (PostgreSQL, Metabase, Python) to make my life easier when generating reports and client insights? Should I try to resurrect and learn dbt to manage my SQL transformations?\n\nc. Given the CTO's task, what kind of \"wrong practices\" should I be looking for in our current data processes?\n\n2.\na. How do I lay the foundation for a future data engineering role, both in terms of learning and advocating for myself?\n\nb. What should I be learning in my spare time to get ready for data engineering tasks (i.e., Python concepts, ETL/ELT, AWS courses)?\n\nc. How do I effectively communicate the need for more proper Data Engineering tools/processes to the higher-ups and how do I make it clear I want to be doing that in the future?\n\nSorry for the long post, and I'm aware of any red flags you see as well, but I need to stay in this role for at least a year or two (for my CV to have that fintech experience) so I want to make the best out of it. Thanks!",
    "author": "No_Net369",
    "timestamp": "2025-10-15T08:11:41",
    "url": "https://reddit.com/r/dataengineering/comments/1o7dp2i/data_person_in_a_small_fintech_how_do_i_shape_my/",
    "score": 38,
    "num_comments": 14,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o82bhc",
    "title": "Help creating a mega app for my company",
    "content": "I am a data analytics apprentice, fairly new to my company. Day to day i dont just do data analysis but basically anything to do with managing my company's data. \n\nCurrently im involved in a large project where i will be the lead from the digital team. The idea is to create a 'mega app' to be used within the product testing process of the company. This process has many stages, with lots of crucial data being stored at each stage. \n\nUltimately, we aim to build a powerful front end. This will allow for everyone involved in the process to input data, read data, see where a product is in the testing process, plus a load more functions. We want this to link with a powerful back end where we can have lots of tables (say 20) which can hold all of the data, be related together where necessary and, most importantly, link well with the front end so that data can be written to and read from the back end using the front end. \n\nThe size of these tables may range from 100 rows to 100s of thousands. Reading and writing data needs to be quick. Also, having the ability to create reports and dashboards from this data is necessary. Finally, we want to be able to have an AI agent integrated into the system to pull answers to user questions from the database.\n\nAfter some research, my manager is interested in using the power platform (power apps for the front end, dataverse for the back end. Also allows for copilot agent integration and powerBI and power automate). However, after trying out this system im slightly questioning if this is the right solution for this scale of project, especially in the long term.\n\nMy main questions are:\n1. Is the power platform capable of creating a system of this scale and is it feasible?\n2. Are there any much better alternatives that we should consider (skill required to use isnt an issue)\n3. Are there any other subreddits where i should put this post?\n\nAll help is appreciated, thank you",
    "author": "ConsiderationMuch937",
    "timestamp": "2025-10-16T03:14:47",
    "url": "https://reddit.com/r/dataengineering/comments/1o82bhc/help_creating_a_mega_app_for_my_company/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7kzhx",
    "title": "Difference dbt athena va dbt redshift",
    "content": "Hi everyone!\n\nAt my job, we’re implementing dbt Athena because dbt Glue was too expensive to run. So we decided to switch to AWS Athena.\n\nRecently, I noticed there’s also dbt Redshift implementation in the tech world so— has anyone here used it and can share the main differences between the two libraries and when use each one?",
    "author": "jonathanrodrigr12",
    "timestamp": "2025-10-15T12:41:17",
    "url": "https://reddit.com/r/dataengineering/comments/1o7kzhx/difference_dbt_athena_va_dbt_redshift/",
    "score": 11,
    "num_comments": 10,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8cc87",
    "title": "Why I'm building a new kind of ETL tool...",
    "content": "At my current org, I developed a dashboard analytics feature from scratch. The dashboards are powered by Elasticsearch, but our primary database is PostgreSQL.\n\nI initially tried using [pgsync](https://pgsync.com/), an open-source library that uses Postgres WAL (Write-Ahead Logging) replication to sync data between Postgres and Elasticsearch, with Redis handling delta changes.\n\nThe issue was managing multi-tenancy in Postgres with this WAL design. It didn't fit our architecture.\n\nWhat ended up working was using Postgres Triggers to save minimal information onto RabbitMQ. When the message was consumed, it would make a back lookup to Postgres to get the complete data. This approach gave us the control we needed and helped scaling for multi-tenancy in Postgres.\n\nThe reason I built it in-house was purely due to complex business needs. None of the existing tools provided control over how quickly or slowly data is synced, and handling migrations was also an issue.\n\nThat's why I started [ETLFunnel](https://etlfunnel.com). It has only one focus: **control must always remain with the developer.**\n\nETLFunnel acts as a library and management tool that guides developers to focus on their business needs, rather than dictating how things should be done.\n\nIf you've had similar experiences with ETL tools not fitting your specific requirements, I'd be interested to hear about it.\n\n# Current Status\n\nI'm building in public and would love feedback from developers who've felt this pain.",
    "author": "Meal_Last",
    "timestamp": "2025-10-16T10:22:35",
    "url": "https://reddit.com/r/dataengineering/comments/1o8cc87/why_im_building_a_new_kind_of_etl_tool/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.3,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o8a5pd",
    "title": "The problem with SQL",
    "content": "\n===========  HEADLINE ===========\nIm unemployed and trying to get a job in DE. How do I get to where I want to be? How do I make that “impression “ to at least get a nibble? \n\n\n===========  BODY ===========\nIm in a little bit of a rut- trying to break into DE- but one issue/challenge I keep encountering: I cannot speak SQL. \n\nIm trying to make the switch from DA/DS (3yrs) and Ive grown to appreciate the logical steps that SQL abstracts, allowing me to focus on what I want and not how to get what I want. This appreciation has only grown as I dive deeper into learning about Spark SQL (glob reading is so rad) , psql, duckdb sql (duck sql????), tsql, snowflake, and SQLITE. From CTEs to wacky ass ‘quirks’/unique capabilities/strengths (Snowflake qualify!!! &lt;- really miss it when I now gotta heavy nest simply for that row_num=1)\n\nThis appreciation has grown to launching setting up and tearing down new DB clusters to learn more and more about the actual DB Engines and administration. Postgres has been by far my favourite: its extension suite is really sweet (hope to get to dig into Apache AGE soon!). \n\nI’m now unemployed and looking for a job (last job was a contract). Every application I send out feels like its destined for nowhere. The other day a recruiter accused me of cheating on a technical assessment and it really was a gut punch. I want to become a data engineer, and Ive been putting so much work into learning all the cool knicks to making full bronze-&gt; gold layers with ‘challenging’ data sets (+ vibed coded backend/front end lol).  So, when someone asks if I know SQL, im inclined to ask what dialect/what part.\n\n\nApologies for the rant but Im just frustrated and feel like no matter how much effort I put into the bare metal of it all, its all for nothing bc I don’t have experience with Databricks (fuck it Ill make my own eco with docker and navigate JAR hell), DBT (never had a reason to use it; I have primarily relied on some greasy ass JSONs) , or some other stack/platform. \n\nPS. One feel good moment did happen though bc I was able to bust out lambda functions on the python segment and idk, it made me realize how far Ive come!\nPPS. Please criticize the hell out of this post, and anything I comment; I am hear to listen.\n\n\n\n\n",
    "author": "Bitter_Marketing_807",
    "timestamp": "2025-10-16T09:03:05",
    "url": "https://reddit.com/r/dataengineering/comments/1o8a5pd/the_problem_with_sql/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o738t6",
    "title": "Do you really need databricks?",
    "content": "\nOkay, so recently I’ve been learning and experimenting with Databricks for data projects. I work mainly with AWS, and I’m having some trouble understanding exactly how Databricks improves a pipeline and in what ways it simplifies development.\n\nRight now, we’re using Athena + dbt, with MWAA for orchestration. We’ve fully adopted Athena, and one of its best features for us is the federated query capability. We currently use that to access all our on-prem data, we’ve successfully connected to SAP Business One, SQL Server and some APIs, and even went as far as building a custom connector using the SDK to query SAP S/4HANA OData as if it were a simple database table.\n\nWe’re implementing the bronze, silver, and gold (with iceberg) layers using dbt, and for cataloging we use AWS Glue databases for metadata, combined with Lake Formation for governance.\n\nAnd so for our dev experience is just making sql code all day long, the source does not matter(really) ... If you want to move data from the OnPrem side to Aws you just do \"create table as... Federated (select * from table) and that's it... You moved data from onprem to aws with a simple Sql, it applies to every source \n\nSo my question is: could you provide clear examples of where Databricks actually makes sense as a framework, and in what scenarios it would bring tangible advantages over our current stack?",
    "author": "whistemalo",
    "timestamp": "2025-10-14T23:06:41",
    "url": "https://reddit.com/r/dataengineering/comments/1o738t6/do_you_really_need_databricks/",
    "score": 98,
    "num_comments": 80,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7f8zi",
    "title": "dbt Coalesce 2025 Highlights: dbt + Fivetran Merger, Open Data Infrastructure, dbt Fusion and more",
    "content": "In case you’re not at Coalesce this week, we put together a quick recap of the opening keynote including details on the dbt + Fivetran merger, dbt Fusion, and what it means for data teams.\n\n👉 [selectstar.com/resources/dbt-coalesce-2025](https://www.selectstar.com/resources/dbt-coalesce-2025)",
    "author": "SelectStarData",
    "timestamp": "2025-10-15T09:08:51",
    "url": "https://reddit.com/r/dataengineering/comments/1o7f8zi/dbt_coalesce_2025_highlights_dbt_fivetran_merger/",
    "score": 15,
    "num_comments": 3,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7ok8g",
    "title": "Can I get a copy of Informatica PowerCenter running locally, or on my hardware for learning/training/development?",
    "content": "Hey Community!\n\nI'm trying to get some hands on practice with Informatica PowerCenter, for a migration project in the future and wanting to see if there is a legit (or not so legit) way to get access to an Informatica PowerCenter environment.\n\nI'm willing to pay for this access (e.g. something like Linux Academy for AWS Training) but I do not want to pay multiple thousands of dollars to learn ancient software.",
    "author": "poppinstacks",
    "timestamp": "2025-10-15T14:59:19",
    "url": "https://reddit.com/r/dataengineering/comments/1o7ok8g/can_i_get_a_copy_of_informatica_powercenter/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o78mvz",
    "title": "Testing data changes without blowing up prod",
    "content": "Every time I tweak something in a pipeline, there’s that tiny fear it’ll break prod. Staging never feels close enough, and cloning full datasets is a pain. I’ve started holding back changes until I can test them safely, but that slows everything down.\n\nHow do you test data updates or schema changes without taking down live tables?",
    "author": "hallelujah-amen",
    "timestamp": "2025-10-15T04:40:41",
    "url": "https://reddit.com/r/dataengineering/comments/1o78mvz/testing_data_changes_without_blowing_up_prod/",
    "score": 23,
    "num_comments": 13,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7nafv",
    "title": "Docker compose for lakehouse like build.",
    "content": "Hi, I'm struggling last few days on getting working \"lakehouse like\" setup using docker. So storage+metastore+spark+jupyter. Does anyone have a ready to go docker compose for that?   \nLLM's are not very helpful in this matter because of outdated etc images.",
    "author": "Kageyoshi777",
    "timestamp": "2025-10-15T14:08:32",
    "url": "https://reddit.com/r/dataengineering/comments/1o7nafv/docker_compose_for_lakehouse_like_build/",
    "score": 2,
    "num_comments": 8,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o77b20",
    "title": "What's the best way to ingest data into a BI platform?",
    "content": "I am trying to make some dashboards from the data of a PostgreSQL DB containing like 20 tables.\n\nI tried using Looker Studio with the correct connector, but it's not able to detect all the tables.\n\nSo do I need to create one superquery that contains denormalised data from all the tables or there is a better way to go about this? ( I had went the superquery route once for a different project with a lot less complex schema). Or should I create a gold layer as a seperate table?\n\nWhat are the best practices to create the gold layer ? ",
    "author": "Potential_Loss6978",
    "timestamp": "2025-10-15T03:27:11",
    "url": "https://reddit.com/r/dataengineering/comments/1o77b20/whats_the_best_way_to_ingest_data_into_a_bi/",
    "score": 18,
    "num_comments": 27,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o7e0zl",
    "title": "Dbt certification exam and certification renewal experience?",
    "content": "I am thinking of taking the dbt certification exam (analytics engineer) but I can’t find anything on their renewal process in the website? Has anyone earned their certification and renewed it, and how does it work? Is renewal free or have you got to pay again to keep your certificate? I am debating because if I have to keep on paying to renew my certification, I don’t see the point in it. ",
    "author": "Capital-Tax-7218",
    "timestamp": "2025-10-15T08:23:46",
    "url": "https://reddit.com/r/dataengineering/comments/1o7e0zl/dbt_certification_exam_and_certification_renewal/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o73qo4",
    "title": "Up-to-date data governance platform pricings help",
    "content": "We're trying to get a sense of how much these tools actually cost before talking to vendors. So far, most sites hide the numbers behind “book a demo” which is little annoying. Does anybody know where we can check accurate prices or what's the usual price range we can expect? Or how much did you end up paying or got quoted for mid-size teams?",
    "author": "Luisy-Prv",
    "timestamp": "2025-10-14T23:37:37",
    "url": "https://reddit.com/r/dataengineering/comments/1o73qo4/uptodate_data_governance_platform_pricings_help/",
    "score": 10,
    "num_comments": 8,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6oqbr",
    "title": "Jupyter Notebooks with the Microsoft Python Driver for SQL",
    "content": "Hi Everyone,\n\nI'm Dave Levy and I'm a product manager for SQL Server drivers at Microsoft. \n\nThis is my first post, but I've been here for a bit learning from you all. \n\nI want to share the latest quickstart that we have released for the Microsoft Python Driver for SQL. The driver is currently in public preview, and we are really looking for the community's help in shaping it to fit your needs...or even [contributing to the project on GitHub](https://github.com/microsoft/mssql-python/).\n\nHere is a link to the quickstart: [https://learn.microsoft.com/sql/connect/python/mssql-python/python-sql-driver-mssql-python-connect-jupyter-notebook](https://learn.microsoft.com/sql/connect/python/mssql-python/python-sql-driver-mssql-python-connect-jupyter-notebook)\n\nIt's great to meet you all!",
    "author": "dlevy-msft",
    "timestamp": "2025-10-14T12:12:12",
    "url": "https://reddit.com/r/dataengineering/comments/1o6oqbr/jupyter_notebooks_with_the_microsoft_python/",
    "score": 58,
    "num_comments": 12,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6zzo8",
    "title": "Has anyone migrated to airflow 3.1?",
    "content": "I know there were some challenges with 3.0. We are planning to migrate from 2.2. I am wondering if there are any gotchas we need to keep in mind or even wait for a future version like 3.2.",
    "author": "Then_Crow6380",
    "timestamp": "2025-10-14T20:07:34",
    "url": "https://reddit.com/r/dataengineering/comments/1o6zzo8/has_anyone_migrated_to_airflow_31/",
    "score": 9,
    "num_comments": 6,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o70g6r",
    "title": "Learn pipelines with databricks and dataflow(gcp)",
    "content": "hello, im sorry for my poor english.\nI would improve my skills as data engineer, and I realized in many jobs ask for databricks, and I dont know where start, I am only use dataflow to create pipelines in my current job and postgres.\nwell, I read your advices.\nthank you!\n",
    "author": "culiacanaz00",
    "timestamp": "2025-10-14T20:30:12",
    "url": "https://reddit.com/r/dataengineering/comments/1o70g6r/learn_pipelines_with_databricks_and_dataflowgcp/",
    "score": 6,
    "num_comments": 1,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6gns4",
    "title": "How are you guys keeping updated?",
    "content": "I am 4 years in this carrer and in a place where i am not sure of anything.\n\nWhat i should actively chasing to keep myself relevant?\n\nWas talking with some recruiters these days and the requirements seem all over the place.\n\n",
    "author": "Old_Tourist_3774",
    "timestamp": "2025-10-14T07:13:57",
    "url": "https://reddit.com/r/dataengineering/comments/1o6gns4/how_are_you_guys_keeping_updated/",
    "score": 69,
    "num_comments": 24,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6boad",
    "title": "Stuck... can' t find a job as a DE",
    "content": "Hi,\n\nI have 5 years of experience in Python : 2 as a Data Scientist and 3 as an “ETL / Cloud Developer” (Airflow, FastAPI, and BigQuery on GCP).\n\nI've been looking for a Data Engineering job in a big city in France for more than 4–5 months, but I’m stuck because I only did a lot of Spark during my studies + a MOOC on Coursera (1 month).\n\nI have multiple GCP certifications (PDE, PDA, ADP),\n\nfinished the Data Scientist and Data Engineer paths on Dataquest,\n\nand hold an MS in CS with a Data Science specialization.\n\nI think Spark in companies isn’t “that hard,” i.e., you rarely use the advanced functionalities.\n\nBut since I don’t have any Spark experience, I can’t even pass the screening phase :/ I really love Data Engineering, but I’m quite burned out from doing online courses especially now that I see they’re worth nothing...I had a promising track but they eventually took someone with DBT expertise\n\nMy last option would be doing the Data Engineering Zoomcamp ... ?\n\nDid anyone experience this and got any advice?",
    "author": "Miserable_Chef_9576",
    "timestamp": "2025-10-14T03:20:52",
    "url": "https://reddit.com/r/dataengineering/comments/1o6boad/stuck_can_t_find_a_job_as_a_de/",
    "score": 109,
    "num_comments": 60,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o77amx",
    "title": "Scraping HTML for NLP training data.",
    "content": "I’m building a custom dataset for NLP and scraping a ton of HTML pages. I’m spending way too much time writing and tweaking parsing rules just to get consistent JSON out of it. There’s gotta be a better way than writing selectors by hand or clicking through GUI tools for every source.",
    "author": "WarAndPeace06",
    "timestamp": "2025-10-15T03:26:32",
    "url": "https://reddit.com/r/dataengineering/comments/1o77amx/scraping_html_for_nlp_training_data/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o62th4",
    "title": "What I think is really going on in the Fivetran+DBT merger",
    "content": "This is a long article, so sit down and get some popcorn 🙂\n\nAt this point everyone here has already read of the newest merger on the block. I think it's been (at least for me) a bit difficult to get the full story of why and whats going. I’m going to try to give what I suspect is really going on here and why it's happening.\n\n*TLDR: Fivetran is getting squeezed on both sides and DBT has hit its peak, so they’re trying to merge to take a chunk off the warehouses and reach Databricks valuation (10b atm -&gt; 100b Databricks/Snowflake)*\n\nFirst, a collect of assumptions from my side:\n\n* Fivetran is getting squeezed at the top by warehouses (Databricks, Snowflake) commoditizing EL for their enterprise contracts. Why ask your enterprise IT team to get legal to review another vendor contract (which will take another few 100ks of the budget) when you can do just 1 vendor? With EL at cost (cause the money is in query compute, not EL)?\n* Fivetran is getting squeezed at the bottom by much cheaper commoditized vendors (Airbyte, DLTHub, Rivery, etc.)\n* DBT has peaked and isn’t really growing much.\n\n\nFor the first, the [proof](https://www.getdbt.com/blog/what-is-open-data-infrastructure) from DBTs article:\n\n&gt; As a result, customers became frustrated with the tool-integration challenges and the inability to solve the larger, cross-domain problems. Customers began demanding more integrated solutions—asking their existing vendors to “do more” and leave in-house teams to solve fewer integration challenges themselves. Vendors saw this as an opportunity to grow into new areas and extend their footprints into new categories. This is neither inherently good nor bad. End-to-end solutions can drive cleaner integration, better user experience, and lower cost. But they can also limit user choice, create vendor lock-in, and drive up costs. The devil is in the details.\n\n&gt; In particular, the data industry has, during the cloud era, been dominated by five huge players, each with well over $1 billion in annual revenue: Databricks, Snowflake, Google Cloud, AWS, and Microsoft Azure. Each of these five players started out by building an analytical compute engine, storage, and a metadata catalog. But over the last five years as the MDS story has played out, each of their customers has asked them to “do more.” And they have responded. Each of these five players now includes solutions across the entire stack: ingestion, transformation, notebooks and BI, orchestration, and more. They have now effectively become “all-in-one data platforms”—bring data, and do everything within their ecosystem.\n\nFor the second point, you only need to go to the pricing page of any of the alternatives. Fivetran is expensive, plan and simple. For the third, I don’t really have any formal proof. You can take it as my opinion I suppose.\n\nWith those 3 facts in mind, it seems like the game for DBTran (I’m using that name from now one 🙂) is then to try to flip the board on the warehouses. Normally, the data warehouse is where things start, with other tools (think data catalogs, transformation layer, semantic layer, etc.) being an add on that they try to commoditize. This is why snowflake and databricks are worth 100b+. Instead, DBTran is trying to make the warehouse be the commodity. This is namely by using a somewhat new tech. Iceberg (not gonna explain iceberg here, feel free to read that elsewhere).\n\n\nIf Iceberg is implemented, then compute and storage are split. The traditional warehouse vendors (bigquery, clickhouse, snowflake, etc.) are simply compute engines on top of the iceberg tables. Merely another component that can be switched out at will. Storage is an s3 bucket. DBTran would then be the rest. It would look a bit like:\n\n* Storage - **S3, GCS, etc.** \n* Compute - **Snowflake, BigQuery, etc.**\n* Iceberg Catalog - **DBTran**\n* EL - **DBTran**\n* Transformation Layer - **DBTran**\n* Semantic Layer - **DBTran**\n\nThey could probably add more stuff here. Buy Lightdash maybe and get into BI? But I don’t imagine they would need to (not a big enough market). Rather, I suspect they want to take a chunk off the big guys. So get that sweet, sweet compute enterprise budget by carving them out in half and eating it.\n\nSo should anyone in this subreddit care? I suppose it depends. If you don’t care about what tool you use, its business as usual. You’ll get something for EL, something for T and so on. Data engineering hasn’t fundamentally changed. If you care about OSS (which I do) then this is worth watching. I’m not sure if this is good or bad. I wouldn’t switch to DBT Fusion anytime soon. But if by any chance DBTran make the semantic layer and the EL OSS (even on an elastic license) then this might actually be a good thing for OSS. Great even.\n\nBut I wouldn’t bet on that. DBT made Metricsflow proprietary. Fivetran is proprietary. If you want OSS, its best to look elsewhere.",
    "author": "BoredAt",
    "timestamp": "2025-10-13T18:49:57",
    "url": "https://reddit.com/r/dataengineering/comments/1o62th4/what_i_think_is_really_going_on_in_the/",
    "score": 165,
    "num_comments": 82,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6d70g",
    "title": "Should I Remove Startup Founder Experience",
    "content": "\n\nI worked 2.5 years as a Data Engineer at Cognizant, then spent 1.2 years running my own startup building websites and apps for 50+ clients.\n\nI’m now looking for a Data Engineer job to learn more, gain fresh experience, and bring new skills back to my startup in the future. But I keep getting rejected. Is it better to leave out my founder experience? If I do, how should I explain the 1.2-year gap in my work history?\n\nAny advice from people who have faced this is appreciated.\n\nThanks!",
    "author": "abhishekp96",
    "timestamp": "2025-10-14T04:42:37",
    "url": "https://reddit.com/r/dataengineering/comments/1o6d70g/should_i_remove_startup_founder_experience/",
    "score": 21,
    "num_comments": 20,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6vjdw",
    "title": "🚀 Real-World use cases at the Apache Iceberg Seattle Meetup — 4 Speakers, 1 Powerful Event",
    "content": "Tired of theory? See how **Uber, DoorDash, Databricks &amp; CelerData** are *actually* using Apache Iceberg in production at our free Seattle meetup.\n\nNo marketing fluff, just deep dives into solving real-world problems:\n\n* **Databricks:** Unveiling the proposed **Iceberg V4 Adaptive Metadata Tree** for faster commits.\n* **Uber:** A look at their **native, cross-DC replication** for disaster recovery at scale.\n* **CelerData:** Crushing the **small-file problem** with benchmarks showing **\\~5x faster writes**.\n* **DoorDash:** Real talk on their multi-engine architecture, use cases, and feature gaps.\n\n**When:** Thurs, Oct 23rd @ 5 PM **Where:** Google Kirkland (with food &amp; drinks)\n\nThis is a chance to hear directly from the engineers in the trenches. Seats are limited and filling up fast.\n\n🔗 **RSVP here to claim your spot:** [`https://luma.com/byyyrlua`](https://luma.com/byyyrlua)",
    "author": "Public_Two_9800",
    "timestamp": "2025-10-14T16:39:27",
    "url": "https://reddit.com/r/dataengineering/comments/1o6vjdw/realworld_use_cases_at_the_apache_iceberg_seattle/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6ntu6",
    "title": "Predict/estimate my baby's delivery time - need real-world contraction time data",
    "content": "\nSo we're going to have a baby in a few weeks, and I was thinking obviously how can I use my data skills for my baby. \n\nI vaguely remembered I saw a video or read an article where someone, somewhere said that they were able to predict their wife's delivery time (with few minutes accuracy) based on accurately measuring contraction start and end times, as contraction lengths tend to be longer and longer as the delivery time approaches. After a quick Google search, I found the video! It was made by [Steve Mould](https://www.youtube.com/watch?v=k7q0Y2W0Rn4) 7 years ago, but somehow I remembered it. If you look at the [chart](https://youtu.be/k7q0Y2W0Rn4?si=vz_S1LCIsLeYs10v&amp;t=275) in the video, the graph and trend lines feel a bit \"exaggerated\", but let's assume it's true.\n\nSo I found a bunch of apps for timing contractions but nothing that provides predictions of the estimated delivery time. I found a [reddit post](https://www.reddit.com/r/dataisbeautiful/comments/lbrp5f/oc_attempt_to_duplicate_steve_moulds_graph_for/) created 5 years ago, but the blog post describing the calculations is not available anymore. \n\nAnyway, I tried to reproduce a similar logic &amp; graph in Python as a Streamlit app, [available in GitHub](https://github.com/vmatt/predict-my-baby). With my synthetic dataset it looks good, but I'd like to get some real data, so I can adjust the regression fitting on proper data. \n\nMy ask would be for the community: \n1. if you know any datasets that are publicly available, could you share with me? I found an [article](https://www.nature.com/articles/sdata201517), but I'm not sure how [can this](https://www.physionet.org/lightwave/) be translated into contraction start and end times. \n2. Or if you already have kid, and you logged contraction lengths (start time/end time) with an app from which you can export into CSV/JSON/whatever format, please share that with me! Also sharing the actual delivery time would be needed so I can actually test it. (and any other data that you are willing to share - age, weight, any treatments during the pregnancy)\n\nI plan to reimplement the final version with html/js, so we can use it offline.\n\nNote: I'm not a data scientist by the way. Just someone who works with data and enjoys these kinds of projects. So I'm sure there are better approaches than simple regression (maybe XGBoost or other ML techniques?), but I'm starting simple. I also know that each pregnancy is unique, contraction lengths and delivery times can vary heavily based on hormones, physique, contractions can stall, speed up randomly, so I have no expectations. But I'd be happy to give it a try, if this can achieve 20-60 minutes of accuracy, I'll be happy.\n\nUpdate: I want to add, that my wife approves this",
    "author": "valko2",
    "timestamp": "2025-10-14T11:37:59",
    "url": "https://reddit.com/r/dataengineering/comments/1o6ntu6/predictestimate_my_babys_delivery_time_need/",
    "score": 5,
    "num_comments": 8,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6pqn2",
    "title": "Optimizing writes to OLAP using buffers",
    "content": "I wrote an article about the best practices for inserts in OLAP (c.f. OLTP), what the technical reasons are behind it (the \"work\" an OLAP database needs to do on insert is more efficient with more data), and how you can implement it using a streaming buffer.\n\nThe heuristic is, at least for ClickHouse:\n\n\\* If you get to 100k rows, write\n\n\\* If you get to 1s, write\n\nWrite when you hit the earlier of either of the above.",
    "author": "oatsandsugar",
    "timestamp": "2025-10-14T12:49:51",
    "url": "https://reddit.com/r/dataengineering/comments/1o6pqn2/optimizing_writes_to_olap_using_buffers/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6sfce",
    "title": "Real-time Data Analytics at Scale: Integrating Apache Flink and Apache Doris with Flink Doris Connector and Flink CDC",
    "content": "In large-scale data analytics, balancing speed, flexibility, and accuracy is always a challenge. Apache Flink and Apache Doris together provide a strong foundation for real-time analytics pipelines. Flink offers powerful stream processing capabilities, while Doris provides low-latency analytics over large datasets.\n\nThis post outlines the main integration patterns between the two systems, focusing on the **Flink Doris Connector** and **Flink CDC** for end-to-end real-time ETL.\n\n# 1. Overview: Flink + Doris in Real-time Analytics\n\n**Apache Flink** is a distributed stream processing engine widely adopted for ingesting and processing data from various sources such as databases, message queues, and event streams.\n\n**Apache Doris** is an MPP-based real-time analytical database that supports fast, high-concurrency queries. Its architecture includes:\n\n* **FE (Frontend):** request routing, query parsing, metadata, scheduling\n* **BE (Backend):** query execution and data storage\n\nTogether, Flink and Doris form a complete path:  \n**Data Collection → Stream Processing → Real-time Storage → Analytics and Query**\n\nhttps://preview.redd.it/g5xktumi95vf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=58bff724160508aa3b65d27668013b2451af496c\n\n# 2. Flink Doris Connector: Scan, Lookup Join, and Real-time Write\n\nThe **Flink Doris Connector** provides three major functions for building data pipelines.\n\n# (1) Scan (Reading from Doris)\n\nInstead of using a traditional JDBC connector (which can hit throughput limits), the Doris Source in Flink distributes read requests across Doris backend nodes.\n\n* The Flink JobManager requests a query plan from Doris FE.\n* The plan is distributed to TaskManagers, each reading data directly from assigned Tablets in parallel.\n\nThis distributed approach significantly increases data read throughput during synchronization or batch analysis.\n\nhttps://preview.redd.it/nm11w5nm95vf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=7de75abee5e481cc1816edb4f5a794450eaf78e1\n\n# (2) Lookup Join (Real-time Stream + Dimension Table Join)\n\nFlink supports joining streaming data with dimension tables in Doris.\n\n* **Traditional JDBC Lookup** performs synchronous single-record queries, which can easily become a bottleneck under heavy load.\n* **Flink Doris Connector** introduces **asynchronous batch lookups**:\n   * Incoming events are queued and processed in batches.\n   * Each batch is sent as a single `UNION ALL` query to Doris.\n\nThis design improves join throughput and reduces latency in stream–dimension table lookups.\n\nhttps://preview.redd.it/g3xrn78p95vf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=5a9b83137da211f91dde0ad32c19c003b8acbe03\n\n(3) Real-time Write (Sink)\n\nFor real-time ingestion, the Connector uses Doris’s **Stream Load** mechanism.\n\n**Process summary:**\n\n* Sink initiates a long-lived Stream Load request.\n* Data is continuously sent in chunks during Flink checkpoints.\n* After a checkpoint is completed, the transaction is committed and becomes visible in Doris.\n\nhttps://preview.redd.it/w5d7rmut95vf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=4c4c308903f6902c398ebd4b0f152fbcb4aac98a\n\nTo ensure **exactly-once semantics**, the connector uses a **two-phase commit**:\n\n1. Pre-commit data during checkpoint (not yet visible)\n2. Commit after checkpoint success\n\nhttps://preview.redd.it/m1nwzo2w95vf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=174fa327670b871eb62744b05a96fc454ef2d2c1\n\n**Balancing real-time and exactly-once:**  \nBecause commits depend on Flink checkpoints, shorter checkpoint intervals yield lower latency but higher resource use.  \nThe Connector adds a **batch caching mechanism**, temporarily buffering records in memory before committing, which improves throughput while maintaining correctness (idempotent writes under Doris primary key model).\n\nhttps://preview.redd.it/lattav7x95vf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=26943c7358c98e9dc2dcd0076861d23f80083524\n\n# 3. Flink CDC: Full and Incremental Synchronization\n\n**Flink CDC (Change Data Capture)** supports both initial full synchronization and continuous incremental updates from databases such as MySQL, Oracle, and PostgreSQL.\n\n# (1) Common challenges in full sync:\n\n* Detecting and syncing new tables automatically\n* Handling metadata and type mapping\n* Supporting DDL propagation (schema changes)\n* Low-code setup with minimal configuration\n\n# (2) Flink CDC capabilities:\n\n* **Incremental snapshot reading** with parallelism and lock-free scanning\n* **Restartable sync** — if interrupted, the task continues from the last offset\n* **Broad source support** (MySQL, Oracle, SQL Server, etc.)\n\nhttps://preview.redd.it/6564nl0z95vf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=93dbfd850c5765db605dd97613c1dc9a3f42e64a\n\n# (3) One-click integration with Doris\n\nWhen combined with the Flink Doris Connector, the system can automatically:\n\n* Create downstream Doris tables if they don’t exist\n* Route multiple tables to different sinks\n* Manage schema mapping transparently\n\nThis reduces configuration complexity and speeds up deployment for large-scale data migration and sync jobs.\n\n# 4. Schema Evolution with Light Schema Change\n\nDoris recently added a **Light Schema Change** mechanism that enables millisecond-level schema updates (add/drop columns) without interrupting ingestion or queries.\n\nWhen integrated with Flink CDC:\n\n1. The Source captures upstream DDL operations.\n2. The Doris Sink parses and applies them via Light Schema Change.\n\nCompared to traditional methods, schema change latency dropped from over 1 second to a few milliseconds, allowing continuous sync even during frequent schema evolution.\n\n# 5. Example: Full MySQL Database Synchronization\n\nA full sync job can be submitted via the Flink client, defining:\n\n* Flink configurations (parallelism, checkpoint interval)\n* Table filtering via regex\n* Source/Sink connector parameters\n\nThis setup allows syncing entire databases to Doris with minimal configuration effort.\n\nhttps://preview.redd.it/8o0fii32a5vf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=0960520b54b0432925a13584963929566966db2b\n\n# 6. Summary\n\n**Key takeaways:**\n\n* **Flink Doris Connector** supports parallel reading, async lookup joins, and exactly-once real-time writing.\n* **Flink CDC** provides lock-free incremental sync, automatic table creation, and DDL propagation.\n* **Light Schema Change** in Doris enables near-instant schema updates with no downtime.\n\nThe combination of Flink and Doris offers a practical, open-source approach to real-time data integration and analytics at scale.",
    "author": "ApacheDoris",
    "timestamp": "2025-10-14T14:30:09",
    "url": "https://reddit.com/r/dataengineering/comments/1o6sfce/realtime_data_analytics_at_scale_integrating/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6m218",
    "title": "Memory Efficient Batch Processing Tools",
    "content": "Hi, I have a ETL pipeline where it basically queries the last day's data(24 hours) from DB and stores it in S3.\n\nThe detailed steps are:\n\n**Query Mysql DB(JSON Response) -&gt; Use jq to remove null values -&gt; Store in temp.json -&gt; Gzip temp.json -&gt; Upload to S3.**\n\nI am currently doing this using a **bash** script and using `mysql` client to query my DB. The issue I am facing is since the query result is large, I am running out of memory. I tried using `--quick` command with `mysql` client to get the data row wise, instead of all at once, but I did not notice any improvement. On average, 1 Million rows seem to be taking 1GB in this case.\n\nMy idea is to stream the query result data from the Mysql DB Server to my Script and then once it hits some number of rows, I gzip and send the data to S3. I do this multiple times until I am through my complete result. I am looking to avoid the limit/offset query route since the dataset is fairly large and limit/offset will just move the issue to DB Server memory.\n\nIs there any way to do this in bash itself or it would be better to move to Python/R or some other language? I am open to any kind of tools, since I want to revamp this, so that this can handle atleast 50-100 million scale.\n\nThanks in advance",
    "author": "darkhorse1997",
    "timestamp": "2025-10-14T10:33:50",
    "url": "https://reddit.com/r/dataengineering/comments/1o6m218/memory_efficient_batch_processing_tools/",
    "score": 4,
    "num_comments": 24,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6in5s",
    "title": "Overcoming the small files problem (GCP, Parquet)",
    "content": "I realised that using Airflow on GCP Composer for loading json files from Google Cloud Storage to BigQuery and then move these files elsewhere every hour was too expensive.\n\nI, then, tried just using BigQuery external tables with dbt for version control over parquet files (with Hive style partitioning in a bucket in GCS), for that I started extracting data and loading it into GCS as parquet files using PyArrow.\n\nThe problem is that these parquet files are way too small (from \\~25 kb to \\~175 kb each) but at the same time, and for now, it seems to be super convenient, but I will soon be facing performance problems.\n\nThe solution I thought was launching a DAG that could merge these files into 1 every day at the end of the day (the resulting file would be around 100 MB which I think is almost ideal) , although I was trying to get away from composer as much as possible, but I guess I could also do a Cloud Function for this.\n\nHave you ever faced a problem like this? I think Databricks Delta Lake can compress parquet files like this automatically, does something like this exist for GCP? Is my solution a good practice? Could something better be done?",
    "author": "No_Engine1637",
    "timestamp": "2025-10-14T08:28:47",
    "url": "https://reddit.com/r/dataengineering/comments/1o6in5s/overcoming_the_small_files_problem_gcp_parquet/",
    "score": 3,
    "num_comments": 22,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6io2w",
    "title": "Software for creating Articlenumbers",
    "content": "Hi I recently started working as a production engineer for a new company, the whole production side I can handle. But now they tasked me with finding a solution for their existing numbering tree. We use this to create all numbers for items that we buy and sell. This is not autogenerated because our ERP doesn't support this. That's why we use XMind as you can see an example in the image above.\n\nIs their any software that I can use to automate this process because Xmind is thrash and a hassle to use? If this is not the right subreddit I am sorry. But I hope you guys can give me some pointers.  \n  \nKind regards ",
    "author": "DowntownEggplant8558",
    "timestamp": "2025-10-14T08:29:41",
    "url": "https://reddit.com/r/dataengineering/comments/1o6io2w/software_for_creating_articlenumbers/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6imt2",
    "title": "Stuck in azure?",
    "content": "Hi!\n\nI have been working as a Data Architect/ Data Engineer/ Data guy for six years now. Before I had worked as a backend .net developer for other 3 years.\n\nI basically work only with Azure, MS Fabric and sometimes with Databricks. And….I'm getting bored, and anxious about the future.\n\nRight now I see only two possible options, migrate to other vendors and learn other tools like AWS, Snowflake or something like that. \n\nOr deep dive into the Dynamics ecosystem trying to evolve into other kind of Microsoft Data IT guy, and sell the part of my soul I keep to MS.\n\nWhat do you think?\n\nPD: greetings from Argentina ",
    "author": "igna_na",
    "timestamp": "2025-10-14T08:28:25",
    "url": "https://reddit.com/r/dataengineering/comments/1o6imt2/stuck_in_azure/",
    "score": 6,
    "num_comments": 10,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6b6bg",
    "title": "How to dynamically set the number of PySpark repartitions to maintain 128 MB file sizes?",
    "content": "I’m working with a large dataset (\\~1B rows, \\~82 GB total)  \nIn one of my PySpark ETL steps, I repartition the DataFrame like this:\n\n    df = df.repartition(600)\n\nOriginally, this resulted in output files around 128 MB each, which was ideal.  \nHowever, as the dataset keeps growing, the files are now around 134 MB, meaning I’d need to keep manually adjusting that static number (600) to maintain \\~128 MB file sizes.\n\nIs there a way in PySpark to determine the DataFrame’s size and calculate the number of partitions dynamically so that each partition is around 128 MB regardless of how the dataset grows?",
    "author": "HMZ_PBI",
    "timestamp": "2025-10-14T02:51:32",
    "url": "https://reddit.com/r/dataengineering/comments/1o6b6bg/how_to_dynamically_set_the_number_of_pyspark/",
    "score": 7,
    "num_comments": 3,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6htme",
    "title": "I was given a task to optimise the code for pipeline and but other pipelines using the same code are running fine",
    "content": "Like the title says there is a global code and every pipeline runs fine except that one pipeline which takes 7 hours, my guide asked me to figure it out myself instead of asking him, please help",
    "author": "Pleasant-Insect136",
    "timestamp": "2025-10-14T07:58:38",
    "url": "https://reddit.com/r/dataengineering/comments/1o6htme/i_was_given_a_task_to_optimise_the_code_for/",
    "score": 1,
    "num_comments": 22,
    "upvote_ratio": 0.54,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6h672",
    "title": "Need Expert Advice — How to Industrialize My Snowflake Data Warehouse (Manual Deployments, No CI/CD, Version Drift)",
    "content": "I’m building a data warehouse in Snowflake, but everything is still manual. Each developer has their own dev database, and we manually create and run SQL scripts, build procedures, and manage releases using folders and a Python script. There’s no CI/CD, no version control for the database schema, and no clear promotion path from dev to prod, so we often don’t know which version of a transformation or table is the latest. This leads to schema drift, inconsistent environments, and lots of manual validation. I’d love advice from data engineers or architects on how to fix these bottlenecks. how to manage database versioning, automate deployments, and ensure consistent environments in Snowflake.",
    "author": "Critical_Anything_70",
    "timestamp": "2025-10-14T07:33:41",
    "url": "https://reddit.com/r/dataengineering/comments/1o6h672/need_expert_advice_how_to_industrialize_my/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5q0ll",
    "title": "Merged : dbt Labs + Fivetran",
    "content": "What do you expect from this announcement?  \n[https://www.getdbt.com/blog/dbt-labs-and-fivetran-merge-announcement](https://www.getdbt.com/blog/dbt-labs-and-fivetran-merge-announcement)  \n",
    "author": "Intelligent_Volume74",
    "timestamp": "2025-10-13T10:21:11",
    "url": "https://reddit.com/r/dataengineering/comments/1o5q0ll/merged_dbt_labs_fivetran/",
    "score": 148,
    "num_comments": 86,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6ef3y",
    "title": "How does your team work?",
    "content": "How are data teams organizing and executing their work these days? Agile? Scrum? Kanban? Scrumban? A bastardized version of everything, or utilizing some other inscrutable series of PM-created acronyms?\n\nMy inbox is always full of the latest hot take on how to organize a data team and where it should sit within the business. But I don't see much shared about the details of how the work is done.\n\nMost organizations I've been affiliated with were (or attempted ) Agile. Usually, it is some flavor of Agile because they found, and I'm inclined to believe, that Agile isn't super well-suited to data engineering workflows. That being said, I do think there's value to pointing work and organizing it into sprints to box, manage, and plan your tasks.\n\nSo what else is out there? Are there any small things that you love or hate about the way you and your team are working these days?",
    "author": "bengen343",
    "timestamp": "2025-10-14T05:41:37",
    "url": "https://reddit.com/r/dataengineering/comments/1o6ef3y/how_does_your_team_work/",
    "score": 4,
    "num_comments": 9,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5u9h4",
    "title": "BigQuery =&gt; DATAFORM vs Snowflakr =&gt; COALESCE ?!",
    "content": "I’m curious to know what the user feedback about COALESCE especially regarding how it works with Snowflake.\nDoes it offer the same features as dbt (Cloud or Core) in terms of modeling, orchestration, and lineage,testing etc?\nAnd how does the pricing and performance compare?\n\nFrom my side, I’ve been using Dataform with BigQuery, and it works perfectly, no need for external tools like dbt in that setup.",
    "author": "Difficult-Ambition61",
    "timestamp": "2025-10-13T12:53:24",
    "url": "https://reddit.com/r/dataengineering/comments/1o5u9h4/bigquery_dataform_vs_snowflakr_coalesce/",
    "score": 77,
    "num_comments": 17,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6lppb",
    "title": "Sharepoint alternatives for mass tabular collaboration (\"business-friendly\")?",
    "content": "Hello, I've recently joined a company as a Data Analyst for a business (commercial) team. From the start, my main challenge I view is data consistency and tabular collaboration.\n\nThe business revolves around a portfolio of ~5000 clients distributed across a team of account executives. Each executive must keep track of individual actions for different clients, and collaborate with data for analytics (my end of the job) and strategic definition.\n\nThis management is done purely with Sharepoint and Excel, and the implementation is rudimentary at best. For instance, the portfolio was uploaded to a Sharepoint list in July to track contract negotiations. This load was done once and in every new portfolio update, data was appended manually. Keys aren't clear throughout and data varies from sheet to sheet, which makes tracking data a challenge.\n\nThe main thing I wanna tackle with a new data structure is standardizing all information and removing as much fields as needed for the account execs to fill, providing less gaps for incorrect data entry and freeing up their own routines as well. My main data layer is the company portfolio fed through Databricks, and from this integration I would upload and constantly update the main table directly from the source. With this first layer of consistency tackled, removing the need for clumsy spreadsheets, I'd move on to individual action trackers, keeping the company data and providing fields for the execs to track their performance.\n\nTldr, I'm looking for a tool to, not only integrate company data, but for it to be scalable and maintanable as well, supporting mass data loads, appends and updates, as well as being friendly enough for non-tech teams to fill out. Is Sharepoint the right tool for this job? What other alternatives could tackle this? Is MS Access a good alternative?",
    "author": "Popular-Plane-6608",
    "timestamp": "2025-10-14T10:21:31",
    "url": "https://reddit.com/r/dataengineering/comments/1o6lppb/sharepoint_alternatives_for_mass_tabular/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6bnir",
    "title": "Are there any open source alternatives to spark for a small cluster?",
    "content": "I'm trying to set up a cluster with a set of workstations to scale up the computation required for some statistical analysis in a research project. Previously I've been using duckdb, but using a single node is no longer possible due to the increasing amount of data we have to analyse. However, setting up spark without docker or kubernetes (it is a limitation of the current setup) is not precisely easy\n\nDo you know any easier to setup alternative to spark compatible with R and CUDA (preferably open source, so we can adapt it to our needs)? Compatibility with python would be nice, but it isn't completely necessary. Additionally, CUDA could be replaced by any other widely available GPU API (we use Nvidia cards, but using opencl instead of CUDA wouldn't be a problem for our workflow)",
    "author": "No_Mongoose6172",
    "timestamp": "2025-10-14T03:19:40",
    "url": "https://reddit.com/r/dataengineering/comments/1o6bnir/are_there_any_open_source_alternatives_to_spark/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o64vny",
    "title": "How are you managing late arriving data, data quality, broken tables, observability?",
    "content": "I'm researching data observability tools and want to understand what is working for people.\n\nCurious how you've managed to fix or at the very least improve things like broken tables (schema drift), data quality, late arriving data, etc.",
    "author": "iblaine_reddit",
    "timestamp": "2025-10-13T20:29:57",
    "url": "https://reddit.com/r/dataengineering/comments/1o64vny/how_are_you_managing_late_arriving_data_data/",
    "score": 11,
    "num_comments": 17,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6azt3",
    "title": "what is the GoodData BI platform like?",
    "content": "So at my work, we are in talks to move away from Power BI to GoodData. \n\nWith as many complaints I have with Power BI and Microsoft and Fabric, this seems like a regression. I dont have any say in the decision. Seems like the executives got upsold on this. \n\nAnyways, anyone have any experience on it? How is it compared to Power BI. \n\nHere is the link to the site: https://www.gooddata.com/platform/",
    "author": "SolitaireKid",
    "timestamp": "2025-10-14T02:40:13",
    "url": "https://reddit.com/r/dataengineering/comments/1o6azt3/what_is_the_gooddata_bi_platform_like/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5x25k",
    "title": "Stuck Between Two Choices",
    "content": "Hi everyone, \n\nToday I received a new job offer with a 25% salary increase, great benefits, and honestly, much better experience and learning opportunities.\nHowever, my current company just offered a 50% salary increase to keep me which surprised me, especially since I had been earning below market rate. They also rewarded me with two extra salaries as appreciation. Now I’m a bit confused and nervous. I truly believe that experience and growth matter more in the long run, but at the same time, financial stability is important too. Still thinking it through.",
    "author": "EdgeCautious7312",
    "timestamp": "2025-10-13T14:36:19",
    "url": "https://reddit.com/r/dataengineering/comments/1o5x25k/stuck_between_two_choices/",
    "score": 26,
    "num_comments": 20,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6sf1g",
    "title": "Business lead vs tech lead: who is more valuable?",
    "content": "In a corporate setup, in the multi-functional project around the business product. Usually tech lead has lower title grade, although expertise in tech lead does not directly translate authority in a team hierarchy. Cheap immigrant resources are blame for this?",
    "author": "Far-Attention-5494",
    "timestamp": "2025-10-14T14:29:51",
    "url": "https://reddit.com/r/dataengineering/comments/1o6sf1g/business_lead_vs_tech_lead_who_is_more_valuable/",
    "score": 0,
    "num_comments": 14,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6k1gx",
    "title": "Confused about which Airflow version to learn",
    "content": "Hey everyone,\n\nI’m new to Data Engineering and currently planning to learn Airflow, but I’m a bit confused about the versions.   \nI noticed the latest version is 3.x but not all switched into yet. Most of the tutorials and resources I found is of 2.0.x. In the sub I saw some are still using 2.2 or 2.8. And other versions. Which version should i install and learn?  \n I heard some of the functions become deprecated or ui elements changed as the version updated.\n\n1 - Which version should I choose for learning?   \n\n2 - Which version is still used in production?  \n\n3 - Is the version gap is relevent?  \n\n4 - what are the things I have to take not ( as version changes)?  \n\n5 - any resource recommendations are appreciated.  \n\nPlease guide me.   \nYour valuable insights and informations are much appreciated, Thanks in advance❤️",
    "author": "Jake-Lokely",
    "timestamp": "2025-10-14T09:20:18",
    "url": "https://reddit.com/r/dataengineering/comments/1o6k1gx/confused_about_which_airflow_version_to_learn/",
    "score": 0,
    "num_comments": 17,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6e2no",
    "title": "Any option to send emails using notebook without the logic apps in synapse?",
    "content": "Just wanted to know if there are any other options to send email in synapse with out using logic apps.\nLike sending emails through pyspark or anyother option . \n\nThank you",
    "author": "data_learner_123",
    "timestamp": "2025-10-14T05:25:51",
    "url": "https://reddit.com/r/dataengineering/comments/1o6e2no/any_option_to_send_emails_using_notebook_without/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6d520",
    "title": "Launching a small experiment: monthly signal checks on what’s really happening in data engineering",
    "content": "Been wanting to do this for a while (set-up a new account for this too). Everything is changing so fast and there's so much going on that I wanted to capture it in realtime. Even if it's just to have something to look back through over time (if this gets legs). \n\nWanted to get your opinions and thoughts on the initial topic (below). I plan to set-up the first poll next week. \n\nAND I WILL PUBLISH THE RESULTS FOR EVERYONE TO BENEFIT\n\nTopics for the first run:  \n\n* Tool fatigue\n* AI in the Data Stack\n* Biggest Challenges in the lifecycle\n* Burnout, workload, satisfaction, team dynamics.\n* Measuring Value v Effort\n* Most used Data Architectures \n\n\n\n  \n",
    "author": "TheDataMind",
    "timestamp": "2025-10-14T04:39:54",
    "url": "https://reddit.com/r/dataengineering/comments/1o6d520/launching_a_small_experiment_monthly_signal/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5jgf3",
    "title": "Imposter syndrome hitting hard",
    "content": "I've been in the data space for about 10 years after an academic journey studying mathematics. The first 8 years of my career was in a consulting company doing a mixture of analytics and data migration activities as part of SaaS implementations (generally ERP/CRM systems). I guess an important thing to note was I genuinely felt like I knew what I was doing and was critical within the implementation projects.\n\n A couple of years ago I switched to a data architecture role at a tech company. Since day 1 I've felt behind my peers who I feel have much stronger skills in data modeling. With my consulting background and lack of formal CS learning, I feel like my knowledge of a traditional development lifecycle are missing and consequently feel like I deliver sub par work, along with other imposter syndrome type effects. Realistically I know the company wanted me for my experience and skills and maybe to be different to existing employees but I can't shake the feeling of unease. \n\nAny suggestions to improve here? Stick it out longer and hope things become clearer (they have over time but it's still hard to keep up), or return to the consulting world where I was more comfortable but now armed with a few more technical skills and less corporate bullshit.",
    "author": "PythagoreanTRex",
    "timestamp": "2025-10-13T06:17:22",
    "url": "https://reddit.com/r/dataengineering/comments/1o5jgf3/imposter_syndrome_hitting_hard/",
    "score": 49,
    "num_comments": 27,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5ms42",
    "title": "Do you use Ibis? How?",
    "content": "Hey folks, \n\nAt snowflake world summit Berlin, I saw Ibis mentioned a lot and after asking several platform teams about it, I found out several are using or exploring it\n\nFor those who don't know, ibis is a portable dataframe that takes SQL or Python and delgates that as SQL code to your underlying runtimes (snowflake, BQ, pyspark etc) Project link: [https://ibis-project.org/](https://ibis-project.org/)\n\nI mostly heard it used to enable development on local via something like duckdb and then deploy to prod and the same code runs on snowflake.\n\nDo you use it in your team? for what?",
    "author": "Thinker_Assignment",
    "timestamp": "2025-10-13T08:25:32",
    "url": "https://reddit.com/r/dataengineering/comments/1o5ms42/do_you_use_ibis_how/",
    "score": 23,
    "num_comments": 34,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5z0hy",
    "title": "Handling multiple date dimensions in a Star schema",
    "content": "I am currently working on building the data model for my company which will serve data from multiple sources into their own semantic models. Some of these models will have fact tables with multiple date keys and my goal is to minimise the workload on BAs and keep it as \"drag and drop\" within PowerBI as possible. How does one handle multiple date keys within the star schema as I assume that only one of the relationships can be active at a time?",
    "author": "Frodan2525",
    "timestamp": "2025-10-13T15:56:29",
    "url": "https://reddit.com/r/dataengineering/comments/1o5z0hy/handling_multiple_date_dimensions_in_a_star_schema/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5r1pp",
    "title": "How to extract records from table without indexes",
    "content": "So basically I've been tasked to move all of the data from one table and move it to a different location. However, the table I am working with is very large (about 50 million rows) and it does not contain indexes and I have no authority to change the structure of this table. I was wondering if anyone has any advice on how I would successfully extract all these records? I don't know where to start.  The full extraction needs to take under 24 hours due to constraints. ",
    "author": "Icy-Crew-1521",
    "timestamp": "2025-10-13T10:57:56",
    "url": "https://reddit.com/r/dataengineering/comments/1o5r1pp/how_to_extract_records_from_table_without_indexes/",
    "score": 9,
    "num_comments": 23,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5lpcm",
    "title": "How to not let work stress me and affect my health",
    "content": "Posting for career advice from devs who gone through similar situations.\n\nAwhile ago our data director chose me to own a migration project (Microsoft SSAS cubes to dbt/snowflake semantic layer). \n\nI do like to ownership and exciting about the the project, at first I spent way extra time because I thought it was interesting but I am still late as each sprint they also give me other tasks where I am the main developer. \n\nAfter a couple of months I felt drained out physically and mentally and had to step back for only my working hours and to protect my health, especially that they don’t give me any extra compensation or promotion for this. \n\nIn this migration project I am working solo and carrying out planning, BA, business interaction, dev, BI, QA, and data gov &amp; administration and the scope is only getting bigger.\n\nLast week there was an ongoing discussion between scrum master trying to highlight it’s already too much for me and that I shouldn’t be engaged as the main developer in other tasks and team lead who said that I am a crucial team member and they need me in other projects as well. \n\nI am connecting with both tmw but I wanna seek your advice on how to best manage these escalating situations and not let it affect my life and health over 0 mental or financial compensation. \n\nOf course I wanna own and deliver but at the same time be considerate to myself and communicative about how complex it is. ",
    "author": "Judessaa",
    "timestamp": "2025-10-13T07:46:24",
    "url": "https://reddit.com/r/dataengineering/comments/1o5lpcm/how_to_not_let_work_stress_me_and_affect_my_health/",
    "score": 10,
    "num_comments": 10,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5gwiz",
    "title": "Local First Analytics for small data",
    "content": "I wrote a blog advocating for the local stack when working with small data instead of spending too much money on big data tool.\n\n[https://medium.com/p/ddc4337c2ad6](https://medium.com/p/ddc4337c2ad6)",
    "author": "Master_Shopping6730",
    "timestamp": "2025-10-13T04:17:01",
    "url": "https://reddit.com/r/dataengineering/comments/1o5gwiz/local_first_analytics_for_small_data/",
    "score": 14,
    "num_comments": 21,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o6arv2",
    "title": "Should i move to Full stack BI engineering?",
    "content": "Hi Guys, I currently have 2 -3 years as a data engineer, i have a opportunity to relocate to a country that i always wanted to move to. The job opportunity is as Full-stack BI engineer. SO I want some advice. Do I make the move to test it out or not? I like the idea of DE as it is high in demand and the future of it looks great. I do wish sometimes I could work with business stakeholders and solving a business problem with my DE skills. So given that i feel DE is a better technical role, but also that i want to work with people more. SO balancing Technical and business awereness. Do i take this new role or not? THe thing that is giving me a bit of hesitation is, that am i going to break my career momentum/trajectory if i move to BI engineering? Also i have to say that i want to lead data teams one day and solve business problems with technical colluges etcc",
    "author": "_GoldenDoorknob_",
    "timestamp": "2025-10-14T02:26:07",
    "url": "https://reddit.com/r/dataengineering/comments/1o6arv2/should_i_move_to_full_stack_bi_engineering/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5ynqd",
    "title": "Getting back in after a few years out?",
    "content": "Forgive the length of this question and the background info. I started my career in the various incarnations of Crystal Decisions, Business Objects, and SAP BusinessObjects, working directly for that company and then customers and partners. I spent about  fifteen years doing it. I was always more interested in and did more work on the server architecture side of the house than the data side of the house, but I had assignments or jobs where I did a fair bit of SQL querying, tweaking ETL jobs in Data Integrator, and writing reports and dashboards. I spent some time in there working as a trainer, too. I worked remotely for a decade before COVID.\n\nIn 2018, the company I worked for started moving away from those SAP products into doing custom analytical stuff in AWS mostly using Docker or Kubernetes (depending on who their client was) to deploy our own analytical tools. I spent 2018 to 2020 learning to build AWS VPCs then DevOps pipelines to deploy those as infrastructure as code. For the last 5 years, I have been completely out of tech, though. I've been doing project and program management. Some of the projects have been tech related, but most haven't been. In 2026, I'd like to get back into working in hands on tech work, preferably something that's data adjacent. If I could learn one technology or product and ride that out for 15 years like I did with the Crystal/Bobj stuff, that would carry me to the end of my career. I want to work remotely and I want to earn a bare minimum of $150k per year doing that. I haven't written non-SQL code since early Visual Basic .Net and I don't want to write code all day, but I don't mind tweaking or troubleshooting it. If you could recommend one product or technology to me that you think has the market share and growth to be around for 10-15 years that I should try to learn to get back into actually doing tech work, what it would be? Snowflake? Data Bricks? Become a SQL guru? Something else entirely?",
    "author": "policywank",
    "timestamp": "2025-10-13T15:41:16",
    "url": "https://reddit.com/r/dataengineering/comments/1o5ynqd/getting_back_in_after_a_few_years_out/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o523v7",
    "title": "I enjoy building End-to-End Pipelines but not SQL-Focused",
    "content": "I’m currently in a Data Engineering bootcamp. So far I’m worried with my skills. While I use SQL regularly, it’s not my strongest suit - I’m less detail-oriented than one of my teammates who focuses more on query precision. My background is CS and I am experienced coding in vscode, building software specifically front end, docker, git commands etc. I have built ERDs before too.\n\nMy main focus on the team is leadership and over seeing designing and building end-to-end data processes from start to finish. I tend to compare myself with that classmate (to be fair, said classmate struggles with git, we help each other out, as she focuses on sql cleaning jobs she volunteered to do). \n\nI guess I’m looking for validation whether I can get a good career with the skillset that I have despite not being too confident with in-depth data cleaning. I do know how to do data cleaning if given more time + data analysid but as I mentioned, i am in a fast tracked bootcamp so I want to focus more on learning the ETL flow. I use the help of ai + self analysis based on the dateset. But i think my data cleaning and analysis skills are a little rusty as of now. I dont know what to focus on learning",
    "author": "zari_tomazplaids",
    "timestamp": "2025-10-12T15:00:47",
    "url": "https://reddit.com/r/dataengineering/comments/1o523v7/i_enjoy_building_endtoend_pipelines_but_not/",
    "score": 75,
    "num_comments": 22,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5gp4l",
    "title": "How I solved the ‘non-Python user’ problem: Jupyter notebooks → PDF",
    "content": "After struggling for weeks to share my Jupyter analysis with our marketing team (they don't have Python installed), I finally found a clean workflow: convert notebooks to PDF before sending. Preserves all the visualizations and formatting. I've been using [Rare2PDF](https://rare2pdf.com/ipynb-to-pdf/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=ipynb_to_pdf_launch) since it doesn't require installation, but there are other options too, like nbconvert if you prefer command line. Anyone else dealing with the 'non-technical stakeholder' export problem?",
    "author": "Old_Activity9411",
    "timestamp": "2025-10-13T04:05:20",
    "url": "https://reddit.com/r/dataengineering/comments/1o5gp4l/how_i_solved_the_nonpython_user_problem_jupyter/",
    "score": 6,
    "num_comments": 9,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5pxlk",
    "title": "I am currently Data engineer with 9 yrs experience. Is it ok to accept Staff Data Engineer role?",
    "content": "I am looking for job change expecting to go for senior Data engineer or ai engineer. I am getting opportunities on product companies as staff Data engineer. I like to code and work on build streaming pipelines. I am not good at meetings level. should I accept this role? Will I regret after joining?",
    "author": "Level_Platypus_9495",
    "timestamp": "2025-10-13T10:18:19",
    "url": "https://reddit.com/r/dataengineering/comments/1o5pxlk/i_am_currently_data_engineer_with_9_yrs/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5rn7l",
    "title": "Down the Rabbit Hole: Dealing with Ad-Hoc Data Requests",
    "content": "",
    "author": "RayisImayev",
    "timestamp": "2025-10-13T11:18:43",
    "url": "https://reddit.com/r/dataengineering/comments/1o5rn7l/down_the_rabbit_hole_dealing_with_adhoc_data/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5pzzv",
    "title": "Navigating the World of Apache Spark: Comprehensive Guide",
    "content": "To help you get the most out of Apache Spark, below is the link of curated guide to all the Spark-related articles, categorizing them by skill level. Consider this your one-stop reference to find exactly what you need, when you need it.\n",
    "author": "Other_Cap7605",
    "timestamp": "2025-10-13T10:20:38",
    "url": "https://reddit.com/r/dataengineering/comments/1o5pzzv/navigating_the_world_of_apache_spark/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5nbos",
    "title": "Wasted two days, I'm frustrated.",
    "content": "Hi, I just got into this new project. And I was asked to work on poc-    \n  \n* connect to sap hana, extract the data from a table   \n* using snowpark load the data into snowflake\n\nI've used spark jdbc to read the hana table and I can connect with snowflake using snowpark(sso). I'm doing all of this locally in VS code. This spark df to snowflake table part is frustrating me. Not sure what's the right approach. Has anyone gone through this same process? **Please help.**\n\nUpdate: Thank you all for the response. I used spark snowflake connector for this poc. That works.\nOther suggested approaches : Fivetran, ADF, Convert spark df to pandas df and then use snowpark",
    "author": "H_potterr",
    "timestamp": "2025-10-13T08:45:18",
    "url": "https://reddit.com/r/dataengineering/comments/1o5nbos/wasted_two_days_im_frustrated/",
    "score": 2,
    "num_comments": 21,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5mxmf",
    "title": "Informatica IDMC MCC",
    "content": "Hello guys. \n\nI need help from people that works with informatica IDMC. I am working on a use case to evaluate timeliness in data quality. The condition is that profiling of a specific dataset should be done before the deadline and the validity and completeness of specific dataset should be above a defined threshold.\n\nI was thinking that if i can get the metadata of profiling job (profiling time, quality percentages found for each dimension) then i could map it to a dataset and and compare the data with a reference table.\n\nBut I didnt find away on how to find or extract this metadata. Any insight will be much appreciated \n\n ",
    "author": "aimenqaii",
    "timestamp": "2025-10-13T08:31:00",
    "url": "https://reddit.com/r/dataengineering/comments/1o5mxmf/informatica_idmc_mcc/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5mjdi",
    "title": "Im struggling to see the the difference between ADF, Databricks, and Dataflows and which combination to use.",
    "content": "I understand that ADF is focused more on pipeline orchestration whereas Databricks is focused on more complex transformations, however I'm struggling to see how both of them integrate. Ill explain my specific situation below to be more specific.\n\n  \nWe are creating tools using data from a multitude of systems. Luckily for us another department has created an SQL server that combines a lot of these systems however we occasionally do require data from other areas of business. We ingest this other data mainly using an ADLS blob storage account. We need to do transformations and combining of this data in some mildly complex ways. The way we have designed this is we will create pipelines to pull in this data from this SQL server and ADLS account into our own SQL server. Some of this data will just be a pure copy, however some of the data does require some transformations to make it useable for us.\n\nThis is where I then came across Dataflows. They looked great to me. Super simple transformations using expression language. Why bother creating a Databricks notebook and code for a column that just needs simple string manipulation? After this I was pretty certain that we would use the above tech stack in the below way:\n\n(Source SQL: The SQL table we are getting data from, Dest SQL: The SQL table we are loading into)\n\n**A pure copy job:** Use **ADF Copy Data** to copy from the **ADLS/Source SQL** to **Dest SQL**.\n\n**Simple Transformation**: Use **Dataflow** which defines the ETL and just call it from a pipeline to do the whole process.\n\n**Complex Transformation:** If data in **Source SQL** table use **ADF Copy Data** to copy it into the **ADLS** then read this file from **Databricks** where we load it into  **Dest SQL**.\n\n  \nHowever upon reflection this feels wrong. It feels like we are loading data in 3 different ways. I get using ADF as the orchestration but using both Dataflows and Databricks seems like doing transformations in two different ways for no reason at all. It feels like we should pick Dataflows **OR** Databricks. If I have to make this decision, we have complex transformations that I don't see being possible in Dataflows so we choose **ADF and Databricks**.\n\nHowever upon further research it looks as if Databricks has its own ETL process similar to ADF under \"Jobs and Pipelines\"? Could this be a viable alternative to ADF and Databricks as then this keeps all the pipeline logic in one place?\n\nI just feel a bit lost with all these tools as it seems like they overlap quite a bit. Upon researching it feels like ADF into Databricks is the answer but then my issue with this is using ADF to copy it into blob storage just to read it from Databricks. It seems like we are copying data just to copy data again. But if it is possible to read straight from the SQL server from Databricks then whats the point of using ADF at all if it can be achieved purely in databricks.\n\nAny help would be appreciated as I know this is quite a general and vague questions.\n\n\n\n\n\n",
    "author": "angryafrican822",
    "timestamp": "2025-10-13T08:16:49",
    "url": "https://reddit.com/r/dataengineering/comments/1o5mjdi/im_struggling_to_see_the_the_difference_between/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5j38n",
    "title": "Sync data from SQL databases to Notion",
    "content": "I'm building an integration for Notion that allows you to automatically sync data from your SQL database into your Notion databases.\n\nWhat it does:\n\n- Works with Postgres, MySQL, SQL Server, and other major databases\n\n- You control the data with SQL queries (filter, join, transform however you want)\n\n- Scheduled syncs keep Notion updated automatically\n\nLooking for early users. There's a lifetime discount for people who join the waitlist!\n\nIf you're currently doing manual exports, using some other solution (n8n automation, make etc) I'd love to hear about your use case.\n\nLet me know if this would be useful for your setup!",
    "author": "Fickle-Distance-7031",
    "timestamp": "2025-10-13T06:02:00",
    "url": "https://reddit.com/r/dataengineering/comments/1o5j38n/sync_data_from_sql_databases_to_notion/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5gww8",
    "title": "Building dataset tracking at scale - lessons learned from adding view/download metrics to an open data platform",
    "content": "Over the last few months, I’ve been working on an open data platform where users can browse and share public datasets. One recent feature we rolled out was **view and download counters** for each dataset  and implementing this turned out to be a surprisingly deep data engineering problem.\n\nA few technical challenges we ran into:\n\n* **Accurate event tracking -** ensuring unique counts without over-counting due to retries or bots.\n* **Efficient aggregation** \\- collecting counts in near-real-time while keeping query latency low.\n* **Schema evolution** \\- integrating counters into our existing dataset metadata model.\n* **Future scalability** \\- planning for sorting/filtering by metrics like views, downloads, or freshness.\n\nI’m curious how others have handled similar tracking or usage-analytics pipelines -especially when you’re balancing simplicity with reliability.\n\nFor transparency: I work on this project (**Opendatabay**) and we’re trying to design the system in a way that scales gracefully as dataset volume grows. Would love to hear how others have approached this type of metadata tracking or lightweight analytics in a data-engineering context.",
    "author": "Winter-Lake-589",
    "timestamp": "2025-10-13T04:17:36",
    "url": "https://reddit.com/r/dataengineering/comments/1o5gww8/building_dataset_tracking_at_scale_lessons/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4j390",
    "title": "Week 3 of learning Pyspark",
    "content": "It's actually week 2+3, took me more than a week to complete.( I also revisted some of the things i learned in the week 1 aswell. The resource(ztm) I've been following previously skipped a lot !) \n\nWhat I learned : \n\n- window functions\n- Working with parquet and ORC\n- writing modes\n- writing by partion and bucketing \n- noop writing\n- cluster managers and deployment modes\n- spark ui (applications, job, stage, task, executors, DAG,spill etc..) \n- shuffle optimization\n- join optimizations\n - shuffle hash join\n - sortmerge join\n - bucketed join\n - broadcast join\n- skewness and spillage optimization\n - salting\n- dynamic resource allocation\n- spark AQE\n- catalogs and types (in memmory, hive) \n- reading writing as  tables\n- spark sql hints \n\n1) Is there anything important i missed? \n2) what tool/tech should i learn next? \n\nPlease guide me.\nYour valuable insights and informations are much appreciated,\nThanks in advance❤️\n\n  \n\n\n\n",
    "author": "Jake-Lokely",
    "timestamp": "2025-10-12T00:46:22",
    "url": "https://reddit.com/r/dataengineering/comments/1o4j390/week_3_of_learning_pyspark/",
    "score": 142,
    "num_comments": 26,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4sjaa",
    "title": "How common is it to store historical snapshots of data?",
    "content": "This question is purely an engineering question: I don't care about analytical benefits, only about ETL pipelines and so on.\n\nAlso, this question is for a low data volume environment and a very small DE/DA team.\n\nI wonder what benefits could I obtain if I stored not only how sales data is right now, but also how it was at any given point in time.\n\nWe have a bucket in S3 where we store all data, and we call it a data lake: I'm not sure if that's accurate, because I understand that, for modern standards, historical snapshots are kinda common. What I don't know is if they are common because business analytical requirements dictate it, or if I, as DE, will benefit from it.\n\nAlso, there's the issue of cost. Using Iceberg (what I would use) on S3 to achieve h.s. must increase costs: on what factor? what does the increase depend on?\n\nEdit 1h later: Thanks to all of you for taking the time to reply.\n\nEdit 1h later number 2: \n\n    Conclusions drawn:\n    \n    The a) absence of a clear answer to the question and the presence of b) b.1) references to data modeling, business requirements and other analytical concepts, plus b.2) an unreasonable amount (&gt;0) of avoidable unkind comments to the question, made me c) form this thin layer of new knowledge:\n    \n    There is no reason to think about historical snapshots of data in an environment where it's not required by downstream analytics or business requirements. Storing historical data is not required to maintain data lake-like structures and the ETL pipelines that move data from source to dashboards.\n\n",
    "author": "escarbadiente",
    "timestamp": "2025-10-12T08:49:19",
    "url": "https://reddit.com/r/dataengineering/comments/1o4sjaa/how_common_is_it_to_store_historical_snapshots_of/",
    "score": 26,
    "num_comments": 37,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4rf13",
    "title": "What Editor Do You Use?",
    "content": "Ive been a vscode user for a long time. recently got into vim keybinds which i love. i want to move off vscode but the 2 biggest things that keep me on it are devcontainers/remote containers and the dbt power user extension since i heavily use dbt.  \n\nneovim, zed and helix all look like a nice alternatives i just havent been able to replicate my workflow fully in any of them. anyone else have this problem or a solution? or most people just using vscode?",
    "author": "shittyfuckdick",
    "timestamp": "2025-10-12T08:05:42",
    "url": "https://reddit.com/r/dataengineering/comments/1o4rf13/what_editor_do_you_use/",
    "score": 24,
    "num_comments": 48,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o51p9l",
    "title": "On-call management when you're alone",
    "content": "Hello fellow data engineers!\n\nI would like to get your point on this subject that I feel many of us have encountered in our career.\n\nI work in a company as their single &amp; first data engineer. They have another team of backend engineers with a dozen employees. This allow the company to have backend engineers taking part of an on call in turns (with a financial compensation). However on my side it's impossible to have such thing in place as it would mean I'd be on call all the time (illegal &amp; not desirable).\n\nThe main pain point is that regularly (2-3 times/month) backend engineers break our data infrastructure on prod with some fix releases they made while on call. I also feel that sometimes they deploy new features as I receive DB schema updates with new tables on the weekend (I don't see many cases where fixing a backend error would imply to create a new table).\n\nSometimes I fix those failures over the weekend on my personal time if I caught the alert notifications but sometimes I just don't check my phone or work laptop. Backend engineers are not responsible for the data infra like me, most of them don't know how it works and they don't have access to it for security reasons.\n\nIn such situation what would be the best solution?\n\nTraining the backend engineers on our data infra and give them access so they fix their mess when it happens ?\nPut myself on call time to time hoping I caught most of the outside working hours errors ?\nInsist to not deploy new features (schema changes) over the weekend ?\n\nFor now I am considering asking for time compensation on case I had to work over the weekend to fix things, but not sure if this is viable on long term, especially as it's not on my contract.\n\nThanks for your insight.",
    "author": "Adrien0623",
    "timestamp": "2025-10-12T14:43:48",
    "url": "https://reddit.com/r/dataengineering/comments/1o51p9l/oncall_management_when_youre_alone/",
    "score": 4,
    "num_comments": 9,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5500a",
    "title": "Multi-customer Apache Airflow deployments?",
    "content": "Hi guys, we develop analytic workflows for customers and deploy to their on-premise (private cloud) K8s cluster, we supply the Airflow deployment as well. Right now every customer gets the same DAGs, but we know at some point there will be divergence based around configuration.\n\n*I was just wondering how best to support similar DAGs, but different configuration based on the customer?*\n\nMy initial idea is to move all the DAGs behind \"factories\", some function that creates and returns the DAG, then a folder for each customer that imports the factory and creates the configured DAG. Then via helm `values.yaml` for airflow update the DAG folder to point to that specific customers folder.\n\n    ./\n    ├─ airflow/\n    │  ├─ dags/\n    │  │  ├─ customer_a/\n    │  │  │  ├─ customer_a_analytics.py\n    │  │  ├─ customer_b/\n    │  │  │  ├─ customer_b_analytics.py\n    │  │  ├─ factory/\n    │  │  │  ├─ analytics_factory.py\n\nMy thinking is, this keeps the core business logic centralized but configurable per customer. We then just point to which ever directory as needed. But jsut wondering if there is an established well used pattern already. But. have a suspicion python imports fail due to this.",
    "author": "SoloAquiParaHablar",
    "timestamp": "2025-10-12T17:10:39",
    "url": "https://reddit.com/r/dataengineering/comments/1o5500a/multicustomer_apache_airflow_deployments/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4f1x3",
    "title": "Am I the only one who spends half their life fixing the same damn dataset every month?",
    "content": "This keeps happening to me and it's annoying as hell.\n\n\n\nI get the same dataset every month (partner data, reports, whatever) and like 30% of the time something small is different. Column name changed. Extra spaces. Different format. And my whole thing breaks.\n\n\n\nThen I spend a few hours figuring out wtf happened and fixing it.\n\n\n\nDoes this happen to other people or is it just me with shitty data sources lol. How do you deal with it?",
    "author": "No-Transition273",
    "timestamp": "2025-10-11T20:43:32",
    "url": "https://reddit.com/r/dataengineering/comments/1o4f1x3/am_i_the_only_one_who_spends_half_their_life/",
    "score": 100,
    "num_comments": 40,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4pdow",
    "title": "how to go from python scripting to working in a team",
    "content": "I have been working with python for years, I can pretty much do anything I need but I've always been a one man show, so never needed to do OOP, CI/CD, logging, or worry about others coding with me, I just push to github in case something broke and that's it.\n\nhow do I take this to the next level?",
    "author": "Fair-Bookkeeper-1833",
    "timestamp": "2025-10-12T06:41:14",
    "url": "https://reddit.com/r/dataengineering/comments/1o4pdow/how_to_go_from_python_scripting_to_working_in_a/",
    "score": 14,
    "num_comments": 34,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5ik56",
    "title": "Non-technical guy needs insight and debate on Palantir Foundry",
    "content": "So I'm an investment analyst studying Palantir and want to understand their product deeper. Among other research I've been browsing this sub and seen that the consensus is it's in the best case a nice but niche product, and in the worst - bad product with good marketing. What I've seen makes me thing their product is legit and its sales are not Karp-marketing driven, so let's debate a little bit. I've written quite a lot, but tried to structure my thoughts and observations so it's easier to get.\n\nI'm not too technical and probably my optics are flawed, but as I see most conclusions on this sub pertain inclusively to managing data (obviously, given this sub name) side of their product. However, their value proposition seem to be broader than that. Seeing their clients' demonstrations like American Airlines on youtube impressed me.\n\nBasically you add a unifying layer on top of all your data and systems (ERP, CRM, etc.), add then feed LLM to it. And after that not only it does the analysis but it actually does the work for you like optimizing flight schedules, escalating only challening/risky cases to human operator with proposed decision. Basically **1)** routine operations become more automated, saving resources and **2)** workflow becomes less fragmented: instead of team A peforming analysis in their system/tool, then writing email to receive approval, then passing the work to team B working in their system/tool, we get much more unified workflow. Moreover, you ask AI agent to create workflow managed by other AI (AI agent will test how effectively workflows is executed by different LLMs and will choose the best one). I'm impressed by that and currently think that it does create value, although only on a large scale workflows given their pricing - **but should I?**\n\nI'm sure it's not as perfect as it seems, because most likely it still takes iterations and time to make it work properly and you will still need their FDE ocassionally (however still less if we compare to pre-AI version of their product). So the argument that they sell you consulting services instead of software seems less compelling.\n\nAnother thing I've seen is Ontology SDK, which allow you to code custom things and applications on top of Foundry which negates the argument that working in Foundry means being limited by their UI and templates, which I've also seen here. Once again, I'm not deep into technicalities of coding/data science, maybe you can correct me.\n\nMaybe you don't really need their ontology/Foundry to automate your business with AI and can just put Agentic AI solutions from MSFT/OpenAI/etc. on top of traditional systems? Maybe you do need an ontology (which is as I heard a relational database), but it is not that hard to create and integrate with AI and your systems for purposes of automation? What do you think?",
    "author": "0utremer",
    "timestamp": "2025-10-13T05:38:59",
    "url": "https://reddit.com/r/dataengineering/comments/1o5ik56/nontechnical_guy_needs_insight_and_debate_on/",
    "score": 0,
    "num_comments": 19,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o3wql2",
    "title": "What makes BigQuery “big“?",
    "content": "",
    "author": "victorviro",
    "timestamp": "2025-10-11T07:16:07",
    "url": "https://reddit.com/r/dataengineering/comments/1o3wql2/what_makes_bigquery_big/",
    "score": 643,
    "num_comments": 33,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4l57x",
    "title": "Fivetran pricing for small data",
    "content": "Currently using Python to extract data from our HR tool Personio through REST API. Now I saw that fivetran is offering a connector, so I am thinking about switching to easen the extraction process.\n\nThing is I dont understand the pricing model. We are less than 1000 employees and I will mainly be looking to extract basic employee data a few times daily. Would it be possible to get away with their free tier? I saw the base spend per month starting at 500, which would be alot given the small data amount.",
    "author": "el_dude1",
    "timestamp": "2025-10-12T02:58:48",
    "url": "https://reddit.com/r/dataengineering/comments/1o4l57x/fivetran_pricing_for_small_data/",
    "score": 14,
    "num_comments": 18,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4clus",
    "title": "When to normalize vs denormalize in database design?",
    "content": "Hi everyone, I'm having trouble understanding data modeling, especially when it comes to deciding when a database should be normalized or denormalized. I'm currently working on a personal project based on a Library system (I asked AI to generate the project criteria, and I'm focusing on building the data model myself).\n\n**The system represents a public library that:**\n\n* Manages a collection of over 20,000 books and magazines\n* Allows users to borrow and reserve books\n* Has employees responsible for system operations\n* Applies fines for late returns\n\n**My main question is:** When designing this kind of system, at what point should I prefer normalization (to avoid redundancy) and when should I consider denormalization (for performance or analytical purposes)? Should I keep everything normalized as in a typical OLTP design, or should I denormalize certain parts for faster reporting?\n\n**For example:** If I have the following tables `publisher` and `user` and they both have `city`, `street`, and `state` fields - should I create another table named `address`? Or leave it as is?\n\nAny guidance would be appreciated!\n\n\\----\n\nEDIT: Thank you so much guys, your answers really shed light on this topic for me\n\n&gt;",
    "author": "Amomn",
    "timestamp": "2025-10-11T18:35:06",
    "url": "https://reddit.com/r/dataengineering/comments/1o4clus/when_to_normalize_vs_denormalize_in_database/",
    "score": 63,
    "num_comments": 25,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4kfc6",
    "title": "How do I actually \"sell\" data engineering/analytics?",
    "content": "Hello! \n\nBeen a reader in this sub for quite some time. I have started a part time job where I am tasked to create a dashboard. No specific software is being required by the client, but I have chosen Looker Studio because the client is using Google as their work environment (sheets + drive). I would love to keep the cost low, or in this case totally free for the client but it's kinda hard working with Looker (I say PBI has better features imo). I am new in this so I don't wanna overcharge the client with my services, thankfully they don't demand much or give a very strict deadline.\n\nI have done all my transforms in my own personal work Gmail using Drive + Sheets + Google Apps Script because all of the raw data are just csv files. My dashboard is working and setup as intended, but it's quite hard to do the \"queries\" I need for each visualization -- I just do a single sheet for each \"query\" because star schema and joins does not work for Looker? I feel like I can do this better, but I am stuck.\n\nHere are my current concerns:\n\n1. If the client asks for more, like automation and additional dashboard features, would you have any suggestions as to how I can properly scale my workflow? I have read about GCP's storage and Bigquery, tried the free trial and I setup it wrong as my credits was depleted in a few days?? I think it's quite costly and overkill for a data that is less than 50k rows according to ChatGPT. \n2. Per my title, how can I \"sell\" this project to the client? What I mean is if in case the client wants to end our contract, like if they are completely satisfied with my simple automation, how can I transfer the ownership to them if I am currently using my personal email?\n\n*PS. I am* ***not*** *a Data analyst by profession nor working in Tech. I am just a guy who likes to try stuff and thankfully I got the chance to work on a real project after doing random Youtube ETL and dashboard projects. Python is my main language, so doing the above work using GAS(Javascript via ChatGPT lol) is quite a new experience to me.*",
    "author": "CumRag_Connoisseur",
    "timestamp": "2025-10-12T02:11:53",
    "url": "https://reddit.com/r/dataengineering/comments/1o4kfc6/how_do_i_actually_sell_data_engineeringanalytics/",
    "score": 16,
    "num_comments": 17,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o5e5qh",
    "title": "I built JSONxplode a tool to flatten any json file to a clean tabular format",
    "content": "Hey. mod team removed the previous post because i used ai to help me write this message but apparently clean and tidy explanation is not something they want so i am writing everything BY HAND THIS TIME.\n\nThis code flattens deep, messy and complex json files into a simple tabular form without the need of providing a schema.\n\nso all you need to do is:\nfrom jsonxplode inport flatten\nflattened_json = flatten(messy_json_data)\n\nonce this code is finished with the json file none of the object or arrays will be left un packed.\n\nyou can access it by doing:\npip install jsonxplode\n\ncode and proper documentation can be found at:\n\nhttps://github.com/ThanatosDrive/jsonxplode\n\nhttps://pypi.org/project/jsonxplode/\n\n\nin the post that was taken down these were some questions and the answers i provided to them\n\nwhy i built this code?\nbecause none of the current json flatteners handle properly deep, messy and complex json files.\n\nhow do i deal with some edge case scenarios of eg \nout of scope duplicate keys?\nthere is a column key counter that increments the column name of it notices that in a row there is 2 of the same columns.\n\nhow does it deal with empty values does it do a none or a blank string?\ndata is returned as a list of dictionaries (an array of objects) and if a key appears in one dictionary but not the other one then it will be present in the first one but not the second one.\n\nif this is a real pain point why is there no bigger conversations about the issue this code fixes?\npeople are talking about it but mostly everyone accepted the issue as something that comes with the job.\n\nhttps://www.reddit.com/r/dataengineering/s/FzZa7pfDYG",
    "author": "Thanatos-Drive",
    "timestamp": "2025-10-13T01:30:28",
    "url": "https://reddit.com/r/dataengineering/comments/1o5e5qh/i_built_jsonxplode_a_tool_to_flatten_any_json/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4r0tp",
    "title": "Inside Data Engineering with Erfan Hesami",
    "content": "Hello!\n\nHope everyone is doing great!\n\n\n\nI have been writing this series \"Inside Data Engineering\" for several months now. Today, sharing the 6th article where Erfan Hesami shares his experience in the world of data engineering, offering insights, exploring challenges, and highlighting emerging industry trends.\n\n  \nThis series focuses on promoting Data Engineering, clarifying the misconception and more.\n\n  \nI would really appreciate if you guys can provide feedback or suggestion on the questions so it can help the new data professionals in the future.\n\n  \nIf you like to be part of it, let me know!\n\n  \nThanks for reading!",
    "author": "mjfnd",
    "timestamp": "2025-10-12T07:50:02",
    "url": "https://reddit.com/r/dataengineering/comments/1o4r0tp/inside_data_engineering_with_erfan_hesami/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.59,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4mpyy",
    "title": "a lot of small files problem",
    "content": "I have 15 million 180gb total json.gz named data but some of them json some of them gzipped. I messed up i know. I want to convert all of them parquet. All my data on google cloud bucket. dataproc maybe right tool but i have 12vcpu limit on google cloud. how can i solve this problem. I want 1000 parquet file for not again living this small a lot file problem. ",
    "author": "whyyoucrazygosleep",
    "timestamp": "2025-10-12T04:33:01",
    "url": "https://reddit.com/r/dataengineering/comments/1o4mpyy/a_lot_of_small_files_problem/",
    "score": 2,
    "num_comments": 9,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4g92c",
    "title": "Data Engineering Crash Course Style Videos?",
    "content": "This might be really niche but I was wondering if there are any YouTube channels out there that have Crash Course or even Khan Academy style videos but for DEs? Like 10-20min long videos explain different concepts and techniques of data engineering. Was looking around on YouTube and just saw mostly those 3+ hour long courses or click baity type videos like \"what I wish I knew 6 months ago as a data engineer\". If this has already been asked, can I have the link to the post?\n\nThanks!",
    "author": "Raginmolases",
    "timestamp": "2025-10-11T21:51:09",
    "url": "https://reddit.com/r/dataengineering/comments/1o4g92c/data_engineering_crash_course_style_videos/",
    "score": 6,
    "num_comments": 4,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o467qd",
    "title": "Data clutter in SCD Type 2",
    "content": "Is there any actual use of end_timestamp and active_status? Why not just append rows with the append_timestamp and derive the active_status and end_timestamp using that?",
    "author": "[deleted]",
    "timestamp": "2025-10-11T13:41:20",
    "url": "https://reddit.com/r/dataengineering/comments/1o467qd/data_clutter_in_scd_type_2/",
    "score": 20,
    "num_comments": 5,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4gfsg",
    "title": "DEBEZIUM column reselector shutting down.",
    "content": "I’m setting up a CDC pipeline from Oracle to Kafka using Debezium. My Oracle table contains LOB and XML columns. When updates occur to non-LOB/XML columns, the redo logs don’t capture those LOB/XML values, so Debezium can’t fetch them. To handle this, I’m using the Reselector post-processor provided by Debezium. However, the issue is that the Oracle connection used by the reselector becomes inactive after some time of being idle, causing the pipeline to break. I then have to restart Kafka Connect to recover. Any workarounds for this issue?\n\n",
    "author": "sp_1218_",
    "timestamp": "2025-10-11T22:01:49",
    "url": "https://reddit.com/r/dataengineering/comments/1o4gfsg/debezium_column_reselector_shutting_down/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4gb9p",
    "title": "Using Airflow XCom from a separate file",
    "content": "In the RedditDataEngineering project [here](https://github.com/airscholar/RedditDataEngineering/tree/main) the author is using a `xcom_pull` from a [pipeline ](https://github.com/airscholar/RedditDataEngineering/blob/main/pipelines/aws_s3_pipeline.py)that is used to upload to S3. From the Airflow documentation it looks like XComs can only be used from the same file that the DAG is defined in but the author's [DAG](https://github.com/airscholar/RedditDataEngineering/blob/main/dags/reddit_dag.py) doesn't define it.\n\nThe project is using Airflow 2.7.2 I trying to get the project working with Airflow 3.0.6 but I can't get `xcom_pull` to be recognized. Is it possible to use XComs from a separate file in newer versions of Airflow?",
    "author": "wanna_get_a_honda",
    "timestamp": "2025-10-11T21:54:37",
    "url": "https://reddit.com/r/dataengineering/comments/1o4gb9p/using_airflow_xcom_from_a_separate_file/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o4678u",
    "title": "Career path advice",
    "content": "I'm at a huge fork in the road in terms of career. I'm going to have to learn something independently but have no idea what path I should take. I've bounced around between different roles frequently, which primes me well for a leadership position but I first have some gaps to fill. \n\nI started as a BI Developer but also did the data warehouse architecture and ETL loading. I designed a data warehouse from the ground up with my manager simply coaching but zero micro managing. \n\nThen I feel like my career took an unexpected turn, I took a job at a smaller firm. It ended up being more system administration with data engineering on the side but definitely more traditional data engineering than I did as a BI dev. I didn't have a manager so the skills I really learned was working with the owners/directly with leadership but it was a small firm and not as formal as I've seen at medium to large companies. No real formal quarterly planning but I was directly responsible for anything that happened or was needed. I really learned to solve whatever problem is thrown at me and I now have the confidence to attack any problem thrown at me. On the technical side I automated their manual workflow, mistakenly using SSIS, because I didn't know better. I later migrated them to the cloud with the support of a very very good consultant. He really liked my ability to learn and solve new problems and I'm still in contact with him today. After migration my job became more of a system administrator but also added new sources to the pipeline. No architecture or modeling responsibilities though. \n\nNext I found a job as an architect based in the cloud. Once again I had no technical people above me. I think this job was a a reach for me, I didn't know what I didn't know and oversold myself. Still I made it work and got my cloud pipelines built but mostly using the interface vs coding the way I expect most companies to work. We used S3, glue, RDS and lambda. This was short lived due to funding for the project and likely because I was over my head. \n\nHer I took a step back and just wanted to be a data engineer at a medium to large company. This was all good until we got offshored. I'm now working through the remaining months of that job while I look for a new one. \n\nUnfortunately it seems the traditional data flow knowledge isn't very wanted anymore and I spent the last 7 years learning more of the administration and leadership side vs technical skills. \n\nAm I simply screwed? Do I have to scramble to learn spark and get AWS engineering certified before I'll be valuable to a company again? I keep getting calls from Linkin recruiters who want me to lead a cloud engineering team but I'm looking for a job as one of those engineers, not their leader. The salary doesn't even seem to be the issue, the issue is no one is looking for general data skills to help train into cloud engineers. I knew u was goin to have to learn throughout my career but I didn't expect this drastic of a shift. I almost fell like front end web developers are in a better position than traditional data people. ",
    "author": "SoggyGrayDuck",
    "timestamp": "2025-10-11T13:40:43",
    "url": "https://reddit.com/r/dataengineering/comments/1o4678u/career_path_advice/",
    "score": 11,
    "num_comments": 3,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o3yz5o",
    "title": "Polars read database and write database bottleneck",
    "content": "Hello guys! I started to use polars to replace pandas on some etl and it’s fantastic it’s performance! So quickly to read and write parquet files and many other operations\n\nBut in am struggling to handle reading and writing databases (sql). The performance is not different from old pandas. \n\nAny tips on such operations than just use connector X? ( I am working with oracle, impala and db2 and have been using sqlalchemy engine and connector x os only for reading )\n\nWould be a option to use pyspark locally just to read and write the databases? \n\nWould be possible to start parallel/async databases read and write (I struggle to handle async codes) ? \n\nThanks in advance. ",
    "author": "BelottoBR",
    "timestamp": "2025-10-11T08:48:02",
    "url": "https://reddit.com/r/dataengineering/comments/1o3yz5o/polars_read_database_and_write_database_bottleneck/",
    "score": 11,
    "num_comments": 20,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o45jkl",
    "title": "Good Hive Metastore Image for Trino + Iceberg",
    "content": "My company has been using Trino + Iceberg for years now. For a long time, we were using Glue as the catalog, but we're trying to be a little bit more cross-platform, so Glue is out. I have currently deployed Project Nessie, but I'm not super happy with it. Does anyone know of a good project for a catalog that has the following:\n\n* actively maintained\n* supports using Postgres as a backend\n* supports (Materialized) Views in Trino",
    "author": "kassett238",
    "timestamp": "2025-10-11T13:13:06",
    "url": "https://reddit.com/r/dataengineering/comments/1o45jkl/good_hive_metastore_image_for_trino_iceberg/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o3tb9l",
    "title": "Write to Fabric warehouse from Fabric Notebook",
    "content": "Hi All,\n\nCurrent project is using Fabric Notebooks for Ingestion and they are triggering these from ADF via the API. When triggering these from the Fabric UI, the notebook can successfully write to the Fabric wh using  .synapsesql(). However whenever this is triggered via ADF using a system assigned managed identity it throws a Request Forbidden error:\n\no7417.synapsesql. : com.microsoft.spark.fabric.tds.error.fabricsparktdsinternalautherror: http request forbidden.\n\nThe ADF Identity has admin access to the workspace and contributer access to the Fabric capacity.\n\nDoes anyone else have this working and can help? \n\nNot sure if maybe it requires storage blob contributed to the Fabric capacity but my user doesn't and it works fine running from my account.\n\nAny help would be great thanks!\n\n",
    "author": "Top-Statistician5848",
    "timestamp": "2025-10-11T04:33:42",
    "url": "https://reddit.com/r/dataengineering/comments/1o3tb9l/write_to_fabric_warehouse_from_fabric_notebook/",
    "score": 5,
    "num_comments": 19,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o3wk8w",
    "title": "Table Engine for small tables in ClickHouse",
    "content": "Hi, i am ingesting a lot of table into ClickHouse. I have a question about relatively small dimension tables that rarely changes, so idea is to make Dictionary out of them, once i ingest them, since they are used for a lot of JOINs with main transactional table. But in what format should i ingest them, so its mostly small narrow tables. Should i just ingest as MergeTree and make dict out of it, or smth like TinyLog, what is the best practice here, since anyway it will be used as Dictionary when its needed?",
    "author": "Hot_While_6471",
    "timestamp": "2025-10-11T07:08:27",
    "url": "https://reddit.com/r/dataengineering/comments/1o3wk8w/table_engine_for_small_tables_in_clickhouse/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o37d19",
    "title": "How do you feel the Job market is at the moment?",
    "content": "Hey guys, 10 years of experience in tech here as a developer, currently switching to Data Engineering. I just wonder how is he job market recently for you guys?\n\nSoftware development is pretty much flooded with outsourcing and AI, wonder if DE is a bit better at finding opportunities. I am currently working quite hard on my SQL, Kafka, Apache etc skills",
    "author": "Weary_Pepper_2581",
    "timestamp": "2025-10-10T10:30:35",
    "url": "https://reddit.com/r/dataengineering/comments/1o37d19/how_do_you_feel_the_job_market_is_at_the_moment/",
    "score": 96,
    "num_comments": 68,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o3lepw",
    "title": "Small data engineering firms",
    "content": "Hey r/dataengineering community,\n\nI’m interested in learning more about how smaller, specialized data engineering teams (think 20 people or fewer) approach designing and maintaining robust data pipelines, especially when it comes to “data-as-state readiness” for things like AI or API enablement.\n\nIf you’re part of a boutique shop or a small consultancy, what are some distinguishing challenges or innovations you’ve experienced in getting client data into a state that’s ready for advanced analytics, automation, or integration?\n\nWould really appreciate hearing about:\n\n\t•\tThe unique architectures or frameworks you rely on (or have built yourselves)\n\n\t•\tApproaches you use for scalable, maintainable data readiness\n\n\t•\tHow small teams manage talent, workload, or project delivery compared to larger orgs\n\nI’d love to connect with others solving these kinds of problems or pushing the envelope in this area. Happy to share more about what we’re seeing too if there’s interest.\n\nThanks for any insights or stories!",
    "author": "red_lasso",
    "timestamp": "2025-10-10T20:37:28",
    "url": "https://reddit.com/r/dataengineering/comments/1o3lepw/small_data_engineering_firms/",
    "score": 13,
    "num_comments": 29,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o3bmkl",
    "title": "Day to Day Life of a Data Engineer",
    "content": "So I’m not a data engineer. I’m a data analyst but at my company we have a program where we get to work with the data engineering team part time for 6 weeks to learn about to build out some of our data infrastructure. For example, building out silver layer data tables that we want access to. This allows us to self serve a little bit so we can help expedite things that we need for our teams. It was a cool experience and I really learned a lot. I didn’t know much about data engineering before hand and I was wondering, how much time do DEs really spending on the “plumbing”? This was my first exposure to the medial data structure as well so idk if it’s different for other places that don’t use that but is that like a huge part of being a data engineer? It’s it mainly building out these cleanses tables? I know when new data sources are brought it that there is set up there, I was part of that too but I feel like the bulk of what was going on was building out silver and gold layers. How much time do you guys actually spend on that kind of work? And is it mundane as it can seem at time? Or did I just have easy work haha",
    "author": "VizlyAI",
    "timestamp": "2025-10-10T13:11:59",
    "url": "https://reddit.com/r/dataengineering/comments/1o3bmkl/day_to_day_life_of_a_data_engineer/",
    "score": 33,
    "num_comments": 9,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o3otpf",
    "title": "Need advice on designing a scalable vector pipeline for an AI chatbot (API-only data ~100GB JSON + PDFs)",
    "content": "Hey folks,\n\nI’m working on a new AI chatbot project from scratch, and I could really use some architecture feedback from people who’ve done similar stuff.\n\nAll the chatbot’s data comes from APIs, roughly 100GB of JSON and PDFs. The tricky part: there’s no change tracking, so right now any update means a full re-ingestion.\n\nStack-wise, we’re on AWS, using Qdrant for the vector store, Temporal for workflow orchestration, and Terraform for IaC. Down the line, we’ll also build a data lake, so I’m trying to keep the chatbot infra modular and future-proof.\n\nMy current idea:  \nAPI → S3 (raw) → chunk + embed → upsert into Qdrant.  \nTemporal would handle orchestration.\n\nI’m debating whether I should spin up a separate metadata DB (like DynamoDB) to track ingestion state, chunk versions, and file progress or just rely on Qdrant payload metadata for now.\n\nIf you’ve built RAG systems or large-scale vector pipelines:\n\n* How did you handle re-ingestion when delta updates weren’t available?\n* Is maintaining a metadata DB worth it early on?\n* Any lessons learned or “wish I’d done this differently” moments?\n\nWould love to hear what’s worked (or not) for others. Thanks!",
    "author": "PrestigiousDemand996",
    "timestamp": "2025-10-10T23:54:17",
    "url": "https://reddit.com/r/dataengineering/comments/1o3otpf/need_advice_on_designing_a_scalable_vector/",
    "score": 4,
    "num_comments": 5,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o33raz",
    "title": "How much data engineers care about costs?",
    "content": "Trying to figure out if there are any data engineers out there that still care (did they ever care?) about building efficient software (AI or not) in the sense of optimized both in terms of scalability/performance and costs.\n\nIt seems that in the age of AI we're myopically looking at maximizing output, not even outcome. Think about it, productivity - let's assume you increase that, you have a way to measure it and decide: yes, it's up. Is anyone looking at costs as well, just to put things into perspective?\n\nOr the predominant mindset of data engineers is: cost is somebody else's problem? When does it become a data engineering problem?\n\n🙏",
    "author": "n4r735",
    "timestamp": "2025-10-10T08:17:24",
    "url": "https://reddit.com/r/dataengineering/comments/1o33raz/how_much_data_engineers_care_about_costs/",
    "score": 42,
    "num_comments": 48,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o385re",
    "title": "AI use in Documentation",
    "content": "I'm starting to use some ai to do the thing I hate (documentation). Has anyone used it heavily for things like drafting design docs from code? If so, what has been your experience/assessment",
    "author": "Suspicious_World9906",
    "timestamp": "2025-10-10T11:00:21",
    "url": "https://reddit.com/r/dataengineering/comments/1o385re/ai_use_in_documentation/",
    "score": 17,
    "num_comments": 18,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o36inq",
    "title": "A JSON validator that actually gets what you meant.",
    "content": "# Ever had a pipeline crash because someone wrote \"yes\" instead of true or \"15 Jan 2024\" instead of \"2024-01-15\"I got tired of seeing “bad data” break dashboards — so I built a hybrid JSON validator that combines rules with a small language model. It doesn’t just validate — it understands what you meant.\n\nFull deep dive here: [https://thearnabsarkar.substack.com/p/json-semantic-validator](https://thearnabsarkar.substack.com/p/json-semantic-validator)\n\n[Hybrid JSON Validator — Rules + Small Language Model for Smarter DataOps](https://reddit.com/link/1o36inq/video/xrhn5muqdbuf1/player)\n\n",
    "author": "arnabsarkar1988",
    "timestamp": "2025-10-10T09:59:17",
    "url": "https://reddit.com/r/dataengineering/comments/1o36inq/a_json_validator_that_actually_gets_what_you_meant/",
    "score": 16,
    "num_comments": 10,
    "upvote_ratio": 0.68,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o3gani",
    "title": "Is ProjectPro worth it to expand the stack and portfolio projects?",
    "content": "Hey Fellas, I am an active Data Engineer working on Databricks and Azure stack for the FMCG sector. Now I want to expand my knowledge and gain solid expertise in AWS and Snowflake, for career growth and freelance purposes. I not only want to grasp knowledge, but I also want to have some real and good case studies or projects for my portfolio as well. For that, I came across ProjectPro's guided projects, which are quite interesting. \n\nNow paying for the subscription to ProjectPro for learning purposes, specifically for the Data Engineering domain, is it worth the price, or not, and what is the quality of the material there?",
    "author": "Uzair_Reaper",
    "timestamp": "2025-10-10T16:23:01",
    "url": "https://reddit.com/r/dataengineering/comments/1o3gani/is_projectpro_worth_it_to_expand_the_stack_and/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o39o9b",
    "title": "Looking for a lightweight open-source metadata catalog (≤1 GB RAM) to pair with Marquez &amp; Delta tables",
    "content": "I’m trying to architect a federated, lightweight open metadata catalog for data discovery.\nConstraints &amp; context:\n\n* Should run as a single-instance service, ideally using ≤1 GB RAM\n* One central DB for discovery (no distributed search infra)\n* Will be used alongside Marquez (for lineage), Delta tables, random files and directories, Postgres BI tables, and PowerBI/Streamlit dashboards\n* Prefer open-source and minimal dependencies\n\nSo far, most tools I found (OpenMetadata, DataHub, Amundsen) feel too heavy for what I’m aiming for.\n\nIs there any tool or minimal setup that actually fits this use case, or am I reinventing the wheel here?",
    "author": "vh_obj",
    "timestamp": "2025-10-10T11:57:27",
    "url": "https://reddit.com/r/dataengineering/comments/1o39o9b/looking_for_a_lightweight_opensource_metadata/",
    "score": 7,
    "num_comments": 6,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o2yrhe",
    "title": "How to model two fact tables with different levels of granularity according to Kimball?",
    "content": "Hi all,\n\nI’m designing a dimensional model for a retail company and have run into a data modeling question related to the Kimball methodology.\n\nI currently have two fact tables:\n\n•\t⁠FactTransaction – contains detailed transaction data (per receipt), with fields such as amount, tax, and a link to a TransactionType dimension (e.g., purchase, sale, return).\n\nThese transactions have a date, so the granularity is daily.\n\n•\t⁠FactTarget – contains target data at a higher level of aggregation (e.g., per year), with fields like target_amount and a link to a TargetType dimension (e.g., purchase, sale). This retail company sets annual targets in dollars for purchases and sales, so these targets are yearly. The fact table als has a Year attribute. A solution might be to use a Date attribute?\n\nUltimately, I need to create a table visualization in PowerBI that combines data from these two fact tables along with some additional measures.\n\nSometimes, I need to filter by type, so TransactionType and TargetType must be linked.\n\nI feel like using a bridge table might be “cheating,” so I’m curious: what would be the correct approach according to Kimball principles?",
    "author": "Quantumizera",
    "timestamp": "2025-10-10T04:51:33",
    "url": "https://reddit.com/r/dataengineering/comments/1o2yrhe/how_to_model_two_fact_tables_with_different/",
    "score": 19,
    "num_comments": 20,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o3ccsr",
    "title": "GUID or URN for business key",
    "content": "I've got a source system which uses GUIDs to define relationships and uniqueness of rows. \n\nBut, I've also got some unique URNs which define certain records in the source system.\n\nThese URNs are meaningful to my business and they are also used as references in other source systems but add joins to pull these in. Whereas the GUIDs are readily available but arent meaningful.\n\nWhich should I use as my business keys in kimball model?",
    "author": "Quiet-Range-4843",
    "timestamp": "2025-10-10T13:39:57",
    "url": "https://reddit.com/r/dataengineering/comments/1o3ccsr/guid_or_urn_for_business_key/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o35mma",
    "title": "How to handle data from different sources and formats?",
    "content": "Hi,\n\nSo we receive data from different sources and in different formats. \n\nBiggest problem is when it comes in pdf format. \n\nCurrently writing scripts to extract data from the pdf’s, but the way it gets exported by client is usually different, resulting in the scripts not working anymore. \n\nSo we have to redo them. \n\nCombine this with 100’s of different clients with different extract forms, and you can see why this is a major headache.\n\nAny recommendations? (And no, we can not tell them how to send us the data)",
    "author": "Dianvs11",
    "timestamp": "2025-10-10T09:26:09",
    "url": "https://reddit.com/r/dataengineering/comments/1o35mma/how_to_handle_data_from_different_sources_and/",
    "score": 6,
    "num_comments": 8,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o2vw26",
    "title": "Looking for tuning advice for ClickHouse",
    "content": "Hey Clickhouse experts,\n\nwe ran some initial TPC-H benchmarks comparing ClickHouse [25.9.3.48](http://25.9.3.48) with Exasol on AWS.  As we are no ClickHouse experts, we probably did things in a not optimal way. Would love input from people who’ve optimized ClickHouse for analytical workloads like this — maybe memory limits, parallelism, or query-level optimizations? Currently, some queries (like Q21, Q8, Q17) are 40–60x slower on the same hardware, while others (Q15, Q16) are roughly on par. Data volume is 10GB.  \nCurrent Clickhouse config highlights:\n\n* `max_threads = 16`\n* `max_memory_usage = 45 GB`\n* `max_server_memory_usage = 106 GB`\n* `max_concurrent_queries = 8`\n* `max_bytes_before_external_sort = 73 GB`\n* `join_use_nulls = 1`\n* `allow_experimental_correlated_subqueries = 1`\n* `optimize_read_in_order = 1`\n\nThe test environment used: AWS r5d.4xlarge (16 vCPUs, 124 GB RAM, RAID0 on two NVMe drives). Report with full setup and results: [Exasol vs ClickHouse Performance Comparison (TPC-H 10 GB)](https://exasol.github.io/benchkit/exa_vs_ch_10g/reports/1-short/REPORT.html)",
    "author": "Practical_Double_595",
    "timestamp": "2025-10-10T02:06:35",
    "url": "https://reddit.com/r/dataengineering/comments/1o2vw26/looking_for_tuning_advice_for_clickhouse/",
    "score": 16,
    "num_comments": 15,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o3hkzz",
    "title": "Self-hosted Community Edition of Athenic AI (BYO-LLM, Dockerized)",
    "content": "I’m the founder of Athenic AI, a tool for exploring and analyzing data using natural language. We’re exploring the idea of a self-hosted community edition and want to get input from people who work with data.\n\nthe community edition would be:\n\n* Bring-Your-Own-LLM (use whichever model you want)\n* Dockerized, self-contained, easy to deploy\n* Designed for teams who want AI-powered insights without relying on a cloud service\n\nIF interested, please let me know:\n\n* Would a self-hosted version be useful?\n* What would you actually use it for?\n* Any must-have features or challenges we should consider?\n\n",
    "author": "JahrudZ",
    "timestamp": "2025-10-10T17:22:36",
    "url": "https://reddit.com/r/dataengineering/comments/1o3hkzz/selfhosted_community_edition_of_athenic_ai_byollm/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o37lnr",
    "title": "Diving deep into theory for associate roles?",
    "content": "I interviewed for a role where I met more or less all the requirements and studied deeply on key etl topics, how to code etc.  But now I’m wondering if I should start studying theory questions again. Like what happens underneath a spark session and how is it structured in terms of staging before signal gets to the nodes etc. \n\nIs this common? Should I be shifting on how I prepare? \n\n",
    "author": "starrorange",
    "timestamp": "2025-10-10T10:39:38",
    "url": "https://reddit.com/r/dataengineering/comments/1o37lnr/diving_deep_into_theory_for_associate_roles/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o38y09",
    "title": "Junior analyst thrown into the deep end &amp; needs help with job/ETL process",
    "content": "Hi everyone. I graduated in 2023 with a business degree. Took a couple Python/SQL/Stats classes in university so when I started my post-grad internship I decided to focus on analytics. Since then I have about 1 year with Tableau, beginner/passable with Python &amp; SQL. I've done a good job for my level (at least that has been my feedback), but now I'm really worried if I can do my new job correctly. \n\nSix months ago I landed a new role that I think I was a bit underqualified for, though I am trying my best. Very large company, and very disorganized data-wise. My role is a new role made specifically for a small team that handles a niche, high volume, sensitive, complicated process. No other analysts - just one systems admin that is good at Power BI and has a ton of domain knowledge. \n\nI'm not really allowed to interface much with the other data analysts/engineers across the company since my boss thinks they won't like that I exist outside of the data-specific teams and could cause issues, at least until I have some real projects finished. So its been hard to understand what tools I can use or what the company uses. For the first 5 months my boss steered me to Dataverse - so I learned (my pro license was approved right away) and created a solution and when we went to push to prod the IT directors told us that we shouldn't be using that. I have access to one database in SMSS, and have been learning Power BI.\n\n**Here is where I'm really not sure what to do.** I was basically hired to work with data from this one external source that I'm only just now getting access to since it was in development. There are hundreds of millions of lines of data across hundreds of tables - this program is huge and really complicated, and the quality is questionable. I'm only just starting to barely understand how it works, and they hired me because I had some existing industry knowledge. My only option is to do the entire ETL process in Power BI and save the data models in Power BI. They want me to do it all - query the data directly from the source, clean/transform, store somewhere, and create dashboards with useful analytics (they already have some KPIs picked out for me to put together). \n\nThe company currently uses a data lake that does not currently include this source, with no plans to set it up anytime soon. They're apparently exploring using Azure Databricks and have a sandbox setup but I'm struggling to gain access to it. I don't know what other tools they may or may not have - everything I've heard is that there is not much of anything. My boss wants me to only use Power BI, because that is what he is familiar with. \n\nI don't want to use Power BI for the entire ETL process, that's not efficient right? I would much rather use Python, and what I see of Databricks that would be great for it, but my access to that is probably not going to be anytime soon. But I'm not an expert on how any of this works. So I'm hoping to ask you guys - what would you do in my position? I want to develop useful skills and use good tools, and to do things efficiently and correctly, but I'm not sure what I have to work with here. Thank you.",
    "author": "jbirdart",
    "timestamp": "2025-10-10T11:29:36",
    "url": "https://reddit.com/r/dataengineering/comments/1o38y09/junior_analyst_thrown_into_the_deep_end_needs/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o23nxt",
    "title": "I'm sick of the misconceptions that laymen have about data engineering",
    "content": "(disclaimer: this is a rant).\n\n***\"Why do I need to care about what the business case is?\"***\n\nThis sentence was just told to me two hours ago when discussing the data \"\"\"\"\"strategy\"\"\"\"\" of a client.\n\nThe conversation happened between me and a backend engineer, and went more or less like this*.* \n\n*\"...and so here we're using CDC to extract data.\"*  \n*\"Why?\"*  \n*\"The client said they don't want to lose any data\"*  \n*\"Which data in specific they don't want to lose?\"*  \n*\"Any data\"*  \n*\"You should ask why and really understand what their goal is. Without understanding the business case you're just building something that most likely will be over-engineered and not useful.\"*  \n***\"Why do I need to care about what the business case is?\"***\n\nThe conversation went on for 15 more minutes but the theme didn't change. For the millionth time, I stumbled upon the usual cdc + spark + kafka bullshit stack built without any rhyme nor reason, and nobody knows or even dared to ask how the data will be used and what is the business case.\n\nAnd then when you ask *\"ok but what's the business case\",* you ALWAYS get the most boilerplate Skyrim-NPC answer like: \"reporting and analytics\".\n\nNow tell me Johnny, does a business that moves slower than my grandma climbs the stairs need real-time reporting? Are they going to make real-time, sub-minute decision with all this CDC updates that you're spending so much money to extract? No? Then why the fuck did you set up a system that requires 5 engineers, 2 project managers and an exorcist to manage?\n\nI'm so fucking sick of this idea that data engineering only consists of Scooby Doo-ing together a bunch of expensive tech and call it a day. JFC.\n\nRant over.",
    "author": "wtfzambo",
    "timestamp": "2025-10-09T04:43:03",
    "url": "https://reddit.com/r/dataengineering/comments/1o23nxt/im_sick_of_the_misconceptions_that_laymen_have/",
    "score": 477,
    "num_comments": 198,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o30rpr",
    "title": "Built an API to query economic/demographic statistics without the CSV hell - looking for feedback **Affiliated**",
    "content": "I spent way too many hours last month pulling GDP data from Eurostat, World Bank, and OECD for a side project. Every source had different CSV formats, inconsistent series IDs, and required writing custom parsers.\n\nSo I built qoery - an API that lets you query statistics in plain English (or SQL) and returns structured data.\n\nFor example:\n\n\\`\\`\\`\n\ncurl -sS \"[https://api.qoery.com/v0/query/nl](https://api.qoery.com/v0/query/nl)\" \\\\\n\n\\-H \"X-API-Key: your-api-key\" \\\\\n\n\\-H \"Content-Type: application/json\" \\\\\n\n\\-d '{\"query\": \"What's the GDP growth rate for France?\"}'  \n\\`\\`\\`\n\nResponse:  \n\\`\\`\\`\n\n\"observations\": \\[\n\n{\n\n\"timestamp\": \"1994-12-31T00:00:00+00:00\",\n\n\"value\": \"2.3800000000\"\n\n},\n\n{\n\n\"timestamp\": \"1995-12-31T00:00:00+00:00\",\n\n\"value\": \"2.3000000000\"\n\n},\n\n...\n\n\\`\\`\\`\n\nCurrently indexed: 50M observations across 1.2M series from \\~10k sources (mostly economic/demographic data - think national statistics offices, central banks, international orgs).",
    "author": "SammieStyles",
    "timestamp": "2025-10-10T06:22:00",
    "url": "https://reddit.com/r/dataengineering/comments/1o30rpr/built_an_api_to_query_economicdemographic/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o2tgtq",
    "title": "Need Airflow DAG monitoring tips",
    "content": "I am new to airflow. And I have a requirement. I have 10 to 12 dags in airflow which are scheduled on daily basis. I need to monitor those 12 dags daily in the morning and evening and report the status of those dags as a single message (lets say in a tabular format) in teams channel. I can use teams workflow to get the alerts in teams channel. \n\nBut kindly give me any tips or ideas on how i can approach the Dag monitoring script. Thank you all in advance. ",
    "author": "SignificanceNo1695",
    "timestamp": "2025-10-09T23:28:33",
    "url": "https://reddit.com/r/dataengineering/comments/1o2tgtq/need_airflow_dag_monitoring_tips/",
    "score": 13,
    "num_comments": 9,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o301gf",
    "title": "GitHub - drainage: Rust + Python Lake House Health Analyzer | Detect • Diagnose • Optimize • Flow",
    "content": "Open source Lake House health checker. For Delta Lake and Apache Iceberg.",
    "author": "averageflatlanders",
    "timestamp": "2025-10-10T05:51:40",
    "url": "https://reddit.com/r/dataengineering/comments/1o301gf/github_drainage_rust_python_lake_house_health/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o2v24z",
    "title": "Tips on how to build our data pipeline",
    "content": "A small company in location intelligence of about 10 people with slow growth.\n\nWe deal with bulk data with geographical features. No streaming.\n\nGeometry sets are identified by the country, the year, the geographical level and the version. Any attribute refers to a geometrical entity and it is identified by the name, the year and the version.\n\nData comes in files, rarely from REST API.\n\nThe workflow is something like this:\n\nA. A file comes in, it is stored in S3 and its metadata recorded -&gt; B. It is prepared with some script or manually cleaned in Excel or other software, depending on who is working on -&gt; C. the cleaned, structured data is stored and ready to be used (in clients dbs, internally for studies, etc.)\n\nI thought something like S3 + Iceberg on S3 for the landing of raw files and their metadata (A). Dagster or Airflow to run scripts to prepare the data when possible or manually record the id of the raw files if the process is manual (B). Postgresql for storing the final data.\n\nI would like to hear comments, suggestion, questions from experienced data engineers, because I don't have much experience. Thank you!",
    "author": "longabout",
    "timestamp": "2025-10-10T01:11:31",
    "url": "https://reddit.com/r/dataengineering/comments/1o2v24z/tips_on_how_to_build_our_data_pipeline/",
    "score": 8,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o38ub7",
    "title": "Required e-mail bills/ statement dataset",
    "content": "I have been trying to build a system which can read ones emails via  login via Gmail, Outlook, yahoo and more and then read the emails and classify them into bills or not bills. Having issues on finding a dataset....\nPs if there something like this already existing please let me know would love to check it out.",
    "author": "palaash_naik",
    "timestamp": "2025-10-10T11:25:47",
    "url": "https://reddit.com/r/dataengineering/comments/1o38ub7/required_email_bills_statement_dataset/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o37g8q",
    "title": "Abinitio",
    "content": "Any one working on abinitio currently,\ni heard there is an older version of abinitio available for downloading.do anyone in this sub had it!!. If yes can you please dm it to me or post it here🫠",
    "author": "Master_Winner3062",
    "timestamp": "2025-10-10T10:33:57",
    "url": "https://reddit.com/r/dataengineering/comments/1o37g8q/abinitio/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o2rv58",
    "title": "Google DATA SCIENTIST AGENT",
    "content": "Have you all heard about that, from yesterday’s live event??\nWhat’s you opinion on it?\nI got bit sus and at the same time fit worried also .\n\nKinda looks like a infinity loop where the agent clean and do processing it’s self . Next step it’s like creating a agent itself like a child  Agent.\n\nEdit :\n\nPracticality is some thing DE have to face for the organization and make them understand that live event is bit friction and reality is but far away like we explain about movies to kids ",
    "author": "PressureCandid1989",
    "timestamp": "2025-10-09T21:53:22",
    "url": "https://reddit.com/r/dataengineering/comments/1o2rv58/google_data_scientist_agent/",
    "score": 7,
    "num_comments": 2,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o35lx2",
    "title": "What is your opinion on the state of Query Federation?",
    "content": "Dremio &amp; Trino had long been the go-to platforms for federating queries across databases, data warehouses, and data lakes. As concepts like lakehouse and data mesh are popularized, more tools are introducing different types of approaches to federation.\n\nWhat is your opinion on the state of things, what is your favorite query federation tools?",
    "author": "AMDataLake",
    "timestamp": "2025-10-10T09:25:24",
    "url": "https://reddit.com/r/dataengineering/comments/1o35lx2/what_is_your_opinion_on_the_state_of_query/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.44,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o3gmi1",
    "title": "Best tool to display tasks like Jira cards?",
    "content": "Hi everyone! I’m looking for recommendations on an app or tool that can help me achieve the goal below. \n\nI have task data (CSV: task name, priority, assignee, due date, blocked). I want a Jira-style board: each card = assignee, with their tasks inside, and overdue/blocked ones highlighted.\n\nIt’ll be displayed on a TV in the office.",
    "author": "danialsiddiki",
    "timestamp": "2025-10-10T16:38:04",
    "url": "https://reddit.com/r/dataengineering/comments/1o3gmi1/best_tool_to_display_tasks_like_jira_cards/",
    "score": 0,
    "num_comments": 11,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o2n6cc",
    "title": "Dbt glue vs dbt Athena",
    "content": "We’ve been working on our Lakehouse, and in the first version, we used dbt with AWS Glue. However, using interactive sessions turned out to be really expensive and hard to manage.\n\nNow we’re planning to migrate to dbt Athena, since according to the documentation, it’s supposed to be cheaper than dbt Glue.\n\nDoes anyone have any advice for migrating or managing costs with dbt Athena?\n\nAlso, if you’ve faced any issues or mistakes while using dbt Athena, I’d love to hear your experience",
    "author": "jonathanrodrigr12",
    "timestamp": "2025-10-09T17:58:03",
    "url": "https://reddit.com/r/dataengineering/comments/1o2n6cc/dbt_glue_vs_dbt_athena/",
    "score": 13,
    "num_comments": 4,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o25ahw",
    "title": "Iceberg is an overkill and most people don't realise it but its metadata model will sneak up on you",
    "content": "\n\nI’ve been following (and using) the Apache Iceberg ecosystem for a while now. Early on, I had the same mindset most teams do: files + a simple SQL engine + a cron is plenty. If you’re under \\~100 GB, have one writer, a few readers, and clear ownership, keep it simple and ship.\n\nBut the thing that was important was ofcourse “scale.” and the metadata.  \nWell i took a good look at a couple of blogs to come to a conclusion for this one and also there came a need of it.\n\nSo iceberg treats metadata as the system of record. Once you see that, a bunch of features stop feeling advanced and just a reminder most of the points here are for when you will scale.\n\n* Well one thing it has is Pruning without reading data, column stats (min/max/null counts) per file let engines skip almost everything before touching storage.\n*  bad load? this was one i came across.. you’re just moving a metadata pointer to a clean snapshot.\n* Concurrent safety on object stores wtih optimistic transactions against the metadata, so it’s all-or-nothing, even with multiple writers.\n* Well nonetheless a lot of other big names do this but just putting it here schema/partition evolution tracked by stable IDs, so renames/reorders don’t break history.\n\nSo if you arae a startup be simple but be prepared and it's okay to start boring. But the moment you feel pain schema churn, slower queries, more writers, hand-rolled cleanups Iceberg’s metadata intelligence starts paying for itself.\n\nIf you’re curious about how the layers fit together (snapshots, manifests, stats, etc.),  \n I wrote up a deeper breakdown in the blog above \n\nDon’t invent distributed systems problems you don’t have but don’t ignore the metadata advantages that are already there when you do.",
    "author": "DevWithIt",
    "timestamp": "2025-10-09T06:00:39",
    "url": "https://reddit.com/r/dataengineering/comments/1o25ahw/iceberg_is_an_overkill_and_most_people_dont/",
    "score": 94,
    "num_comments": 37,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o30g66",
    "title": "Why Data Contracts are foundation of a Data Mesh",
    "content": "What do you think?",
    "author": "theo______",
    "timestamp": "2025-10-10T06:08:53",
    "url": "https://reddit.com/r/dataengineering/comments/1o30g66/why_data_contracts_are_foundation_of_a_data_mesh/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o2z1ay",
    "title": "I built SemanticCache, a high-performance semantic caching library for Go",
    "content": "I’ve been working on a project called [SemanticCache](https://github.com/botirk38/semanticcache), a Go library that lets you cache and retrieve values based on meaning, not exact keys.\n\nTraditional caches only match identical keys — SemanticCache uses vector embeddings under the hood so it can find semantically similar entries.  \nFor example, caching a response for “The weather is sunny today” can also match “Nice weather outdoors” without recomputation.\n\nIt’s built for LLM and RAG pipelines that repeatedly process similar prompts or queries.  \nSupports multiple backends (LRU, LFU, FIFO, Redis), async and batch APIs, and integrates directly with OpenAI or custom embedding providers.\n\nUse cases include:\n\n* Semantic caching for LLM responses\n* Semantic search over cached content\n* Hybrid caching for AI inference APIs\n* Async caching for high-throughput workloads\n\nRepo: [https://github.com/botirk38/semanticcache](https://github.com/botirk38/semanticcache)  \nLicense: MIT\n\nWould love feedback or suggestions from anyone working on AI infra or caching layers. How would you apply semantic caching in your stack?",
    "author": "botirkhaltaev",
    "timestamp": "2025-10-10T05:05:07",
    "url": "https://reddit.com/r/dataengineering/comments/1o2z1ay/i_built_semanticcache_a_highperformance_semantic/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o2sqkb",
    "title": "Talend Metadata Bridge",
    "content": "Has anyone used Talend Metadata Bridge to migrate from Informatica Powercenter to Talend Data Fabric?",
    "author": "GarpA13",
    "timestamp": "2025-10-09T22:44:03",
    "url": "https://reddit.com/r/dataengineering/comments/1o2sqkb/talend_metadata_bridge/",
    "score": 3,
    "num_comments": 12,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o2h3ro",
    "title": "For anyone who moved from BI to DE, what roles have you had?",
    "content": "I'm currently working as a BI Analyst. I'm kind of stuck at my job because of the job market. In the meantime, I'm hoping to use about a year to learn dbt and a few other things, so I can move away from BI Analyst positions. \n\nI'm fortunate that at work I've been assigned to more work on the back-end, so I'm not necessarily doing analysis. However, this was actually a disadvantage when I was looking for BI roles earlier this year, so I have to refocus my job research.\n\nprojects I've worked on\n\nMoving data models from Tableau to the database.\n\nExtracted metadata from the Tableau API. With that data, we've been able to see the impact of changes to our data models. I've also used the data to automate some Tableau admin tasks and give the team visibility to security.\n\nI built a process that automatically pulls user data from multiple sources, combines it into one table, and flags errors in username assignments. the process before excel-based approach and now links everything directly to verified HR records.\n\nI'd like to pivot to more technical roles with my end goal as a Data Engineer, but I want to avoid going back to analyst roles. I do plan on going back to school maybe in the UK for a comp sci conversion masters for formal education. I'm hoping to land a role DE after 5-6 years.",
    "author": "datarocks456",
    "timestamp": "2025-10-09T13:35:32",
    "url": "https://reddit.com/r/dataengineering/comments/1o2h3ro/for_anyone_who_moved_from_bi_to_de_what_roles/",
    "score": 14,
    "num_comments": 14,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o2vmg0",
    "title": "Shared paths with Python, dbt, and uv?",
    "content": "Hi all, what's the easiest way to share paths between Python modules/scripts and dbt, particularly when using uv?\n\nNote to mods: I asked a similar question earlier and it was removed. Since this is a DE subreddit, I figured there would be people here with experience using these tools and they could share what they've learned. \n\nThanks all 🙏",
    "author": "9070932767",
    "timestamp": "2025-10-10T01:49:20",
    "url": "https://reddit.com/r/dataengineering/comments/1o2vmg0/shared_paths_with_python_dbt_and_uv/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o25ss6",
    "title": "What do you think about the Open Semantic Interchange (OSI)?",
    "content": "The [initiative by Snowflake](https://www.snowflake.com/en/blog/open-semantic-interchange-ai-standard/) tries to **interoperability and open standards are essential to unlocking AI with data**, and that OSI is a collaborative effort to address the lack of a common semantic standard, enabling a more connected, open ecosystem.\n\nEssentially, trying to **standardize semantic model exchange through a vendor-agnostic specification** and a YAML-based OSI model, plus read/write mapping modules that will be part of the Apache open-source project.\n\nIn part, it's perfect, so we don't have dbt, Cube, or LookML-flavored syntax, but it's hard to grasp. Currently joined vendors are Alation, Atlan, BlackRock, Blue Yonder, Cube, dbt Labs, Elementum AI, Hex, Honeydew, Mistral AI, Omni, RelationalAI, Salesforce, Select Star, Sigma, and ThoughtSpot.\n\nWhat do you think? Will it help to harmonize metrics definitions? Or consolidating on specs for BI tools as well?",
    "author": "sspaeti",
    "timestamp": "2025-10-09T06:22:28",
    "url": "https://reddit.com/r/dataengineering/comments/1o25ss6/what_do_you_think_about_the_open_semantic/",
    "score": 16,
    "num_comments": 10,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o27u1n",
    "title": "Snowflake (or any DWH) Data Compression on Parquet files",
    "content": "Hi everyone,\n\nMy company is looking into using Snowflake as our main data warehouse, and I'm trying to accurately forecast our potential storage costs.\n\nHere's our situation: \nwe'll be collecting sensor data every five minutes from over 5000 pieces of equipment through their web APIs. My proposed plan is to first pull that data, use a library like pandas to do some initial cleaning and organization, and then convert it into compressed Parquet files. We'd then place these files in a staging area and most likely our cloud blob storage but we're flexible and could use Snowflake's internal stage as well.\n\nMy specific question is about what happens to the data size when we copy it from those Parquet files into the actual Snowflake tables. I assume that when Snowflake loads the data, it's stored according to its data type (varchar, number, etc.) and then Snowflake applies its own compression.\n\nSo, would the final size of the data in the Snowflake table end up being more, less, or about the same as the size of the original Parquet file? Let’s say, if I start with a 1 GB Parquet file, will the data consume more or less than 1 GB of storage inside Snowflake tables?\n\nI'm really just looking for a sanity check to see if my understanding of this entire process is on the right track.\n\nThanks!",
    "author": "rtripat",
    "timestamp": "2025-10-09T07:44:45",
    "url": "https://reddit.com/r/dataengineering/comments/1o27u1n/snowflake_or_any_dwh_data_compression_on_parquet/",
    "score": 11,
    "num_comments": 12,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o1udcq",
    "title": "Eventually got a DE job, but what's next?",
    "content": "After a Bootcamp and more than 6 months of job hunting, got rejected multiple times, I eventually landed a job in a public organization. But the first 3 months is way busier than I thought, I need to fit in quickly as there are so many jobs left from the last DE, and as the only DE in the team, I need to provide data internally and externally with a wide range of tools: legacy VBA code, SPSS script, code written in Jupyter notebook, Python script scheduled to run by scheduler and Dagster. And for sure, lots of SQL queries. And in the near future, we are going to retire some of the flat files and migrate them to our data warehouse, and we are aiming to improve our current ML model as well. I really enjoy what I'm doing, and have no complaints about the work environment. But I am wondering if I stay here for too long, do I even have the courage to pursue other postions in a more challenging Tech company? Do they even care about what I did at my current job? If you were me, will you aim for jobs with better pay and just settle in the same environment and see if I can get a promotion or find a better role internally?\n\n\\--------------------Edit--------------------\n\nI dm the comments asking about the Bootcamp, I will not post it here as it is not my intention. In such tough job market, everyone needs to work harder to get a job, not sure if a bootcamp can land you a job.",
    "author": "Internal-Daikon7152",
    "timestamp": "2025-10-08T19:22:50",
    "url": "https://reddit.com/r/dataengineering/comments/1o1udcq/eventually_got_a_de_job_but_whats_next/",
    "score": 43,
    "num_comments": 25,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o1u64i",
    "title": "We built Arc, a high-throughput time-series warehouse on DuckDB + Parquet (1.9M rec/sec)",
    "content": "Hey everyone,  I’m Ignacio, founder at Basekick Labs.\n\nOver the last few months I’ve been building Arc, a high-performance time-series warehouse that combines:\n\n* Parquet for columnar storage\n*  DuckDB for analytics\n* MinIO/S3 for unlimited retention\n* MessagePack ingestion for speed (1.89 M records/sec on c6a.4xlarge)\n\nIt started as a bridge for InfluxDB and Timescale for long term storage in s3, but it evolved into a full data warehouse for observability, IoT, and real-time analytics.\n\nArc Core is open-source (AGPL-3.0) and available here &gt; [https://github.com/Basekick-Labs/arc](https://github.com/Basekick-Labs/arc)\n\nBenchmarks, architecture, and quick-start guide are in the repo.\n\nWould love feedback from this community, especially around ingestion patterns, schema evolution, and how you’d use Arc in your stack.\n\nCheers, Ignacio",
    "author": "Icy_Addition_3974",
    "timestamp": "2025-10-08T19:13:11",
    "url": "https://reddit.com/r/dataengineering/comments/1o1u64i/we_built_arc_a_highthroughput_timeseries/",
    "score": 46,
    "num_comments": 15,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o1yixn",
    "title": "Has anyone built python models with DBT",
    "content": "So far I have been learning to build DBT models with SQL until now when I discovered you could do that with python. Was just curious to know from community if anyone has done it, how’s it like.",
    "author": "Fireball_x_bose",
    "timestamp": "2025-10-08T23:16:27",
    "url": "https://reddit.com/r/dataengineering/comments/1o1yixn/has_anyone_built_python_models_with_dbt/",
    "score": 10,
    "num_comments": 11,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o28s9e",
    "title": "Data Cleanup for AI/Automation Prep?",
    "content": "Who's doing data cleanup for AI readiness or optimization? \n\nAgencies? Consultants? In-house teams? \n\nI want to talk to a few people that are/have been doing data cleanup/standardization projects to help companies prep or get more out of their AI and automaton tools. \n\nWho should I be talking to? ",
    "author": "seanrrwilkins",
    "timestamp": "2025-10-09T08:20:36",
    "url": "https://reddit.com/r/dataengineering/comments/1o28s9e/data_cleanup_for_aiautomation_prep/",
    "score": 1,
    "num_comments": 7,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o1gcub",
    "title": "The AI promise vs reality: 45% of teams have zero non-technical user adoption",
    "content": "Sharing a clip from the recent [Data Stack Report webinar](https://www.youtube.com/watch?v=0Cx5CI1HdXc). \n\nKey stat: 45% of surveyed orgs have zero non-technical AI adoption for data work.\n\nThe promise was that AI would eliminate the need for SQL skills and make data accessible to everyone. Reality check: business users still aren't self-serving their data needs, even with AI \"superpowers.\"\n\nMaybe the barrier was never technical complexity. Maybe it's trust, workflow integration, or just that people prefer asking humans for answers.\n\nThoughts? Is this matching what you're seeing?\n\n\\--&gt; [full report](https://www.metabase.com/data-stack-report-2025)",
    "author": "Miserable_Fold4086",
    "timestamp": "2025-10-08T10:05:53",
    "url": "https://reddit.com/r/dataengineering/comments/1o1gcub/the_ai_promise_vs_reality_45_of_teams_have_zero/",
    "score": 86,
    "num_comments": 40,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o21bdo",
    "title": "Data pipelines(AWS)",
    "content": "We have multiple data sources using different patterns, and most users want to query and share data via Snowflake. What is the most reliable data pipeline between connecting and storing data in Snowflake, staging it in S3 or Iceberg, then connecting it to Snowflake? \n\nAnd is there such a thing as Data Ingestion as a platform or service?",
    "author": "OkWoodpecker6123",
    "timestamp": "2025-10-09T02:23:31",
    "url": "https://reddit.com/r/dataengineering/comments/1o21bdo/data_pipelinesaws/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o255dh",
    "title": "Open-source python data profiling tools",
    "content": "I have been wondering lately, why there is so much of space in data profiling tools even in FY25 when GenAI has been creeping in every corner of development works. I have gone through few libs like the GE, Talend and Y-data profiling, Pandas, etc. Most of them are pretty complex to integrate into your solution as a module component, lack robustness, or have a license demand. Help me please to locate an open-source data profiling option which would serve stably my project which deals with tons of data.",
    "author": "Theunknown2609",
    "timestamp": "2025-10-09T05:54:23",
    "url": "https://reddit.com/r/dataengineering/comments/1o255dh/opensource_python_data_profiling_tools/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o20ilj",
    "title": "Poor update performance with clickhouse",
    "content": "Clickhouse have performance problem with random updates, i changed to \"insert new records then delete old record\" method, but performance still poor. Are there any db out there that have decent random updates performance AND can handle all sorts of query fast\n\nClickhouse has performance problem with random updates. I use two sql (insert &amp; delete)  instead of one UPDATE sql in hope to improve random update performance\n\n1. edit old record by inserting new records (**value of order by column unchanged**)\n2. delete old record\n\nAre there any db out there that have decent random updates performance AND can handle all sorts of query fast\n\ni use MergeTree engine currently:\n\n    CREATE TABLE hellobike.t_records\n    (\n        `create_time` DateTime COMMENT 'record time',\n        ...and more...\n    )\n    ENGINE = MergeTree()\n    ORDER BY create_time\n    SETTINGS index_granularity = 8192;",
    "author": "National_Assist5363",
    "timestamp": "2025-10-09T01:29:16",
    "url": "https://reddit.com/r/dataengineering/comments/1o20ilj/poor_update_performance_with_clickhouse/",
    "score": 3,
    "num_comments": 14,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o1q6l7",
    "title": "What's this bullshit, Google?",
    "content": "Why do I need to fill out a questionnaire, provide you with branding materials, create a dedicated webpage, and submit all of these things to you for \"verification\" *just* so that I can enable OAuth for calling the BigQuery API?\n\nAlso, I have to get branding information published for the \"app\" *separately* from verifying it?\n\nI'm not even publishing a god damn application! I'm just doing a small reverse ETL into another third party tool that doesn't natively support service account authentication. The scope is literally just bigquery.readonly.\n\nWay to create a walled garden. 😮‍💨\n\nIs anyone else exasperated by the number of *purely* software development specific concepts/patterns/\"requirements\" that seems to continuously creep into the data space?\n\nSure, DE is arguably a subset of SWE, but sometimes stuff like this makes me wonder whether anyone with a data background is actually at the helm. Why would anyone *need* branding information for authenticating with a database?",
    "author": "hcf_0",
    "timestamp": "2025-10-08T16:04:21",
    "url": "https://reddit.com/r/dataengineering/comments/1o1q6l7/whats_this_bullshit_google/",
    "score": 19,
    "num_comments": 25,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o24far",
    "title": "Replacing Legacy Message Queueing Solutions with RabbitMQ - Upcoming Conference Talk for Data Engineers!",
    "content": "Struggling with integrating legacy message queueing systems into modern data pipelines? Brett Cameron, Chief Application Services Officer at VMS Software Inc. and RabbitMQ/Erlang expert, will be presenting a talk on modernizing these systems using RabbitMQ.\n\n**Talk:** Replacing Legacy Message Queueing Solutions with RabbitMQ  \nData engineers and pipeline architects will benefit from practical insights on how RabbitMQ can solve traditional middleware challenges and streamline enterprise data workflows. Real-world use-cases and common integration hurdles will be covered.\n\nSave your spot for MQ Summit [https://mqsummit.com/talks/replacing-legacy-message-queueing-solutions-with-rabbitmq/](https://mqsummit.com/talks/replacing-legacy-message-queueing-solutions-with-rabbitmq/)",
    "author": "Code_Sync",
    "timestamp": "2025-10-09T05:20:55",
    "url": "https://reddit.com/r/dataengineering/comments/1o24far/replacing_legacy_message_queueing_solutions_with/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o1x0t8",
    "title": "Stay at current job or take new hybrid offer in a different industry?",
    "content": "I currently work full time as an operational analyst in the energy industry. This is my first job out of college, and I make around mid 79K(base). I’m also in grad school for Data Science and AI, and my classes are in person. My job isn’t very technical right now. It’s more operational and repetitive, and my manager doesn’t really let me get involved in data or reporting work. My long term goal is to move into a machine learning engineer or data engineering role.\n\nI recently got an offer from another company in a different industry. The pay is in the low 80s and the role is hybrid with about two to three days in the office. It’s a bit more technical than what I do now since it focuses on Power BI and reporting, but it’s still not super advanced or coding heavy. The new job offers more PTO and I’d have more autonomy to build models and learn skills on my own. The only catch is that raises aren’t guaranteed or significant.\n\nHere’s my situation. My current company is fully in person but it’s less than 10 miles from home and school. The new job is 30 to 40 miles each way, so the commute would be a lot longer even though it’s hybrid. At the beginning of next year, I’ll be eligible to apply for internal transfers into more data driven departments. However, I’m not sure how guaranteed that process really is since this is my first job in the industry. If I do move into a different role internally, the pay becomes much more competitive, but again it’s not something I can fully rely on. I’m also due for a raise of around 4 percent, a bonus, and about 3K in tuition reimbursement that I’d lose if I left now.\n\nFinancially, the new offer doesn’t change much. Maybe a few hundred more a month after taxes, but it offers hybrid flexibility, slightly more technical work, and a bit more freedom.\n\nWould you stay until the beginning of next year to collect the raise and bonus and then try to move internally into a more data focused role? Or would you take the hybrid offer in a new domain for the Power BI experience and flexibility, even though the commute is longer and the pay difference is small?\n\nTL;DR: First job out of college making mid 70K offered a low 80s hybrid role that’s a little more technical (Power BI and reporting) but in a new industry with longer commute and no guaranteed raises. Current job is closer to home and school, and I’ll get a raise, bonus, and tuition reimbursement if I stay until the beginning of next year plus a chance to transfer internally, though I’m not sure how guaranteed that is. If I move internally, the pay would be much more competitive, but it’s still a risk. Long term goal is to move into a machine learning engineer or data engineering role. Not sure if I should stay or take the new role.",
    "author": "ShipPuzzleheaded2022",
    "timestamp": "2025-10-08T21:45:30",
    "url": "https://reddit.com/r/dataengineering/comments/1o1x0t8/stay_at_current_job_or_take_new_hybrid_offer_in_a/",
    "score": 3,
    "num_comments": 10,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o22h4j",
    "title": "TPC-DS Benchmark: Trino 477 and Hive 4 on MR3 2.2",
    "content": "In this article, we report the results of evaluating the performance of the latest releases of Trino and Hive-MR3 using 10TB TPC-DS benchmark.\n\n1. **Trino 477** (released in September 2025)\n2. **Hive 4.0.0 on MR3 2.2** (released in October 2025)\n\nAt the end of the article, we show the progress of Trino and Hive on MR3 for the past two and a half years.",
    "author": "ForeignCapital8624",
    "timestamp": "2025-10-09T03:37:16",
    "url": "https://reddit.com/r/dataengineering/comments/1o22h4j/tpcds_benchmark_trino_477_and_hive_4_on_mr3_22/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o22cm9",
    "title": "How to deny Lineage Node Serialization/Deserialization in OpenLineage/Spark",
    "content": "Hey, I'm looking for a specific configuration detail within the OpenLineage Spark Integration and hoping someone here knows the trick.\nMy Spark jobs are running fine performance-wise, but I need to deny nodes that shows serializing and deserializing  while the job executes.\nIs there a specific Spark config property through which I can deny these nodes?",
    "author": "berserker467",
    "timestamp": "2025-10-09T03:30:05",
    "url": "https://reddit.com/r/dataengineering/comments/1o22cm9/how_to_deny_lineage_node/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o1aqvl",
    "title": "Why python dev need DuckDB (and not just another dataFrame library)",
    "content": "",
    "author": "TransportationOk2403",
    "timestamp": "2025-10-08T06:38:18",
    "url": "https://reddit.com/r/dataengineering/comments/1o1aqvl/why_python_dev_need_duckdb_and_not_just_another/",
    "score": 33,
    "num_comments": 11,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o1rzve",
    "title": "SCD Type 3 vs an alternate approach?",
    "content": "Hey guys,\n\nI am doing some data modelling, and I have a situation where there is a table field that analysts expect to update via manual entry. This will happen *once at most for any record*.\n\nI understand SCD Type 3 is used for such cases.\n\nSomething like the following:\n\n|value|prev\\_value|\n|:-|:-|\n|A|null|\n\nThen, after updating the record:\n\n|value|prev\\_value|\n|:-|:-|\n|B|A|\n\nBut I'm thinking of an alternative which more explicitly captures the binary (initial vs final) state of the record: something like value and orig\\_value. Set value = orig\\_value, unless business updates the record. \n\nSomething like:\n\n|value|orig\\_value|\n|:-|:-|\n|A|A|\n\nThen, after updating the record:\n\n|value|orig\\_value|\n|:-|:-|\n|B|A|\n\nIs there any reason NOT to do it this way? Business will make the updates to records by editing an upstream table via a file upload. I feel that this approach would simplify the SQL logic; a simple coalesce would do the job. Plus having only one column change as opposed to multiple feels cleaner, and the column names can communicate the intent of these fields better.",
    "author": "Spooked_DE",
    "timestamp": "2025-10-08T17:28:53",
    "url": "https://reddit.com/r/dataengineering/comments/1o1rzve/scd_type_3_vs_an_alternate_approach/",
    "score": 2,
    "num_comments": 15,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o19pjx",
    "title": "From DevOps to Data Engineering or Data Analyst?",
    "content": "I'm a DevOps Engineer with two years of experience. I switched to DevOps engineering from non tech specialisation and got my AWS certs, learned the DevOps culture and tools and I consider myself mid DevOps engineer with the experience I got with Cloud, IaC, CI/CD pipelines, Linux, Containers and other related areas.\n\nI had a single professional experience in DevOps and was laid off about 5 months ago. But I find it very difficult to land a new job despite building production level projects, upskilling, certification, and showcasing. \n\nThe reason is most of the job posts require senior positions with 3.5+ or 7+ years of experience. In addition to variety of skills required in every DevOps role. And I notice the same problem about it with other applicants.\n\nI am thinking about switching to Data Analytics or Data Engineering. \n\nI am looking for less stressful job (not looking to learn every trendy tool all the time, less uncertainities), with sustainable job demand.\n\nI always loved working with excel and building sheets. I am good with python, and I have theoritical knowledge about sql but have not practiced it. \n\nDo I pursue Data Engineering or Data analytics or keep trying with DevOps?",
    "author": "Accomplished_Fixx",
    "timestamp": "2025-10-08T05:55:39",
    "url": "https://reddit.com/r/dataengineering/comments/1o19pjx/from_devops_to_data_engineering_or_data_analyst/",
    "score": 18,
    "num_comments": 10,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o1b94r",
    "title": "Semistructured data in raw layer",
    "content": "Hello! Always getting great advice here, so here comes one more question from me.\n\nI’m building a system in which I use dlt to ingest data from various sources (that are either RMDBS, API or file-based) to Microsoft Azure SQL DB. Now lets say that I have this JSON response that consists of pleeeenty of nested data (4 or 5 levels deep of arrays). Now what dlthub does is that it automatically normalizes the data and loads the arrays into subtables. I like this very much, but now upon some reading I found out that the general advice is to stick as much as possible to the raw format of the data, so in this case loading the nested arrays in JSON format in the db, or even loading the whole response as one value to a raw table with one column.\n\nWha do you think about that? What I’m losing by normalizing it at this step, except the fact that I have a shitton of tables and I guess it’s impossible to recreate something if  I don’t like the normalize logic? Am I missing something? I’m not doing any transformations except this, mind you.\n\nThanks!",
    "author": "thursday22",
    "timestamp": "2025-10-08T06:58:42",
    "url": "https://reddit.com/r/dataengineering/comments/1o1b94r/semistructured_data_in_raw_layer/",
    "score": 13,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0yip2",
    "title": "Wake up babe, new format-aware compression framework by meta just dropped",
    "content": "",
    "author": "psychuil",
    "timestamp": "2025-10-07T19:20:57",
    "url": "https://reddit.com/r/dataengineering/comments/1o0yip2/wake_up_babe_new_formataware_compression/",
    "score": 98,
    "num_comments": 15,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o19ziq",
    "title": "Director and principle Data engineers",
    "content": "What are your job responsibilities and what tools are you using to manage/remember all the information about projects and teams?\n\nAre you still involved in development ?",
    "author": "educationruinedme1",
    "timestamp": "2025-10-08T06:06:48",
    "url": "https://reddit.com/r/dataengineering/comments/1o19ziq/director_and_principle_data_engineers/",
    "score": 11,
    "num_comments": 5,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0y1jn",
    "title": "How is Capital One for data engineering? I've heard they're meh-to-bad for tech jobs in general, but is this domain a bit of an exception?",
    "content": "I ask because I currently have a remote job (I've only been here for 6 months - I don't like it and am expecting to lose it soon), but I have an outstanding offer from Capital One for a Senior Data Engineer position that's valid until March or April.\n\nI wasn't sure about taking it since it's not remote and the higher responsibilities with the culture I hear on r/cscareerquestions makes me worry about my time there, but due to my looming circumstances, I may just take that offer.\n\nI'd rather have a remote job so I'm thinking of living off savings for a bit and applying/studying, assuming the offer-on-hold is as solid as they say.",
    "author": "paxmlank",
    "timestamp": "2025-10-07T18:57:40",
    "url": "https://reddit.com/r/dataengineering/comments/1o0y1jn/how_is_capital_one_for_data_engineering_ive_heard/",
    "score": 54,
    "num_comments": 42,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o17fcf",
    "title": "The Single Node Rebellion",
    "content": "The road to freedom is not going to be easy but the direction is clear.\n\nhttps://preview.redd.it/zgi2teihfvtf1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=d77d685ee36c0a380c2da63c029771ae672ccc94\n\n",
    "author": "Nekobul",
    "timestamp": "2025-10-08T04:06:39",
    "url": "https://reddit.com/r/dataengineering/comments/1o17fcf/the_single_node_rebellion/",
    "score": 6,
    "num_comments": 1,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o18ukl",
    "title": "Mcp integration with snowflake",
    "content": "How’s it going everyone?\nMe and my team are currently thinking about setting up an MCP server and integrating it with a snowflake warehouse.\nWe wanted to know if someone tried it before and had any recommendations, practices or good things to know before taking any actions.\nThanks!",
    "author": "FollowingExisting869",
    "timestamp": "2025-10-08T05:18:10",
    "url": "https://reddit.com/r/dataengineering/comments/1o18ukl/mcp_integration_with_snowflake/",
    "score": 4,
    "num_comments": 9,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0iula",
    "title": "I just rolled out my first production data pipeline, and I expected the hardest things would be writing ETL scripts or managing schema changes. I soon discovered the hardest things were usually things that had not crossed my mind:",
    "content": "Dirty or inconsistent data that makes downstream jobs fail\n\nMaking the pipeline idempotent so reruns do not clone or poison data\n\nIncluding monitoring and alerting that actually catch real failure\n\nWorking with inexperienced teams with DAGs, schemas, and pipelines.\n\nEven though I have read the tutorials and blog entries, these issues did not appear until the pipeline was live.\n\n",
    "author": "Flat_Direction_7696",
    "timestamp": "2025-10-07T09:01:14",
    "url": "https://reddit.com/r/dataengineering/comments/1o0iula/i_just_rolled_out_my_first_production_data/",
    "score": 198,
    "num_comments": 35,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0kyd0",
    "title": "I think we need other data infrastructure for AI (table-first infra)",
    "content": "hi!  \nI do some data consultancy for llm startups. They do llm finetuning for different use cases, and I build their data pipelines. I keep running into the same pain: just a pile of big text files. Files and object storage look simple, but in practice they slow me down. One task turns into many blobs across different places – messy. No clear schema. Even with databases, small join changes break things. The orchestrator can’t “see” the data, so batching is poor, retries are clumsy, and my GPUs sit idle.\n\nMy friend helped me rethink the whole setup. What finally worked was treating everything as tables with transactions – one namespace, clear schema for samples, runs, evals, and lineage. I snapshot first, then measure, so numbers don’t drift. Queues are data-aware: group by token length or expected latency, retry per row. After this, fewer mystery bugs, better GPU use, cleaner comparisons.\n\nHe wrote his view here: [https://tracto.ai/blog/better-data-infra](https://tracto.ai/blog/better-data-infra)\n\nDoes anyone here run AI workloads on transactional, table-first storage instead of files? What stack do you use, and what went wrong or right?",
    "author": "Fabulous_Pollution10",
    "timestamp": "2025-10-07T10:17:18",
    "url": "https://reddit.com/r/dataengineering/comments/1o0kyd0/i_think_we_need_other_data_infrastructure_for_ai/",
    "score": 153,
    "num_comments": 5,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o1693s",
    "title": "Spark Job Execution When OpenLineage (Marquez) API is Down?",
    "content": "I've been working with OpenLineage and Marquez to get robust data lineage for our Spark jobs. However, a question popped into my head regarding resilience and error handling.\nWhat exactly happens to a running Spark job if the OpenLineage (Marquez) API endpoint becomes unavailable or unresponsive?\nSpecifically, I'm curious about:\n\n * Does the Spark job itself fail or stop? Or does it continue to execute successfully, just without emitting lineage events?\n*  Are there any performance impacts if the listener is constantly trying (and failing) to send events?\n\n",
    "author": "berserker467",
    "timestamp": "2025-10-08T02:58:23",
    "url": "https://reddit.com/r/dataengineering/comments/1o1693s/spark_job_execution_when_openlineage_marquez_api/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0jhuk",
    "title": "Is there anything actually new in data engineering?",
    "content": "I have been looking around for a while now and I am trying to see if there is anything actually new in the data engineering space. I see a tremendous amount of renaming and fresh coats of paint on old concepts but nothing that is original. For example, what used to be called feeds is now called pipelines. New name, same concept. Three tier data warehousing (stage, core, semantic) is now being called medallion. I really want to believe that we haven't reached the end of the line on creativity but it seems like there a nothing new under the sun. I see open source making a bunch of noise on ideas and techniques that have been around in the commercial sector for literally decades.  I really hope I am just missing something here.",
    "author": "marketlurker",
    "timestamp": "2025-10-07T09:25:08",
    "url": "https://reddit.com/r/dataengineering/comments/1o0jhuk/is_there_anything_actually_new_in_data_engineering/",
    "score": 112,
    "num_comments": 64,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o14vvh",
    "title": "[FOSS] Flint: A 100% Config-Driven ETL Framework (Seeking Contributors)",
    "content": "I'd like to share a project I've been working on called **Flint**:\n\nFlint transforms data engineering by shifting from custom code to declarative configuration for complete ETL pipeline workflows. The framework handles all execution details while you focus on what your data should do, not how to implement it. This configuration-driven approach standardizes pipeline patterns across teams, reduces complexity for ETL jobs, improves maintainability, and makes data workflows accessible to users with limited programming experience.\n\nThe processing engine is abstracted away through configuration, making it easy to switch engines or run the same pipeline in different environments. The current version supports Apache Spark, with Polars support in development.\n\nIt is not intended to replace all pipeline programming work but rather make straightforward ETL tasks easier so engineers can focus on more interesting and complex problems.\n\nSee an example configuration at the bottom of the post. Check out the repo, star it if you like it, and let me know if you're interested in contributing.\n[GitHub Link: config-driven-ETL-framework](https://github.com/krijnvanderburg/config-driven-ETL-framework)\n\n## Why I Built It\n\nTraditional ETL development has several pain points:\n- Engineers spend too much time writing boilerplate code for basic ETL tasks, taking away time from more interesting problems\n- Pipeline logic is buried in code, inaccessible to non-developers\n- Inconsistent patterns across teams and projects\n- Difficult to maintain as requirements change\n\n## Key Features\n- **Pure Configuration**: Define sources, transformations, and destinations in JSON or YAML\n- **Multi-Engine Support**: Run the same pipeline on Pandas, Polars, or other engines\n- **100% Test Coverage**: Both unit and e2e tests at 100%\n- **Well-Documented**: Complete class diagrams, sequence diagrams, and design principles\n- **Strongly Typed**: Full type safety throughout the codebase\n- **Comprehensive Alerts**: Email, webhooks, files based on configurable triggers\n- **Event Hooks**: Custom actions at key pipeline stages (onStart, onSuccess, etc.)\n\n## Looking for Contributors!\nThe foundation is solid - 100% test coverage, strong typing, and comprehensive documentation - but I'm looking for contributors to help take this to the next level. Whether you want to add new engines, add tracing and metrics, change CLI to use click library, extend the transformation library to Polars,  I'd love your help!\n\nCheck out the repo, star it if you like it, and let me know if you're interested in contributing.\n\n[GitHub Link: config-driven-ETL-framework](https://github.com/krijnvanderburg/config-driven-ETL-framework)\n\n\n```jsonc\n{\n    \"runtime\": {\n        \"id\": \"customer-orders-pipeline\",\n        \"description\": \"ETL pipeline for processing customer orders data\",\n        \"enabled\": true,\n        \"jobs\": [\n            {\n                \"id\": \"silver\",\n                \"description\": \"Combine customer and order source data into a single dataset\",\n                \"enabled\": true,\n                \"engine_type\": \"spark\", // Specifies the processing engine to use\n                \"extracts\": [\n                    {\n                        \"id\": \"extract-customers\",\n                        \"extract_type\": \"file\", // Read from file system\n                        \"data_format\": \"csv\", // CSV input format\n                        \"location\": \"examples/join_select/customers/\", // Source directory\n                        \"method\": \"batch\", // Process all files at once\n                        \"options\": {\n                            \"delimiter\": \",\", // CSV delimiter character\n                            \"header\": true, // First row contains column names\n                            \"inferSchema\": false // Use provided schema instead of inferring\n                        },\n                        \"schema\": \"examples/join_select/customers_schema.json\" // Path to schema definition\n                    }\n                ],\n                \"transforms\": [\n                    {\n                        \"id\": \"transform-join-orders\",\n                        \"upstream_id\": \"extract-customers\", // First input dataset from extract stage\n                        \"options\": {},\n                        \"functions\": [\n                            {\"function_type\": \"join\", \"arguments\": {\"other_upstream_id\": \"extract-orders\", \"on\": [\"customer_id\"], \"how\": \"inner\"}},\n                            {\"function_type\": \"select\", \"arguments\": {\"columns\": [\"name\", \"email\", \"signup_date\", \"order_id\", \"order_date\", \"amount\"]}}\n                        ]\n                    }\n                ],\n                \"loads\": [\n                    {\n                        \"id\": \"load-customer-orders\",\n                        \"upstream_id\": \"transform-join-orders\", // Input dataset for this load\n                        \"load_type\": \"file\", // Write to file system\n                        \"data_format\": \"csv\", // Output as CSV\n                        \"location\": \"examples/join_select/output\", // Output directory\n                        \"method\": \"batch\", // Write all data at once\n                        \"mode\": \"overwrite\", // Replace existing files if any\n                        \"options\": {\n                            \"header\": true // Include header row with column names\n                        },\n                        \"schema_export\": \"\" // No schema export\n                    }\n                ],\n                \"hooks\": {\n                    \"onStart\": [], // Actions to execute before pipeline starts\n                    \"onFailure\": [], // Actions to execute if pipeline fails\n                    \"onSuccess\": [],  // Actions to execute if pipeline succeeds\n                    \"onFinally\": [] // Actions to execute after pipeline completes (success or failure)\n                }\n            }\n        ]\n    }\n}\n```",
    "author": "TeamFlint",
    "timestamp": "2025-10-08T01:28:42",
    "url": "https://reddit.com/r/dataengineering/comments/1o14vvh/foss_flint_a_100_configdriven_etl_framework/",
    "score": 2,
    "num_comments": 10,
    "upvote_ratio": 0.59,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o1619y",
    "title": "Choosing between two offer for data engineering roles",
    "content": "Hi there, this is my first post here\n\nI want to know what the community thinks, so some background: \n\nI am a data engineer with 4 years of experience, for the first 3 years I mainly worked with older side of data engineering (think Apache Cloudera, Hive, Impala, and its ecosystem)\n\nAnd this past year I've had the pleasure of working in Databricks &amp; Azure cloud environment, also diving into dimensional modeling\n\nNow, I am presented with basically two choices:\n1. Keep working on the dimensional modelling side of DE, since there is a new project involved in business department. So basically will be working mostly on business understanding &amp; data transformation hence the dimensional model\n2. Move to the DE in IT department &amp; will mostly work with more upstream layer (think bronze layer) &amp; ETL pipelines moving data from different sources\n\nI'm currently more inclined towards choice 1, but what do you guys think about the future prospects?\n\nThanks in advance",
    "author": "Low-Palpitation6322",
    "timestamp": "2025-10-08T02:44:27",
    "url": "https://reddit.com/r/dataengineering/comments/1o1619y/choosing_between_two_offer_for_data_engineering/",
    "score": 1,
    "num_comments": 3,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0aq9n",
    "title": "5 Takeaways from Big Data London 2025 You’ll Soon Regret Reading",
    "content": "Wrote this article with a review of the conference... I had to take 10s of ambush enterprise demos to get some insights, but at least was fun :) Here is the article: [link](https://medium.com/@ggbaro/5-takeaways-from-big-data-london-2025-youll-soon-regret-reading-f08e05e6c68e)\n\nThe amount of hype is at its peak, I think some big changes will come in the near future\n\n*Disclaimer: The core article is not brand affiliate, but I work for* [*hiop*](https://www.hiop.io)*, which is mentioned in the article along our position on certain topics*",
    "author": "ggbaro",
    "timestamp": "2025-10-07T03:12:38",
    "url": "https://reddit.com/r/dataengineering/comments/1o0aq9n/5_takeaways_from_big_data_london_2025_youll_soon/",
    "score": 122,
    "num_comments": 34,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0qoxf",
    "title": "Do you know any really messy databases I could use for testing?",
    "content": "Hey everyone,\n\nAfter my previous [post ](https://www.reddit.com/r/dataengineering/comments/1nxshi2/how_to_deal_with_messy_database/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button)about working with databases that had no foreign keys, inconsistent table names, random fields everywhere, and zero documentation, I would like to practice on another really messy, real-world database, but unfortunately, I no longer have access to the hospital one I worked on.\n\nSo I’m wondering, does anyone know of any public or open databases that are *actually very messy*?\n\nIdeally something with:\n\n* Dozens or hundreds of tables\n* Missing or wrong foreign keys\n* Inconsistent naming\n* Legacy or weird structure\n\nAny suggestions or links would be super appreciated. I searched on Google, but most of the database I found was okay/not too bad.",
    "author": "Which-Breadfruit-926",
    "timestamp": "2025-10-07T13:44:24",
    "url": "https://reddit.com/r/dataengineering/comments/1o0qoxf/do_you_know_any_really_messy_databases_i_could/",
    "score": 18,
    "num_comments": 19,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o178u7",
    "title": "Advice on Improving Data Search",
    "content": "I am currently working on a data search tool \n\nFront end (Nexjs) + AI enabled insight + analytics enabled\n\nBackend (Express JS) + Postgres\n\n  \nI have data in different formats (csv, xlsx, jsonl, json, sql, pdf etc)\n\nI take the data and paste in a folder within my project then process it from there \n\nI have several challenges:\n\n1. My data ingest approach is not optimized. I tried using first approach: **node igestion (npm run:ingest)&gt; put it into a staging table and then copy the stagoimg table to the real table, but this approach is taking too long to load the data into progress**\n\n**2. Second approach I use is take for instance a csv  &gt; clean it into a new csv &gt; load it directly into postgres (better)**\n\n**3. Third approach is take the data  &gt; clean it &gt; turn it into json file &gt; convert this into sql &gt; and use psql commands to insert the data into the database** \n\n  \nThe other challenges I am facing is search (The search is taking too approx 6 secs),  I am considering using paradeDB to improve the search , would this help as the data grows ?  \n\n  \nExperienced engineers please advice on this ",
    "author": "Dangerous-Remote-608",
    "timestamp": "2025-10-08T03:56:52",
    "url": "https://reddit.com/r/dataengineering/comments/1o178u7/advice_on_improving_data_search/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0tkwd",
    "title": "Purview or ...",
    "content": "We are about to dump Collibra as our governance tool and we get Purview as part of our MS licensing but I like the look of Openmetadata. Boss won't go with an opensource solution but I get the impression Purview is less usable than Collibra.. I can also get most of the lineage in GCP and users can use AI to explore data.\n\nAnyone like Purview.. we are not an MS shop other than office stuff and identity.. mix of AWS with a GCP data platform",
    "author": "wa-jonk",
    "timestamp": "2025-10-07T15:36:38",
    "url": "https://reddit.com/r/dataengineering/comments/1o0tkwd/purview_or/",
    "score": 9,
    "num_comments": 8,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0m9c4",
    "title": "Future of data in combination with AI",
    "content": "I keep seeing posts of people worried that AI is going to replace data jobs. \n\nI do not see this happening, I actually see the inverse happening. \n\nWhy? \n\nThere are areas or industries that are difficult to surface to consumers or businesses because they're complicated. The subjects themselves and/or the underlying subject information. Science, finance, etc. There's lots of areas. AI is expected to help breakdown those barriers to increase the consumption of complicated subject matters. \n\nGuess what's required to enable this? ...data. \n\nNot just any data, good data. High integrity data, ultra high integrity data. The higher, the more valuable. Garbage data isn't going to work anymore, in any industry, as the years roll on.\n\nThis isn't just true for those complicated areas, all industries will need better data. \n\nAnyone who wants to be a player in the future is going to have to upgrade and/or completely re-write their existing systems since the vast majority of data systems today produce garbage data. Partly due to businesses in-adequality budgeting for it. There is a good portion of companies that will have to completely restart their data operations, relegating their current data useless and/or obsolete. Operational, transactional, analytical, etc. \n\nThis is just to get high integrity data. To implement data into products needing application/operational data feeds where AI is also expected to expand? Is an additional area. \n\nData engineering isn't going anywhere.",
    "author": "DataIron",
    "timestamp": "2025-10-07T11:03:27",
    "url": "https://reddit.com/r/dataengineering/comments/1o0m9c4/future_of_data_in_combination_with_ai/",
    "score": 17,
    "num_comments": 26,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o03g0b",
    "title": "I can’t* understand the hype on Snowflake",
    "content": "I’ve seen a lot of roles demanding Snowflake exp, so okay, I just accept that I will need to work with that \n\nBut seriously, Snowflake has pretty simple and limited Data Governance, don’t have too much options on performance/cost optimization (can get pricey fast), has a huge vendor lock in and in a world where the world is talking about AI, why would someone fallback to simple Data Warehouse? No need to mention what it’s concurrent are offering in terms of AI/ML…\n\nI get the sense that Snowflake is a great stepping stone. Beautiful when you start, but you will need more as your data grows.\n\nI know that Data Analyst loves Snowflake because it’s simple and easy to use, but I feel the market will demand even more tech skills, not less.\n\n\n*actually, I can ;) ",
    "author": "NoGanache5113",
    "timestamp": "2025-10-06T19:57:58",
    "url": "https://reddit.com/r/dataengineering/comments/1o03g0b/i_cant_understand_the_hype_on_snowflake/",
    "score": 181,
    "num_comments": 125,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o07rp2",
    "title": "What AI Slop can do?",
    "content": "I'm now ended up in a situation to deal with a messy Chatgpt created ETL that went to production without proper Data Quality checks, this ETL has easily missed thousands of records per day for the last 3 months. \n\nI would not be shocked if this ETL was deployed by our junior but it was designed and deployed by our senior with 8+ YOE. \nPreviously, I used to admire his best practices and approaches in designing ETLs, now it is sad what AI Slop has done to our senior.\n\nI'm now forced to backfill and fix the existing systems ASAP because he is having some other priorities 🙂\n\n",
    "author": "ProgrammerDouble4812",
    "timestamp": "2025-10-07T00:02:50",
    "url": "https://reddit.com/r/dataengineering/comments/1o07rp2/what_ai_slop_can_do/",
    "score": 82,
    "num_comments": 39,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0rhhv",
    "title": "What are the best practices when it comes to applying complex algorithms in data pipelines?",
    "content": "Basically I'm wondering how to handle anything complex enough inside a data pipeline that is beyond the scope of regular SQL, spark, etc.\n\nOf course using SQL and spark is preferred but may not always feasible. Here are some example use cases I have in mind.\n\nFor dataset with certain groups perform the task for each group:\n\n* apply a machine learning model\n* solve a non linear optimization problem\n* solve differential equations\n* apply complex algorithm that cover thousand of lines of code in Python\n\nAfter doing a bit of research, it seems like the solution space for the use case is rather poor with options like (pandas) udf which have their own problems (bad performance due to overhead).\n\nAm I overlooking better options or are the data engineering tools just underdeveloped for such (niche?) use cases?",
    "author": "22Maxx",
    "timestamp": "2025-10-07T14:14:09",
    "url": "https://reddit.com/r/dataengineering/comments/1o0rhhv/what_are_the_best_practices_when_it_comes_to/",
    "score": 7,
    "num_comments": 12,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0g1u4",
    "title": "Footgun AI",
    "content": "",
    "author": "Thinker_Assignment",
    "timestamp": "2025-10-07T07:17:45",
    "url": "https://reddit.com/r/dataengineering/comments/1o0g1u4/footgun_ai/",
    "score": 14,
    "num_comments": 2,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o03svz",
    "title": "How do you actually use dbt in your daily work?",
    "content": "Hey everyone,\n\nIn my current role, my team wants to encourage me to start using dbt, and they’re even willing to pay for a training course so I can learn how to implement it properly.\n\nFor context, I’m currently working as a Data Analyst, but I know dbt is usually more common in Analytics Engineer and Data Engineer roles and that’s why I wanted to ask here , for those of you who use dbt day-to-day, what do you actually do with it?\n\nDo you really use **everything** dbt has to offer  like macros, snapshots, seeds, tests, docs, exposures, etc.? Or do you mostly stick to modeling and testing?\n\nBasically, I’m trying to understand what parts of dbt are truly essential to learn first, especially for someone coming from a data analyst background who might eventually move into an Analytics Engineer role.\n\nWould really appreciate any insights or real-world examples of how you integrate dbt into your workflows.\n\nThanks in advance",
    "author": "LongCalligrapher2544",
    "timestamp": "2025-10-06T20:15:56",
    "url": "https://reddit.com/r/dataengineering/comments/1o03svz/how_do_you_actually_use_dbt_in_your_daily_work/",
    "score": 73,
    "num_comments": 37,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0cyj4",
    "title": "Did you build your own data infrastructure?",
    "content": "I've seen posts from the past about engineering jobs becoming infra jobs over time. I'm curious - did you have to build your own infra? Are you the one maintaining at the company? Are you facing problems because of this? ",
    "author": "Character-Zombie1330",
    "timestamp": "2025-10-07T05:09:40",
    "url": "https://reddit.com/r/dataengineering/comments/1o0cyj4/did_you_build_your_own_data_infrastructure/",
    "score": 13,
    "num_comments": 31,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0nx5y",
    "title": "Large Scale with Dagster",
    "content": "I am currently setting up a data pipeline with Dagster and am faced with the question of how best to structure it when I have multiple data sources (e.g., different APIs, databases, Files).\nEach source in turn has several tables/structures that need to be processed.\n\nMy question:\nShould I create a separate asset (or asset graph) for each source, or would it be better to generate the assets dynamically/automatically based on metadata (e.g., configuration or schema information)?\nMy main concerns are maintainability, clarity, and scalability if additional sources or tables are added later.\n\nI would be interested to know\n- how you have implemented something like this in Dagster\n- whether you define assets statically per source or generate them dynamically\n- and what your experiences have been (e.g., with regard to partitioning, sensors, or testing).",
    "author": "SurroundFun9276",
    "timestamp": "2025-10-07T12:02:17",
    "url": "https://reddit.com/r/dataengineering/comments/1o0nx5y/large_scale_with_dagster/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0ixsr",
    "title": "KESTRA VS. TEMPORAL",
    "content": "Has anyone here actually used Kestra or Temporal in production?\n\nI’m trying to understand how these two compare in practicen Kestra looks like a modern, declarative replacement for Airflow (YAML-first, good UI, lighter ops), while Temporal feels more like an execution engine for long-running, stateful workflows (durable replay, SDK-based)\n\nFor teams doing data orchestration + AI/agent workflows, where do you draw the line between the two? Do you ever see them co-existing (Kestra for pipelines, Temporal for async AI tasks), or is one clearly better for end-to-end automation?",
    "author": "Glum_Shopping_7833",
    "timestamp": "2025-10-07T09:04:32",
    "url": "https://reddit.com/r/dataengineering/comments/1o0ixsr/kestra_vs_temporal/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o023we",
    "title": "I keep making mistakes that impact production jobs…losing confidence in my abilities",
    "content": "I am a junior data engineer with a little over a year worth of experience. My role started off as a support data engineer but in the past few months, my manager has been giving the support team more development tasks since we all wanted to grow our technical skills. I have also been assigned some development tasks in the past few months, mostly fixing a bug or adding validation frameworks in different parts of a production job. \n\nBefore I was the one asking for more challenging tasks and wanted to work on development tasks but now that I have been given the work, I feel like I have only disappointed my manager. In the past few months, I feel like pretty much every PR I merged ended up having some issue that either broke the job or didn’t capture the full intention of the assigned task. \n\nAt first, I thought I should be testing better. Our testing environments are currently so rough to deal with that just setting them up to test a small piece of code can take a full day of work. Anyway, I did all that but even then I feel like I keep missing some random edge case or something that I failed to consider which ends up leading to a failure downstream. And I just constantly feel so dumb in front of my manager. He ends up having to invest so much time in fixing things I break and he doesn’t even berate me for it but I just feel so bad. I know people say that if your manager reviewed your code then its their responsibility too, but I feel like I should have tested more and that I should be more holistic in my considerations. I just feel so self-conscious and low on confidence. \n\nThe annoying thing is that the recent validation thing I worked on, we introduced it to other teams too since it would affect their day-to-day tasks but turns out, my current validation framework technically works but it will also result in some false positives that I now need to work on. But other teams know that I am the one who set this up and that I failed to consider something so anytime, these false positives show up (until I fix it), it will be because of me. I just find it so embarrassing and I know it will happen again because no matter how much I test my code, there is always something that I will miss. It almost makes me want to never PR into production and just never write development code, keep doing my support work even though I find that tedious and boring but at least its relatively low stakes…\n\nI am just not feeling very good and doesn’t help that I feel like I am the only one making these kind of mistakes in my team and being a burden on my manager, and ultimately creating more work for him with my mistakes…Like I think even the new person on the team isn’t making as many mistakes as I am..",
    "author": "eatdrinksleepp",
    "timestamp": "2025-10-06T18:53:54",
    "url": "https://reddit.com/r/dataengineering/comments/1o023we/i_keep_making_mistakes_that_impact_production/",
    "score": 27,
    "num_comments": 15,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0heu3",
    "title": "Walrus: A 1 Million ops/sec, 1 GB/s Write Ahead Log in Rust",
    "content": "Hey r/dataengineering,\n\nI made walrus: a fast Write Ahead Log (WAL) in Rust built from first principles which achieves 1M ops/sec and 1 GB/s write bandwidth on consumer laptop.\n\nfind it here: [https://github.com/nubskr/walrus](https://github.com/nubskr/walrus)\n\nI also wrote a blog post explaining the architecture: [https://nubskr.com/2025/10/06/walrus.html](https://nubskr.com/2025/10/06/walrus.html)\n\nyou can try it out with:\n\n    cargo add walrus-rust\n\njust wanted to share it with the community and know their thoughts about it :)",
    "author": "Ok_Marionberry8922",
    "timestamp": "2025-10-07T08:08:31",
    "url": "https://reddit.com/r/dataengineering/comments/1o0heu3/walrus_a_1_million_opssec_1_gbs_write_ahead_log/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o07r26",
    "title": "backfilling cumulative table design",
    "content": "Hey everyone,\n\nHas anyone here worked with **cumulative dimensions** in production?\n\nI just found this [video](https://www.youtube.com/watch?v=dWj8VeBEQCc) where the creator demonstrates a technique for building a cumulative dimension. It looks really cool, but I was wondering how you would handle **backfilling** in such a setup.\n\nMy first thought was to run a loop like the creator run his manually creation of the cumulative table shown in the video, but that could become inefficient as data grows. I also discovered that you can achieve something similar for backfills using`ARRAY_AGG()` in Snowflake, though I’m not sure what potential downsides there might be.\n\nDoes anyone have a code example or a preferred approach for this kind of scenario?\n\nThanks in advance ❤️",
    "author": "Fun-Jeweler3794",
    "timestamp": "2025-10-07T00:01:45",
    "url": "https://reddit.com/r/dataengineering/comments/1o07r26/backfilling_cumulative_table_design/",
    "score": 8,
    "num_comments": 8,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0894u",
    "title": "What actually causes “data downtime” in your stack? Looking for real failure modes + mitigations",
    "content": "I’m \\~3 years into DE. Current setup is pretty simple: managed ELT → cloud warehouse, mostly CDC/batch, transforms in dbt on a scheduler. Typical end-to-end freshness is \\~5–10 min during the day. Volume is modest (\\~40–50M rows/month). In the last year we’ve only had a handful of isolated incidents (expired creds, upstream schema drift, and one backfill that impacted partitions) but nothing too crazy.\n\nI’m trying to sanity-check whether we’re just small/lucky. For folks running bigger/streaming or more heterogenous stacks, what *actually* bites you?\n\nIf you’re willing to share: how often you face real downtime, typical MTTR, and one mitigation that actually moved the needle. Trying to build better playbooks before we scale.",
    "author": "plot_twist_incom1ng",
    "timestamp": "2025-10-07T00:33:59",
    "url": "https://reddit.com/r/dataengineering/comments/1o0894u/what_actually_causes_data_downtime_in_your_stack/",
    "score": 5,
    "num_comments": 6,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o06kdv",
    "title": "Job Switch - Study Partner",
    "content": "Looking for a dedicated study partner who is a working professional and is currently preparing for a job switch- Let's stay consistent, share resources, and keep each other accountable.",
    "author": "Medical_Ad2859",
    "timestamp": "2025-10-06T22:47:58",
    "url": "https://reddit.com/r/dataengineering/comments/1o06kdv/job_switch_study_partner/",
    "score": 4,
    "num_comments": 7,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0b0du",
    "title": "Setting up seamless Dagster deployments",
    "content": "Hey folks,\n\nI recently implemented a CI/CD pipeline for my team’s Dagster setup. It uses a webhook on our GitHub repo which triggers a build job on Jenkins. The Jenkins pipeline builds a Docker image and uploads it to a registry. From there, it gets pulled onto the target machine. The existing container is stopped and a new container is started from the pulled image. \n\nIt’s fairly simple and works as intended. But, I foresee an issue in the future. For now, I’m the only developer so I time the deployments for when there are no jobs running on Dagster. But when the number of jobs and developers increase I don’t think that will be possible. If a container gets taken down while a job is running, that just causes issues. So I’m interested to know how are you guys handling this ? What is your deployment process like ?",
    "author": "YameteGPT",
    "timestamp": "2025-10-07T03:29:03",
    "url": "https://reddit.com/r/dataengineering/comments/1o0b0du/setting_up_seamless_dagster_deployments/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o07wb7",
    "title": "An aspiring DE looking to pick the thoughts of DE professionals.",
    "content": "I have a degree from the humanities and discovered my passion for building things later on. I'm a self-taught software engineer without any professional experience looking to transition into the DE field. \n\nI started practicing with python and built a few fairly simple data pipelines like pulling data from Kaggle API, transforming it, and loading it to MongoDB  Atlas. This has given me some understanding and experience with a library like pandas. I recognize my skills currently aren't all that and so I'm actively developing other skills required to succeed in this role.\n\n\nI'm actively hunting for entry-level roles in DE. As a professional who's working in this field, I'd like to kindly pick your thoughts on what entry-level roles I might target to land my first job in DE and what advice you might offer moving forward in terms of career path.\n\n\nThank you for your time.\n",
    "author": "Playful_Concert3298",
    "timestamp": "2025-10-07T00:11:03",
    "url": "https://reddit.com/r/dataengineering/comments/1o07wb7/an_aspiring_de_looking_to_pick_the_thoughts_of_de/",
    "score": 3,
    "num_comments": 11,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1o0dy9y",
    "title": "Unified Prediction Market Python Library",
    "content": "",
    "author": "Ashercn97",
    "timestamp": "2025-10-07T05:53:32",
    "url": "https://reddit.com/r/dataengineering/comments/1o0dy9y/unified_prediction_market_python_library/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nzp92t",
    "title": "We just launched Daft’s distributed engine v1.5: an open-source engine for running models on data at scale",
    "content": "Hi all! I work on Daft full-time, and since we just shipped a big feature, I wanted to share what’s new. Daft’s been mentioned here a couple of times, so AMA too.\n\nDaft is an open-source Rust-based data engine for multimodal data (docs, images, video, audio) and running models on them. We built it because getting data into GPUs efficiently at scale is painful, especially when working with data sitting in object stores, and usually requires custom I/O + preprocessing setups.\n\nSo what’s new? Two big things.  \n  \n**1. A new distributed engine for running models at scale**\n\nWe’ve been using Ray for distributed data processing but consistently hit scalability issues. So we switched from using Ray Tasks for data processing operators to running one Daft engine instance per node, then scheduling work across these Daft engine instances. Fun fact: we named our single-node engine “Swordfish” and our distributed runner “Flotilla” (i.e. a school of swordfish).\n\nWe now also use [morsel-driven parallelism](https://15721.courses.cs.cmu.edu/spring2016/papers/p743-leis.pdf) and dynamic batch sizing to deal with varying data sizes and skew.\n\nAnd we have smarter shuffles using either the Ray Object Store or our new Flight Shuffle (Arrow Flight RPC + NVMe spill + direct node-to-node transfer).  \n  \n**2. Benchmarks for AI workloads**\n\nWe just designed and ran some swanky new AI benchmarks. Data engine companies love to bicker about TPC-DI, TPC-DS, TPC-H performance. That’s great, who doesn’t love a throwdown between Databricks and Snowflake.\n\nSo we’re throwing a new benchmark into the mix for audio transcription, document embedding, image classification, and video object detection. More details linked at the bottom of this post, but tldr Daft is 2-7x faster than Ray Data and 4-18x faster than Spark on AI workloads.\n\nhttps://preview.redd.it/ohjpa08jyitf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=5109e9cb6a8e9ca435b7ec77f522b1830b4d9a72\n\nAll source code is public. If you think you can beat it, we take all comers 😉\n\n**Links**\n\nCheck out our architecture blog! [https://www.daft.ai/blog/introducing-flotilla-simplifying-multimodal-data-processing-at-scale](https://www.daft.ai/blog/introducing-flotilla-simplifying-multimodal-data-processing-at-scale)\n\nOr our benchmark blog [https://www.daft.ai/blog/benchmarks-for-multimodal-ai-workloads](https://www.daft.ai/blog/benchmarks-for-multimodal-ai-workloads)\n\nOr check us out [https://github.com/Eventual-Inc/Daft](https://github.com/Eventual-Inc/Daft) :)",
    "author": "sanityking",
    "timestamp": "2025-10-06T10:19:07",
    "url": "https://reddit.com/r/dataengineering/comments/1nzp92t/we_just_launched_dafts_distributed_engine_v15_an/",
    "score": 21,
    "num_comments": 6,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nzmz47",
    "title": "How to cope with messing up?",
    "content": "Been on two large scale projects.\n\nProject 1 - Moving a data share into Databricks\n\nThis has been about a 3 months process. All the data is being shared through databricks on a monthly cadence. There was testing and sign off from vendor side.\n\nI did 1:1 data comparison on all the files except 1 grouping of them which is just a data dump of all our data. One of those files had a bunch of nulls and its honestly something I should have caught. I only did a cursory manual review before send because there were no changes and it already was signed off on. I feel horrible and sick right now about it.\n\nProject 2 - Long term full accounts reconciliation of all our data.\n\nProject 1s fuck up wouldnt make me feel as bad if i wasn't 3 weeks behind and struggling with project 2. Its a massive 12 month project and im behind on vendor test start cause the business logic is 20 years old and impossible to replicate.\n\nThe stress is eating me alive.",
    "author": "Dashncrash-",
    "timestamp": "2025-10-06T08:55:40",
    "url": "https://reddit.com/r/dataengineering/comments/1nzmz47/how_to_cope_with_messing_up/",
    "score": 24,
    "num_comments": 24,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nzn8mj",
    "title": "How I am building a data engineering job board",
    "content": "Hello fellow data engineers! Since I received positive feedback from my last year post about a [FAANG job board](https://www.reddit.com/r/dataengineering/comments/1hzgnmv/faang_data_engineering_job_board/) I decided to share updates on expanding it.\n\nYou can check it out here: [https://hire.watch/?categories=Data+Engineering](https://hire.watch/?categories=Data+Engineering)\n\nApart from the new companies I am processing, there is a new filter by goal salary - you just set your goal amount, the rate (per hour, per month, per year) and the currency (e.g. USD, EUR) and whether you want the currency in the job posting to match exactly.\n\nSo the full list of filters is:\n\n1. Full-text search\n2. Location - on-site\n3. Remote - from a given city, US state, EU, etc.\n4. Category - you can check out the data engineering category here: [https://hire.watch/?categories=Data+Engineering](https://hire.watch/?categories=Data+Engineering)\n5. Years of experience and seniority\n6. Target gross salary\n7. Date posted and date modified\n\nOn a techincal level, I use Dagster + DBT + the Python ecosystem (Polars, numpy, etc.) for most of the ETL, as well as LLMs for enriching and organizing the job postings.\n\nI prioritize features and next batch of companies to include by doing polls in the Discord community: [https://discord.gg/cN2E5YfF](https://discord.gg/cN2E5YfF) , so you can join there and vote if you want to see a feature you want earlier.\n\nLooking forward to your feedback :)",
    "author": "dev-ai",
    "timestamp": "2025-10-06T09:05:04",
    "url": "https://reddit.com/r/dataengineering/comments/1nzn8mj/how_i_am_building_a_data_engineering_job_board/",
    "score": 20,
    "num_comments": 13,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nzipj5",
    "title": "About to be let go",
    "content": "Hi all, \n\nI am currently working as a data engineer. I have worked for about 2-3 years in this position and due to restructuring, the person that hired me left the company 1 year after hiring me. I understand that learning comes from yourself and this is a wake up call for me. I would like to ask for some advice on what is required to be a successful data engineer in this day and age and what the job market is leaning towards. I don’t have much time in this company and would like some advice on how to proceed to get my next position. \n\nThanks! 🙏 ",
    "author": "No-Importance2124",
    "timestamp": "2025-10-06T06:13:24",
    "url": "https://reddit.com/r/dataengineering/comments/1nzipj5/about_to_be_let_go/",
    "score": 31,
    "num_comments": 21,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nzqt08",
    "title": "Casual DE Meetups in the NYC area?",
    "content": "Hey folks,\n\nI was wondering if anyone knows of any data engineering meetups in the NYC area. I’ve checked [Meetup.com](http://Meetup.com), but most of the events there seem to be hosted or sponsored by large organizations. I’m looking for something more casual—just a group of data engineering professionals getting together to share experiences and insights (over mini golf, or a walk through central park, etc.), similar to what you’d find in r/ProgrammingBuddies.",
    "author": "jduran9987",
    "timestamp": "2025-10-06T11:16:37",
    "url": "https://reddit.com/r/dataengineering/comments/1nzqt08/casual_de_meetups_in_the_nyc_area/",
    "score": 10,
    "num_comments": 5,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nztpo1",
    "title": "Unexpected data from source with different type",
    "content": "How are you guys dealing with unexpected data from the source? \n\nMy company has quite a few airflow DAGs with code to read data from an Oracle table into a BigQuery table.\nAll are mostly \"SELECT * FROM oracle_table\", get it into a pandas dataframe and use pandas method for Bigquery sink \"df.to_gbq(...)\"\n\nIt's a clear weak strategy regarding data quality. A few errors I've come across are when unexpected data pop into a column, such as an integer in a data column. So the destiny table can't accept it due to its defined schema.\n\nHow are you dealing with expectations for data? Schema evolution maybe? Quality tasks before layers?",
    "author": "meet_me_at_seven",
    "timestamp": "2025-10-06T13:03:53",
    "url": "https://reddit.com/r/dataengineering/comments/1nztpo1/unexpected_data_from_source_with_different_type/",
    "score": 5,
    "num_comments": 6,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nzcncy",
    "title": "Differentiating between analytics engineer vs data engineer",
    "content": "In my company, i am the only “data” person responsible for analytics and data models. There are 30 people in our company currently \n\nOur current tech stack is fivetran plus bigquery data transfer service to ingest salesforce data to bigquery.\n\nFor the most part, BigQuery’s native EL tool can replicate the salesforce data accurately and i would just need to do simple joins and normalize timestamp columns\n\nCurious if we were to ever scale the company, i am deciding between hiring a data engineer or an analytics engineer. Fivetran and DTS work for my use case and i dont really need to create custom pipelines; just need help in “cleaning” the data to be used for analytics for our BI analyst (another role to hire)\n\nWhich role would be more impactful for my scenario? Or is “analytics engineer“ just another buzz term?\n",
    "author": "tytds",
    "timestamp": "2025-10-06T00:32:14",
    "url": "https://reddit.com/r/dataengineering/comments/1nzcncy/differentiating_between_analytics_engineer_vs/",
    "score": 37,
    "num_comments": 33,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nznqhm",
    "title": "Launching an AI Data meet in Manchester",
    "content": "Hi Everyone, \n\nHope you don't mind me sharing, I have been empowered to create a space for data enthusiasts to explore the new and exciting world of Data and AI.\n\nI want to create a regular event where anyone and everyone can discuss, present and network around the evolving themes this subject throws up!\n\nIf you are based in and around Manchester and want to be involved and attend, please feel free to reach out to me or book a free space [here](https://www.meetup.com/ai-data-hive-manchester/events/311408010/?utm_medium=referral&amp;utm_campaign=share-btn_savedevents_share_modal&amp;utm_source=link&amp;utm_version=v2).\n\nI will also be providing free pizza and drinks! whats not to love, right?\n\nhttps://preview.redd.it/v8j6gvo1pitf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=7788020fd6dac87696786221f9aec2384231b554\n\nWhat's",
    "author": "Joe_Matillion",
    "timestamp": "2025-10-06T09:23:09",
    "url": "https://reddit.com/r/dataengineering/comments/1nznqhm/launching_an_ai_data_meet_in_manchester/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nzjh13",
    "title": "I built an open source AI data layer",
    "content": "Excited to share a project I’ve been solo building for months! Would love to receive honest feedback :) \n\nMy motivation: AI is clearly going to be the interface for data. But earlier attempts (text-to-SQL, etc.) fell short - they treated it like magic. The space has matured: teams now realize that AI + data needs structure, context, and rules. So I built a product to help teams deliver “chat with data” solutions fast with full control and observability -- am I wrong?\n\nThe product allows you to connect any LLM to any data source with centralized context (instructions, dbt, code, AGENTS.md, Tableau) and governance. Users can chat with their data to build charts, dashboards, and scheduled reports — all via an agentic, observable loop. With slack integration as well!\n\n* Centralize context management: instructions + external sources (dbt, Tableau, code, AGENTS.md), and self-learning\n* Agentic workflows (ReAct loops): reasoning, tool use, reflection\n* Generate visuals, dashboards, scheduled reports via chat/commands \n* Quality, accuracy, and performance scoring (llm judges) to ensure reliability\n* Advanced access &amp; governance: RBAC, SSO/OIDC, audit logs, rule enforcement \n* Deploy in your environment (Docker, Kubernetes, VPC) — full control over infrastructure \n\nhttps://reddit.com/link/1nzjh13/video/wfoxi3hjuhtf1/player\n\n**GitHub:** [github.com/bagofwords1/bagofwords](http://github.com/bagofwords1/bagofwords)   \n**Docs / architecture / quickstart:** [docs.bagofwords.com](http://docs.bagofwords.com) ",
    "author": "Hot_Dependent9514",
    "timestamp": "2025-10-06T06:44:56",
    "url": "https://reddit.com/r/dataengineering/comments/1nzjh13/i_built_an_open_source_ai_data_layer/",
    "score": 8,
    "num_comments": 9,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nzwm5s",
    "title": "SSIS on databricks",
    "content": "I have few data pipelines that creates csv files ( in blob or azure file share ) in data factory using azure SSIS IR .\n\nOne of my project is moving to databricks instead of SQl Server .\nI was wondering if I also need to rewrite those scripts or if there is a way somehow to run them over databrick\n\n",
    "author": "Upper_Pair",
    "timestamp": "2025-10-06T14:50:53",
    "url": "https://reddit.com/r/dataengineering/comments/1nzwm5s/ssis_on_databricks/",
    "score": 0,
    "num_comments": 40,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nzelaz",
    "title": "Optimizing Large-Scale Data Inserts into PostgreSQL: What’s Worked for You?",
    "content": "When working with PostgreSQL at scale, efficiently inserting millions of rows can be surprisingly tricky. I’m curious about what strategies data engineers have used to speed up bulk inserts or reduce locking/contention issues. Did you rely on `COPY` versus batched `INSERT`s, use partitioned tables, tweak `work_mem` or `maintenance_work_mem`, or implement custom batching in Python/ETL scripts?\n\nIf possible, share concrete numbers: dataset size, batch size, insert throughput (rows/sec), and any noticeable impact on downstream queries or table bloat. Also, did you run into trade-offs, like memory usage versus insert speed, or transaction management versus parallelism?\n\nI’m hoping to gather real-world insights that go beyond theory and show what truly scales in production PostgreSQL environments.",
    "author": "AliAliyev100",
    "timestamp": "2025-10-06T02:41:53",
    "url": "https://reddit.com/r/dataengineering/comments/1nzelaz/optimizing_largescale_data_inserts_into/",
    "score": 17,
    "num_comments": 23,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nzdz97",
    "title": "A simple Python code to build your own AI agent - text to SQL example",
    "content": "For anyone wanting to learn more about AI engineering, I wrote this article on how to build your own AI agent with Python.  \nIt shares a 200-line simple Python script to build an conversational analytics agent on BigQuery, with simple pre-prompt, context and tools. The full code is available on my Git repo if you want to start working on it",
    "author": "clr0101",
    "timestamp": "2025-10-06T02:01:06",
    "url": "https://reddit.com/r/dataengineering/comments/1nzdz97/a_simple_python_code_to_build_your_own_ai_agent/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.61,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nz6k8f",
    "title": "Informatica +snowflake +dbt",
    "content": "Hello\n\nOur current tech stack is azure and snowflake . We are onboarding informatica in an attempt to modernize our data architecture. Our initial plan is to use informatica for ingestion and transformation through medallion so we can use cdgc, data lineage, data quality and profiling but as we went through the initial development we recognized the best apporach is to use informatica for ingestion and for transformations use snowflake sp. \n\nBut I think using using a proven tool like DBT will be help better with data quality and data lineage. With new features like canvas and copilot I feel we can make our development quicker and most robust with git integrations.\n\nDoes informatica integrate well with DBt? Can we kick of DBT loads from informatica after ingesting the data? Is it DBT better or should we need to stick with snowflake sps? \n\n--------------------UPDATE--------------------------\n\n\nWhen I say Informatica, I am talking about Informatica CLOUD, not legacy PowerCenter. Business like to onboard Informatica as it comes with a suite with features like Data Ingestions, profiling, data quality , data governance etc. ",
    "author": "Libertalia_rajiv",
    "timestamp": "2025-10-05T18:54:15",
    "url": "https://reddit.com/r/dataengineering/comments/1nz6k8f/informatica_snowflake_dbt/",
    "score": 17,
    "num_comments": 57,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nyujeq",
    "title": "What Advice can you give to 0-2 Years Exp Data Engineer",
    "content": "Hello Folks,\n\nI am A Talend Data Engineer focusing on ETL pipelines , making Lift/shift - Pipelines using Talend Studio and Talend Cloud Setup.\nHow ever ETL is a broad Career but i dont know what to pivot on in my next career, I don't just want to build only pipelines. What other things i can explore which will also give monetary returns.\n",
    "author": "Chi3ee",
    "timestamp": "2025-10-05T10:35:17",
    "url": "https://reddit.com/r/dataengineering/comments/1nyujeq/what_advice_can_you_give_to_02_years_exp_data/",
    "score": 66,
    "num_comments": 43,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nyvel2",
    "title": "How many data pipelines does your company have?",
    "content": "I was asked this question by my manager and I had no idea how to answer. I just know we have a lot of pipelines, but I’m not even sure how many of them are actually functional.\n\nIs this the kind of question you’re able to answer in your company? Do you have visibility over all your pipelines, or do you use any kind of solution/tooling for data pipeline governance?",
    "author": "m1fc",
    "timestamp": "2025-10-05T11:07:12",
    "url": "https://reddit.com/r/dataengineering/comments/1nyvel2/how_many_data_pipelines_does_your_company_have/",
    "score": 38,
    "num_comments": 40,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nz6clt",
    "title": "Python Object query engine",
    "content": "Hi all, about a year ago I was hit with a task to align 500k file movements (src, dest,  timestamp) in a csv file and track a file through folders. Pandas made this less than optimal to query fast and still took a fair amount of time to build the flow tree.\n\nMany months of engineering later, I released PyThermite, a fully in memory query engine that indexed pure python objects, not dataframes or arbitrary data proxies. This also means that object attribute updates will automatically update the search index, eliminating the need for multi pass data creation.\n\nhttps://github.com/tylerrobbins5678/PyThermite\n\nPerformance appears be be absolutely destroying pandas and even polars in query performance. 6x -70x on 10M objects objects with a 19 part query. Index / dataframe build performance is significantly slower as expected, but thats the upfront cost with constant time lookup capability.\n\nWhat's everyone's thoughts on this? I am in the ETL space in my career and have always leaned more into the OOP concepts which are discarded in favor of row/col data. Is this a solution thats reusable or just only for those holding onto OOP hope?",
    "author": "Interesting-Frame190",
    "timestamp": "2025-10-05T18:43:49",
    "url": "https://reddit.com/r/dataengineering/comments/1nz6clt/python_object_query_engine/",
    "score": 3,
    "num_comments": 6,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nykijn",
    "title": "If you're a business owner, will you hire a data engineer and a data analyst?",
    "content": "Curious whether the community will have different opinion about their role, justification on hiring one and the need to build a data team.\n\nDo you think data role is only needed when the company has been large and quite digitalized?",
    "author": "ketopraktanjungduren",
    "timestamp": "2025-10-05T03:17:25",
    "url": "https://reddit.com/r/dataengineering/comments/1nykijn/if_youre_a_business_owner_will_you_hire_a_data/",
    "score": 42,
    "num_comments": 41,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nyx8t0",
    "title": "Is it common for a web app to trigger a data pipeline? Are there use case examples available?",
    "content": "So there is a text description to be provided by a web app user, to which I wish to find the most similar text in a table and bring up its id with the help of a LLM. Thus I believe a data pipeline should be triggered as soon as the user hits send and output the id for them.\nI'm also wondering whether this is the correct approach to look for similar text in database, I know about open search, but I need some smarts to identify the right text based on further instructions as well.",
    "author": "meet_me_at_seven",
    "timestamp": "2025-10-05T12:16:15",
    "url": "https://reddit.com/r/dataengineering/comments/1nyx8t0/is_it_common_for_a_web_app_to_trigger_a_data/",
    "score": 5,
    "num_comments": 9,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nyn26d",
    "title": "Conference talks",
    "content": "Hey, I've recently listened to some of the talks from the dbt conference Coalesce 2024 and found some of them inspiring. (https://youtube.com/playlist?list=PL0QYlrC86xQnWJ72sJlzDqPS0peE7j9Ed\n\nCan you recommend more freely available recordings of talks from conferences that deal with data engineering? Preferably from the last 2-3 years. ",
    "author": "Upbeat-Conquest-654",
    "timestamp": "2025-10-05T05:36:19",
    "url": "https://reddit.com/r/dataengineering/comments/1nyn26d/conference_talks/",
    "score": 10,
    "num_comments": 2,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nykvb6",
    "title": "Data mapping tools. Need help!",
    "content": "Hey guys. My team has been tasked with migrating on-prem ERP system to snowflake for client. \n\nThe source data is in total disaster. I'm talking at least 10 years of inconsistent data entry and bizarre schema choices.  We have many issues at hand like addresses combined in a text block, different date formats and weird column names that mean nothing.\n\nI think writing python scripts to map the data and fix all of this would take a lot of dev time. Should we opt for data mapping tools? Should also be able to apply conditional logic. Also, genAI be used for data cleaning (like address parsing) or would it be too risky for production?\n\nWhat would you recommend?",
    "author": "One-Builder-7807",
    "timestamp": "2025-10-05T03:38:14",
    "url": "https://reddit.com/r/dataengineering/comments/1nykvb6/data_mapping_tools_need_help/",
    "score": 14,
    "num_comments": 17,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nyv607",
    "title": "Streaming real time data into vector database",
    "content": "Hi Everyone. Curious to know anyone has tried streaming realtime data into vector database like pinecone, milvus, qdrsnt. or tried to integrate them as  with ETL pipelines as a data sink. Any specific use case. ",
    "author": "DistrictUnable3236",
    "timestamp": "2025-10-05T10:58:47",
    "url": "https://reddit.com/r/dataengineering/comments/1nyv607/streaming_real_time_data_into_vector_database/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nyn9mm",
    "title": "Advice on Picking a Product Architecture Playbook",
    "content": "\nI work on a data and analytics team in ~300 person org, at a major company that handles, let’s say, a critical back office business function. The org is undergoing a technical up-skill transformation. In yesteryear, business users came to us for dashboards, any ETL needed to power them and basic automation, maybe setting up API clients… so nothing terribly complex. Now the org is going to hire dozens of technical folks who will need to do this kind of thing on their own, and my own team must also transition, for our survival, to being the providers of a central repository for data, customized modules, maybe APIs, etc. \n\nFor context, my team’s technical level is on average mid level, we certainly aren’t Sr SWEs, but we are excited about this opportunity and have a high capacity to learn. And fortunately, we have access to a wide range of technology.  Mainly what would hold us back is our own limited vision and time. \n\nSo, I think we need to find and follow a playbook for what kind of architecture to learn about and go build, and I’m looking for suggestions on what that might be. TIA!",
    "author": "gaokai85",
    "timestamp": "2025-10-05T05:45:52",
    "url": "https://reddit.com/r/dataengineering/comments/1nyn9mm/advice_on_picking_a_product_architecture_playbook/",
    "score": 6,
    "num_comments": 6,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nykvos",
    "title": "Find the best solution for the storage issue",
    "content": "I am looking to design a data pipeline that handles both structured and unstructured data. By unstructured data, I mean types like images, voice, and text. For storage, I need the best tools that allow me to develop on my own S3 setup. I’ve come across different tools such as LakeFS (free version), Delta Lake, DVC, and Hudi, but I’m struggling to find the best solution because the requirements I have are specific:\n\n1. The tool must be fully open-source.\n2. It should support multi-user environments, Single Sign-On (SSO), and versioning.\n3. It must include a rollback option.\n\nGiven these requirements, what would be the best solution?",
    "author": "Helpful_Ad_982",
    "timestamp": "2025-10-05T03:38:53",
    "url": "https://reddit.com/r/dataengineering/comments/1nykvos/find_the_best_solution_for_the_storage_issue/",
    "score": 5,
    "num_comments": 5,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nyixbo",
    "title": "Polymo: declarative API ingestion for pyspark",
    "content": "API ingestion with pyspark currently sucks. Thats why I created Polymo, an open source library for Pyspark that adds a declarative layer on top of the custom data source reader. Just provide a yaml file and Polymo takes care of all the technical details. It comes with a lightweight UI to create, test and validate your configuration. \n\nCheck it out here: https://dan1elt0m.github.io/polymo/\n\nFeedback is very welcome! ",
    "author": "Rozijntjes",
    "timestamp": "2025-10-05T01:37:28",
    "url": "https://reddit.com/r/dataengineering/comments/1nyixbo/polymo_declarative_api_ingestion_for_pyspark/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nz4ooa",
    "title": "Would small data teams benefit from an all-in-one pipeline tool?",
    "content": "When I look at the modern data stack, it feels overly complex. There are separate tools for each part of the data engineering process, which seems unnecessarily complicated and not ideal for small teams.\n\nWould anyone benefit from a simple tool that handles raw extracts, allows transformations in SQL, and lets you add data tests at any step in the process—all with a workflow engine that manages the flow end to end?\n\nI spent the last few years building a tool that does exactly this. It's not perfect, but the main purpose is to help small data teams get started quickly by automating repetitive pieces of the data pipeline process, so they can focus on complex data integration work that needs more attention.\n\nI'm thinking about open sourcing it. Since data engineers really like to tinker, I figure the ability to modify any generated SQL at each step would be important. The tool is currently opinionated about using best practices for loading data (always use a work table in Redshift/Snowflake, BCP for SQL Server, defaulting to audit columns for every load, etc.).\n\nWould this be useful to anyone else?",
    "author": "CombinationFlaky3441",
    "timestamp": "2025-10-05T17:24:12",
    "url": "https://reddit.com/r/dataengineering/comments/1nz4ooa/would_small_data_teams_benefit_from_an_allinone/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nxshi2",
    "title": "How to deal with messy database?",
    "content": "Hi everyone, during my internship in a health institute, my main task was to clean up and document medical databases so they could later be used for clinical studies (using DBT and related tools).\n\nThe problem was that the databases I worked with were really messy, they came directly from hospital software systems. There was basically no documentation at all, and the schema was a mess, moreover, the database was huge, thousands of fields and hundred of tables.\n\n Here are some examples of bad design: \n\n* No foreign keys defined between tables that clearly had relationships.\n* Some tables had a column that just stored the *name* of another table to indicate a link (instead of a proper relation).\n* Other tables existed in total isolation, but were obviously meant to be connected.\n\nTo deal with it, I literally had to spend my weeks opening each table, looking at the data, and trying to guess its purpose, then writing comments and documentation as I went along.\n\nSo my questions are:\n\n* Is this kind of challenge (analyzing and documenting undocumented databases) something you often encounter in data engineering / data science work?\n* If you’ve faced this situation before, how did you approach it? Did you have strategies or tools that made the process more efficient than just manual exploration?",
    "author": "Which-Breadfruit-926",
    "timestamp": "2025-10-04T05:19:54",
    "url": "https://reddit.com/r/dataengineering/comments/1nxshi2/how_to_deal_with_messy_database/",
    "score": 69,
    "num_comments": 55,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ny7fwb",
    "title": "Workflow help/examples?",
    "content": "Hello,\n\nFor context I’m entirely self taught data engineer with a focus in Business intelligence and data warehousing, almost exclusively on the Microsoft stack. Current stack is SSIS, Azure SQL MI, and Power BI, and the team uses ADO for stories.  I’m aware of tools like git, and processes like version control and CICD, but I don’t know how to weave it all together and actually develop with these things in mind.  I’ve tried unsuccessfully to get ssis solutions and sql database projects into version control in a sustainable way. I’d also like to be able to publish release notes to users and stakeholders. \n\nSo the question is, what does a development workflow that touches all these bases look like? Any suggestions would help, I know there’s not an easy answer and I’m willing to learn. \n\n",
    "author": "WorkRelatedRedditor",
    "timestamp": "2025-10-04T15:25:27",
    "url": "https://reddit.com/r/dataengineering/comments/1ny7fwb/workflow_helpexamples/",
    "score": 6,
    "num_comments": 7,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ny34j6",
    "title": "How is Snowflake managing their COS storage cost?",
    "content": "I am doing a technical research on Storage for Data Warehouses. I was confused on how snowflake manages to provide a flat rate ($23/TB/month) for [storage](https://www.snowflake.com/en/pricing-options/)?  \nI know [COS API calls ](https://aws.amazon.com/s3/pricing/?nc=sn&amp;loc=4#:~:text=Requests%20%26%20data%20retrievals)(GET,SELECT PUT, LIST...) cost a lot especially for smaller file sizes. So how is snowflake able to abstract these API charges and give a flat rate to customer? (Or are there hidden terms and conditions?)  \n\n\nAdditionally, does Snowflake charge for Data transfer from Customer's storage to SF storage or are they billed separately by the COS provider?(S3,Blobe...)  \n",
    "author": "Express_Lock_6631",
    "timestamp": "2025-10-04T12:29:22",
    "url": "https://reddit.com/r/dataengineering/comments/1ny34j6/how_is_snowflake_managing_their_cos_storage_cost/",
    "score": 9,
    "num_comments": 16,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nxxj0q",
    "title": "First time doing an integration (API to ERP). Any tips from veterans?",
    "content": "Hey guys,\n\nI have experience with automating reading data from APIs for the purpose of reporting. But now I’ve been tasked with pushing data from an API into our ERP. \n\nWhile it seems ‘much the same’, to me it’s a lot more daunting as now I’m creating official documents so much more at stake. The data only has to be updated daily from the 3rd party to our ERP. It involves posting purchase orders. \n\nIn general, any tips that might help? I’ve accounted for:\n\n- Logging of success/failure to db\n-detailed logger in the python script\n-checking for updates/vs new records. \n\nIt’s all running on a VM, Python for the script and just plain old task scheduler. \n\nAny help would be greatly appreciated. \n",
    "author": "thenumbers_dontaddup",
    "timestamp": "2025-10-04T08:50:12",
    "url": "https://reddit.com/r/dataengineering/comments/1nxxj0q/first_time_doing_an_integration_api_to_erp_any/",
    "score": 14,
    "num_comments": 11,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ny3ats",
    "title": "DAMA DMBOK in ePub format",
    "content": "I already purchased at DAMA de pdf version of the DMBOK, but it is almost impossible to read on a small screen, looking for an ePub version, even if I have to purchase it again, thanks",
    "author": "br_web",
    "timestamp": "2025-10-04T12:36:09",
    "url": "https://reddit.com/r/dataengineering/comments/1ny3ats/dama_dmbok_in_epub_format/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nyk5dm",
    "title": "Do immigrants with foreign (third-world) degrees face disadvantages in the U.S. tech job market?",
    "content": "I’m moving to the U.S. in January 2026 as a green card holder from Nepal. I have an engineering degree from a Nepali university and several years of experience in data engineering and analytics. The companies I’ve worked for in Nepal were offshore teams for large Australian and American firms, so I’ve been following global tech standards.\n\nWill having a foreign (third-world) degree like mine put me at a disadvantage when applying for tech jobs in the U.S., or do employers mainly value skills and experience?",
    "author": "No-Zookeepergame198",
    "timestamp": "2025-10-05T02:54:47",
    "url": "https://reddit.com/r/dataengineering/comments/1nyk5dm/do_immigrants_with_foreign_thirdworld_degrees/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ny091v",
    "title": "best practices for storing data from on premise server to cloud storage",
    "content": "Hello,  \n  \nI would like to discuss the industry standard/best practices for extracting daily data from an on-premise OLTP database like PostgreSQL or DB2 and storing the data in cloud storage systems like Amazon S3 or Google Cloud Storage.\n\nI have a few questions since I am quite a newbie in data engineering:\n\n1. Would I extract files from the database through custom scripts (Python, shell) which access the production database and copy data to a dedicated file system?\n2. Would the file system be on the same server as the database or on a separate server?\n3. Is it better to extract the data from a replica or would it also be acceptable to access the production database?\n4. How do I connect an on-premise server with cloud storage?\n5. How do I transfer the extracted data that is now on the file system to cloud storage? Again custom scripts?\n6. What about tools like Fivetran and Airbyte?",
    "author": "greywind1903",
    "timestamp": "2025-10-04T10:37:25",
    "url": "https://reddit.com/r/dataengineering/comments/1ny091v/best_practices_for_storing_data_from_on_premise/",
    "score": 4,
    "num_comments": 8,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ny1e63",
    "title": "MySQL + Excel Automation: IDEs or Tools with Complex Export Scripting?",
    "content": "I'm looking for recommendations on a MySQL IDE, editor, or client that can both execute SQL queries and automate interactions with Excel. My ideal solution would include a robust data export wizard that supports complex, code-based instructions or scripting. I need to efficiently run queries, then automatically export, sync, or transform the results in Excel for use in reports or workflow automation.\n\nDoes anyone have experience with tools or workflows that work well for this, especially when advanced automation or customization is required? Any suggestions, features to look for, or sample workflow/code examples would be greatly appreciated!",
    "author": "Training_Ad6701",
    "timestamp": "2025-10-04T11:21:33",
    "url": "https://reddit.com/r/dataengineering/comments/1ny1e63/mysql_excel_automation_ides_or_tools_with_complex/",
    "score": 2,
    "num_comments": 6,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nxp7t3",
    "title": "What do we think about this post - \"Why AI will fail without engineering principles?\"",
    "content": "So, in todays market, the message here seems a bit old hat. However; this was written only 2 months ago.\n\nIt's from a vendor, so \\*obviously\\* it's biased.  But the arguments are well written, and it's slightly just a massive list of tech without actually addressing the problem, but interesting nontheless.\n\nTLDR: Is promoting good engineering a dead end these days?\n\n[https://archive.ph/P02wz](https://archive.ph/P02wz)",
    "author": "codek1",
    "timestamp": "2025-10-04T02:11:47",
    "url": "https://reddit.com/r/dataengineering/comments/1nxp7t3/what_do_we_think_about_this_post_why_ai_will_fail/",
    "score": 8,
    "num_comments": 7,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nxs2h8",
    "title": "Delhi Snowflake Meetup",
    "content": "Hello everyone, \nI am organising is snowflake meet up in Delhi, India.\nWe will discuss genAI with snowflake. There will be free lunch and snacks along with a Snowflake branded gift.\nIt is an official event of snowflake. \nEven if you are a college student, \nBeginner in data engineering, or an expert in it.\nDetails:\nOctober 11, 9:30 IST.\nVenue details will be shared after registration. DM me for link.",
    "author": "My_name_is_Ayan",
    "timestamp": "2025-10-04T04:59:25",
    "url": "https://reddit.com/r/dataengineering/comments/1nxs2h8/delhi_snowflake_meetup/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nx25u5",
    "title": "Best GUI-based Cloud ETL/ELT",
    "content": "I work in a shop where we used to build data warehouses with Informatica PowerCenter.  We moved to a cloud stack years back and implemented these complex transformations into Scala in Databricks although we have been doing more and more Pyspark.  Over time, we've had issues deploying new gold-tier models in our medallion architecture.  Whenever there are highly complex transformations, it takes us a lot longer to develop and deploy. Data quality is lower. Even with lineage graphs, we cannot answer quickly and well for complex derivations if someone asks how we came up with a value in a field.  Nothing we do on our new stack compared to the speed and quality when we used to have a good GUI-based ETL tool.  Basically myself and 1 other team member could build data warehouses quickly and after moving to the cloud, we have tons of engineers and it takes longer with worse results.\n\nWhat we are considering now is to continue using Databricks for ingest and maybe bronze/silver layers and when building gold layer models with complex transformations, we use a GUI and cloud-based ETL/ELT solution.  We want something like the old PowerCenter.  Matillion was mentioned.  Also, Informatica has a cloud solution.\n\nAny advice?  What is the best GUI-based tool for ETL/ELT with the most advanced transformations available like what PowerCenter used to have with expression tranformations, aggregations, filtering, complex functions, etc.\n\nWe don't care about interfaces because data will already be in the data lake.  The focus is specifically on very complex transformations and complex business rules and building gold models from silver data.",
    "author": "PepperAffectionate25",
    "timestamp": "2025-10-03T08:24:33",
    "url": "https://reddit.com/r/dataengineering/comments/1nx25u5/best_guibased_cloud_etlelt/",
    "score": 29,
    "num_comments": 52,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nx7hhq",
    "title": "Lightweight Data Quality Testing Framework (dq_tester)",
    "content": "I put together a simple Python framework for writing lightweight data quality tests. It’s intended to be easy to plug into existing pipelines, and lets you define reusable checks on your database or csv files using sql.  \n\nIt’s meant for cases where you don't want the overhead of larger frameworks and just want to configure some basic testing in your pipeline. I've also included example prompt instructions in case you want to configure your tests in a project in claude.\n\nRepo: [https://github.com/koddachad/dq\\_tester](https://github.com/koddachad/dq_tester)",
    "author": "CombinationFlaky3441",
    "timestamp": "2025-10-03T11:42:12",
    "url": "https://reddit.com/r/dataengineering/comments/1nx7hhq/lightweight_data_quality_testing_framework_dq/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nx5fx1",
    "title": "Quick Q: How are you all using Fivetran History Mode",
    "content": "I’m fairly new to the data engineering/analytics space. Anyone here using Fivetran’s [History Mode](https://fivetran.com/docs/core-concepts/sync-modes/history-mode#historymode)? From what I can tell it’s kinda like SCD Type 1, but not sure if that’s exactly right. Curious how folks are actually using it in practice and if there are any gotchas downstream.",
    "author": "OrganizationSea8705",
    "timestamp": "2025-10-03T10:25:36",
    "url": "https://reddit.com/r/dataengineering/comments/1nx5fx1/quick_q_how_are_you_all_using_fivetran_history/",
    "score": 9,
    "num_comments": 5,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nws1e8",
    "title": "Replace Data Factory with python?",
    "content": "I have used both Azure Data Factory and Fabric Data Factory (two different but very similar products) and I don't like the visual language. I would prefer 100% python but can't deny that all the connectors to source systems in Data Factory is a strong point. \n\nWhat's your experience doing ingestions in python? Where do you host the code? What are you using to schedule it? \n\nAny particular python package that can read from all/most of the source systems or is it on a case by case basis?",
    "author": "loudandclear11",
    "timestamp": "2025-10-02T23:54:48",
    "url": "https://reddit.com/r/dataengineering/comments/1nws1e8/replace_data_factory_with_python/",
    "score": 47,
    "num_comments": 39,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nwp1wp",
    "title": "Explain Azure Data Engineering project in the real-life corporate world.",
    "content": "I'm trying to learn Azure Data Engineering. I've happened to go across some courses which taught Azure Data Factory (ADF), Databricks and Synapse. I learned about the Medallion Architecture ie,. Data from on-premises to bronze -&gt; silver -&gt; gold (delta). Finally the curated tables are exposed to Analysts via Synapse.\n\nThough I understand the working in individual tools, not sure how exactly work with all together, for example:  \nWhen to create pipelines, when to create multiple notebooks, how does the requirement come, how many delta tables need to be created as per the requirement, how do I attach delta tables to synapse, what kind of activities to perform in dev/testing/prod stages.\n\nThank you in advance.",
    "author": "Chan350",
    "timestamp": "2025-10-02T20:58:32",
    "url": "https://reddit.com/r/dataengineering/comments/1nwp1wp/explain_azure_data_engineering_project_in_the/",
    "score": 37,
    "num_comments": 12,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nwtxhz",
    "title": "Feedback on self learning / project work",
    "content": "Hi everyone,\n\nI'm from the UK and was recently made redundant after 6 years in the world of technical consulting for a software company. I've taken the few months since to take up learning python, then data manipulation into data engineering. \n\nI've done a project that I would love some feedback on. I know it is bare bones and not at a high level but it is on what I have learnt and picked up so far. The project link is here: [https://github.com/Griff-Kyal/Data-Engineering/tree/main/nyc-tlc-pipeline](https://github.com/Griff-Kyal/Data-Engineering/tree/main/nyc-tlc-pipeline) . I'd love to know what to learn / implement for my next project to get it at a level which would get recognised by potential employee.\n\nAlso, since I don't have a qualification in the field, I have been looking into the 'Microsoft Certified: Fabric Data Engineer Associate' course and wondered if its something I should look at doing to boost my CV/ potential hire-ability ?\n\n  \nThanks for taking the time and i appreciate all and any feedback",
    "author": "Kyal2k",
    "timestamp": "2025-10-03T01:58:14",
    "url": "https://reddit.com/r/dataengineering/comments/1nwtxhz/feedback_on_self_learning_project_work/",
    "score": 7,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nw8twq",
    "title": "Landed a \"real\" DE job after a year as a glorified data wrangler - worried about future performance",
    "content": "Edit: Removing all of this just cus, but thank you to everyone who replied! I feel much better about the position after reading through everything. This community is awesome :)",
    "author": "echanuda",
    "timestamp": "2025-10-02T09:39:13",
    "url": "https://reddit.com/r/dataengineering/comments/1nw8twq/landed_a_real_de_job_after_a_year_as_a_glorified/",
    "score": 63,
    "num_comments": 17,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nwm4fg",
    "title": "Conversion to Fabric",
    "content": "Anyone’s company made a conversion from Snowflake/Databricks to Fabric?  Genuinely curious what the justification/selling point would be to make the change as they seem to all be extremely comparable overall (at best).  Our company is getting sold hard on Fabric but the feature set isn’t compelling enough (imo) to even consider it.  \n\nAlso would be curious if anyone has been on Fabric and switched over to one of the other platforms.  I know Fabric has had some issues and outages that may have influenced it, but if there were other reasons I’d be interested in learning more. \n\nNote: not intending this to be a bashing session on the platforms, more wanting to see if I’m missing some sort of differentiator between Fabric and the others!",
    "author": "dopedankfrfr",
    "timestamp": "2025-10-02T18:30:51",
    "url": "https://reddit.com/r/dataengineering/comments/1nwm4fg/conversion_to_fabric/",
    "score": 10,
    "num_comments": 19,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nwbq4r",
    "title": "How do you test ETL pipelines?",
    "content": "The title, how does ETL pipeline testing work? Do you have ONE script prepared for both prod/dev modes?\n\nDo you write to different target tables depending on the mode?\n\nhow many iterations does it take for an ETL pipeline in development?\n\nHow many times do you guys test ETL pipelines?\n\nI know it's an open question, so don't be afraid to give broad or particular answers based on your particular knowledge and/or experience.\n\nAll answers are mega appreciated!!!!\n\nFor instance, I'm doing Postgresql source (40 tables) -&gt; S3 -&gt; transformation (all of those into OBT) -&gt; S3 -&gt; Oracle DB, and what I do to test this is:\n\n* extraction, transform and load: partition by run\\_date and run\\_ts\n* load: write to different tables based on mode (production, dev)\n* all three scripts (E, T, L) write quite a bit of metadata to \\_audit.\n\nAnything you guys can add, either broad or specific, or point me to resources that are either broad or specific, is appreciated. Keep the GPT garbage to yourself.\n\nCheers\n\n  \nEdit Oct 3: I cannot stress enough how appreciated I am to see the responses. People sitting down to help or share expecting nothing in return. Thank you all.",
    "author": "escarbadiente",
    "timestamp": "2025-10-02T11:25:39",
    "url": "https://reddit.com/r/dataengineering/comments/1nwbq4r/how_do_you_test_etl_pipelines/",
    "score": 43,
    "num_comments": 31,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nwauwr",
    "title": "Beginning the Job Hunt",
    "content": "Hey all, glad to be a part of the community. I have spent the last 6 months - 1 year studying data engineering through various channels (Codecademy, docs, Claude, etc.) mostly self-paced and self-taught. I have designed a few ETL/ELT pipelines and feel like I'm ready to seek work as a junior data engineer. I'm currently polishing up the ole LinkedIn and CV, hoping to start job hunting this next week. I would love any advice or stories from established DEs on their personal journeys. \n\nI would also love any and all feedback on my stock market analytics pipeline. [www.github.com/tmoore-prog/stock\\_market\\_pipeline](http://www.github.com/tmoore-prog/stock_market_pipeline) \n\nLooking forward to being a part of the community discussions!\n\n",
    "author": "Efficient_Arrival_83",
    "timestamp": "2025-10-02T10:53:33",
    "url": "https://reddit.com/r/dataengineering/comments/1nwauwr/beginning_the_job_hunt/",
    "score": 30,
    "num_comments": 16,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nwp2l2",
    "title": "Rough DE day",
    "content": "It wasn’t actually that bad. But I spent all day working a vendor Oracle view that my org has heavily modified. It’s slow, unless you ditch 40/180 columns. It’s got at least one source of unintended non-determinism, which makes concrete forensics more than a few steps away. It’s got a few bad sub-query columns (meaning the whole select fails if one of these bad records is in the mix). A bit over 1M rows. Did I mention it’s slow? Takes 10 seconds just to get a count.  This database is our production enterprise datawarehouse RAC environment, 5 DBAs on staff, which should tell you how twisted this view is. Anyway, just means things will take longer, Saul Goodman… I bet a few out there can relate. Tomorrows Friday!",
    "author": "Strange_Bru0101",
    "timestamp": "2025-10-02T20:59:36",
    "url": "https://reddit.com/r/dataengineering/comments/1nwp2l2/rough_de_day/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nwu2oe",
    "title": "Need advice on career progression while juggling uni, moving to germany, wanting to to possobly start contract work/startup",
    "content": "Background:\n\nI’ve been working as a Data Engineer for about 3.5 years, mainly on data migrations and warehouse engineering for analytics.\n\nEven though I’m still technically a junior, for the last couple of years I’ve worked on fairly big projects with a lot of responsibility, often figuring things out on my own and delivering without much help.\n\nI’m on £40k and recently started doing a degree alongside work. I’m in a decent position to move up.\n\nThe company is big but my team is small (1 manager, 1 senior, 2 juniors). It’s generally a good place to work, though promotions and recognition are quite slow — most people move internally to progress. As the other junior and senior are on a single project, I'm doing all others currently.\n\nI normally get bored after about a year in a job, but I’ve been here for 2 years and still enjoy most of the work despite a few frustrations.\n\nCurrent situation:\nMy girlfriend lives in Germany (we’ve been together for 4 years), and I want to move there. My current job doesn’t allow working abroad, so I’ll need to find something a way to make it happen. I do fortunately have EU citizenship\n\nI’ve had a few opportunites in Germany. Some looked promising but didn’t work out (e.g. they needed someone to start immediately, or misrepresented parts of the process). Overall, though, I seem to get decent interest.\n\nMain issue:\n\nA lot of roles in Germany require a degree (I’m working on one but don’t have it yet).\nMany jobs also want fluent German. Mine is still pretty basic, but I’m learning.\n\nI'm considering:\nEU contracting - I like the idea of doing different projects every 6–12 months while living in Germany. I haven’t looked properly into the legal/tax side yet, but it sounds like it could fit well.\n\nBuilding a product/startup-  I’ve built a very basic MVP that provides analytics (including some predictive analysis) for small–mid sized e-commerce companies. It’s early, but I think it could be developed into more of a template/solution to offer as a service potentially.\n\nCareer progression - I don’t want to stay as a junior any longer and its so low priority for the company currently. I want to keep build towards something bigger but feel like times not on my side\n \n\nI’m juggling a lot right now: work, uni, the product idea, and the thought of switching to contracting and moving abroad. I want to move things forward without getting stuck in the same place for too long or burning out trying to do everything at once.\n\nAny advice on \n\n* Moving to Germany as a data professional without fluent German\n* Whether EU contracting is a good stepping stone or just a distraction right now\n* If it’s smarter to build the product before or after relocating\n* General advice on avoiding career stagnation while juggling multiple priorities\n\n\nTL;DR:\n3.5 yrs as a Data Engineer, junior title, £40k, started a degree. Want to move to Germany (girlfriend), progress career, maybe try contracting or build a startup/product. Feels like a lot to juggle and I don’t want to get stuck. Looking for advice from people who’ve been through similar moves or decisions.\n\n",
    "author": "Material_Direction_1",
    "timestamp": "2025-10-03T02:07:14",
    "url": "https://reddit.com/r/dataengineering/comments/1nwu2oe/need_advice_on_career_progression_while_juggling/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.44,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nwfqaz",
    "title": "In response to F3, the new file format",
    "content": "",
    "author": "aleda145",
    "timestamp": "2025-10-02T13:53:57",
    "url": "https://reddit.com/r/dataengineering/comments/1nwfqaz/in_response_to_f3_the_new_file_format/",
    "score": 10,
    "num_comments": 3,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nwi7t3",
    "title": "Openmetadata &amp; GitSync",
    "content": "We’ve been exploring OpenMetadata for our data catalogs and are impressed by their many connector options. For our current testing set up, we have OM deployed using the helm chart that comes shipped with airflow. When trying to set up GitSync for DAGs, despite having dag_generated_config folder set separated for dynamic dags generated from OM, it is still trying to write them into the default location where the GitSync DAG would write into, and this would cause permission errors. Looking thru several posts in this forum, I’m aware that there should be a separate airflow for the pipeline. However, Im still wondering, if it’s still possible to have GitSync and dynamic dags from OM coexist. ",
    "author": "Linhphambuzz",
    "timestamp": "2025-10-02T15:32:16",
    "url": "https://reddit.com/r/dataengineering/comments/1nwi7t3/openmetadata_gitsync/",
    "score": 6,
    "num_comments": 1,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nvr141",
    "title": "This is one of the best free videos series of Mastering Databricks and Spark step by step",
    "content": "I came across this series by Bryan Cafferky on Databricks and Apache Spark, want to share with reddit community. \n\nHope people will find them useful and please spread the word:\n\nhttps://www.youtube.com/watch?v=JUObqnrChc8&amp;list=PL7_h0bRfL52qWoCcS18nXcT1s-5rSa1yp&amp;index=29",
    "author": "69odysseus",
    "timestamp": "2025-10-01T18:47:24",
    "url": "https://reddit.com/r/dataengineering/comments/1nvr141/this_is_one_of_the_best_free_videos_series_of/",
    "score": 221,
    "num_comments": 15,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nw7oi2",
    "title": "DBT project: Unnesting array column",
    "content": "I'm building a side project to get familiar with DBT, but I have some doubts about my project data layers. Currently, I'm fetching data from the YouTube API and storing it in a raw schema table in a Postgres database, with every column stored as a text field except for one. The exception is a column that stores an array of Wikipedia links describing the video.\n\nFor my staging models in DBT, I decided to assign proper data types to all fields and also split the topics column into its own table. However, after reading the DBT documentation and other resources, I noticed it's generally recommended to keep staging models as close to the source as possible.\n\nSo my question is: should I keep the array column unnested in staging and instead move the separation into my intermediate or semantic layer? That way, the topics table (a dimension basically) would exist there.",
    "author": "HanDw",
    "timestamp": "2025-10-02T08:56:34",
    "url": "https://reddit.com/r/dataengineering/comments/1nw7oi2/dbt_project_unnesting_array_column/",
    "score": 12,
    "num_comments": 14,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nwf67i",
    "title": "Continue as a tool based MDM Developer 3.5 YOE or Switch to core data engineering? Detailed post",
    "content": "I am writing this post so any other MDM developer in future gets clarity on where they are and where they need to go.\n\nCareer advice needed. I am a 3.5 years experienced Informatica MDM SaaS developer who specializes in all things related to MDM but on informatica cloud only.\n\nStrengths:\n- I would say I can very understand how MDM works.\n-  I have good knowledge on building MDM integrations for enterprise internal applications as well.\n- I can pick up a new tool within weeks and start developing MDM components (I got this chance only once in my career)\n- building pipelines to get data to MDM, export data from MDM\n- enable other systems in an enterprise to use MDM.\n- I am able to get good understanding of business requirements and think from MDM perspective to give pros and cons.\n\nWeaknesses:\n- Less exposure to different types of MDM implemtations\n- Less exposure to other aspects of data management like data governance\n- I can do data engineering stuff (ETL, Data Quality, Orchestration etc) only within informatica cloud environment \n- Lack of exposure to core data engineering components like data storage/data warehousing, standard AWS/Azure/GCP cloud platforms and file storage systems (used them only as source and targets from MDM perspective),  ETL pipelines using python-apache spark, orchestration tools like airflow. Never got a chance to create something with them.\n\nCrux of the matter (My question)-\n\nNow I am at a point in my career where I am not feeling confident with MDM as a career. I feel like I am lacking something when I m working. Coding is limited, my thinking is limited to the tool that is being used, I feel like I am playing a workaround simulator with the MDM tool. I am able to understand what is being done, what we are solving, and how we are helping business but I don't get more problem solving. \n\nShould I continue on this path?\nShould I prepare and change my career to data engineering?\n\n\nWhy data engineering?\n- Although MDM is a more specialised branch of data engineering but it is not exactly data engineering. \n- More career opportunities with data engineering\n- I feel I will get a sense of satisfaction after working as a data engineer when I solve more problems (grass is always greener on the other side)\n\nCan experienced folks give some suggestions?\n",
    "author": "dumb_cyka_2697",
    "timestamp": "2025-10-02T13:34:12",
    "url": "https://reddit.com/r/dataengineering/comments/1nwf67i/continue_as_a_tool_based_mdm_developer_35_yoe_or/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nwsux4",
    "title": "A new solution for trading off between rigid schemas and schemaless mess",
    "content": "I always remember that the DBA team slows me down from applying DDLs to alter columns. When I switch to NoSQL databases that require no schema, however, I often forget what I had stored later.\n\nMany data teams face the same painful choice: rigid schemas that break when business requirements evolve, or schemaless approaches that turn your data lake into a swamp of unknown structures.\n\nAt ScopeDB, we deliver a full-featured, flexible schema solution to support you in evolving your data schema alongside your business, without any downtime. We call it \"Schema On The Fly\":\n\n* Gradual Typing System: Fixed columns for predictable data, variant object columns for everything else. Get structure where you need it, flexibility where you don't.\n\n* Online Schema Evolution: Add indexes on nested fields online. Factor out frequently-used paths to dedicated columns. Zero downtime, zero migrations.\n\n* Schema On Write: Transform raw events during ingestion with ScopeQL rules. Extract fixed fields, apply filters, and version your transformation logic alongside your application code. No separate ETL needed.\n\n* Schema On Read: Use bracket notation to explore nested data. Our variant type system means you can query any structure efficiently, even if it wasn't planned for.\n\nRead how we're making data schemas work for developers, not against them.",
    "author": "tison1096",
    "timestamp": "2025-10-03T00:48:11",
    "url": "https://reddit.com/r/dataengineering/comments/1nwsux4/a_new_solution_for_trading_off_between_rigid/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nvwbcr",
    "title": "ELI5: what is CDC and how is it different?",
    "content": "Could someone please explain what CDC is exactly?\n\nIs it a set of tools, a methodology, a design pattern? How does it differ from microbatches based on timestamps or event streaming?\n\nThanks!",
    "author": "dadadawe",
    "timestamp": "2025-10-01T23:30:02",
    "url": "https://reddit.com/r/dataengineering/comments/1nvwbcr/eli5_what_is_cdc_and_how_is_it_different/",
    "score": 25,
    "num_comments": 21,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nvnxla",
    "title": "How to convince my team to stop using conda as an environment manager",
    "content": "Does anyone actually use conda anymore? We aren’t in college anymore",
    "author": "N3Flip",
    "timestamp": "2025-10-01T16:24:00",
    "url": "https://reddit.com/r/dataengineering/comments/1nvnxla/how_to_convince_my_team_to_stop_using_conda_as_an/",
    "score": 81,
    "num_comments": 80,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nvi2g7",
    "title": "Career path for a mid-level, mediocre DE?",
    "content": "As the title says, I consider myself a mediocre DE. I am self taught. Started 7 years ago as a data analyst.\n\nOver the years I’ve come to accept that I won’t be able to churn out pipelines the way my peers do. My team can code circles around me.\n\nHowever, I’m often praised for my communication and business understanding by management and stakeholders.\n\nSo what is a good career path in this space that is still technical in nature but allows you to flex non-technical skills as well?\n\nI worry about hitting a ceiling and getting stuck if I don’t make a strategic move in the next 3-5 years.\n\n\nEDIT: Thank you everyone for the feedback! Your replies have given me a lot to think about. ",
    "author": "nostalgicwander",
    "timestamp": "2025-10-01T12:33:06",
    "url": "https://reddit.com/r/dataengineering/comments/1nvi2g7/career_path_for_a_midlevel_mediocre_de/",
    "score": 119,
    "num_comments": 33,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nveze1",
    "title": "Why Spark and many other tools when SQL can do the work ?",
    "content": "I have worked in multiple enterprise level data projects where Advanced SQL in Snowflake can handle all the transformations on available data. \n\nI haven't worked on Spark. \n\nBut I wonder why would Spark and other tools be required such as Airflow, DBT, when SQL(in Snowflake) itself is so powerful to handle complex data transformations.\n\nCan someone help me understand on this part ?\n\nThanks you!\n\nGlad to be part of such an amazing community.",
    "author": "Flashy_Scarcity777",
    "timestamp": "2025-10-01T10:41:52",
    "url": "https://reddit.com/r/dataengineering/comments/1nveze1/why_spark_and_many_other_tools_when_sql_can_do/",
    "score": 157,
    "num_comments": 103,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nvprhr",
    "title": "A deep dive into backfilling data with Kafka and S3",
    "content": "",
    "author": "nejcko",
    "timestamp": "2025-10-01T17:48:14",
    "url": "https://reddit.com/r/dataengineering/comments/1nvprhr/a_deep_dive_into_backfilling_data_with_kafka_and/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nvialm",
    "title": "Git branching with dbt... moving from stage/uat environment to prod?",
    "content": "So, we have multiple dbt projects at my employer, one which has three environments (dev, stage and prod). The issue we're having is merging from the staging env to prod. For reference, in most of our other projects, we simply have dev and prod. Every branch gets tested and reviewed in a PR (we also have a CI environment and job that runs and checks to make sure nothing will break in Prod from changes being implemented) and then merged into a main branch, which is Production.   \n  \nA couple months back we implemented \"stage\" or a UAT environment for one of primary/largest dbt projects. The environment works fine the issue is that in git, once a developer's PR is reviewed and approved in dev and their code is merged into stage, it gets merged into a single stage branch in git.  \n  \nThis is somewhat problematic since we'll typically end up with a backlog of changes over time which all need to go to Prod, but not all changes are tested/UAT'd at the same time.   \nSo, you end up having some changes that are ready for prod while others are awaiting UAT review.   \nSince all changes in stage exist in a single branch, anything that was merged from dev to stage has to go to Prod all at once.   \nI've been trying to figure out if there's a way to \"cherry pick\" a handful of commits in the stage branch and merge only those to prod in a PR. A colleague suggested using git releases to do this functionality but that doesn't seem to be (based on videos I've watched) what we need.  \n  \nHow are people handling this type of functionality? Once your changes go to your stage/uat environment do you have a way of handling merging individual commits to production?    \n",
    "author": "SeaYouLaterAllig8tor",
    "timestamp": "2025-10-01T12:41:52",
    "url": "https://reddit.com/r/dataengineering/comments/1nvialm/git_branching_with_dbt_moving_from_stageuat/",
    "score": 15,
    "num_comments": 15,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nvk6sa",
    "title": "Iceberg x power bi",
    "content": "Hi all,\n\nI am currently building a data platform where the storage is based on Iceberg in a MinIO bucket. I am looking for advice on connecting Power BI (I have no choice regarding the solution) to my data.\n\nI saw that there is a Trino Power BI extension, but it is not compatible with Power BI Report Server. Do you have any other alternatives to suggest? One option would be to expose my datamarts in Postgres, but if I can centralize everything in Iceberg, that would be better.\n\nThank you for your help.",
    "author": "GeneralFlight2313",
    "timestamp": "2025-10-01T13:52:25",
    "url": "https://reddit.com/r/dataengineering/comments/1nvk6sa/iceberg_x_power_bi/",
    "score": 6,
    "num_comments": 6,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nvir7y",
    "title": "Any recommendations on sources for learning clean code to work with python in airflow? Use cases maybe?",
    "content": "I mean writing good DAGs and specially handling errors",
    "author": "meet_me_at_seven",
    "timestamp": "2025-10-01T12:59:34",
    "url": "https://reddit.com/r/dataengineering/comments/1nvir7y/any_recommendations_on_sources_for_learning_clean/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nvx64e",
    "title": "Palantir Foundry Devs - what's our future?",
    "content": "Hey guys! I've been working as a DE and AE on Foundry for the past year, got certified as DE, and now picking up another job closer to App Dev, also Foundry. \n\nAnybody wondering what's the future looking like for devs working on Foundry? Do you think the demand for us will keep rising (considering how hard it is to even start working on the platform without having a rich enough client first)? Is Foundry as a platform going to continue prospering? Is this the niche to be in for the next 5-10 years? ",
    "author": "ColumbRoff",
    "timestamp": "2025-10-02T00:23:42",
    "url": "https://reddit.com/r/dataengineering/comments/1nvx64e/palantir_foundry_devs_whats_our_future/",
    "score": 0,
    "num_comments": 22,
    "upvote_ratio": 0.47,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nulrd5",
    "title": "The Great Consolidation is underway",
    "content": "Finding these moves interesting. Seems like maybe a sign that the data engineering market isn't that big after all?",
    "author": "full_arc",
    "timestamp": "2025-09-30T11:53:42",
    "url": "https://reddit.com/r/dataengineering/comments/1nulrd5/the_great_consolidation_is_underway/",
    "score": 411,
    "num_comments": 42,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nucisl",
    "title": "“Achievement”",
    "content": "",
    "author": "Background_Artist801",
    "timestamp": "2025-09-30T05:59:10",
    "url": "https://reddit.com/r/dataengineering/comments/1nucisl/achievement/",
    "score": 1196,
    "num_comments": 32,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nuz48r",
    "title": "Data Rage",
    "content": "We need a flair for just raging into the sky.  I am getting historic data from Oracle to a unity catalog table in Databricks.  A column has hours.  So I'm expecting the values to be between 0 and 23.  Why the fuck are there hours with 24 and 25!?!?! 🤬🤬🤬",
    "author": "RobotechRicky",
    "timestamp": "2025-09-30T21:43:55",
    "url": "https://reddit.com/r/dataengineering/comments/1nuz48r/data_rage/",
    "score": 64,
    "num_comments": 20,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nupeh2",
    "title": "Could Senior Data Engineers share examples of projects on GitHub?",
    "content": "Hi everyone !\n\nI’m a semi senior DE and currently building some personal projects to keep improving my skills. It would really help me to see how more experienced engineers approach their projects — how they structure them, what tools they use, and the overall thinking behind the architecture.\n\nI’d love to check out some Senior Data Engineers’ GitHub repos (or any public projects you’ve got) to learn from real-world examples and compare with what I’ve been doing myself.\n\nWhat I’m most interested in:\n\n* How you structure your projects\n* How you build and document ETL/ELT pipelines\n* What tools/tech stack you go with (and why)\n\nThis is just for learning , and I think it could also be useful for others at a similar level.\n\nThanks a lot to anyone who shares !",
    "author": "gbj784",
    "timestamp": "2025-09-30T14:11:42",
    "url": "https://reddit.com/r/dataengineering/comments/1nupeh2/could_senior_data_engineers_share_examples_of/",
    "score": 196,
    "num_comments": 48,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nvaqj8",
    "title": "Text based search for drugs and matching",
    "content": "Hello,\n\nCurrently i'm working on something that has to match drug description from a free text with some data that is cleaned and structured with column for each type of information for the drug. The free text usually contains dosage, quantity, name, brand, tablet/capsule and other info like that in different formats, sometimes they are split between ',' sometimes there is no dosage at all and many other formats.   \nThe free text cannot be changed to something more standard.  \nAnd based on the free text i have to match it to something in the database but idk which would be the best solution.  \nFrom the research that i've done so far i came across databricks and using the vector search functionality from there.   \nAre there any other services / principles that would help in a context like that?",
    "author": "StefanSG2",
    "timestamp": "2025-10-01T08:06:30",
    "url": "https://reddit.com/r/dataengineering/comments/1nvaqj8/text_based_search_for_drugs_and_matching/",
    "score": 8,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nvioip",
    "title": "Kubrick group - London",
    "content": "Anyone familiar with Kubrick group? Are they really producing that many senior data engineers or are they just inflating their staff so they can get hired better. ",
    "author": "r_mashu",
    "timestamp": "2025-10-01T12:56:41",
    "url": "https://reddit.com/r/dataengineering/comments/1nvioip/kubrick_group_london/",
    "score": 3,
    "num_comments": 7,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nus5j4",
    "title": "Is it just me or do younger hiring managers try too hard during DE interviews?",
    "content": "I’ve noticed quite a pattern with interviews for DE roles. It’s always the younger hiring managers that try really hard to throw you off your game during interviews. They’ll ask trick questions or just constantly drill into your answers. It’s like they’re looking for the wrong answer instead of the right one. I almost feel like they’re trying to prove something like that they’re the real deal.\n\nWhen it comes to the older ones it’s not so much that. They actually take the time to want to get to know you and see if you’re a good culture fit.  I find that I do much better with them and I’m able to actually be myself as opposed to walking on egg shells.\n\nwith that being said anyone else experience the same thing?",
    "author": "burningburnerbern",
    "timestamp": "2025-09-30T16:05:59",
    "url": "https://reddit.com/r/dataengineering/comments/1nus5j4/is_it_just_me_or_do_younger_hiring_managers_try/",
    "score": 84,
    "num_comments": 34,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nv9ggg",
    "title": "Log-Based CDC vs. Traditional ETL: A Technical Deep Dive",
    "content": "",
    "author": "dan_the_lion",
    "timestamp": "2025-10-01T07:18:10",
    "url": "https://reddit.com/r/dataengineering/comments/1nv9ggg/logbased_cdc_vs_traditional_etl_a_technical_deep/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nuj7jq",
    "title": "We just shipped Apache Gravitino 1.0 – an open-source alternative to Unity Catalog",
    "content": "Hey folks,As part of the Apache Gravitino project, I’ve been contributing to what we call a **“catalog of catalogs”** – a unified metadata layer that sits on top of your existing systems. With 1.0 now released, I wanted to share why I think it matters for anyone in the Databricks / Snowflake ecosystem.\n\n**Where Gravitino differs from Unity Catalog by Databricks**\n\n* **Open &amp; neutral**: Unity Catalog is excellent inside the Databricks ecosystem. And it was not open sourced until last year. Gravitino is Apache-licensed, open-sourced from day 1, and works across Hive, Iceberg, Kafka, S3, ML model registries, and more.\n* **Extensible connectors**: Out-of-the-box connectors for multiple platforms, plus an API layer to plug into whatever you need.\n* **Metadata-driven actions**: Define compaction/TTL policies, run governance jobs, or enforce PII cleanup directly inside Gravitino. Unity Catalog focuses on access control; Gravitino extends to automated actions.\n* **Agent-ready**: With the MCP server, you can connect LLMs or AI agents to metadata. Unity Catalog doesn’t (yet) expose metadata for conversational use.\n\n**What’s new in 1.0**\n\n* Unified access control with enforced RBAC across catalogs/schemas.\n* Broader ecosystem support (Iceberg 1.9, StarRocks catalog).\n* Metadata-driven action system (statistics + policy + job engine).\n* MCP server integration to let AI tools talk to metadata directly.\n\nHere’s a simplified architecture view we’ve been sharing:*(diagram of catalogs, schemas, tables, filesets, models, Kafka topics unified under one metadata brain)*\n\n**Why I’m excited** Gravitino doesn’t replace Unity Catalog or Snowflake’s governance. Instead, it complements them by acting as a **layer above multiple systems**, so enterprises with hybrid stacks can finally have one source of truth.\n\nRepo: [https://github.com/apache/gravitino](https://github.com/apache/gravitino)\n\nWould love feedback from folks who are deep in Databricks or Snowflake or any other data engineering fields. What gaps do you see in current catalog systems?\n\nhttps://preview.redd.it/p6uqzj5n5csf1.png?width=2368&amp;format=png&amp;auto=webp&amp;s=bab993cd44bc393cc8c3f1317c458fe872afbbb3",
    "author": "Q-U-A-N",
    "timestamp": "2025-09-30T10:19:08",
    "url": "https://reddit.com/r/dataengineering/comments/1nuj7jq/we_just_shipped_apache_gravitino_10_an_opensource/",
    "score": 84,
    "num_comments": 14,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nv4lwu",
    "title": "Open source AI Data Generator",
    "content": "We built an AI-powered dataset generator that creates realistic datasets for dashboards, demos, and training, then shared the open source repo. The response was incredible, but we kept hearing: 'Love this, but can I just use it without the setup?'\n\nSo we [hosted it](https://www.metabase.com/ai-data-generator) as a free service ✌️\n\nOf course, it's still [100% open source](https://github.com/metabase/dataset-generator) for anyone who wants to hack on it.\n\nOpen to feedback and feature suggestions from the BI community!",
    "author": "Ramirond",
    "timestamp": "2025-10-01T03:32:07",
    "url": "https://reddit.com/r/dataengineering/comments/1nv4lwu/open_source_ai_data_generator/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.54,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nuzgb8",
    "title": "Deep dive iceberg format",
    "content": "Here is one of my blog posts deep diving into iceberg format.\nLooked into metadata, snapshot files, manifest lists, and delete and data files. Feel free to add suggestions, clap and share. \n\nhttps://towardsdev.com/apache-iceberg-for-data-lakehouse-fc63d95751e8 \n\nThanks",
    "author": "mani-2512",
    "timestamp": "2025-09-30T22:02:40",
    "url": "https://reddit.com/r/dataengineering/comments/1nuzgb8/deep_dive_iceberg_format/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nu91ht",
    "title": "Interesting Links in Data Engineering - September 2025",
    "content": "In the very nick of time, here are a bunch of things that I've found in September that are interesting to read.\nIt's all there: Kafka, Flink, Iceberg (so. much. iceberg.), Medallion Architecture discussions, DuckDB 1.4 with Iceberg write support, the challenge of Fast Changing Dimensions in Iceberg, The Last Days of Social Media… and lots more.\n\n👉 Enjoy 😁 https://rmoff.net/2025/09/29/interesting-links-september-2025/",
    "author": "rmoff",
    "timestamp": "2025-09-30T02:56:26",
    "url": "https://reddit.com/r/dataengineering/comments/1nu91ht/interesting_links_in_data_engineering_september/",
    "score": 40,
    "num_comments": 3,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nupqde",
    "title": "How does pyarrow data type convert to pyiceberg",
    "content": "https://medium.com/@shubhamg2404/the-bridge-between-pyarrow-and-pyiceberg-a-deep-dive-into-data-type-conversions-957c72f8dd9e",
    "author": "Born_Shelter_8354",
    "timestamp": "2025-09-30T14:24:38",
    "url": "https://reddit.com/r/dataengineering/comments/1nupqde/how_does_pyarrow_data_type_convert_to_pyiceberg/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nu8s8q",
    "title": "Databricks cost vs Redshift",
    "content": "I am thinking of moving away from Redshift because query performance is bad and it is looking increasingly like and engineering dead end. I have been looking at Databricks which from the outside looking looks brilliant. \n\nHowever I can't get any sense of costs, we currently have $10,000 a year Redshift contract and we only have 1TB of data. In there. Tbh Redshift was a bit overkill for our needs in the first place, but you inherit what you inherit! \n\nWhat do you reckon, worth the move? ",
    "author": "Humble_Exchange_2087",
    "timestamp": "2025-09-30T02:39:57",
    "url": "https://reddit.com/r/dataengineering/comments/1nu8s8q/databricks_cost_vs_redshift/",
    "score": 28,
    "num_comments": 36,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nuis74",
    "title": "Migration of database keeps getting slower",
    "content": "TL;DR: Migrating a large project backend from Google Sheets to self-hosted Appwrite. The migration script slows down drastically when adding documents with relationships. Tried multiple approaches (HTTP, Python, Dart, Node.js, even direct MariaDB injection) but relationships mapping is the bottleneck. Looking for guidance on why it’s happening and how to fix it.\n\n\nHello, I am a hobbyist who have been making apps for personal use, using flutter since 7 years. \n\nI have a project which used Google sheet as backend. The database has grown quite large and I've been trying to migrate to self-hosted appwrite. The database has multiple collections with relationships between few of them. \n\nThe issue I'm facing is that the part of the migration script which adds documents that has to map the relationships keeps getting slower and slower to an unfeasible rate. I've been trying to find a fix since over 2 weeks and have tried http post, python, dart and node js but with no relief. Also tried direct injection into mariadb but for stuck at mapping relationships. \n\nCan someone please guide me why this is happening and how can I circumvent this? \n\nThanks\n\nContext-\nhttps://pastebin.com/binVPdnd",
    "author": "JacuzziGuy",
    "timestamp": "2025-09-30T10:03:11",
    "url": "https://reddit.com/r/dataengineering/comments/1nuis74/migration_of_database_keeps_getting_slower/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nuikzn",
    "title": "How to handle tables in long format where the value column contains numbers and strings?",
    "content": "Dear community\n\nI work on a factsheet-like report which shall be distributed via PDF and therefore I chose Power BI Report Builder which works great for pixel perfect print optimized reports. For PBI Report Builder and my report design in general it is best to work with flat tables. The input comes from various Excel files and I process them with Python in our Lakehouse. That works great. The output column structure is like this:\n\n- Hierarchy level 1 (string)\n- Hierarchy level 2 (string)\n- Attribute group (string)\n- Attribute (string)\n- Value (mostly integers some strings)\n\nFor calculations in the report it is best to have the value column only being integers. However, some values cannot be expressed as number and are certain keywords instead stored as strings. I thought about having a value_int and value_str column to solve this.\n\nDo you have any tips or own experiences? I'm relatively new to data transformations and maybe not aware of some more advanced concepts.\n\nThanks!\n",
    "author": "aegi_97",
    "timestamp": "2025-09-30T09:55:36",
    "url": "https://reddit.com/r/dataengineering/comments/1nuikzn/how_to_handle_tables_in_long_format_where_the/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nu86pr",
    "title": "Custom extract tool",
    "content": "We extract reports from Databricks to various state regulatory agencies. These agencies have very specific and odd requirements for these reports. Beyond the typical header, body, and summary data, they also need certain rows hard coded with static or semi-static values. For example, they want the date (in a specific format) and our company name in the first couple of cells before the header rows. Another example is they want a static row between the body of the report and the summary section. It personally makes my skin crawl but the requirements are the requirements; there’s not much room for negotiation when it comes to state agencies.\n\nToday we do this with a notebook and custom code. It works but it’s not awesome. I’m curious if there are any extraction or report generation tools that would have the required amount of flexibility. Any thoughts?",
    "author": "raginjason",
    "timestamp": "2025-09-30T02:00:38",
    "url": "https://reddit.com/r/dataengineering/comments/1nu86pr/custom_extract_tool/",
    "score": 5,
    "num_comments": 12,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nu88jo",
    "title": "Getting handson experience on MDM tools",
    "content": "Hello peeps,\n\n  \nbackground : I am new to data world and since my start in 2022, have been working with syndigo MDM for a retailer. This is a on-the-job learning phase for me and am now interested to explore &amp; get handson experience on other MDM tools available \\[STIBO, Reltio, Informatica , Semarchy ....\\]\n\n  \nI keep looking up job postings periodically just to stay aware of how the market is ( in the domain that I am into). Everytime I only come across Reltio or Informatica MDM openings (sometimes semarchy &amp; Profisee too) but never on Syndigo MDM.\n\nIts bugging me to keep working on a tool that barely got any new openings in the market\n\nHence I am interested to gather some handson exp on other MDM tools available &amp; tending to your suggestions or experiences if you had ever tried this path in your personal time.\n\n  \nTIA",
    "author": "hkesani",
    "timestamp": "2025-09-30T02:03:52",
    "url": "https://reddit.com/r/dataengineering/comments/1nu88jo/getting_handson_experience_on_mdm_tools/",
    "score": 3,
    "num_comments": 9,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntw5sv",
    "title": "Pontoon, an open-source data export platform",
    "content": "Hi, we're Alex and Kalan, the creators of Pontoon ([https://github.com/pontoon-data/Pontoon](https://github.com/pontoon-data/Pontoon)). Pontoon is an **open source, self-hosted, data export platform**. We built Pontoon from the ground up for the use case of shipping data products to enterprise customers. Check out our [demo ](https://app.storylane.io/share/onova7c23ai6)or try it out with docker [here](https://pontoon-data.github.io/Pontoon/getting-started/quick-start/).\n\nWhile at our prior roles as data engineers, we’ve both felt the pain of data APIs. We either had to spend weeks building out data pipelines in house or spend a lot on ETL tools like [Fivetran](https://www.fivetran.com/). However, there were a few companies that offered data syncs that would sync directly to our data warehouse (eg. Redshift, Snowflake, etc.), and when that was an option, we always chose it. This led us to wonder “**Why don’t more companies offer data syncs?**”. So we created Pontoon to be a platform that any company can self host to provide data syncs to their customers!\n\nWe designed Pontoon to be:\n\n* **Easily Deployed**: We provide a single, self-contained [Docker image](https://github.com/pontoon-data/Pontoon/pkgs/container/pontoon%2Fpontoon-unified)\n* **Support Modern Data Warehouses:** Supports Snowflake, BigQuery, Redshift, (we're working on S3, GGS)\n* **Multi-cloud**: Can send data from any cloud to any cloud\n* **Developer Friendly:** Data syncs can also be built via the API\n* **Open Source:** Pontoon is free to use by anyone\n\nUnder the hood, we use [Apache Arrow ](https://arrow.apache.org/)and [SQLAlchemy](https://github.com/sqlalchemy/sqlalchemy) to move data. Arrow has been fantastic, being very helpful with managing the slightly different data / column types between different databases. Arrow has also been really performant, averaging around 1 million records per minute on our benchmark.\n\nIn the shorter-term, there are several improvements we want to make, like:\n\n* Adding support for DBT models to make adding data models easier\n* UX improvements like better error messaging and monitoring of data syncs\n* More sources and destination (S3, GCS, Databricks, etc.)\n\nIn the longer-term, we want to make data sharing as easy as possible. As data engineers, we sometimes felt like second class citizens with how we were told to get the data we needed - “just loop through this api 1000 times”, “you probably won’t get rate limited” (we did), “we can schedule an email to send you a csv every day”. We want to change how modern data sharing is done and make it simple for everyone.\n\nGive it a try [https://github.com/pontoon-data/Pontoon](https://github.com/pontoon-data/Pontoon) and let us know if you have any feedback. Cheers!",
    "author": "alexdriedger",
    "timestamp": "2025-09-29T15:29:51",
    "url": "https://reddit.com/r/dataengineering/comments/1ntw5sv/pontoon_an_opensource_data_export_platform/",
    "score": 26,
    "num_comments": 9,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntpa1m",
    "title": "Rant: tired of half-*ssed solutions",
    "content": "Throwaway account.\n\nI love being a DE, with the good and the bad. \n\nExcept for the past few of years. I have been working for an employer who doesn’t give a 💩 about methodology or standards.\n\nTo please “customers”, I have written Python or SQL scripts with hardcoded values, emailed files periodically because my employer is too cheap to buy a scheduler, let alone a hosted server, ETL jobs get hopelessly delayed because our number of Looker users has skyrocketed and both jobs and Looker queries compete for resources constantly (“select * from information schema” takes 10 minutes average to complete) and we won’t upgrade our Snowflake account because it’s too much money.\n\nThe list goes on.\n\nWhy do I stay? The money. I am well paid and the benefits are hard to beat. \n\nI long for the days when we had code reviews, had to use a coding style guide, could use a properly designed database schema without any dangling relationships.\n\nI spoke to my boss about this. He thinks it’s because we are all remote. I don’t know if I agree. \n\nI have been a DE for almost 2 decades. You’d think I’ve seen it all but apparently not. I guess I am getting too old for this.\n\nAnyhow. Rant over.",
    "author": "[deleted]",
    "timestamp": "2025-09-29T11:05:05",
    "url": "https://reddit.com/r/dataengineering/comments/1ntpa1m/rant_tired_of_halfssed_solutions/",
    "score": 53,
    "num_comments": 32,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nteddq",
    "title": "New Community Rule. Rule 9: No low effort/AI posts",
    "content": "Hello all,\n\nAnnouncing we have a new rule where we're cracking down on low effort and AI generated content primarily fuelled from the [discussion here](https://www.reddit.com/r/dataengineering/comments/1nk4ai8/meta_ai_slop_report_option/) and created a new rule for it which can be found in the sidebar under Rule 9.\n\nWe'd like to invite the community to use the report function where you feel a post or comment may be AI generated so the mod team can review and remove accordingly.\n\nCheers all.  Have a great week and thank you for everybody positively contributing to making the subreddit better.",
    "author": "AutoModerator",
    "timestamp": "2025-09-29T03:25:21",
    "url": "https://reddit.com/r/dataengineering/comments/1nteddq/new_community_rule_rule_9_no_low_effortai_posts/",
    "score": 227,
    "num_comments": 14,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntgqgt",
    "title": "Is the Senior Job Market Dead Right Now?",
    "content": "Ive been a DE for 8 years now. ive been trying to find a new job but have received 0 callbacks after applying for a week. \n\nI have all the major skills: airflow, dbt, snowflake, python, etc. Im used to getting blown up by recruiters when i look for a job but right now its just crickets. ",
    "author": "shittyfuckdick",
    "timestamp": "2025-09-29T05:31:22",
    "url": "https://reddit.com/r/dataengineering/comments/1ntgqgt/is_the_senior_job_market_dead_right_now/",
    "score": 137,
    "num_comments": 105,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nuew5x",
    "title": "Alembic alternatives for managing data models",
    "content": "What do folks use to manage their data models? \n\nI've come from teams that just used plan SQL and didn't really version control their data models over time. Obviously, that's not preferred. \n\nBut I recently joined a place that uses alembic and I'm not positive it's all that much better that pure SQL with no version control. (Only kind of joking.) It has weird quirks with it's autogenerated revisions, nullability and other updating aspects. The most annoying issue being that its autogenerated revision file for updates is always just creating every table again, which we haven't been able to solve, so we just have to write it ourselves every time.\n\nWe use Microsoft SQL Server for our DB if that makes any difference. I've seen some mentions of Atlas? Any other tools folks love for this?\n\n",
    "author": "ursamajorm82",
    "timestamp": "2025-09-30T07:36:00",
    "url": "https://reddit.com/r/dataengineering/comments/1nuew5x/alembic_alternatives_for_managing_data_models/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nu5761",
    "title": "Lake Formation Column Security Not Working with DataZone/SageMaker Studio &amp; Redshift",
    "content": "Hey all,\n\nI've hit a wall on what seems like a core use case for the modern AWS data stack, and I'm hoping someone here has seen this specific failure mode before. I've been troubleshooting for days and have exhausted the official documentation.\n\nMy Goal (What I'm trying to achieve): An analyst logs into AWS via IAM Identity Center. They open our Amazon DataZone project (which uses the SageMaker Unified Studio interface). They run a SELECT * FROM customers query against a Redshift external schema. Lake Formation should intercept this and, based on their group membership, return only the 2 columns they are allowed to see (revenue and signup_date).\n\nThe Problem (The \"Smoking Gun\"): The user (analyst1) can log in and access the project. However, the system is behaving as if Trusted Identity Propagation (TIP) is completely disabled, even though all settings appear correct. I can prove this with two states: \n\n1.If I give the project's execution role (datazone_usr_role_...) SELECT in Lake Formation: The query runs, but it returns ALL columns. The user's fine-grained permission is ignored. \n\n2.If I revoke SELECT from the execution role: The query fails with TABLE_NOT_FOUND: Table '...customers' does not exist. The Data Explorer UI confirms the user can't see any tables. This proves Lake Formation is only ever seeing the service role's identity, never the end user's. \n\n\nThe Architecture: \n•Identity: IAM Identity Center (User: analyst1, Group: Analysts). \n•UI: Amazon DataZone project using a SageMaker Unified Domain. \n•Query Engine: Amazon Redshift with an external schema pointing to Glue. \n•Data Catalog: AWS Glue. \n•Governance: AWS Lake Formation. \n\nWhat I Have Already Done (The Exhaustive List): I'm 99% sure this is not a basic permissions issue. We have meticulously configured every documented prerequisite for TIP: \n\n•Created a new DataZone/SageMaker Domain specifically with IAM Identity Center authentication.\n•Enabled Domain-Level TIP: The \"Enable trusted identity propagation for all users on this domain\" checkbox is checked. \n•Enabled Project Profile-Level TIP: The Project Profile has the enableTrustedIdentityPropagationPermissions blueprint parameter set to True. \n•Created a NEW Project: The project we are testing was created after the profile was updated with the TIP flag. \n•Updated the Execution Role Trust Policy: The datazone_usr_role_... has been verified to include sts:SetContext in its trust relationship for the sagemaker.amazonaws.com principal. \n•Assigned the SSO Application: The Analysts group is correctly assigned to the Amazon SageMaker Studio application in the IAM Identity Center console. \n•Tried All LF Permission Combos: We have tried every permutation of Lake Formation grants to the user's SSO role (AWSReservedSSO_...) and the service role (datazone_usr_role_...). The result is always one of the two failure states described above.\n\nMy Final Question: Given that every documented switch for enabling Trusted Identity Propagation has been flipped, what is the final, non-obvious, expert-level piece of the puzzle I am missing? Is there a known bug or a subtle configuration in one of these places? \n•The Redshift external schema itself? \n•The DataZone \"Data Source\" connection settings? \n•A specific IAM permission missing from the user's Permission Set that's needed to carry the identity token? \n•A known issue with this specific stack (DataZone + Redshift + LF)?\n\nI'm at the end of my rope here and would be grateful for any insights from someone who has successfully built similar architecture. Thanks in advance!!",
    "author": "XxThatWeirdGuyxX",
    "timestamp": "2025-09-29T22:48:23",
    "url": "https://reddit.com/r/dataengineering/comments/1nu5761/lake_formation_column_security_not_working_with/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntp9ry",
    "title": "How to handle 53 event types and still have a social life?",
    "content": "We’re setting up event tracking: 13 structured events covering the most important things, e.g. view\\_product, click\\_product, begin\\_checkout. This will likely grow to 27, 45, 53, ... event types because of tracking niche feature interactions. Volume-wise, we are talking hundreds of millions of events daily.\n\n2 pain points I'd love input on:\n\n1. Every event lands in its own table, but we are rarely interested in one event. Unioning all to create this sequence of events feels rough as event types grow. Is it? Any scalable patterns people swear by?\n2. We have no explicit link between events, e.g. views and clicks, or clicks and page loads; causality is guessed by joining on many fields or connecting timestamps. How is this commonly solved? Should we push back for source-sided identifiers to handle this?\n\nWe are optimizing for scalability, usability, and simplicity for analytics. Really curious about different perspectives on this.\n\n**EDIT:** To provide additional information, we do have a sessionId. However, within a session we still rely on timestamps for inference. \"Did this view lead to this click?\" Unlike an additional, common identifier between views and clicks specifically for example (like a hook that 1:1 matches both). I am wondering if the latter is common.\n\nAlso, we actually are plugging into existing solutions like Segment, RudderStack, Snowplow, Amplitude (one of them not all 4) that provides us the ability to create structured tracking plans for events. Every event defined in this plan currently lands as a separate table in BQ. It's then that we start to make sense of it, potentially creating one big table of them by unioning. Am I missing possibilities, e.g. having them land as one table in the first place? Does this change anything?",
    "author": "Nero-Azzuro",
    "timestamp": "2025-09-29T11:04:48",
    "url": "https://reddit.com/r/dataengineering/comments/1ntp9ry/how_to_handle_53_event_types_and_still_have_a/",
    "score": 35,
    "num_comments": 28,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nu170m",
    "title": "sparkenforce: Type Annotations &amp; Runtime Schema Validation for PySpark DataFrames",
    "content": "sparkenforce is a PySpark type annotation package that lets you specify and enforce DataFrame schemas using Python type hints.\n\n## What My Project Does\n\nWorking with PySpark DataFrames can be frustrating when schemas don’t match what you expect, especially when they lead to runtime errors downstream.\n\nsparkenforce solves this by:\n\n* Adding type annotations for DataFrames (columns + types) using Python type hints.\n* Providing a `@validate` decorator to enforce schemas at runtime for function arguments and return values.\n* Offering clear error messages when mismatches occur (missing/extra columns, wrong types, etc.).\n* Supporting flexible schemas with ..., optional columns, and even custom Python ↔ Spark type mappings.\n\nExample:\n\n```\nfrom sparkenforce import validate\nfrom pyspark.sql import DataFrame, functions as fn\n\n@validate\ndef add_length(df: DataFrame[\"firstname\": str]) -&gt; DataFrame[\"name\": str, \"length\": int]:\n    return df.select(\n        df.firstname.alias(\"name\"),\n        fn.length(\"firstname\").alias(\"length\")\n    )\n```\n\nIf the input DataFrame doesn’t contain \"firstname\", you’ll get a `DataFrameValidationError` immediately.\n\n## Target Audience\n\n* PySpark developers who want stronger contracts between DataFrame transformations.\n* Data engineers maintaining ETL pipelines, where schema changes often breaks stuff.\n* Teams that want to make their PySpark code more self-documenting and easier to understand.\n\n## Comparison\n\n* Inspired by [dataenforce](https://github.com/CedricFR/dataenforce) (Pandas-oriented), but extended for PySpark DataFrames.\n* Unlike static type checkers (e.g. mypy), sparkenforce enforces schemas at runtime, catching real mismatches in Spark pipelines.\n* [spark-expectations](https://github.com/Nike-Inc/spark-expectations) has a wider aproach, tackling various data quality rules (validating the data itself, adding observability, etc.). sparkenforce focuses only on schema or structure data contracts.\n\n## Links\n\n* PyPI: [sparkenforce](https://pypi.org/project/sparkenforce/)\n* Source code: [GitHub repo](https://github.com/agustin-recoba/sparkenforce)\n* Demo notebook: [Examples](https://github.com/agustin-recoba/sparkenforce/blob/main/src/demo/demo_notebook.ipynb)",
    "author": "nopasanaranja20",
    "timestamp": "2025-09-29T19:17:21",
    "url": "https://reddit.com/r/dataengineering/comments/1nu170m/sparkenforce_type_annotations_runtime_schema/",
    "score": 8,
    "num_comments": 4,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntw4d6",
    "title": "dbt orchestration in Snowflake",
    "content": "Hey everyone, I’m looking to get into dbt as it seems to bring a lot of benefits. Things like version control, CI/CD, lineage, documentation, etc.\n\nI’ve noticed more and more people using dbt with Snowflake, but since I don’t have hands-on experience yet, I was wondering how do you usually orchestrate dbt runs when you’re using dbt core and Airflow isn’t an option?\n\nDo you rely on Snowflake’s native features to schedule updates with dbt? If so, how scalable and easy is it to manage orchestration this way?\n\nSorry if this sounds a bit off but still new to dbt and just trying to wrap my head around it!",
    "author": "Realistic_Function",
    "timestamp": "2025-09-29T15:28:05",
    "url": "https://reddit.com/r/dataengineering/comments/1ntw4d6/dbt_orchestration_in_snowflake/",
    "score": 12,
    "num_comments": 20,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntk7yy",
    "title": "How Spark Really Runs Your Code: A Deep Dive into Jobs, Stages, and Tasks",
    "content": "Apache Spark is one of the most powerful engines for big data processing, but to use it effectively you need to understand what’s happening under the hood. Spark doesn’t just “run your code” — it breaks it down into a hierarchy of **jobs, stages, and tasks** that get executed across the cluster.",
    "author": "Lenkz",
    "timestamp": "2025-09-29T07:56:31",
    "url": "https://reddit.com/r/dataengineering/comments/1ntk7yy/how_spark_really_runs_your_code_a_deep_dive_into/",
    "score": 38,
    "num_comments": 1,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nte7px",
    "title": "Are big take home projects a red flag?",
    "content": "Many months ago I was rejected after doing a take home project. My friends say I dodged a bullet but it did a number on my self esteem.\n\nI was purposefully tasked with building a ppipeline in a technology I didn’t know to see how well I learn new tech, and I had to use formulas from a physics article they supplied to see how well I learn new domains (I’m not a physicist). I also had to evaluate the data quality.\n\nIt took me about half a day to learn the tech through tutorials and examples, and a couple of hours to find all the incomplete rows, missing rows, and duplicate rows. I then had to visit family for a week, so I only had a day to work on it.\n\nWhen I talked with the company again they praised my code and engineering, but they were disappointed that I didn’t use the physics article to find out which values are reasonable and then apply outlier detection, filters or something else to evaluate the output better.\n\nI was a bit taken aback because that would’ve required a lot more work for a take home project that I purposefully was not prepared for. I felt like I am not that good since I needed so much time to learn the tech and domain, but my friendstell me I dodged a bullet because if they expect this much from a take home project they would’ve worked me to the bone once I was on the payroll.\n\nWhat do you guys think? Is a big take home project a red flag?\n\n",
    "author": "pimmen89",
    "timestamp": "2025-09-29T03:15:35",
    "url": "https://reddit.com/r/dataengineering/comments/1nte7px/are_big_take_home_projects_a_red_flag/",
    "score": 63,
    "num_comments": 36,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntqrh1",
    "title": "API Waterfall - Endpoints that depends on others... some hints?",
    "content": "How do you guys handle this szenario:\n\nYou need to fetch `/api/products` with different query parameters:\n\n* `?category=electronics&amp;region=EU`\n* `?category=electronics&amp;region=US`\n* `?category=furniture&amp;region=EU`\n* ...and a million other combinations\n\nEach response is paginated across 10-20 pages. Then you realize: to get complete product data, you need to call `/api/products/{id}/details` for each individual product because the list endpoint only gives you summaries.\n\nThen you have dependencies... like syncing endpoint B needs data from endpoint A...\n\nThen you have rate limits... 10 requests per seconds on endpoint A, 20 on endpoint b... i am crying\n\nThen you do not want to full load every night, so you need dynamic upSince query parameter based on the last successfull sync...\n\nI tried severald products like airbyte, fivetrain, hevo and I tried to implement something with n8n. But none of these tools are handling the dependency stuff i need...\n\nI wrote a ton of scripts but they getting messy as hell and I dont want to touch them anymore\n\nim lost - how do you manage this?",
    "author": "domsen123",
    "timestamp": "2025-09-29T12:00:48",
    "url": "https://reddit.com/r/dataengineering/comments/1ntqrh1/api_waterfall_endpoints_that_depends_on_others/",
    "score": 7,
    "num_comments": 10,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nugg6x",
    "title": "The Python Apocolypse",
    "content": "We've been talking a lot about Python on this sub for data engineering. In my latest episode of Unapologetically Technical, Holden Karau and I discuss what I'm calling the [Python Apocalypse](https://www.youtube.com/watch?v=CpaXP_LKz6Y), a mountain of technical debt created by using Python with its lack of good typing (hints are not types), poorly generated LLM code, and bad code created by data scientists or data engineers.\n\nMy basic thesis is that codebases larger than \\~100 lines of code become unmaintainable quickly in Python. Python's type hinting and \"compilers\" just aren't up to the task. I plan to write a more in-depth post, but I'd love to see the discussion here so that I can include it in the post.",
    "author": "eljefe6a",
    "timestamp": "2025-09-30T08:34:53",
    "url": "https://reddit.com/r/dataengineering/comments/1nugg6x/the_python_apocolypse/",
    "score": 0,
    "num_comments": 19,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nti5y8",
    "title": "Flattening SAP hierarchies (open source)",
    "content": "Hi all,\n\nI just released an open source product for flattening SAP hierarchies, i.e. for when migrating from BW to something like Snowflake (or any other non-SAP stack where you have to roll your own ETL)\n\n[https://github.com/jchesch/sap-hierarchy-flattener](https://github.com/jchesch/sap-hierarchy-flattener)\n\nMIT License, so do whatever you want with it!\n\nHope it saves some headaches for folks having to mess with SETHEADER, SETNODE, SETLEAF, etc.",
    "author": "jodyhesch",
    "timestamp": "2025-09-29T06:33:53",
    "url": "https://reddit.com/r/dataengineering/comments/1nti5y8/flattening_sap_hierarchies_open_source/",
    "score": 20,
    "num_comments": 5,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntjvhb",
    "title": "When ETL Turns into a Land Grab",
    "content": "",
    "author": "rotzak",
    "timestamp": "2025-09-29T07:43:04",
    "url": "https://reddit.com/r/dataengineering/comments/1ntjvhb/when_etl_turns_into_a_land_grab/",
    "score": 8,
    "num_comments": 0,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nu0860",
    "title": "Complete Guide to the Netflix Data Engineer Role (Process, Prep &amp; Tips)",
    "content": "I recently put together a step-by-step guide for those curious about Netflix Data Engineering roles\n",
    "author": "Altruistic_Potato_67",
    "timestamp": "2025-09-29T18:32:18",
    "url": "https://reddit.com/r/dataengineering/comments/1nu0860/complete_guide_to_the_netflix_data_engineer_role/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntr4jl",
    "title": "Browser Caching Specific Airflow Run URLs",
    "content": "Hey y'all. Coming at you with a niche complaint curious to hear if others have solutions.\n\nWe use airflow for a lot of jobs and my browser (arc) always saves the url of random runs in the history. As a result i'll get into situations where when I type in the link to my search bar it will autocomplete to an old run giving a distorted view since i'm looking at old runs. \n\n  \nHas anyone else run into this or has solution?",
    "author": "Formal_Salad",
    "timestamp": "2025-09-29T12:14:23",
    "url": "https://reddit.com/r/dataengineering/comments/1ntr4jl/browser_caching_specific_airflow_run_urls/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntjg60",
    "title": "Is flattening an event_param struct in bigquery the best option for data modelling?",
    "content": "In BQ, I have firebase event logs in a date-sharded table which I'm set up an incremental dbt job to reformat as a partitioned table.\n\nThe event_params contain different keys for different events, and sometimes the same event will have different keys depending on app-version and other context details.\n\nI'm using dbt to build some data models on these events, and figure that flattening out the event params into one big table with a column for each param key will make querying most efficient. Especially for events that I'm not sure what params will be present, this will let me see everything present without any unknowns. The models will have an incremental load that add new columns on schema change - whenever a new param is introduced.\n\nDoes this approach seem sound? I know the structs must be used because they are more efficient, and I'm worried I might be taking the path of least resistance and most compute.",
    "author": "PalaceCarebear",
    "timestamp": "2025-09-29T07:26:20",
    "url": "https://reddit.com/r/dataengineering/comments/1ntjg60/is_flattening_an_event_param_struct_in_bigquery/",
    "score": 6,
    "num_comments": 6,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntiu0o",
    "title": "(Blockchain) data engineering",
    "content": "Hi all,\n\nI currently work as a data engineer in a big firm (+10.000 employees) in the finance sector.\n\nI would consider myself a T-shaped developer, with a deep knowledge of data modelling and an ability to turn scattered data into valuable high quality datasets. I have a masters degree in finance, are self tought on the technical side - and are therefore lacking my co-workers when it comes to skills in software engineering.\n\nAt some point, I would like to work in the blockchain industry.\n\nDo any of you have tips and tricks to position my profile to be a fit into data engineering roles in the crypto/blockchain industry?\n\nAnything will be appreciated, thanks :)",
    "author": "DanishGuy3232",
    "timestamp": "2025-09-29T07:01:41",
    "url": "https://reddit.com/r/dataengineering/comments/1ntiu0o/blockchain_data_engineering/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nu7z2p",
    "title": "New resource: Learn AI Data Engineering in a Month of Lunches",
    "content": "Hey r/dataengineering 👋,\n\nStjepan from Manning here. \n\nFirstly, a MASSIVE thank you to moderators for letting me post this. \n\nI wanted to share a new book from Manning that many here will find useful: *Learn AI Data Engineering in a Month of Lunches* by **David Melillo**.\n\nThe book is designed to help data engineers (and aspiring ones) **bridge the gap between traditional data pipelines and AI/ML workloads**. It’s structured in the *“Month of Lunches”* format — short, digestible lessons you can work through on a lunch break, with practical exercises instead of theory-heavy chapters.\n\n[Learn AI Data Engineering in a Month of Lunches](https://preview.redd.it/9vfkgzfkk9sf1.jpg?width=718&amp;format=pjpg&amp;auto=webp&amp;s=d6061f296f366e6552896ff157b61df20110a7e6)\n\n**A few highlights:**\n\n* Building **data pipelines for AI and ML**\n* Preparing and managing datasets for model training\n* Working with embeddings, vector databases, and large language models\n* Scaling pipelines for real-world production environments\n* Hands-on projects that reinforce each concept\n\nWhat I like about this one is that it doesn’t assume you’re a data scientist — it’s written squarely for **data engineers who want to make AI part of their toolkit**.\n\n👉 Save 50% today with code **MLMELILLO50RE** here: [Learn AI Data Engineering in a Month of Lunches](https://hubs.la/Q03LsPp70)\n\nCurious to hear from the community: **how are you currently approaching AI/ML workloads in your pipelines?** Are you experimenting with vector databases, LLMs, or keeping things more traditional?\n\nThank you all for having us.\n\nCheers,",
    "author": "ManningBooks",
    "timestamp": "2025-09-30T01:46:33",
    "url": "https://reddit.com/r/dataengineering/comments/1nu7z2p/new_resource_learn_ai_data_engineering_in_a_month/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.26,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nt7gsm",
    "title": "Is there really space/need for dedicated BI, Analytics, and AI/ML departments?",
    "content": "My company has distinct departments for BI, analytics and a newer AI/ML group. There’s already a fair amount of overlap between Analytics and BI. Currently analytics owns much of the production models, but I anticipate AI/ML will build new better models. To clarify AI/ML at my company is not tied to analytics at all at this point. They are building out their own ML platform and will have their own models. All three groups rely on DE which my company is actively revamping. Wanted to ask the DEs of Reddit: Do you think there is reason to have these 3 different groups? I think the lines of distinction are getting increasingly blurry. Do your companies have dedicated analytics, BI, and AI/ML groups/depts? ",
    "author": "Budget_Yoghurt_9348",
    "timestamp": "2025-09-28T20:13:11",
    "url": "https://reddit.com/r/dataengineering/comments/1nt7gsm/is_there_really_spaceneed_for_dedicated_bi/",
    "score": 19,
    "num_comments": 12,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nsn801",
    "title": "Week 1 of learning pyspark.",
    "content": "Week 1 of learning pyspark.\n\n-Running on default mode in databricks free edition \n-using csv\n\nWhat did i learned : \n\n- spark architecture\n     - cluster \n     - driver\n     - executors\n- read / write data\n     -schema\n-API\n     -RDD(just brushed past, heard it become   ) \n     - dataframe (focused on this) \n     - datasets (skipped) \n-lazy processing \n     -transformation and actions\n-basic operations, grouping, agg, join etc.. \n-data shuffle \n    -narrow / wide transformation \n    - data skewness\n-task, stage, job\n-data accumulators\n-user defined functions\n-complex data types (arrays and structs) \n-spark-submit\n-spark SQL\n-optimization \n     -predicate push down\n     -cache(), persist() \n     -broadcast join\n     -broadcast variables \n\nDoubts :\n1- is there anything important i missed? \n2- do i need to learn sparkML? \n3- what are your insights as professionals who works with spark? \n4-how do you handle corrupted data? \n5- how do i proceed from here on? \n\nPlans for Week 2 :\n\n-learn more about spark optimization, the things i missed and how these actually used in actual spark workflow\n( need to look into real industrial spark applications and how they transform and optimize. if you could provide some of your works that actually used on companies on real data, to refer, that would be great)\n\n-working more with parquet. \n(do we convert the data like csv or other into parquet(with basic filtering) before doing transformation or we work on the data as it as then save it as parquet?) \n\n-running spark application on cluster\n(i looked little into data lakes and using s3 and EMR servelerless, but i heard that EMR not included in aws free tier, is it affordable? (just graduated/jobless). \nAny altranatives ? Do i have to use it to showcase my projects? ) \n \n- get advices and reflect \n\n\nPlease guide me.\nYour valuable insights and informations are much appreciated,\nThanks in advance❤️\n\n  ",
    "author": "Jake-Lokely",
    "timestamp": "2025-09-28T05:34:32",
    "url": "https://reddit.com/r/dataengineering/comments/1nsn801/week_1_of_learning_pyspark/",
    "score": 255,
    "num_comments": 34,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nteq94",
    "title": "Do i need to over complicate the pipeline? Worried about costs.",
    "content": "**Developing** a custom dashboard with back-end on Cloudflare Workers, for our hopefully future customers, and honestly i got stuck on designing the data pipeline from the provider to all of the features we decided on.  \n\n\n**SHORT DESCRIPTION**  \nEach of the sensor sends current reading via a webhook every 30 seconds (temp &amp; humidity) and network status (signal strength , battery and metadata) \\~ 5 min.  \nEach of the sensor haves label's which we plan to utilize as influxdb tags. (Big warehouse ,3 sensors on 1m, 8m ,15m from the floor, across \\~110 steel beams)  \n  \nI have quite a list of **features** i want to support for our customers, and want to use InfluxDB Cloud to store RAW data in a 30 day bucket (without any further historical storage). \n\n* Live data updating in front-end graphs and charts. (Webhook endpoint -&gt; CFW Endpoint -&gt;  Durable Object (websocket) -&gt; Frontend (Sensor overview page) Only activated when user on sensor page.\n* The main dashboard would mimic a single Grafana dashboard, allowing users to configure their own panels, and some basic operations, but making it more user friendly (select's sensor1 , sensor5, sensor8 calculates average t&amp;h)  for important displaying, with live data updating (separate bucket, with agregation cold start (when user select's the desired building) \n* Alerts, with resolvable states (idea to use Redis , but i think a separate bucket might do the trick)\n* Data Export with some manipulation (daily high's and low's, custom down sample, etc)\n\n  \nNow this is all fun and games, for a single client, with not too big of a dataset, but the system might need to provide bigger retention policy for some future clients of raw data, I would guess the key is limiting all of the dynamical pages to use several buckets. \n\n  \nThis is my first bigger project where i need to think about the scalability of the system as i do not want to get back and redo the pipeline unless i absolutely need to.\n\nAny recommendations are welcome.",
    "author": "nemanja030998",
    "timestamp": "2025-09-29T03:47:25",
    "url": "https://reddit.com/r/dataengineering/comments/1nteq94/do_i_need_to_over_complicate_the_pipeline_worried/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntevmj",
    "title": "Is there any need for Data Quality/QA Analyst role?",
    "content": "Because I think I would like to do that.\n\nI like looking at data, though I no longer work professionally in a data analytics or data engineering role. However, I still feel like I could bring value in that area, on a fraction scale. I wonder if there is a role like a Data QA Analyst as a sidehustle/fractional role.\n\nMy plan is to pitch the idea that I will write the analytics code that evaluates the quality of data pipelines every day. I think in day-to-day DE operation, the tests folks write are mostly about pipeline health. With everyone integrating AI-based transformation, there is value in having someone test the output.\n\nSo, I was wondering if data quality analysis is even a thing? I think this is not a role to have someone entirely dedicated to full-time, but rather someone familiar with the feature or product to data analytics test code and look at data.\n\nMy plan is to:\n- Stare the at the data produced from DE operations\n- Come up with different questions and tests cases\n- Write simple code for those tests cases\n- And flag them to DE or production side\n\n\nWhen I was doing web scraping work, I used to write operations that simply scraped the data. Whenever security measures were enforced, the automation program I used was smart enough to adapt - utilizing tricks like fooling captchas or rotating proxies. However, I have recently learned that in flight ticket data scraping, if the system detects a scraping operation in progress, premiums are dynamically added to the ticket prices. They do not raise any security measures, but instead corrupt the data from the source.\n\nIf you are running a large-scale data scraping operation, it is unreasonable to expect the person doing the scraping to be aware of these issues. The reality is that you need someone to develop an test case that can monitor pricing data volatility to detect abnormalities. Most Data Analysts simply take the data provided by Data Engineers at face value and do not conduct a thorough analysis of it and nor should they.\n\nBut then again, this is just an idea. Please let me know what you think. I might pitch this idea to my employer. I do not need a two-day weekend, just one day is enough.",
    "author": "anyfactor",
    "timestamp": "2025-09-29T03:56:14",
    "url": "https://reddit.com/r/dataengineering/comments/1ntevmj/is_there_any_need_for_data_qualityqa_analyst_role/",
    "score": 4,
    "num_comments": 13,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nt2r0m",
    "title": "Microsoft’s Dynamics 365 Export Disaster Timeline",
    "content": "Microsoft has this convoluted mess of an ERP called Dynamics 365. It's expensive as shit, slow to work in, complicated to deploy customizations to. Worst of all, everyone in your company heavily relies on data export for reporting. Unfortunately getting that data has been an agonizing process since *forever*. The timeline (give or take) has been something like this:\n\n**ODATA (circa 2017)**  \n\\- Paintfully slow and just plain stupid for any serious data export.  \n\\- Relies on *URLs* for paging..  \n\\- Completely unusable if you had more than toy-sized data.\n\n**BYOD (2017-2020)** “Bring Your Own Database” aka *Bring Your Own Pain.*  \n\\- No delta feed just brute-force emptied and inserted data again and again.  \n\\- Bogged down performance of the entire system while exports ran until batch servers were introduced. You had to stagger the timing of exports and run cleanup jobs.  \n\\- You could only export \"entities\" , custom tables required you to deploy packages.  \n\\- You had to manage everything (schema, indexes, perf, costs).\n\n**Export to Data Lake (2021–2023)**  \n\\- Finally, the least bad option. Just dumped CSV files into ADLS.  \n\\- You had to parse out the data using Synapse which was slow  \n\\- Not perfect, but at least it was predictable to build pipelines on. Eventually some delta functionality hacks were implemented.\n\n**Fabric (2023 → today)**  \n**-** Scrap all that, because FU. Everything must go into Fabric now:  \n\\- Missing columns, messed up enums, table schemas don't match, missing rows etc.  \n\\- **Forced** deprication of Export to Data Lake, alienating and enraging all their customers causing them to lose all trust, causing panic  \n\\- More expensive in every way, from data storage, to  parquet conversion  \n\\- Fabric still in alpha. Buggy as shit. Limited T-SQL scope. Fragile and can cause loss of data.  \n\\- A hopeless development team on the Microsoft payroll that don't solve anything and outright lie and pretend everything is working and that this is so much better than what we had before.\n\nIn practice, every few years an organization has to re-adapt their entire workflow. Rebuild reports, views and whatnot. Hundreds of hours of work. All of this because Microsoft refuses to allow access to production database or read-only replicas. To your own data. Has anyone been through this clown show? If you have to vent I am here to listen.",
    "author": "agneum",
    "timestamp": "2025-09-28T16:20:03",
    "url": "https://reddit.com/r/dataengineering/comments/1nt2r0m/microsofts_dynamics_365_export_disaster_timeline/",
    "score": 14,
    "num_comments": 37,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nt7qu1",
    "title": "Introducing Pixeltable: Open Source Data Infrastructure for Multimodal Workloads",
    "content": "**TL;DR**: Open-source **declarative data infrastructure** for multimodal AI applications. Define what you want computed once, engine handles incremental updates, dependency tracking, and optimization automatically. Replace your vector DB + orchestration + storage stack with one `pip install`. Built by folks behind Parquet/Impala + ML infra leads from Twitter/Airbnb/Amazon and founding engineers of MapR, Dremio, and Yellowbrick.\n\nWe found that working with multimodal AI data sucks with traditional tools. You end up writing tons of imperative Python and glue code that breaks easily, tracks nothing, doesn't perform well without custom infrastructure, or requires stitching individual tools together.\n\n* What if this fails halfway through?\n* What if I add one new video/image/doc?\n* What if I want to change the model?\n\n**With Pixeltable you define what you want, engine figures out how:**\n\n    import pixeltable as pxt\n    \n    # Table with multimodal column types (Image, Video, Audio, Document)\n    t = pxt.create_table('images', {'input_image': pxt.Image})\n    \n    # Computed columns: define transformation logic once, runs on all data\n    from pixeltable.functions import huggingface\n    \n    # Object detection with automatic model management\n    t.add_computed_column(\n        detections=huggingface.detr_for_object_detection(\n            t.input_image,\n            model_id='facebook/detr-resnet-50'\n        )\n    )\n    \n    # Extract specific fields from detection results\n    t.add_computed_column(detections_labels=t.detections.labels)\n    \n    # OpenAI Vision API integration with built-in rate limiting and async management\n    from pixeltable.functions import openai\n    \n    t.add_computed_column(\n        vision=openai.vision(\n            prompt=\"Describe what's in this image.\",\n            image=t.input_image,\n            model='gpt-4o-mini'\n        )\n    )\n    \n    # Insert data directly from an external URL\n    # Automatically triggers computation of all computed columns\n    t.insert({'input_image': 'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000025.jpg'})\n    \n    # Query - All data, metadata, and computed results are persistently stored\n    results = t.select(t.input_image, t.detections_labels, t.vision).collect()\n\n**Why This Matters Beyond Computer Vision and ML Pipelines:**\n\nSame declarative approach works for agent/LLM infrastructure and context engineering:\n\n    from pixeltable.functions import openai\n    \n    # Agent memory that doesn't require separate vector databases\n    memory = pxt.create_table('agent_memory', {\n        'message': pxt.String,\n        'attachments': pxt.Json\n    })\n    \n    # Automatic embedding index for context retrieval\n    memory.add_embedding_index(\n        'message', \n        string_embed=openai.embeddings(model='text-embedding-ada-002')\n    )\n    \n    # Regular UDF tool\n    @pxt.udf\n    def web_search(query: str) -&gt; dict:\n        return search_api.query(query)\n    \n    # Query function for RAG retrieval\n    @pxt.query\n    def search_memory(query_text: str, limit: int = 5):\n        \"\"\"Search agent memory for relevant context\"\"\"\n        sim = memory.message.similarity(query_text)\n        return (memory\n                .order_by(sim, asc=False)\n                .limit(limit)\n                .select(memory.message, memory.attachments))\n    \n    # Load MCP tools from server\n    mcp_tools = pxt.mcp_udfs('http://localhost:8000/mcp')\n    \n    # Register all tools together: UDFs, Query functions, and MCP tools  \n    tools = pxt.tools(web_search, search_memory, *mcp_tools)\n    \n    # Agent workflow with comprehensive tool calling\n    agent_table = pxt.create_table('agent_conversations', {\n        'user_message': pxt.String\n    })\n    \n    # LLM with access to all tool types\n    agent_table.add_computed_column(\n        response=openai.chat_completions(\n            model='gpt-4o',\n            messages=[{\n                'role': 'system', \n                'content': 'You have access to web search, memory retrieval, and various MCP tools.'\n            }, {\n                'role': 'user', \n                'content': agent_table.user_message\n            }],\n            tools=tools\n        )\n    )\n    \n    # Execute tool calls chosen by LLM\n    from pixeltable.functions.anthropic import invoke_tools\n    agent_table.add_computed_column(\n        tool_results=invoke_tools(tools, agent_table.response)\n    )\n    \n    etc..\n\nNo more manually syncing vector databases with your data. No more rebuilding embeddings when you add new context. What I've shown:\n\n* **Regular UDF**: `web_search()` \\- custom Python function\n* **Query function**: `search_memory()` \\- retrieves from Pixeltable tables/views\n* **MCP tools**: `pxt.mcp_udfs()` \\- loads tools from MCP server\n* **Combined registration**: `pxt.tools()` accepts all types\n* **Tool execution**: `invoke_tools()` executes whatever tools the LLM chose\n* **Context integration**: Query functions provide RAG-style context retrieval\n\nThe LLM can now choose between web search, memory retrieval, or any MCP server tools automatically based on the user's question.\n\n**Why does it matter?**\n\n* **Incremental processing** \\- only recompute what changed\n* **Automatic dependency tracking** \\- changes propagate through pipeline\n* **Multimodal storage** \\- Video/Audio/Images/Documents/JSON/Array as first-class types\n* **Built-in vector search** \\- no separate ETL and Vector DB needed\n* **Versioning &amp; lineage** \\- full data history tracking and operational integrity\n\n**Good for**: AI applications with mixed data types, anything needing incremental processing, complex dependency chains\n\n**Skip if**: Purely structured data, simple one-off jobs, real-time streaming\n\nWould love feedback/2cts! Thanks for your attention :)\n\n**GitHub**: [https://github.com/pixeltable/pixeltable](https://github.com/pixeltable/pixeltable)",
    "author": "Norqj",
    "timestamp": "2025-09-28T20:28:04",
    "url": "https://reddit.com/r/dataengineering/comments/1nt7qu1/introducing_pixeltable_open_source_data/",
    "score": 6,
    "num_comments": 9,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nstjqb",
    "title": "Struggling with poor mentorship",
    "content": "I'm three weeks into my data engineering internship working on a data catalog platform, coming from a year in software development. My current tasks involve writing DAGs and Python scripts for Airflow, with some backend work in Go planned for the future.   \n  \nI was hoping to learn from an experienced mentor to understand data engineering as a profession, but my current mentor heavily relies on LLMs for everything and provides only surface-level explanations. He openly encourages me to use AI for my tasks without caring about the source, as long as it works. This concerns me greatly, as I had hoped for someone to teach me the fundamentals and provide focused guidance. I don't feel he offers much in terms of actual professional knowledge. Since we work in different offices, I also have limited interaction with him to build any meaningful connection.   \n  \nI left my previous job seeking better learning opportunities because I felt stagnant, but I'm worried this situation may actually be a downgrade. I definitely will raise my concern, but I am not sure how I should go about it to make the best out of the 6 months I am contracted to. Any advice?",
    "author": "Successful-Drop-3856",
    "timestamp": "2025-09-28T10:03:48",
    "url": "https://reddit.com/r/dataengineering/comments/1nstjqb/struggling_with_poor_mentorship/",
    "score": 32,
    "num_comments": 22,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntf4pw",
    "title": "Is it possible to write directly to the Snowflake's internal staging storage system from IDMC?",
    "content": "# Is it possible to write directly to Snowflake's internal staging storage system from IDMC?\n\n",
    "author": "EmbarrassedDance498",
    "timestamp": "2025-09-29T04:10:20",
    "url": "https://reddit.com/r/dataengineering/comments/1ntf4pw/is_it_possible_to_write_directly_to_the/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ntnhup",
    "title": "Starting on dbt with AI",
    "content": "For people new to dbt / starting to implementing it in their companies, I wrote an article on how you can fast-track implementation with AI tools. Basically the good AI agent plugged to your data warehouse can init your dbt, help you build the right transformations with dbt best practices and handle all the data quality checks / git versioning work. Hope it's helpful!",
    "author": "clr0101",
    "timestamp": "2025-09-29T09:59:20",
    "url": "https://reddit.com/r/dataengineering/comments/1ntnhup/starting_on_dbt_with_ai/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nshcm1",
    "title": "Fivetran to buy dbt? Spill the Tea",
    "content": "Source:  \n[https://www.theinformation.com/articles/data-startup-fivetran-talks-buy-dbt-labs-multibillion-dollar-deal](https://www.theinformation.com/articles/data-startup-fivetran-talks-buy-dbt-labs-multibillion-dollar-deal)\n\n  \n",
    "author": "engineer_of-sorts",
    "timestamp": "2025-09-27T23:36:37",
    "url": "https://reddit.com/r/dataengineering/comments/1nshcm1/fivetran_to_buy_dbt_spill_the_tea/",
    "score": 95,
    "num_comments": 131,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nsji5v",
    "title": "Palantir used by the United Kingdom National Health Service?!",
    "content": "The National Health Service in the United Kingdom have recently announced the deployment of a full data platform migration and consolidation to Palantir Foundry in order to challenge operational challenges such as in-day appointment cancellations and federate data beteeen different NHS England Trusts (regional based parts of the NHS). \n\nIn November 2023, NHS England awarded Palantir a £330m contract to deploy a Federated Data Platform that aims to provide “joined up” NHS services. The NHS has many operational challenges around data such as the frequency of data for in-day decisions in hospitals and consuming health services in multiple regions or hospital departments because of siloed data. \n\nAs a Platform Engineer now, having built data platforms and conducted cloud migrations in a few UK private sectors and coming to understand how much vendor lock in can have significant ramifications for an organisation. \n\nI’m astounded at the decision to see a public service consuming a platform with complete vendor lock in.\n\nThis seems completely bonkers; please tell me you can host Palantir services in your own cloud accounts and within your own internal networks! \n\nFrom what I’ve read, Palantir is just a shiny wrapper built on Spark and Delta Lake hosted on k8’s with the choice of leaving insanely hard. \n\nWhat value-add does Palantir provide that I’m missing here? The NHS has been continually shifting towards the cloud for the last ten years and from my point of view, this was simply an architectural problem to solve to federate NHS trusts rather than buy  into a noddy spark wrapper? \n\n Palantir doesn’t have much market penetration in the United Kingdom in the private sector, Beyond its nefarious political associations, I’m very curious to see what Americans think of this decision? \n\nWhat should we be worried about; politically and technically. ",
    "author": "gluka",
    "timestamp": "2025-09-28T01:55:18",
    "url": "https://reddit.com/r/dataengineering/comments/1nsji5v/palantir_used_by_the_united_kingdom_national/",
    "score": 42,
    "num_comments": 23,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nsmtya",
    "title": "dbt-Cloud pros/cons what's your honest take?",
    "content": "I’ve been a long-time lurker here and finally wanted to ask for some help.\n\nI’m doing some exploratory research into dbt Cloud and I’d love to hear from people who use it day-to-day. I’m especially interested in the issues or pain points you’ve run into, and how you feel it compares to other approaches.\n\nI’ve got a few questions lined up for dbt Cloud users and would really appreciate your experiences. If you’d rather not post publicly, I’m happy to DM instead. And if you’d like to verify who I am first, I can share my LinkedIn.\n\nThanks in advance to anyone who shares their thoughts — it’ll be super helpful.",
    "author": "SnooPineapples1366",
    "timestamp": "2025-09-28T05:15:11",
    "url": "https://reddit.com/r/dataengineering/comments/1nsmtya/dbtcloud_proscons_whats_your_honest_take/",
    "score": 18,
    "num_comments": 26,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nsw6tf",
    "title": "ETL helpful articles",
    "content": "Hi,\n\nI am building ETL pipelines using aws state machines and aurora serverless postgres. \n\nI am always looking for new patterns or helpful tips and tricks for design, performance, data storage such as raw, curated data. \n\nI’m wondering if you have books, articles, or videos you’ve enjoyed that could help me out. \n\nI’d appreciate any pointers. \n\nThanks\n",
    "author": "Randomengineer84",
    "timestamp": "2025-09-28T11:47:26",
    "url": "https://reddit.com/r/dataengineering/comments/1nsw6tf/etl_helpful_articles/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nshgtc",
    "title": "Talend or Spark Job Offer",
    "content": "Hey guys. I got 1 job offers here and I really need your advice. \n\n\nOffer: Bank. \nTech Stacks: Talend + GCP.  \nSalary: around 30% more than B. \n\n\n\nCurrent Company: Consulting.   \nTech Stacks: Azure, Spark.  \nIm on bench for 5 months now as I'm a junior.\n\n\nI'm inclined to accept offer A but Talend is my biggest worry. If I stay for 1 more year at B, I might get 80% more than my current salary. What do you all think?\n\n",
    "author": "Dont_say_Maths927",
    "timestamp": "2025-09-27T23:43:55",
    "url": "https://reddit.com/r/dataengineering/comments/1nshgtc/talend_or_spark_job_offer/",
    "score": 30,
    "num_comments": 34,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nstbbu",
    "title": "Are You Writing Your Data Right? Here’s How to Save Cost &amp; Time",
    "content": "There are many ways to write the data on disk, but have you ever thought about what can be the most efficient way to store your data, so that you can optimize your processing effort and cost?\n\nIn my 4+ years of experience as a Data Engineer, I have seen many data enthusiasts make this common mistake of simply saving the dataframe and reading it back for use later, but what if we can optimize it somehow and save the cost of future processing? Partitioning and Bucketing are the Answer to this.\n\nIf you’re curious and want a deep dive, check out my article here:  \n[Partitioning vs Bucketing in Spark](https://medium.com/@vyasnikhil30/partitioning-vs-bucketing-in-spark-a66e2fca5fe3)\n\nShow some love if you find it helpful! ❤️",
    "author": "Status_Air9764",
    "timestamp": "2025-09-28T09:54:13",
    "url": "https://reddit.com/r/dataengineering/comments/1nstbbu/are_you_writing_your_data_right_heres_how_to_save/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ns89n4",
    "title": "dbt project blueprint",
    "content": "I've read quite a few posts and discussions in the comments about dbt and I have to say that some of the takes are a little off the mark. Since I’ve been working with it for a couple years now, I decided to put together a project showing a blueprint of how dbt core can be used for a data warehouse running on Databricks Serverless SQL. \n\nIt’s far from complete and not meant to be a full showcase of every dbt feature, but more of a realistic example of how it’s actually used in industry (or at least at my company).\n\nSome of the things it covers:\n\n* Medallion architecture\n* Data contracts enforced through schema configs and tests\n* Exposures to document downstream dependencies\n* Data tests (both generic and custom)\n* Unit tests for both models and macros\n* PR pipeline that builds into a separate target schema (My meager attempt of showing how you could write to different schemas if you had a multi-env setup)\n* Versioning to handle breaking schema changes safely\n* Aggregations in the gold/mart layer\n* Facts and dimensions in consumable models for analytics (start schema)\n\nThe repo is here if you’re interested: [https://github.com/Alex-Teodosiu/dbt-blueprint](https://github.com/Alex-Teodosiu/dbt-blueprint)\n\nI'm interested to hear how others are approaching data pipelines and warehousing. What tools or alternatives are you using? How are you using dbt Core differently? And has anyone here tried dbt Fusion yet in a professional setting? \n\nJust want to spark a conversation around best practices, paradigms, tools, pros/cons etc...\n\n",
    "author": "ActRepresentative378",
    "timestamp": "2025-09-27T15:30:32",
    "url": "https://reddit.com/r/dataengineering/comments/1ns89n4/dbt_project_blueprint/",
    "score": 91,
    "num_comments": 32,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nt0wc9",
    "title": "ArgosOS an app that lets you search your docs intelligently",
    "content": "Hey everyone, I built this indie project called ArgosOS a semantic OS, kind of like dropbox+LLM. Its a desktop app that lets you search stuff intelligently. e.g. Put all your grocery bills and find out how much you spent on milk?\n\nThe architecture is different. Instead of using a vector Database, I went with a different approach. I used a tag based solution.  \nThe process looks like this.\n\nIngestion side:\n\n1. Upload a doc and trigger ingestion agent\n2. ingestion agent calls the LLM to creates relevant tags. These tags are stored in a sqllite db with the relevant tags.\n\nQuery side:  \nRunning a query triggers two agent retrieval agent and post\\_processor agent.\n\n1. Retrieval agent processes the query with all available tags and extracts relevant tags using LLM\n2. Post processor agent searches the sqllite db to get all docs with the tags and extracts useful content.\n3. After extracting relevant content post processor agent does any math operation. In the grocery case, if it finds milk in 10 reciepets. It adds them returns result.\n\nTag based architecture seems pretty accurate for small scale use case like mine. Let me know your thoughts. Thanks",
    "author": "Dry_Mixture130",
    "timestamp": "2025-09-28T14:57:18",
    "url": "https://reddit.com/r/dataengineering/comments/1nt0wc9/argosos_an_app_that_lets_you_search_your_docs/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nsi5lo",
    "title": "Data engineer in China? (UK foreigner)",
    "content": "Hey does anyone have any experience working as a data engineer in China, as western foreigner? Job availability etc please, is it worth trying?\n\nNot looking to get rich, I just want to relocate, just hope the salary is comfortable \n\nThanks",
    "author": "Iintahlo",
    "timestamp": "2025-09-28T00:27:04",
    "url": "https://reddit.com/r/dataengineering/comments/1nsi5lo/data_engineer_in_china_uk_foreigner/",
    "score": 13,
    "num_comments": 14,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nsn2hm",
    "title": "GCP ETL doubts",
    "content": "Hi guys, I have very less experience with GCP especially in the context of building ETL pipelines (&lt; 1 yoe). So please help with below doubts:\n\nWe used Dataflow for ingestion, and Dataform for transformations and load into BQ for RDBMS data ingestion (like Postgres, MySQL etc). Custom code was written which was further templatised and provided for data ingestion. \n\nHow would dataflow handle schema drift (addition, renaming, deletion of columns from source)\n\nWhat GCP services can be used for API data ingestion (please provide simple ETL architecture)\n\nWhen would we use Dataproc \n\nHandling schema drift incase of API, Files, Tables data ingestions.\n\nThanks in Advance! ",
    "author": "FeeOk6875",
    "timestamp": "2025-09-28T05:26:55",
    "url": "https://reddit.com/r/dataengineering/comments/1nsn2hm/gcp_etl_doubts/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nsgmm5",
    "title": "Has anyone used Kedro data pipelining tool?",
    "content": "We are currently using Airbyte, which has numerous issues and frequently breaks for even straightforward tasks. I have been exploring projects which are cost-efficient and can be picked up by data engineers easily. \n\nI wanted to ask the opinion of people who are using it, and if there are any underlying issues which may not have been seen through their documentation.",
    "author": "FuzzyCraft68",
    "timestamp": "2025-09-27T22:52:02",
    "url": "https://reddit.com/r/dataengineering/comments/1nsgmm5/has_anyone_used_kedro_data_pipelining_tool/",
    "score": 3,
    "num_comments": 6,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nsmog2",
    "title": "What's the simplest gpu provider?",
    "content": "Hey,  \nlooking for the easiest way to run gpu jobs. Ideally it’s couple of clicks from cli/vs code. Not chasing the absolute cheapest, just simple + predictable pricing. eu data residency/sovereignty would be great.\n\nI use modal today, just found lyceum, pretty new, but so far looks promising (auto hardware pick, runtime estimate). Also eyeing runpod, lambda, and ovhcloud. maybe vast or paperspace?\n\nwhat’s been the least painful for you?",
    "author": "test12319",
    "timestamp": "2025-09-28T05:07:17",
    "url": "https://reddit.com/r/dataengineering/comments/1nsmog2/whats_the_simplest_gpu_provider/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ns82i7",
    "title": "Is it better to build a data lake with historical backfill already in source folders or to create the pipeline steps first with a single file then ingest historical data later",
    "content": "I am using AWS services here as examples because that is what I am familiar with. I need two glue crawlers for two database tables, one for raw, one for transformed. I just don't know if my initial raw crawl should include every single file I can currently put it in to the directory or use a single file as having a representative schema (there is no schema evolution for this data) and process the backfill data with thousands of API requests",
    "author": "sumant28",
    "timestamp": "2025-09-27T15:21:33",
    "url": "https://reddit.com/r/dataengineering/comments/1ns82i7/is_it_better_to_build_a_data_lake_with_historical/",
    "score": 10,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nseha7",
    "title": "Where to download Databricks summit 2025 slides pdf",
    "content": "I want to systematically learn the slides from Databricks Summit 2025. Does anyone know where I can access them? ",
    "author": "databend-cloud",
    "timestamp": "2025-09-27T20:47:37",
    "url": "https://reddit.com/r/dataengineering/comments/1nseha7/where_to_download_databricks_summit_2025_slides/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nsj0tx",
    "title": "On-Call Rotation for a DE?",
    "content": "I've recently got an offer for a DE position in a mid-sized product company (Europe). The offer is nice, the team seems strong, so I would love to join. The only doubt I have is their on-call system, where engineers rotate monitoring the pipelines (obviously there is logging/alerting in place). They've told me they would not put me solo in the first 6-9 months. I don't have experience being on-call; I've only heard about it from YouTube videos about Big Tech work and that's it. In the place I am currently employed, we are kind of reacting after something bad happened with a delay - for example, if a pipeline failed on Saturday, we would only check it on Monday.\n\nAnd I guess the other point, since I am already making this post - how hard is DBT? I've never worked with it, but they use it in combination with Airflow as the main ETL tool.\n\nAny help is appreciated, thanks!",
    "author": "Fiarmis",
    "timestamp": "2025-09-28T01:23:51",
    "url": "https://reddit.com/r/dataengineering/comments/1nsj0tx/oncall_rotation_for_a_de/",
    "score": 3,
    "num_comments": 15,
    "upvote_ratio": 0.61,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nrnvv3",
    "title": "Have you ever build good Data Warehouse?",
    "content": "- not breaking every day\n- meaningful data quality tests\n- code was po well written (efficient) from DB perspective \n- well documented \n- was bringing real business value\n\nI am DE for 5 years - worked in 5 companies. And every time I was contributing to something that was already build for at least 2 years except one company where we build everything from scratch. And each time I had this feeling that everything is glued together with tape and will that everything will be all right.\n\nThere was one project that was build from scratch where Team Lead was one of best developers I ever know (enforced standards, PR and Code Reviews was standard procedure), all documented, all guys were seniors with 8+ years of experience. Team Lead also convinced Stake holders that we need to rebuild all from scratch after external company was building it for 2 years and left some code that was garbage.\n\nIn all other companies I felt that we are should start by refactor. I would not trust this data to plan groceries, all calculate personal finances not saying about business decisions of multi bilion companies…\n\nI would love to crack it how to make couple of developers build together good product that can be called finished.\n\nWhat where your success of failure stores…",
    "author": "Certain_Mix4668",
    "timestamp": "2025-09-26T23:16:27",
    "url": "https://reddit.com/r/dataengineering/comments/1nrnvv3/have_you_ever_build_good_data_warehouse/",
    "score": 89,
    "num_comments": 38,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nrpqaq",
    "title": "Low cost hobby project",
    "content": "I work in a small company where myself and a colleague are essentially the only ones doing data engineering.   Recently she has got a new job.   We’re good friends as well as colleagues and really enjoy writing code together, so we’ve agreed to start a “hobby project” in our own time.   Not looking to create a product as such, just wanting to try out stuff we haven’t worked with before in case it proves useful for our future career direction.\n\nWe’re particularly looking to work with data and platforms that we don’t normally encounter at work.   We are largely AWS based so we have lots of experience in things like Glue, Athena, Redshift etc but are keen to try something else.  Both of us also have great Python skills including polars/pandas and all the usual stuff.   However we don’t have much experience in orchestration tools like Airflow as most of our pipelines are just orchestrated in Azure DevOos.\n\nObviously with us funding any costs ourselves out of pocket, keeping the ongoing spend low is a priority.  Any recommendations for any free/low cost platforms we can use. - eg I’m aware there’s a free tier for Databricks.  Also any good “big” public datasets  to play with would be appreciated.   Thanks!",
    "author": "dataisok",
    "timestamp": "2025-09-27T01:13:11",
    "url": "https://reddit.com/r/dataengineering/comments/1nrpqaq/low_cost_hobby_project/",
    "score": 31,
    "num_comments": 8,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nrpb0q",
    "title": "Geospatial python library",
    "content": "Anyone have experience with city2graph (not my project, I will not promote) for converting geospatial datasets (they usually come in geography or geometry formats, with various shapes like polygons or lines or point clouds) into actual graphs that graph software can do things with? Used to work on geospatial stuff, so this is quite interesting to me. It's hard math and lots of linear algebra. Wonder if this Python library is being used by anyone here.",
    "author": "datancoffee",
    "timestamp": "2025-09-27T00:45:43",
    "url": "https://reddit.com/r/dataengineering/comments/1nrpb0q/geospatial_python_library/",
    "score": 14,
    "num_comments": 18,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1ns7m0m",
    "title": "Has a European company or non-Chinese corporation used Alibaba Cloud or Tencent Cloud?Are they secure and reliable for westerners? Does their support speak English?",
    "content": "So im looking at cloud computing services to run VMs and I found out Alibaba and Tencent has cloud computing services.Also what about Baidu Cloud?",
    "author": "Additional-Pick-3596",
    "timestamp": "2025-09-27T15:01:19",
    "url": "https://reddit.com/r/dataengineering/comments/1ns7m0m/has_a_european_company_or_nonchinese_corporation/",
    "score": 1,
    "num_comments": 5,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nro937",
    "title": "Which are the best open source database engineering techstack to process huge data volume ?",
    "content": "Wondering in Data Engineering stream which are the open-source tech stack in terms of \nData base, \nProgramming language supporting processing huge data volume,\nReporting \n\nI am thinking loud on \nVector databases-\n\n Open source \nMOJO programming language for speed and processing huge data volume \nAny AI backed open source tools \n\nAny thoughts on better ways of tech stack ?\n",
    "author": "moldov-w",
    "timestamp": "2025-09-26T23:39:16",
    "url": "https://reddit.com/r/dataengineering/comments/1nro937/which_are_the_best_open_source_database/",
    "score": 10,
    "num_comments": 47,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nr9ciu",
    "title": "We built a new geospatial DataFrame library called SedonaDB",
    "content": "SedonaDB is a fast geospatial query engine that is written in Rust.\n\nSedonaDB has Python/R/SQL APIs, always maintains the Coordinate Reference System, is interoperable with GeoPandas, and is blazing fast for spatial queries.  \n\nThere are already excellent geospatial DataFrame libraries/engines, such as PostGIS, DuckDB Spatial, and GeoPandas.  All of those libraries have great use cases, but SedonaDB fills in some gaps.  It’s not always an either/or decision with technology.  You can easily use SedonaDB to speed up a pipeline with a slow GeoPandas join, for example.\n\nCheck out [the release blog](https://sedona.apache.org/latest/blog/2025/09/24/introducing-sedonadb-a-single-node-analytical-database-engine-with-geospatial-as-a-first-class-citizen/) to learn more!\n\nAnother post on why we decided to build SedonaDB in Rust is coming soon.\n\n",
    "author": "MrPowersAAHHH",
    "timestamp": "2025-09-26T11:41:40",
    "url": "https://reddit.com/r/dataengineering/comments/1nr9ciu/we_built_a_new_geospatial_dataframe_library/",
    "score": 56,
    "num_comments": 8,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqrfv9",
    "title": "Reality Nowadays…",
    "content": "Chef with expired ingredients ",
    "author": "Background_Artist801",
    "timestamp": "2025-09-25T20:56:33",
    "url": "https://reddit.com/r/dataengineering/comments/1nqrfv9/reality_nowadays/",
    "score": 783,
    "num_comments": 19,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nrrhbh",
    "title": "Best Course Resources for Part-Time Learning Data Engg",
    "content": "TL;DR\nI know enough about Python and SQL upto Joins but no standard database knowledge all through Chatgpt/Gemini and screwing up with some data that was handed to me. I want to learn more about other tools as well as using cloud. Have no industry experience per se and would love some advice on how to get to a level of building reliable pipelines for real world use. I havent used a single Apache tool, just theoretical knowledge and YT. Thats how bad it is.\n\n\nHi everyone, \n\nIm ngl this thread alone has taught me so much for the work I've done. Im a self taught programmer (~4 years now). I started off with Python had absolutely no idea about SQL (still kinda don't). \n\nWhen I started to learn programming (~2021) I had just finished uni with Bio degree and I began to take keen interest into it as my thesis was based on computational simulation of binding molecules and I was heavily limited by the software GUI which my lecturer showed me could have been much more efficient using Python. Hence, began my journey. I started off learning HTML, CSS and JS (that alone killed my interest for a while), but then I stumbled onto Python. Keep in mind late 2020 to early 2021 had a massive hype of online ML courses and thats how I forayed into the world of Python.\n\nGiven its high-level and massive community made it easier to understand a lot of concepts and it has a library for the most random shit you'd wanna not code yourself. However, I have realized my biggest limiting factor was:\n\n1. Tutorial Hell\n2. Never knowing if I know enough? (Primarily because of not having any industry experience with SQL and Git, as well as QA with unit testing/TDD. These were just concepts I've about).\n\nTo put it frankly I was/am extremely underconfident of being able to build reliable code that can be used in the real world. \n\nBut I have a very stubborn attitude and for better or for worse that has pushed me. My Python knowledge and my subject expertise gave me an advantage to quickly understand high level ML/DL topics to train and experiment with models, but I always enjoyed data engineering i.e., building the pipelines that feed the right data to AI.\n\nBut I constantly feel like I am lacking. I started small[ish] since last December. MY mom runs a small cafe but we struggled to keep track of financials. Few reasons being, barebones POS system, with a basic analytics dashboard, handwritten inventory tracking, no accurate insights from sales through delivery partners. \nI initially thought I could just export the excel files and clean and analyze it in Python. But there were a lot of issues and so I picked up Postgres (open-source few!) with the basics (upto Joins, I use CTEs cause for the life of me I don't see myself using views etc.). The data totals up i.e., from all data sources to ~100k rows. I used sqlalchemy to pushed the cleaning datasets to a postgres database and I used duckdb for in memory transformations to build the fact tables (3 of them for the orders, items, and added financial expenses).\n\nThis was way more tedious than Ive explained. Primarily due to a lot of issues like duplicated invoice no.s (the POS system was restarted this year on the advice of my mom, but thats another story for another day), basically no definitive primary key (created a composite key with the date), the delivery partners order ids are not shown in the same report as the master report, and so on. Without getting much into detail,\n\n\nHere is my current situation and why I have asked this question on this thread:\n\nI was using Gemini to help me structure the Python code I wrote in my notebook and write the SQL queries (only to realize it was not upto the mark so I pretty much wrote 70% of the CTE myself) and used duckdb engine to query the data from the staging tables directly into a fact table. But I learnt all these terminologies because of Gemini. I just didnt share any financial data with it which is probably why it gave me the garbage[ish] query. But the point being I learnt that. I was setting the data types configs using Pandas and I didn't create any tables in SQL it was directly mapped by SQLalchemy.\n\nThen I came across dimension tables, data marts, etc. I feel like I am damn close and I can pick this up but the learning feels extremely ad hoc and I keep doubting my existing code infrastructure a lot.\n\nSo my question is should I continue to learn like this (making a ridiculously insane amount of mistake only to realize there are existing theories on how to model data, transform data, etc., later on). Or is it wise to actually take on a certification course? I also have zero actual cloud knowledge (have just tinkered with BigQuery on Googles Cloud skill boos courses)\n\nAs much as it frustrates me I love seeing data coming together like to provide useful, viable information as an output. But I feel like my knowledge is my limitation.\n\nI would love to hear your inputs, personal experiences, book reccos (I am a better visual learner tbh). Most of what I can find have very basic intros to Python, SQL, etc. and yes I can always be better with my basics but if I start off like and get bored I know I am going to slack off and never finish the course.\n\nI think weirdly I am asking people to rate my level (cant believe im seeking validation on a data engg thread) and suggest any good learning sources. \n\nFYI If you have read it through from the start till here. Thank you and I hope all your dreams come true! Cuz you're a legend!",
    "author": "reficul97",
    "timestamp": "2025-09-27T03:06:21",
    "url": "https://reddit.com/r/dataengineering/comments/1nrrhbh/best_course_resources_for_parttime_learning_data/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nr3w6v",
    "title": "My company didn't use industry standard tools and I feel I'm way behind",
    "content": "My company was pretty disorganized and didn't really do standardization. We trained on stuff like Microsoft Azure and then just...didn't really use it. \n\nNow I'm unemployed (well, I do Lyft, so self employed technically) and I feel like I'm fucked in every meeting looking for a job (the i word apparently isn't allowed). Thinking of just overstating how much we used Microsoft Azure so I can kinda creep the experience in. I got certified on it, so I kinda know the ins and outs of it. We just didn't do anything with it - we just stuck to 100% manual work and SQL. ",
    "author": "Calm_Description_866",
    "timestamp": "2025-09-26T08:10:18",
    "url": "https://reddit.com/r/dataengineering/comments/1nr3w6v/my_company_didnt_use_industry_standard_tools_and/",
    "score": 77,
    "num_comments": 23,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nrptsl",
    "title": "Looking for advice on scaling SEC data app (10 rps limit)",
    "content": "I’ve built a financial app that pulls company financials from the SEC—nearly verbatim (a few tags can be missing)—covering the XBRL era (2009/2010 to present). I’m launching a site to show detailed quarterly and annual statements.\n\nConstraint: The SEC allows ~10 requests/second per IP, so I’m worried I can only support a few hundred concurrent users if I fetch on demand.\n\nGoal: Scale beyond that without blasting the SEC and without storing/downloading the entire corpus.\n\nWhat’s the best approach to:\n\t•\tstay under ~10 rps to the SEC,\n\t•\tkeep storage minimal, and\n\t•\tstill serve fast, detailed statements to lots of users?\n\nAny proven patterns (caching, precomputed aggregates, CDN, etc.) you’d recommend?",
    "author": "Ok-Access5317",
    "timestamp": "2025-09-27T01:19:27",
    "url": "https://reddit.com/r/dataengineering/comments/1nrptsl/looking_for_advice_on_scaling_sec_data_app_10_rps/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nrpe1q",
    "title": "Looking for a community for SAP Datasphere",
    "content": "Hey everyone,\n\nI’m planning to start learning SAP Datasphere, but so far all I’ve found are YouTube videos. I’m looking for any PDFs, docs, or other files that could help me study.\n\nAlso, does anyone know if there’s a Discord server where people talk about SAP Datasphere? Would love to join and learn with others.\n\n\n\n",
    "author": "akdVortex",
    "timestamp": "2025-09-27T00:51:09",
    "url": "https://reddit.com/r/dataengineering/comments/1nrpe1q/looking_for_a_community_for_sap_datasphere/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nrh28u",
    "title": "Am I overreacting?",
    "content": "This seems like a nightmare and is stressing me out. I could use some advice.\n\nOur head of CS manages all of our clients. She has used this huge, slow, unvalidated query that I wrote for her to create reports with AI. She always wants stuff added to it so it keeps growing. She manually downloads data from customers into csv. AI wrote python to make html reports from csv.\n\nShe’s made good reports for customers but it all lives entirely outside of our app. Shes having issues making it work for all clients, so they want me to get involved.\n\nMy thinking is to let her do her thing, and then once designed, build the reports into our app. With the goal being:\n1) Using simple, validated functions/queries (that we spent a lot of time making test cases to validate) and not this big ass query\n2) Each report component is modularized and easily reusable in other reports\n3) Generating a report is all obviously automated. \n\nNow, they messaged me today about providing estimates on delivering something similar to the app’s reporting structure for her to use offline, just generating the html from csv, using the monster query. With the goal that:\n\n1) She can continue to craft reports with AI having all data points readily available \n2) The reports can easily be plugged into the app’s reporting infrastructure \n\nAnother idea that they thought of that I didn’t think much of at first was to just copy her AI generated html into the app so it has a place to live for clients.\n\nMy biggest concerns are the AI not understanding our schema, what is available to use as far as validated functions, etc. Having to manage stuff offline vs in the app. Using this unnecessary big ass query. Having to work with what the AI produces.\n\nShould I push going full AI route and not dealing with the app at all? Or try to keep the AI just for design and lean heavier on the app side?\n\nAm I overreacting?  Please help. ",
    "author": "CEOnnor",
    "timestamp": "2025-09-26T17:07:34",
    "url": "https://reddit.com/r/dataengineering/comments/1nrh28u/am_i_overreacting/",
    "score": 7,
    "num_comments": 14,
    "upvote_ratio": 0.74,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nrcvuf",
    "title": "The Ultimate Guide to Open Table Formats: Iceberg, Delta Lake, Hudi, Paimon, and DuckLake",
    "content": "We’ll start beginner-friendly, clarifying what a table format is and why it’s essential, then progressively dive into expert-level topics: metadata internals (snapshots, logs, manifests, LSM levels), row-level change strategies (COW, MOR, delete vectors), performance trade-offs, ecosystem support (Spark, Flink, Trino/Presto, DuckDB, warehouses), and adoption trends you should factor into your roadmap.\n\nBy the end, you’ll have a practical mental model to choose the right format for your workloads, whether you’re optimizing petabyte-scale analytics, enabling near-real-time CDC, or simplifying your metadata layer for developer velocity.\n\n",
    "author": "AMDataLake",
    "timestamp": "2025-09-26T14:00:40",
    "url": "https://reddit.com/r/dataengineering/comments/1nrcvuf/the_ultimate_guide_to_open_table_formats_iceberg/",
    "score": 16,
    "num_comments": 2,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqzudt",
    "title": "my freebies haul from big data ldn! (peep the stickers)",
    "content": "honestly i could've gotten more shirts but it was a pain to lug it all around ",
    "author": "sajiaoo",
    "timestamp": "2025-09-26T05:19:42",
    "url": "https://reddit.com/r/dataengineering/comments/1nqzudt/my_freebies_haul_from_big_data_ldn_peep_the/",
    "score": 40,
    "num_comments": 2,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nrit7u",
    "title": "Polaris Catalog",
    "content": "Are you familiar with any companies using or adopting Apache Polaris catalog? \n\nIt seems promising, but I haven’t seen much to indicate that there is any adoption currently happening. ",
    "author": "CDCheerios",
    "timestamp": "2025-09-26T18:35:06",
    "url": "https://reddit.com/r/dataengineering/comments/1nrit7u/polaris_catalog/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqsuyj",
    "title": "In way over my head, feel like a fraud",
    "content": "My career has definitely taken a weird set of turns over the last few years to get me to end up where I have today. Initially, I started off building Tableau dashboards with datasets handed to me and things were good. After a while, I picked up Alteryx to better develop datasets meant specifically for Tableau reports. All good, no problems there. Eventually, I got hired at by a company to keep doing those two things, building reports and the workflows to support them.\n\n\nNow this company has had a lot of vendors in the past which means its data architecture and pipelines have spaghettied out of control even before I arrived. The company isn't a tech company, and there are a lot of boomers in it who can barely work Excel. It still makes a lot of money though, since it's primarily in the retail/sales space of luxury items. Once I took over, I've tried to do my best to keep things organized but it's a real mess. I should note that it's just me that manages these pipelines and databases, no one else really touches them. If there's ever a data question, they just ask me to figure it out.\n\n\nFast forward to earlier this year, and my bosses tell me that they want to me explore Azure, the cloud, and see if we can move our analytics ahead. I have spent hours researching and trying to learn as much as  I can. I created a Databricks instance and started writing notebooks to recreate some of the ETL processes that exist on our on-prem servers. I've definitely gotten more comfortable with writing code, databricks in general, and slowly understanding that world more, but the more I read online the more I feel like a total hack and fraud.\n\n\nI don't do anything with Git, I vaguely know that it's meant for version control but nothing past that. CI/CD is foreign to me. Unit tests, what are those? There are so many terms that I see in this subreddit that feel like complete jibberish to me, and I'm totally disheartened. How can I possibly bridge this gap? I feel like they gave me keys to a Ferrari and I've just been driving a Vespa up to this point. I do understand the concepts of data modeling, dim and fact tables, prod and dev, but I've never learned any formal testing. I constantly run into issues of a table updating incorrectly, or the numbers not matching between two reports, etc and I just fly by the seat of my pants. We don't have one source of truth or anything like that, the requirements constantly shift, the stakeholders constantly jump from one project to the other, it's all a big whirlwind.\n\n\nCan anyone else sympathize? What should I do? Hiring a vendor to come and teach me isn't an option, and I can't just quit to find something else, the market is terrible and I have another baby on the way. Like honestly, what the fuck do I do?",
    "author": "SoloArtist91",
    "timestamp": "2025-09-25T22:16:12",
    "url": "https://reddit.com/r/dataengineering/comments/1nqsuyj/in_way_over_my_head_feel_like_a_fraud/",
    "score": 88,
    "num_comments": 35,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqt4s0",
    "title": "Unemployment thoughts",
    "content": "\nI had been a good Data Engineer back in India. The day after finishing my final bachelor’s exam, I joined a big tech company where I got the opportunity to work on Azure, SQL, and Power BI. I gained a lot of experience there. I used to work 16 hours a day with a tight schedule, but my productivity never dropped. However, as we all know, freshers usually get paid peanuts for the work they do.\n\nI wanted to complete one year there, and then I shifted to a startup company with a 100% hike, though with the same workload. At the startup, I got the opportunity to handle a Snowflake migration project, which made me really happy as Snowflake was booming at that time. I worked there for 1.3 years.\n\nWith the money and experience I gained, I achieved my dream of coming to the USA. I resigned, but since the project had a lot of dependencies, they requested me to continue for 3 more months, which I was happy to do.\nAnd by the god grace i was also worked as GA for 2 semester while doing my masters.\n\nNow, I have completed my master’s degree and am looking for a job, but it feels like nobody cares about my 3 years of experience in India. Most of my applications are directly rejected. It’s been 9 months, and I feel like I’m losing hope and even some of my knowledge and skills, as I keep applying for hundreds of jobs daily.\n\nAt this point, I want to restart, but I’m missing my consistency. I’m not sure whether I should completely focus on Azure, Python, Snowflake, or something else. Maybe I’m doing something wrong. \n",
    "author": "charan_redit",
    "timestamp": "2025-09-25T22:32:30",
    "url": "https://reddit.com/r/dataengineering/comments/1nqt4s0/unemployment_thoughts/",
    "score": 50,
    "num_comments": 36,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqvkbr",
    "title": "Any good ways to make a 300+ page PDF AI readable?",
    "content": "Hi, this seems like the place to ask this so sorry if it is not.\n\nMy company publishes a lot of PDFs on its website, many of which are quite large (the example use case i was given is 378 pages). I have been tasked with identifying methods to try and make these files more readable as we are a regulator and want people to get accurate information when they ask GenAI about our rules.\n\nBasically I want to try and make our PDFs as readable as possible for any GenAI our audience chucks their PDF into, without moving from PDF as we dont want the document to be easily editable.\n\nI have already found some methods like using accessibility tags that should help, but I imagine 300 pages will still be a stretch for most tools.\n\nMy boss currently doesn't want to edit the website if we can avoid it to avoid having to work with our web developer contractor who they apparently hate for some reason, so adding metadata on the website end is out for the moment.\n\nIs there any method that I can use to sneak in the full plaintext of the file where an AI can consistently find it? Or have any of you come across other methods that can make PDFs more readable?\n\nApologies if this has been asked before but I can only find questions from the opposite side of reading unstructured PDFs.",
    "author": "drwicksy",
    "timestamp": "2025-09-26T01:06:42",
    "url": "https://reddit.com/r/dataengineering/comments/1nqvkbr/any_good_ways_to_make_a_300_page_pdf_ai_readable/",
    "score": 28,
    "num_comments": 20,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nquwi3",
    "title": "How to deal with non engineer people",
    "content": "Hi, maybe some of you have been in a similar situation.\n\nI am working with a team coming from a university background. They have never worked with databases, and I was hired as a data engineer to support them. My approach was to design and build a database for their project.\n\nThe project goal is to run a model more than 3,000 times with different setups. I designed an architecture to store each setup, so results can be validated later and shared across departments. The company itself is only at the very early stages of building a data warehouse—there is not yet much awareness or culture around data-driven processes.\n\nThe challenge: every meeting feels like a struggle. From their perspective, they are unsure whether a database is necessary and would prefer to save each run in a separate file instead. But I cannot imagine handling 3,000 separate files—and if reruns are required, this could easily grow to 30,000 files, which would be impossible to manage effectively.\n\nOn top of that, they want to execute all runs over 30 days straight, without using any workflow orchestration tools like Airflow. To me, this feels unmanageable and unsustainable. Right now, my only thought is to let them experience it themselves before they see the need for a proper solution. What are your thoughts? How would you deal with it?",
    "author": "sundowner_99",
    "timestamp": "2025-09-26T00:23:09",
    "url": "https://reddit.com/r/dataengineering/comments/1nquwi3/how_to_deal_with_non_engineer_people/",
    "score": 31,
    "num_comments": 39,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqu1v9",
    "title": "Hive or Iceberg for production ?",
    "content": "Hey everyone,\n\nI’ve been working on a use case at the company I’m with (a mid-sized food delivery service) and right now we’re still on Apache Hive. But honestly, looking at where the industry is going, it feels like a no-brainer that we’ll be moving toward **Apache Iceberg** sooner or later. The adoption is hiuge  and has a great community imo.\n\nBefore we fully pitch this switch internally though, I’d love to hear from people still using Hive how has the cost difference been for you? Has Hive really been cost-effective in the long run, or do you also feel the pull toward Iceberg? We’re also open to hearing about any tools or approaches that helped you with migration if you’ve gone through it already.\n\nI came across this blog as were shared by perplexity that compared Hive and Iceberg and found it pretty useful :   \n  \n[https://olake.io/blog/apache-iceberg-hive-comparison](https://olake.io/blog/apache-iceberg-hive-comparison).   \n[https://www.starburst.io/blog/hive-vs-iceberg/](https://www.starburst.io/blog/hive-vs-iceberg/)  \n[https://olake.io/iceberg/hive-partitioning-vs-iceberg-partitioning](https://olake.io/iceberg/hive-partitioning-vs-iceberg-partitioning)  \n  \nSharing it here in case others are in the same boat.\n\nCurious to hear your experiences are you still making Hive work, or already making the shift to Iceberg?",
    "author": "DevWithIt",
    "timestamp": "2025-09-25T23:28:28",
    "url": "https://reddit.com/r/dataengineering/comments/1nqu1v9/hive_or_iceberg_for_production/",
    "score": 11,
    "num_comments": 18,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqw5c7",
    "title": "How to replicate/mirror OLD as400 database to latest SQL databases or any compatible databases",
    "content": "We have an old as400 database which is very unresponsive and slow for any Data extraction.\nIs there any way to mirror old as400 database so that we can extract data from mirrored database.",
    "author": "moldov-w",
    "timestamp": "2025-09-26T01:45:16",
    "url": "https://reddit.com/r/dataengineering/comments/1nqw5c7/how_to_replicatemirror_old_as400_database_to/",
    "score": 6,
    "num_comments": 8,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqdtfk",
    "title": "Cloudflare announces Data Platform: ingest, store, and query data directly on Cloudflare",
    "content": "",
    "author": "vaibeslop",
    "timestamp": "2025-09-25T10:59:53",
    "url": "https://reddit.com/r/dataengineering/comments/1nqdtfk/cloudflare_announces_data_platform_ingest_store/",
    "score": 88,
    "num_comments": 35,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqj6qk",
    "title": "Fastest way to generate surrogate keys in Delta table with billions of rows?",
    "content": "Hello fellow data engineers,\n\nI’m working with a Delta table that has billions of rows and I need to generate surrogate keys efficiently. Here’s what I’ve tried so far:\n\t1.\tROW_NUMBER() – works, but takes hours at this scale.\n\t2.\tIdentity column in DDL – but I see gaps in the sequence.\n\t3.\tmonotonically_increasing_id() – also results in gaps (and maybe I’m misspelling it).\n\nMy requirement: a fast way to generate sequential surrogate keys with no gaps for very large datasets.\n\nHas anyone found a better/faster approach for this at scale?\n\nThanks in advance! 🙏\n",
    "author": "Numerous-Round-8373",
    "timestamp": "2025-09-25T14:28:39",
    "url": "https://reddit.com/r/dataengineering/comments/1nqj6qk/fastest_way_to_generate_surrogate_keys_in_delta/",
    "score": 35,
    "num_comments": 19,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqu5kx",
    "title": "The Evolution of Search - A Brief History of Information Retrieval",
    "content": "",
    "author": "kushalgoenka",
    "timestamp": "2025-09-25T23:34:46",
    "url": "https://reddit.com/r/dataengineering/comments/1nqu5kx/the_evolution_of_search_a_brief_history_of/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq9qx7",
    "title": "Is this a poor onboarding process or a sign I’m not suited for technical work?",
    "content": "To add some background, this is my second data related role, I am two months into a new data migration role that is heavily SQL-based, with an onboarding process that's expected to last three months. So far, I’ve encountered several challenges that have made it difficult to get fully up to speed. Documentation is limited and inconsistent, with some scripts containing comments while others are over a thousand lines without any context. Communication is also spread across multiple messaging platforms, which makes it difficult to identify a single source of truth or establish consistent channels of collaboration.\n\nIn addition, I have not yet had the opportunity to shadow a full migration, which has limited my ability to see how the process comes together end to end. Team responsiveness has been inconsistent, and despite several requests to connect, I have had minimal interaction with my manager. Altogether, these factors have made onboarding less structured than anticipated and have slowed my ability to contribute at the level I would like.\n\nI’ve started applying again, but my question to anyone reading is whether this experience seems like an outlier or if it is more typical of the field, in which case I may need to adjust my expectations.",
    "author": "Oh_reaaaally",
    "timestamp": "2025-09-25T08:25:03",
    "url": "https://reddit.com/r/dataengineering/comments/1nq9qx7/is_this_a_poor_onboarding_process_or_a_sign_im/",
    "score": 42,
    "num_comments": 44,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqc347",
    "title": "Are there companies really using DOMO??!",
    "content": "Recently been freelancing for a big company, and they are using DOMO for ETL purposes .. Probably the worse tool I have ever used, it's an Aliexpress version of Dataiku ... \n\nAnyone else using it ? Why would anyone choose this ? I don;t understand",
    "author": "jdaksparro",
    "timestamp": "2025-09-25T09:54:30",
    "url": "https://reddit.com/r/dataengineering/comments/1nqc347/are_there_companies_really_using_domo/",
    "score": 29,
    "num_comments": 45,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqnqin",
    "title": "Kafka BQ sink connector multiple tables from MySQL",
    "content": "I am tasked to move data from MySQL into BigQuery, so far, it's just 3 tables, well, when I try adding the parameters\n\n    upsertEnabled: true\n    deleteEnabled: true\n\nerrors out to\n\n    kafkaKeyFieldName must be specified when upsertEnabled is set to true kafkaKeyFieldName must be specified when deleteEnabled is set to true\n\nI do not have a single key for all my tables. I indeed have pk per each, any suggestions or someone with experience have had this issue bef? An easy solution would be to create a connector per table, but I believe that will not scale well if i plan to add 100 more tables, am I just left to read off each topic using something like spark, dlt or bytewax to do the upserts myself into BQ?",
    "author": "josejo9423",
    "timestamp": "2025-09-25T17:50:25",
    "url": "https://reddit.com/r/dataengineering/comments/1nqnqin/kafka_bq_sink_connector_multiple_tables_from_mysql/",
    "score": 4,
    "num_comments": 15,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqy2cl",
    "title": "Feedback Request: Automating PDF Reporting in Data Pipelines",
    "content": "In many projects I’ve seen, PDF reporting is still stitched together with ad-hoc scripts or legacy tools. It often slows down the pipeline and adds fragile steps at the very end.\n\nWe’ve built **CxReports**, a production platform that automates PDF generation from data sources in a more governed way. It’s already being used in compliance-heavy environments, but we’d like feedback from this community to understand how it fits (or doesn’t fit) into real data engineering workflows.\n\n* Where do PDFs show up in your pipelines, and what’s painful about that step?  \n* Do current approaches introduce overhead or limit scalability?  \n* What would “good” reporting automation look like in the context of ETL/ELT?  \n\nWe’ll share what we’ve learned so far, but more importantly, we want to hear how you solve it today. Your input helps us make sure CxReports stays relevant to actual engineering practice, not just theoretical use cases.\n\n",
    "author": "Carageavk",
    "timestamp": "2025-09-26T03:45:42",
    "url": "https://reddit.com/r/dataengineering/comments/1nqy2cl/feedback_request_automating_pdf_reporting_in_data/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq4g9r",
    "title": "Choosing Between Two Offers - Growth vs Stability",
    "content": "Hi everyone!\n\nI'm a data engineer with a couple years of experience, mostly with enterprise dwh and ETL, and I have two offers on the table for roughly the same compensation. Looking for community input on which would be better for long-term career growth:\n\n**Company A - Enterprise Data Platform company (PE-owned, $1B+ revenue, 5000+ employees)**\n\n* **Role**: Building internal data warehouse for business operations\n* **Tech stack**: Hadoop ecosystem (Spark, Hive, Kafka), SQL-heavy, HDFS/Parquet/Kudu\n* **Focus**: Internal analytics, ETL pipelines, supporting business teams\n* **Environment**: Stable, Fortune 500 clients, traditional enterprise\n* Working on company's own data infrastructure, not customer-facing\n* Good Work-life balance, nice people, relaxed work-ethic\n\n**Company B - Product company (\\~500 employees)**\n\n* **Role**: Building customer-facing data platform (remote, EU-based)\n* **Tech stack**: Cloud platforms (Snowflake/BigQuery/Redshift), Python/Scala, Spark, Kafka, real-time streaming\n* **Focus**: ETL/ELT pipelines, data validation, lineage tracking for fraud detection platform\n* **Environment**: Fast-growth, 900+ real-time signals\n* Working on core platform that thousands of companies use\n* Worse work-life balance, higher pressure work-ethic\n\n**Key Differences I'm Weighing:**\n\n* Internal tooling (Company A) vs customer-facing platform (Company B)\n* On-premise/Hadoop focus vs cloud-native architecture\n* Enterprise stability vs scale-up growth\n* Supporting business teams vs building product features\n\n**My considerations:**\n\n* Interested in international opportunities in 2-3 years (due to being in a post-soviet economy) maybe possible with Company A\n* Want to develop modern, transferable data engineering skills\n* Wondering if internal data team experience or platform engineering is more valuable in NA region?\n\nWhat would you choose and why?\n\nParticularly interested in hearing from people who've worked in both internal data teams and platform/product companies. Is it more stressful but better for learning?\n\nThanks!",
    "author": "NightL4",
    "timestamp": "2025-09-25T04:41:22",
    "url": "https://reddit.com/r/dataengineering/comments/1nq4g9r/choosing_between_two_offers_growth_vs_stability/",
    "score": 27,
    "num_comments": 27,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq6ceg",
    "title": "From your experience, how do you monitor data quality in big data environnement.",
    "content": "Hello, so I'm curious to know what tools or processes you guys use in a big data environment to check data quality. Usually when using spark, we just implement the checks before storing the dataframes and logging results to Elastic, etc.\nI did some testing with PyDeequ and Spark; Know about Griffin but never used it. \n\nHow do you guys handle that part? What's your workflow or architecture for data quality monitoring?",
    "author": "Man_InTheMirror",
    "timestamp": "2025-09-25T06:09:48",
    "url": "https://reddit.com/r/dataengineering/comments/1nq6ceg/from_your_experience_how_do_you_monitor_data/",
    "score": 18,
    "num_comments": 13,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqrnpe",
    "title": "Iceberg based Datalake project vs a mature Data streaming service",
    "content": "I’m having to decide between two companies where I have option to choose projects between Iceberg based data lake(Apple) vs Streaming service based on Flink (mid scale company)\nWhat do you think would be better for a data engineering career? \nI do come from a data engineering background and have used Iceberg recently.\n\nLet’s keep pays scale out of scope.",
    "author": "jay-lamba",
    "timestamp": "2025-09-25T21:07:52",
    "url": "https://reddit.com/r/dataengineering/comments/1nqrnpe/iceberg_based_datalake_project_vs_a_mature_data/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqd5hq",
    "title": "Does DLThub support OpenLineage out of the box?",
    "content": "Hi 👋\n\ndoes DLThub natively generate OpenLineage events? I couldn’t find anything explicit in the docs.\n\nIf not, has anyone here tried implementing OpenLineage facets with DLThub? Would love to hear about your setup, gotchas, or any lessons learned.\n\nI’m looking at DLThub for orchestrating some pipelines and want to make sure I can plug into an existing data observability stack without reinventing the wheel.\n\nThanks in advance 🙏\n",
    "author": "Aggressive-Practice3",
    "timestamp": "2025-09-25T10:34:47",
    "url": "https://reddit.com/r/dataengineering/comments/1nqd5hq/does_dlthub_support_openlineage_out_of_the_box/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nplp34",
    "title": "How do I go from a code junkie to answering questions like these as a junior?",
    "content": "Code junkie -&gt; I am annoyingly good at coding up whatever ( be it Pyspark or SQL )\n\nIn my job I don't think I will get exposure to stuff like this even if I stay here 10 years( I have 1 YOE currently in a SBC)",
    "author": "Potential_Loss6978",
    "timestamp": "2025-09-24T12:33:58",
    "url": "https://reddit.com/r/dataengineering/comments/1nplp34/how_do_i_go_from_a_code_junkie_to_answering/",
    "score": 310,
    "num_comments": 107,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqddyr",
    "title": "Master SQL Aggregations &amp; Window Functions - A Practical Guide",
    "content": "If you’re new to SQL or want to get more confident with **Aggregations and Window functions,** this guide is for you.  \n\nhttps://preview.redd.it/q9z7s2clkcrf1.png?width=668&amp;format=png&amp;auto=webp&amp;s=87cd2fb311f5cebb699f564182f7baa26f409718\n\nInside, you’ll learn:  \n\n\\- How to use **COUNT(), SUM(), AVG(), STRING\\_AGG()** with simple examples  \n\n\\- **GROUP BY** tricks like **ROLLUP, CUBE, GROUPING SETS** explained clearly  \n\n\\- How window functions like **ROW\\_NUMBER(), RANK(), DENSE\\_RANK(), NTILE()** work  \n\n\\- Practical tips to make your queries cleaner and faster  \n\n\n\n📖 Check it out here: \\[Master SQL Aggregations &amp; Window Functions\\] \\[[medium link](https://medium.com/@eya.nani/master-sql-aggregations-and-window-functions-to-level-up-your-data-analysis-skills-e426a2b2e31f)\\]  \n\n\n\n💬 What’s the first SQL trick you learned that made your work easier? Share below 👇",
    "author": "Additional-Funny-578",
    "timestamp": "2025-09-25T10:43:41",
    "url": "https://reddit.com/r/dataengineering/comments/1nqddyr/master_sql_aggregations_window_functions_a/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 0.68,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq4hx4",
    "title": "The 2025 &amp; 2026 Ultimate Guide to the Data Lakehouse and the Data Lakehouse Ecosystem",
    "content": "By 2025, this model matured from a promise into a proven architecture. With formats like **Apache Iceberg, Delta Lake, Hudi, and Paimon**, data teams now have open standards for transactional data at scale. Streaming-first ingestion, autonomous optimization, and catalog-driven governance have become baseline requirements. Looking ahead to 2026, the lakehouse is no longer just a central repository, it extends outward to power **real-time analytics, agentic AI, and even edge inference**.",
    "author": "AMDataLake",
    "timestamp": "2025-09-25T04:43:43",
    "url": "https://reddit.com/r/dataengineering/comments/1nq4hx4/the_2025_2026_ultimate_guide_to_the_data/",
    "score": 8,
    "num_comments": 0,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npse7i",
    "title": "Is Data Engineering in SAP a dead zone career wise?",
    "content": "Currently a BI Developer using Microsoft fabric/Power BI but a higher paying opportunity in data engineering popped up at my company, but it used primarily SAP BODS as its tool for ETL. \n\nFrom what I understand some members on the team still use Python and SQL to load the data out of SAP but it seems like it’s primarily operating within an SAP environment.\n\nWould switching to a SAP data engineering position lock me out of progressing vs just staying a lower paid BI analyst operating within a Fabric environment?",
    "author": "ToothPickLegs",
    "timestamp": "2025-09-24T17:14:01",
    "url": "https://reddit.com/r/dataengineering/comments/1npse7i/is_data_engineering_in_sap_a_dead_zone_career_wise/",
    "score": 63,
    "num_comments": 51,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq3o2p",
    "title": "Do you use Kafka as data source for your AI agents and RAG applications",
    "content": "Hey everyone, would love to know if you have a scenario where your rag apps/ agents constantly need fresh data to work, if yes why and how do you currently ingest realtime data for Kafka, What tools, database and frameworks do you use. ",
    "author": "DistrictUnable3236",
    "timestamp": "2025-09-25T03:59:15",
    "url": "https://reddit.com/r/dataengineering/comments/1nq3o2p/do_you_use_kafka_as_data_source_for_your_ai/",
    "score": 8,
    "num_comments": 4,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq3v60",
    "title": "POC Suggestions",
    "content": "Hey,   \nI am currently working as a Senior Data Engineer for one of the early stage service companies . I currently have a team of 10 members out of which 5 are working on different projects across multiple domains and the remaining 5 are on bench . My manager has asked me and the team to deliver some PoC along with the projects we are currently working on/ tagged to . He says those PoC should somecase some solutioning capabilities which can be used to attract clients or customers to solve their problems and that it should have an AI flavour and also that it has to solve some real business problems . \n\nAbout the resources - Majority of the team is less than 3 years of experience . I have 6 years of experience . \n\n  \nI have some ideas but not sure if these are valid or if they can be used at all . I would like to get some ideas or your thoughts about the PoC topics and their outcomes I have in mind which I have listed below \n\n1. Snowflake vs Databricks Comparison PoC - Act as an guide on*When to use Snowflake, when to use Databricks.*  \n2. AI-Powered Data Quality Monitoring - Trustworthy data with AI-powered validation.  \n3. Self Healing Pipelines - Pipelines detect failures (late arrivals, schema drift), classify cause with ML, and auto-retry with adjustments.  \n 4.Metadata-Driven Orchestration- Based on the metadata, pipelines or DAGs run dynamically . \n\n  \nLet me know your thoughts. ",
    "author": "Weak_Balance_2489",
    "timestamp": "2025-09-25T04:09:55",
    "url": "https://reddit.com/r/dataengineering/comments/1nq3v60/poc_suggestions/",
    "score": 6,
    "num_comments": 4,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq7s5z",
    "title": "Syncing db layout a to b",
    "content": "I need help. I am by far not a programmer but i have been tasked by our company to find the solution to syncing dbs (which is probably not the right term)\n\nWhat i need is a program that looks at the layout ( think its called the scheme or schema) of database a ( which would be our db that has all the correct fields and tables) and then at database B (which would have data in it but might be missing tables or fields ) and then add all the tables and fields from db a to db b without messing up the data in db b \n\n",
    "author": "ProfessionalSmooth46",
    "timestamp": "2025-09-25T07:09:00",
    "url": "https://reddit.com/r/dataengineering/comments/1nq7s5z/syncing_db_layout_a_to_b/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npyqzj",
    "title": "Career crossroad",
    "content": "Amassed around 6.5 of work ex. Out of which I've spent almost 5 as a data modeler. Mainly used SQL, Excel, SSMS, a bit of databricks to create models or define KPI logic. There were times when I worked heavily on excel and that made me crave for something more challenging. \nThe last engagement I had, was a high stakes-high visibility one and I was supposed to work as a Senior Data Engineer. I didn't have time to grasp and found it hard to cope with. My intention of joining the team was to learn a bit of DE(Azure Databricks and ADF) but, it was almost too challenging. (Add a bit of office politics as well)\nI'm now senior enough to lead products in theory but, my confidence has taken a hit. \nI'm not naturally inclined to Python or PySpark. I'm most comfortable with SQL. \nI find myself at an odd juncture.\nWhat should I do? \n\nEdit: My engagement is due to end in a few weeks and I'll have to look for a new one soon. \nI'm now questioning what kind of role would I be suited for, in the long term given the advent of AI.",
    "author": "ScroLin247",
    "timestamp": "2025-09-24T22:44:02",
    "url": "https://reddit.com/r/dataengineering/comments/1npyqzj/career_crossroad/",
    "score": 10,
    "num_comments": 2,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqbe6d",
    "title": "Data Engineer in Dilemma",
    "content": "Hi Folks,\n\nThis is actually my first post here, seeking some advice to think through my career dilemma.\n\nIm currently a Data Engineer (entering my 4th working year) with solid experience in building ETL/ELT pipelines and optimising data platform (Mainly Azure).\n\nAt the same time, I have been hands-on with AI project such as LLM, Agentic AI, RAG system. Personally I do enjoyed building quality data pipeline and serve the semantic layer. Things getting more interesting for me when i get to see the end-to-end stuff when I know how my data brings value and utilised by the Agentic AI. (However I am unsure on this pathway since these term and career trajectory is getting bombastic ever since the OpenAI blooming era)\n\nSeeking advice on:\n1. Specialize - Focus deeply on either Data engineering or AI/ML Engineering?\n2. Stay Hybrid - Continue in strengthening my DE skills while taking AI projects on the side? (Possibly be Data &amp; AI engineer)\n\nSome questions in my mind and open for discussion \n1. What is the current market demand for hybrid Data+AI Engineers versus specialist?\n2. What does a typical DE career trajectory look like?\n3. How about AI/ML engineer career path? Especially on the GenAI and production deployment?\n4. Are there real advantages to specialising early or is a hybrid skillset more valueable today?\n\nWould be really grateful for any insights, advice and personal experiences that you can share.\n\nThank you in advance!\n\n[View Poll](https://www.reddit.com/poll/1nqbe6d)",
    "author": "Background_Artist801",
    "timestamp": "2025-09-25T09:28:02",
    "url": "https://reddit.com/r/dataengineering/comments/1nqbe6d/data_engineer_in_dilemma/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npoias",
    "title": "What's new in Postgres 18",
    "content": "",
    "author": "Chemical-Treat6596",
    "timestamp": "2025-09-24T14:24:24",
    "url": "https://reddit.com/r/dataengineering/comments/1npoias/whats_new_in_postgres_18/",
    "score": 33,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npsjij",
    "title": "How do you manage your DDLs?",
    "content": "How is everyone else managing their DDLs when creating data pipelines?\n\nDo you embed CREATE statements within your pipeline? Do you have a separate repo for DDLs that's ran separately from your pipelines? In either case, how do you handle schema evolution?\n\nThis assumes a DWH like Snowflake. \n\nWe currently do the latter. The problem is that it's a pain to do ALTER statements since our pipeline runs all SQLs on deploy. I wonder how everyone else is managing. ",
    "author": "mrbananamonkey",
    "timestamp": "2025-09-24T17:21:02",
    "url": "https://reddit.com/r/dataengineering/comments/1npsjij/how_do_you_manage_your_ddls/",
    "score": 15,
    "num_comments": 25,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npz9sh",
    "title": "Are there any online resources for learning data bricks free edition and making pipeline without using cloud services?",
    "content": "I got selected for data engineering role and I wanted to know if there are any YouTube resources for learning data bricks and making pipeline in free edition of data bricks",
    "author": "Pleasant-Insect136",
    "timestamp": "2025-09-24T23:16:49",
    "url": "https://reddit.com/r/dataengineering/comments/1npz9sh/are_there_any_online_resources_for_learning_data/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq150s",
    "title": "Need Advice on ADF",
    "content": "https://preview.redd.it/5f5ge4g7s9rf1.png?width=768&amp;format=png&amp;auto=webp&amp;s=ca8ac80779e9066dede1c7ebde704a42a808489c\n\nThis is my first time working with Azure and I have never worked with Pipelines before so I am not sure what I am doing (please dont roast me, I am still a junior). Essentially we have some 10 machines somewhere that sends data periodically once a day, I suggested my manager we use Azure Functions (Durable Functions to READ and one for Fetching Acitivity from REST APIs) but he suggested that since it's a proof of concept to the customer we should go for a managed services (idk what his logic is) so I choose Azure Data Factory so this is my diagram, we have some sort of \"ingestor\" that ingest data and writes to SQL database.\n\nPlease give me insight as to if this is a good approach, some drawbacks or some other insights. I am not sure if I am in the right direction as I don't have solution architect experience I only have less than one year Cloud Engineering experience.",
    "author": "Cold-Somewhere8170",
    "timestamp": "2025-09-25T01:19:12",
    "url": "https://reddit.com/r/dataengineering/comments/1nq150s/need_advice_on_adf/",
    "score": 3,
    "num_comments": 11,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1noq2sq",
    "title": "It's All About Data...",
    "content": "",
    "author": "growth_man",
    "timestamp": "2025-09-23T11:58:27",
    "url": "https://reddit.com/r/dataengineering/comments/1noq2sq/its_all_about_data/",
    "score": 1860,
    "num_comments": 44,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nqi5y7",
    "title": "Biggest Data Engineering Pain Points",
    "content": "I’m working on a project to tackle some of the everyday frustrations in data engineering — things like repetitive boilerplate, debugging pipelines at 2 AM, cost optimization, schema drift, etc.\n\nYour answer can help me focusing on the right tool.\n\nThanks in advance, and I'd love to hear more in comments.\n\n[View Poll](https://www.reddit.com/poll/1nqi5y7)",
    "author": "noswear94",
    "timestamp": "2025-09-25T13:47:01",
    "url": "https://reddit.com/r/dataengineering/comments/1nqi5y7/biggest_data_engineering_pain_points/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.23,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq01gg",
    "title": "I built a mobile app(1k+ downloaded) to manage PostgreSQL databases",
    "content": "🔌 Direct Database Connection\n\n* No proxy servers, no middleware, no BS - just direct TCP connections\n* Save multiple connection profiles\n\n🔐 SSH Tunnel Support\n\n* Built-in SSH tunneling for secure remote connections\n* SSL/TLS support for encrypted connections\n\n📝 Full SQL Editor\n\n* Syntax highlighting and auto-completion\n* Multiple script tabs\n\n📊 Data Management\n\n* DataGrid for handling large result sets\n* Export to CSV/Excel\n* Table data editing\n\nLink is [Play Store](https://play.google.com/store/apps/details?id=com.mk.hipsql)",
    "author": "himkii",
    "timestamp": "2025-09-25T00:05:44",
    "url": "https://reddit.com/r/dataengineering/comments/1nq01gg/i_built_a_mobile_app1k_downloaded_to_manage/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npudzq",
    "title": "Migrate legacy ETL pipelines",
    "content": "We have a legacy product which has ETL pipelines built using Informatica Powercenter. Now management has finally decided that it’s time to upgrade to a cloud native solution but not IDMC. But there’s hardly any documentation out there for these ETL’s running in production for more than a decade. Is there an option on the market, OSS or otherwise that will help in migrating all the logic?",
    "author": "kash80",
    "timestamp": "2025-09-24T18:49:04",
    "url": "https://reddit.com/r/dataengineering/comments/1npudzq/migrate_legacy_etl_pipelines/",
    "score": 6,
    "num_comments": 11,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npoka6",
    "title": "I built an open source ai web scraper with json schema validation",
    "content": "I've been working on an open source vibescraping tool on the side, I'm usually collecting data from many different websites. Enough that it became a nuisance to manage even with Claude Code. \n\nGetting claude to iteratively fix the parsing for each site took a good bit of time, and there was no validation. I also don't really want to manage the pipeline, I just want the data in an api that I can read and collect from. So I figured it would save some time since I'm always setting up new scrapers which is a pain. It's early but when it works, it's pretty cool and should be more stable soon.\n\nBuilt with aisdk, hono, react, and typescript. If you're interested to use it, give it a star. It's free to use. I plan to add playwright support soon for javascript websites as I'm intending to monitor data on some of them.\n\n[github.com/gvkhna/vibescraper](http://github.com/gvkhna/vibescraper)\n\n",
    "author": "gvkhna",
    "timestamp": "2025-09-24T14:26:37",
    "url": "https://reddit.com/r/dataengineering/comments/1npoka6/i_built_an_open_source_ai_web_scraper_with_json/",
    "score": 8,
    "num_comments": 3,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npvr9w",
    "title": "SFTP cleaning with rules.",
    "content": "We have many clients sending data files to our SFTP, recently moved using SFTPGo for account management which so far I really like so far. We have an homebuild ETL that grabs those files into our database. Now this ETL tool can compress, move or delete these files but our developers like to keep those files on the SFTP for x days. Are there any tools where you can compress, move or delete files with simple rules with a nice GUI, looked at SFTPGo events but got lost there.",
    "author": "No_Disaster_9715",
    "timestamp": "2025-09-24T19:56:27",
    "url": "https://reddit.com/r/dataengineering/comments/1npvr9w/sftp_cleaning_with_rules/",
    "score": 3,
    "num_comments": 6,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npkazt",
    "title": "How to upskill",
    "content": "Hi all,\n\nI am a technical program manager and was almost a director position in my firm. I had to quit because of too much politics and sales pressure. I took up just delivery focused role and realised that I became techno functional in my previous role in healthcare ( worked for 14 years) where I led large scale programs in cloud but always had architects on the team. I like to be on the strategy side of the projects but feels like I have lost touch with the technical aspects. I feel like doing a cloud certification to feel more confident when talking about architectures in detail. Are there other TPMs who are well versed with cloud tech stack and anyone has any good course recommendations? ( Not looking for self paced programs but an instructor led training to keep me on track). Most of my programs have been on Azure and databricks so looking for recommendations there.",
    "author": "Mammoth_Student_7390",
    "timestamp": "2025-09-24T11:41:07",
    "url": "https://reddit.com/r/dataengineering/comments/1npkazt/how_to_upskill/",
    "score": 5,
    "num_comments": 4,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npl2y6",
    "title": "Can someone explain what does AtScale really do?",
    "content": "I mean I get all the spiel about the semantic layer and all that jazz but IMO it’s more about someone (whatever role does that in your company) assessing and defining it. So I don’t get what is the tech about it.\n\nCan someone help me clear the marketing talk and help me understand what does it REALLY do tech wise?",
    "author": "Royal-Parsnip3639",
    "timestamp": "2025-09-24T12:10:04",
    "url": "https://reddit.com/r/dataengineering/comments/1npl2y6/can_someone_explain_what_does_atscale_really_do/",
    "score": 5,
    "num_comments": 15,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq1b7z",
    "title": "First Data Engineering Project with Python and Pandas - Titanic Dataset",
    "content": "Hi everyone! I'm new to data engineering and just completed my first project using Python and pandas. I worked with the Titanic dataset from Kaggle, filtering passengers over 30 years old and handling missing values in the 'Cabin' column by replacing NaN with 'Unknown'.  \nYou can check out the code here: [https://github.com/Parsaeii/titanic-data-engineering](https://github.com/Parsaeii/titanic-data-engineering)  \nI'd love to hear your feedback or suggestions for my next project. Any advice for a beginner like me? Thanks! 😊",
    "author": "LynxEmotional4523",
    "timestamp": "2025-09-25T01:30:44",
    "url": "https://reddit.com/r/dataengineering/comments/1nq1b7z/first_data_engineering_project_with_python_and/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npxicf",
    "title": "Collibra Free trial",
    "content": "How do we get free collibra trial version can some guide through the process and services offered in free trial. Also what will be subscription and services offered in paid versions\n\nI tried checking in multiple forums and Collibra website too but not getting any concrete solution to it",
    "author": "Last_Recording5989",
    "timestamp": "2025-09-24T21:29:52",
    "url": "https://reddit.com/r/dataengineering/comments/1npxicf/collibra_free_trial/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npo82u",
    "title": "Sanofi Hyd review for data engineer?",
    "content": "Hi All,\n\nI recently joined a xxx company 3 months back and now I got a great opportunity with Sanofi hyd\n\nExperience: 12 years 2 months\nRole : Data engineer \nSalary offered: 41 fixed +8 variable \nI have almost same salary in the company I joined recently which is relatively small in revenue and profits compared to sanofi\n\nI saw like sanofi is pharma related company and has good revenue, so hopefully have scope for career.. \n\nIs sanofi GCC worth to shift after 3 months of working in a company?\n\n\nI am looking for job stability at this higher packages.",
    "author": "Internal_Builder_848",
    "timestamp": "2025-09-24T14:12:53",
    "url": "https://reddit.com/r/dataengineering/comments/1npo82u/sanofi_hyd_review_for_data_engineer/",
    "score": 4,
    "num_comments": 1,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npmejz",
    "title": "Collibra - Pros and Cons",
    "content": "What are the challenges during and post implementation ? \nWhat alternatives would you suggest ? \n\nLet’s assume - Data Governance and documentation is not the issue . I would appreciate practical inputs and advices . ",
    "author": "Exotic_Pi_9",
    "timestamp": "2025-09-24T13:01:31",
    "url": "https://reddit.com/r/dataengineering/comments/1npmejz/collibra_pros_and_cons/",
    "score": 3,
    "num_comments": 5,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1np67wl",
    "title": "Choosing Between Data Engineering and Platform Engineering",
    "content": "First of all thanks for reading my wall of text :)\n\nI did various internships in Data Engineering and Data Platform during the last 4 years of University and contributed regularly to large open source projects in that area. I was never that fascinated by writing sql transformations but rather tooling, optimizations and infra and moved more and more to building platforms for data engineers.\n\nI now have 2 offers at hand (both pay equal). The first one is as a data engineer. I would be the only data guy in a department of 30 people and there is a large initiative to automate some financial reporting. The tasks are building dbt models with Trino. Also building some dashboards which I have never done. I would be responsible which is cool, but the tasks don’t seem to deep. Sure I could probably come up with e.g a testing pipeline for dbt models and implement that on my own to have some technical challenges but that is it. There is a department taking care of all services and development of the platform. I am a bit afraid that I will be stuck in writing pipelines when I take that job and will not be invited to tooling / infra heavy roles.\n\nThe other one is as a platform engineer where I would work in a platform team to build multi cloud K8s microservices and handle monitoring and logging etc. That seems to be more challenging from a technical perspective but I would not be in the data sphere anymore. Do you think a switch back to data / data platform engineering is possible from there. Especially if I continue with open source?",
    "author": "Creative-Dentist-383",
    "timestamp": "2025-09-24T00:52:29",
    "url": "https://reddit.com/r/dataengineering/comments/1np67wl/choosing_between_data_engineering_and_platform/",
    "score": 24,
    "num_comments": 15,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npoeae",
    "title": "Using Iceberg Time Travel for Historical Trends",
    "content": "I am relatively new to Apache Iceberg and data engineering in general. I'm assigned a new project recently at work where that want to roll out an internal BI system.\n\nI'm looking at Apache Iceberg and one of the business requirements is to be able to create trend graphs based on historical data. From what I have read, in Iceberg there's a functionality called time travel that let you use the exact same query with \"AS OF your_timestamp\" to get the results of the past. It seems to me that it can be useful in generating historical trends over time.\n\nHowever, I also read that in the long term, for example when you have data that spans over years, using time travel to generate historical trends is actually a very bad idea in terms of performance and is an anti-pattern. I also tried asking AIs, which some of them told me it's fine and some of them tell me to look at Type 2 Slowly Changing Dimensions when building the tables.\n\nI am a bit lost here and some help and suggestions will be greatly appreciated.\n\n",
    "author": "Fair-Mathematician68",
    "timestamp": "2025-09-24T14:19:56",
    "url": "https://reddit.com/r/dataengineering/comments/1npoeae/using_iceberg_time_travel_for_historical_trends/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npb5c9",
    "title": "Should applications consume data from the DWH or directly from object storage services?",
    "content": "If I have a cloud storage that centralizes all my company’s raw data and a data warehouse that processes the data for analysis, would it be better to feed other applications (e.g. Salesforce) from the DWH or directly from the object storage?\n\nFrom what I understand, both options are valid with pros and cons, and both require using an ETL tool. My concern is that I’ve always seen the DWH as a tool for reporting, not as a centralized source of data from which non-BI applications can be fed, but I can see that doing everything through the DWH might be simpler during the transformation phase rather than creating separate ad hoc pipelines in parallel.",
    "author": "LuckyAd5693",
    "timestamp": "2025-09-24T05:44:48",
    "url": "https://reddit.com/r/dataengineering/comments/1npb5c9/should_applications_consume_data_from_the_dwh_or/",
    "score": 9,
    "num_comments": 11,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1np45xe",
    "title": "What is the need for using hashing algorithms to create primary keys or surrogate keys?",
    "content": "I am currently learning data engineering. I have some technical skills and use sql for pulling reports in my current job. I am currently learning more about data modeling, Normalization, star schema, data vault etc. In star schema the examples I saw are using a MD5 hash function to convert the source data primary key to the fact table primary key or dimension table primary key. In data vaults also similar things they are doing for hubs satellite and link tables. I don't quite understand why do additional processing by converting an existing primary key into a hash key? Instead, can't they use a continuous sequence as a primary key? What are the practical benefits of using a hashed value as a primary key? As far as I know hashing is one way and we cannot derive the business primary key value back from the hash key. So I assume it is primarily an organizational need. But for what? What problem is a hashed primary key solving?",
    "author": "tinkerjreddit",
    "timestamp": "2025-09-23T22:40:39",
    "url": "https://reddit.com/r/dataengineering/comments/1np45xe/what_is_the_need_for_using_hashing_algorithms_to/",
    "score": 27,
    "num_comments": 39,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npl2or",
    "title": "I am trying to setup Data Replication from IBM AS400 to an Iceberg Data Lakehouse",
    "content": "Hi,\n\nit's my first post here. I come from a DevOps background but am getting more and more Data Engineering tasks recently.\n\nI am trying to setup database replication to a data lakehouse.\n\nFirst of all, here are some specifications about my current situation :\n\n* The  source database is configured on relevant tables with a CDC system.\n* The IT Team managing this database is against direct connection so they are redirecting the CDC to another database to act as a buffer/audit step. Before an ETL pipeline will load the relevant data and send files to S3 compatible Buckets.\n* The source data is very well defined, with global standards applied to all tables and columns in the database.\n* The data lakehouse is using Apache Iceberg, with Spark and Trino for transformation and exploration. We are running everything in Kubernetes (except the buckets).\n\nWe want to be able to replicate relevant tables to our data lakehouse in an automated way. The resfresh rate could be every hour, half-hour, 5 minutes, etc ... No need for streaming right now.\n\nI found some important points to look for :\n\n* how do we represent the transformation in the exchanged files (SQL transactions, before/after data) ?\n* how do we represent table schema ?\n* how do we make the correct type conversion from source format to Iceberg format ?\n* how do we detect and adapt to schema evolution ?\n\nI am lost thinking about all possible solutions and all of them seem to *reinvent the wheel*:\n\n* use the strong standards applied to the source database. modification timestamp columns are present in every table and could allow us to not need CDC tools. A simple ETL pipeline could query the inserted/updated/deleted data since the last batch. This would lead us to Ad Hoc solutions : simple but limited with evolution.\n* use Kafka (or Postgresql FOR UPDATE SKIP LOCKED trick) with a custom Json like file format to represent the CDC aggregated output. Once the file format defined, we would use Spark to ingest the data into Iceberg.\n\nI am sure there as to be existing solutions and patterns to this problem.\n\nThanks a lot for any advice !\n\nPS : I rewrote the post to remove the unecessary on premise/cloud specification. Still the source database is an on premise IBM AS400 database if anyone is interested.  \nPPS : also why can't I use any bold characters ?? Reddit keep telling me my text is AI content if I set any character to bold  \nPPPS : sorry dear admin, keep up the good work",
    "author": "FlowBigby",
    "timestamp": "2025-09-24T12:09:47",
    "url": "https://reddit.com/r/dataengineering/comments/1npl2or/i_am_trying_to_setup_data_replication_from_ibm/",
    "score": 2,
    "num_comments": 8,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npq48w",
    "title": "Do you have a Single Prod VM",
    "content": "Hi. I was recently spoke with  another data engineer at an event. They told me that they currently run Dagster on a single windows VM for production. They have Keeper for secrets management, but no SSO. Only those with access to the internal VM IP address can access the machine.\n\nThis sparked a question that I’ve thought of before and decided might be good to ask here.  How many of you are actually running production grade work flows on a single VM? What is your set up? Airflow, Dagster, cron, etc….? I’m very curious as to how common this is and just how much people are doing with one vm.  \n\nI’ve heard and been told that something like Airflow works best on a cluster but I’ve also seen a few people say that they run it on a single VM with docker.  Anyway I’m just curious about your experiences and what issues (aside from scalability) you may have run into if you are into this situation. \n\n\nTLDR: Are you running production workflow on one VM? If yes, what is your stack and how much are you processing with it?",
    "author": "lilde1297",
    "timestamp": "2025-09-24T15:30:47",
    "url": "https://reddit.com/r/dataengineering/comments/1npq48w/do_you_have_a_single_prod_vm/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq1n9u",
    "title": "Should I quit my job to do this Database Start up?",
    "content": "Hi guys,  \nI am in the middle of designing a database system built in rust that should be able to store, KV, Vector Graph and more with a high NO-SQL write speed it is built off a LSM-Tree that I made some modifications to.\n\nIt's alot of work and I have to say I am enjoying the process but I am just wondering if there is any desire for me to opensource it / push to make it commercially viable?\n\nThe ideal for me would be something similar to serealDB:\n\nEssentially the DB Takes advantage of LogStructured Merges ability to take large data but rather than utilising compaction I built a placement engine in the middle to allow me to allocate things to graph, key-value, vector, blockchain, etc\n\nI work in an AI company as a CTO and it solved our compaction issues with a popular NoSQL DB but I was wondering if anyone else would be interested?\n\nIf so I'll leave my company and opensource it",
    "author": "Actual_Ad5259",
    "timestamp": "2025-09-25T01:53:09",
    "url": "https://reddit.com/r/dataengineering/comments/1nq1n9u/should_i_quit_my_job_to_do_this_database_start_up/",
    "score": 0,
    "num_comments": 19,
    "upvote_ratio": 0.3,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npjhs8",
    "title": "Where does your Extract Layer live? Custom code, SaaS, platform connectors?",
    "content": "It was always a mystery to me as a Data Analyst until I started my first Data Engineer job about a year ago. I am a data team of one inside a small-mid sized non-tech company. \n\nI am using Microsoft Fabric Copy Jobs since we were already set on Azure/PowerBI and they are dead simple. Fivetran or Airbyte seemed to make sense but looked like overkill for this scope/budget.  \n\n\nGiven Fabric is the only tool I have used, and it still feels half-baked for most other features , I am curious: how big is your team/org and how do you handle data extraction from source systems?\n\n* Run custom API extractors on VMs/containers (Python, Airflow, etc.)?\n* Use managed ELT tools like Fivetran, Airbyte, Stitch, Hevo, etc. ?\n* Rely on native connectors in platforms like Fabric, Snowflake, Databricks?\n* Something else entirely?\n\nWould you make the same choice again?",
    "author": "greatlakesdataio",
    "timestamp": "2025-09-24T11:10:34",
    "url": "https://reddit.com/r/dataengineering/comments/1npjhs8/where_does_your_extract_layer_live_custom_code/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nolrzf",
    "title": "LMFAO offshoring",
    "content": "Got tasked with developing a full test concept for our shiny new cloud data management platform.\n\nFocus: anonymized data for offshoring.\nTranslation: make sure other offshore employes can access it without breaking any laws.\n\nFeels like I’m digging my own grave here 😂😂\n",
    "author": "Salt_Anteater3307",
    "timestamp": "2025-09-23T09:17:18",
    "url": "https://reddit.com/r/dataengineering/comments/1nolrzf/lmfao_offshoring/",
    "score": 213,
    "num_comments": 39,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nq12cj",
    "title": "helping founders and people with data",
    "content": "Finally, a way to query databases without writing SQL! Just ask questions in plain English and get instant results with charts and reports.\nBuilt this because I was tired of seeing people struggle to access their own data. Now anyone can be data-driven!\nWhat do you think? Would you use something like this?",
    "author": "chinm333-startup-hub",
    "timestamp": "2025-09-25T01:14:08",
    "url": "https://reddit.com/r/dataengineering/comments/1nq12cj/helping_founders_and_people_with_data/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.28,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npkp4e",
    "title": "Tried building a better Julius (conversational analytics). Thoughts?",
    "content": "Being able to talk to data without having to learn a query language is one of my favorite use-cases of LLMs. I was looking up conversational analytics tools online, and stumbled upon Julius AI, which I found to be really impressive. It gave me the idea to build my own POC with a better UX\n\nI’d already hooked up some tools that fetch stock market data using financial-datasets, but recently added a file upload feature as well, which lets you upload an Excel or CSV sheet and ask questions about your own data (this currently has size limitations due to context window, but improvements are planned).\n\nMy main focus was on presenting the data in a format that’s easier and quicker to digest and structuring my example in a way that lets people conveniently hook up their own data sources.\n\nSince it is open source, you can customize this to use your own data source by editing config.ts and config.server.ts files. All you need to do is define tool calls, or fetch tools from an MCP server and return them in the fetchTools function in config.server.ts.\n\nLet me know what you think! If you have any feature recommendations or bug reports, please feel free to raise an issue or a PR.\n\n\n\n🔗 Link to source code and live demo in the comments",
    "author": "AviusAnima",
    "timestamp": "2025-09-24T11:55:51",
    "url": "https://reddit.com/r/dataengineering/comments/1npkp4e/tried_building_a_better_julius_conversational/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npgsfy",
    "title": "What data do you copy/paste between systems every week?",
    "content": "Just curious what everyone’s most annoying copy/paste routine is at work.\nI feel like everyone has at least one data task they do over and over that makes them want to scream. What’s the one that drives you crazy?",
    "author": "Perfect_Figure182",
    "timestamp": "2025-09-24T09:27:35",
    "url": "https://reddit.com/r/dataengineering/comments/1npgsfy/what_data_do_you_copypaste_between_systems_every/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npflh7",
    "title": "Thoughts - can/will cloud data platforms start to offer \"owned\" solutions vs. pay as you go?",
    "content": "TL/DR - will cloud data platforms (ie: snowflake) start to address the extreme cost challenges some customers are facing with their solutions with a \"buy\" the compute resource model to augment the current \"rent\" the compute resource model pricing structure?\n\n  \n  \nA theory / futuristic question, wondering if anyone has thoughts on this...  \n  \nI absolutely love Snowflake, am experiencing tangible benefits over our on-prem SQL implementation - but am noticing that it is introducing significant cost challenges that were not present in our previous on-prem solution.   \n  \nThere has been ton's of discussion on this sub and others about how cost is essentially the customers fault - they are not taking the effort to understand Snowflake cost and optimize their Snowflake implementation accordingly, or that cost is a \"benefit\" since it scales in relation to value delivered -- but I want to take a different approach for this post.\n\nMy Fortune 400 global company is spending too much time managing our Snowflake bill, we never did that in our on-prem SQL environment, and it's waste. We don't want layers of senior leadership spending valuable time worrying about this, we don't want teams of off-shore people constantly monitoring and turning every query not because the query needs tuning but rather we are trying to squeeze every penny out of our snowflake bill, we don't want to layoff onshore resources and replace them with cheaper offshore resources simply because that's our only option to balance our budget now that we are renting a infrastructure with variable, unpredictable, and constantly increasing costs.  We want to focus our time creating business value, not managing our Snowflake costs!\n\nGiven this, does anyone think the next major step in cloud data platform evolution is to rethink the costing of the product? For example, in Snowflake my virtual compute engine is ultimately running on physical hardware somewhere. Would it be technically possible, and advantageous, to offer a model where the customer has a one-time purchase of hardware resources which would be hosted/maintained by Snowflake, or perhaps hosted/maintained inhouse, and then the customer could elect to link compute resources to this \"owned\" hardware. For example, most of my companies processing is on a X-Small warehouse, which in this idea, we could own, and essentially forget about from budgetary perspective. Our company could \"buy\" one with a one-time 100K-ish spend, and then use it until it dies for free (not including the cost of snowflake operating/maintaining the hardware if applicable). From Snowflake's perspective this locks us in as a customer since they are hosting hardware we paid for, and from our perspective this drastically lowers our monthly bill. We would effectively \"rent\" any larger sized compute which would be a more predictable cost to manage for my leadership. Obviously, there are other pros/cons to a situation where we hosted the hardware inhouse and Snowflake owned the application layer. \n\nFurthermore, if this idea is technically possible, and provides value to the customer - is it only a matter of time before one of the big vendors offers it for competitive differentiation? \n\nThoughts?\n\n",
    "author": "rotr0102",
    "timestamp": "2025-09-24T08:43:25",
    "url": "https://reddit.com/r/dataengineering/comments/1npflh7/thoughts_canwill_cloud_data_platforms_start_to/",
    "score": 0,
    "num_comments": 15,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nos7dw",
    "title": "Data Engineers: Struggles with Salesforce data",
    "content": "I’m researching pain points around getting Salesforce data into warehouses like Snowflake. I’m somewhat new to the data engineering world, I have some experience but am by no means an expert. I was tasked with doing some preliminary research before our project kicks off. What tools are you guys using? What takes the most time? What are the biggest hurdles?\n\nBefore I jump into this I would like to know a little about what lays ahead. \n\nI appreciate any help out there. ",
    "author": "VizlyAI",
    "timestamp": "2025-09-23T13:19:00",
    "url": "https://reddit.com/r/dataengineering/comments/1nos7dw/data_engineers_struggles_with_salesforce_data/",
    "score": 30,
    "num_comments": 58,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nojoum",
    "title": "BigQuery vs snowflake vs Databricks, which one is more dominant in the industry and market?",
    "content": "i dont really care about difficulty, all I want is how much its used in the industry wand which is more spreaded, I don't know anything about these tools, but in cloud I use and lean toward AWS if that helps\n\n  \nI am mostly a data scientist who works with llms,  nlp and most text tasks, I use python SQL and excel and other tools",
    "author": "Beyond_Birthday_13",
    "timestamp": "2025-09-23T07:59:14",
    "url": "https://reddit.com/r/dataengineering/comments/1nojoum/bigquery_vs_snowflake_vs_databricks_which_one_is/",
    "score": 65,
    "num_comments": 73,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nosyuk",
    "title": "How to learn something new nowadays?",
    "content": "In the past, if I had to implement something new, I had to read tutorials, documentation, StackOverflow questions, and try the code many times until it worked. Things stuck in your brain and you actually learned.\n\nBut nowadays? If it's something I dont know about, I'll just ask whatever AI Agent to do the code for me, review it, and if it looks OK I'll accept it and move to the next task. I won't be able to write myself the same code again, of course. And I dont have a deep understanding of what's happening in reality, but I'm more productive and able to deliver more for the company.\n\nHave you been able to overcome this situation in which more productivity takes over your learning? If so, how?",
    "author": "L3GOLAS234",
    "timestamp": "2025-09-23T13:48:40",
    "url": "https://reddit.com/r/dataengineering/comments/1nosyuk/how_to_learn_something_new_nowadays/",
    "score": 18,
    "num_comments": 9,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1np3zw4",
    "title": "GitHub - Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler",
    "content": "",
    "author": "PsychologicalTap1541",
    "timestamp": "2025-09-23T22:30:28",
    "url": "https://reddit.com/r/dataengineering/comments/1np3zw4/github_websitecrawler_extract_data_from_websites/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1noxj84",
    "title": "ELT in snowflake",
    "content": "Hi,\n\nMy company is moving towards snowflake as data warehouse. They have developed a bunch of scripts to load data in raw layer format and then let individual team to do further processing to take it to golden layer. What tools should I be using for transformation (raw to silver to golden schema)?",
    "author": "Wht_the_heck_gng_on",
    "timestamp": "2025-09-23T17:00:36",
    "url": "https://reddit.com/r/dataengineering/comments/1noxj84/elt_in_snowflake/",
    "score": 6,
    "num_comments": 26,
    "upvote_ratio": 0.69,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nogqqj",
    "title": "Meetings instead of answering a simple question",
    "content": "This is just a rant but it seems like especially management loves to schedule meetings, sometimes presential, for things that could be answered in a simple message or email.  \n  \n—We need this data in our metrics.  \n  \n—Ok, send me the API-credentials and description and I'll handle it.  \n  \n—That would be productive. Let's have a meeting in three weeks instead.  \n  \n*three weeks later*  \n  \n—I'm sorry, I have no clue why we scheduled this meeting and didn't do my homework. How about a meeting in three weeks? Come to the office, let's get high on caffeine and let me tell you everything about my dog.  \n  \nHave you experienced something like this? ",
    "author": "dfwtjms",
    "timestamp": "2025-09-23T06:00:14",
    "url": "https://reddit.com/r/dataengineering/comments/1nogqqj/meetings_instead_of_answering_a_simple_question/",
    "score": 44,
    "num_comments": 19,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nodiup",
    "title": "Fabric is the new standard for Microsoft in Data Engineering?",
    "content": "Hey, I have some doubts regarding Microsoft Fabric, Azure and Databricks.\n\nIn my company all the pojects lately has being with Fabric\n\nIn other offers as a Senior DE I've seen a lot of Fabric for different type of companies\n\nMicrosoft 'removed' the DP-203 certification (Azure Data Engineer) for the DP-700 (Fabric Data Engineer)\n\nAzure as a platform to use Data Factory and Synapse seems will be elegacy product, instead of it I think being an expert in Fabric will make for us very good opportunities.\n\nWhat happens with Databricks then? I see that Fabric is cool to interconnect Data Engineering, Data Analysis and Machine Learning but is not that powerful as Databricks. Do you think guys is good to be an expert in Fabric and in other way in Databricks?",
    "author": "Irachar",
    "timestamp": "2025-09-23T03:15:56",
    "url": "https://reddit.com/r/dataengineering/comments/1nodiup/fabric_is_the_new_standard_for_microsoft_in_data/",
    "score": 62,
    "num_comments": 42,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npftls",
    "title": "Why Python?",
    "content": "Why is the standard for data engineering to use python? all of our orchestration tools are python, libraries are python, even dbt and frontend stuff are python. \n\nwhy would we not use lower level languages like C or Rust? especially when it comes to orchestration tools which need to be precise on execution. or dataframe tools which need to be as memory efficient as possible (thank you duckdb and polars for making waves here).\n\nit seems almost counterintuitive python became the standard. i imagine its because theres so much overlap with data science and machine learning so the conversion was easier? \n\nedit: every response is just parroting the same thing that python is easy for noobs to pick up and understand. this doesnt really explain why our orchestrations tools and everything else need to use python. a good example here would be neovim, which is written in C but then easily extended via lua so people can rapidly iterate on it. why not have airflow written in c or rust and have dags written python for easy development? everyone seems to take this argumentative when i combat the idea that a lot of DE tools are unnecessarily written in python. ",
    "author": "shittyfuckdick",
    "timestamp": "2025-09-24T08:51:50",
    "url": "https://reddit.com/r/dataengineering/comments/1npftls/why_python/",
    "score": 0,
    "num_comments": 130,
    "upvote_ratio": 0.46,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1norvu7",
    "title": "What was you stack, tools,languages or framworks you knew when you got your first job?",
    "content": "These days when i read junior or entry jobs they need everything in one man, sql, python cloud and big data, more,  so this got me wondering what you guys had in your first jobs, and was it enough?",
    "author": "Beyond_Birthday_13",
    "timestamp": "2025-09-23T13:06:56",
    "url": "https://reddit.com/r/dataengineering/comments/1norvu7/what_was_you_stack_toolslanguages_or_framworks/",
    "score": 8,
    "num_comments": 8,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1noq2ax",
    "title": "Salesforce to Snowflake...",
    "content": "Currently we use DBAMP from SQL Server to query live data from our three salesforce instances. \n\nRight now the only Salesforce connection we have in Snowflake is a nightly load into our DataLake (This is handled by an outside company who manage those pipelines). We have expressed interest in moving over to Snowflake but we have concerns since the data that would be queried is in a Datalake format and a day behind. What are some solutions to having as close to possible live data in Snowflake? These are the current solutions I would think we have: \n\n*  Use Azure Data Factory to Pump important identified tables into snowflake every few hours. (This would be a lot of custom mapping and coding to get it to move over unless there was a magic select \\* into snowflake button. I wouldn't know if there is as I am new to ADF). \n* I have seen solutions for Zero Copy into Snowflake from Data Cloud but unsure on this as our Data Cloud is not set up. Would this be hard to set up? Expensive? ",
    "author": "Prize-Ad-5787",
    "timestamp": "2025-09-23T11:57:56",
    "url": "https://reddit.com/r/dataengineering/comments/1noq2ax/salesforce_to_snowflake/",
    "score": 8,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1no36ho",
    "title": "Please explain normalization to me like I'm a child :(",
    "content": "Hi guys! :) I hope it's the right place for this question.\nSo I have a databases and webtechnolgies exam on thursday and it's freaking me out. This is the first and probably last time I'm in touch with databases since it has absolutely nothing to do with my degree but I have to take this exam anyway. So you're taking to a noob :/\n\nI've been having my issues with normalization. I get the concept, I also kind of get what I'm supposed to do and somehow I manage to do it correctly. But I just don't understand and it freaks me out that I can normalize but don't know what I'm doing at the same time. \nSo the first normal form (english is not my mother tongue so ig thats what you'd call it in english) is to check every attribute of a table for atomicity. So I make another columns and so on. I get this one, it's easy. I think I have to do it so I avoid that there aren't many values? That's where it begins, I don't even know what one, I just do it and it's correct.  \nThen I go on and check for the second normal form. It has something to do with dependencies and keys. At this point I check the table and something in me says \"yeah girl, looks logical, do it\" and I make a second or third table so attributes that work together are in one table. Same problem, I don't know why I do it. And this is also where the struggle begins. I don't even know what I'm doing, I'm just doing it right, but I'm never doing it because I know. But it gets horrible with the third normal form. Transitive dependencies??? I don't even know what that exactly means. At this point I feel like I have to make my tables smaller and smaller and look for the minimal amount of attributes that need to be together to make sense. And I kind of get these right too ¡-¡ But I have make the most mistakes in the third form. \nBut the worst is this one way of spelling my professor uses sometimes. Something like A -&gt; B, B -&gt; CD or whatever.  It describes my tables and also dependencies? But I really don't get this one. We also have exercises where this spelling is the only thing given and I have to normalize only with that. I need my tables to manage this. \nMaybe you understand what I don't understand? I don't know why I exactly do it and I don't know what I actually have to look for. It freaks me out. I've been watching videos, asking ChatGPT, asking friends in my course and I just don't understand. At least I'm doing it right at some point. \n\nDo you think you can explain it to me? :(\n\n\nEdit: \nThanks to everyone who explained it to me!!! I finally understand and I'm so happy that I understand now! Makes everything so much easier, I never thought I'd ever get it, but I do! \nThank you &lt;3\n\nFor everyone that helped me,, I PASSED MY EXAM",
    "author": "soosmagmangos",
    "timestamp": "2025-09-22T17:25:38",
    "url": "https://reddit.com/r/dataengineering/comments/1no36ho/please_explain_normalization_to_me_like_im_a_child/",
    "score": 169,
    "num_comments": 50,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nouuv9",
    "title": "When you look at your current data pipelines and supporting tools, do you feel they do a good job of carrying not just the data itself, but also the metadata and semantics (context, meaning, definitions, lineage) from producers to consumers?",
    "content": "If you have achieved this, what tools/practices/choices got you there? And if not, where do you think are the biggest gaps? \n\n",
    "author": "brother_maynerd",
    "timestamp": "2025-09-23T15:03:27",
    "url": "https://reddit.com/r/dataengineering/comments/1nouuv9/when_you_look_at_your_current_data_pipelines_and/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1npeqdi",
    "title": "Prove me wrong - The entire big data industry is pointless merge sort passes over a shared mutable heap to restore per user physical locality",
    "content": "I just finished mangling a 100TB dataset with 300GB daily of ingest, my process was as follows:\n\n1. Freeze the postgres database by querying foreign keys, indexes, columns, tables and most importantly the mutable sequences of each table. Write the output to a file. At the same time, create a wal2json change data capture slot.\n\n2. Begin consuming the slot, during each transaction try to find the user_id, if found, serialize and write to an S3 user extent, checkpoint the slot and continue.\n\n3. Export the mutable row data using RDS to S3 (parquet) or querying raw page ranges over each table between id &gt; 0 and id &lt; step1.table.seq. \n\n3. Use spark or a network of EC2 nodes with thread pools/local scratch disks to read random pages above, perform multiple local merge sort passes to disk, then shuffle over the network until each node gets local data to resolve tables with orphaned foreign key records until you get all the user data on a single thread.\n\n4. Group the above by (user_id, the order the tables were designed/written to, then the row primary key). Write these to S3 like you did in step 1.\n\n5. All queries are now embarrassingly parallel and can be parallelized up to the total number of users in your data set because each users data is not mixed with other users.\n\nThis industry acts as though paying millions in spark/kafka/god knows what else clusters or the black box of snowflake is “a best practice”, but actual problem is the destroyed physical locality due to the mutable canonical schema in SQL databases that maintain a shared mutable heap underneath.\n\nThe future is event sourcing/log structured storage. Prove me wrong.",
    "author": "Due_Carrot_3544",
    "timestamp": "2025-09-24T08:10:35",
    "url": "https://reddit.com/r/dataengineering/comments/1npeqdi/prove_me_wrong_the_entire_big_data_industry_is/",
    "score": 0,
    "num_comments": 33,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nok03l",
    "title": "Bytewax is really cool - good bye PyFlink",
    "content": "I spent hours trying to make PyFlink work, what a pain to have a Python wrapper on top of Java JAR files. So many cryptic issues, we lost a week trying to make it work.\n\nWe then switched to Bytewax, everything got so much simpler, Dockerfile, Python code, and performance was even better!\n\nOf course, we can afford to make the switch because we had simple stateless real-time filtering &amp; dispatch use cases (quite classic really).\n\nThank you Bytewax, you saved us. That was my testimony.\n\nUPDATE: Thanks u/gangtao, pointing out that Bytewas is no longer maintained... Sad. Even though I will still use a fixed version image for my simple Kafka-&gt;Kafka filtering dispatch use case. (works nicely so far)",
    "author": "askolein",
    "timestamp": "2025-09-23T08:10:36",
    "url": "https://reddit.com/r/dataengineering/comments/1nok03l/bytewax_is_really_cool_good_bye_pyflink/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1noqi94",
    "title": "Made a self-hosted API for CRUD-ing JSON data. Useful for small but simple data storage.",
    "content": "I made a self-hosted API in go for **CRUD-ing JSON data**. It's optimized for simplicity and easy-use. I've added some helpful functions (like for appending, or incrementing values, ...). Perfect for small personal projects.\n\nTo get an idea, the API is based on your JSON structure. So the example below is for CRUD-ing \\[key1\\]\\[key2\\] in file.json.\n\n`DELETE/PUT/GET: /api/file/key1/key2/...`",
    "author": "_Rush2112_",
    "timestamp": "2025-09-23T12:14:31",
    "url": "https://reddit.com/r/dataengineering/comments/1noqi94/made_a_selfhosted_api_for_cruding_json_data/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1noicw9",
    "title": "Ideas for new stuff to do",
    "content": "Hi friends, I’m a data engineering team lead, I have about 5 DE right now. Most of us juniors, myself included (1.5 Years of experience before getting the position).\n\nRecently, one of my team members told me that she is feeling shcuka, because the work I assign her feels too easy and repetitive. She doesn’t feel technically challenged, and fearing she won’t progress as a DE. Sadly she’s right. Our PMs are weak, and mostly give us tasks like “add this new field to GraphQL query from data center X” or “add this field to SQL query”, and it’s really entry level stuff. AI could easily do it if it were integrated.\n\nSo I’m asking you, do you have ideas for stuff I can give here to do, or giving me sources of inspiration? Our stack is Vertica as DB, and airflow 2.10.4 for orchestration, and SQL or python for pipelines and ETLs. We also in advanced levels of evaluation of S3 and Spark.\n\nI’ll also add she is going through tough times, but I want advice about her growth as a data engineer.",
    "author": "Zestyclose-Basil-896",
    "timestamp": "2025-09-23T07:06:59",
    "url": "https://reddit.com/r/dataengineering/comments/1noicw9/ideas_for_new_stuff_to_do/",
    "score": 4,
    "num_comments": 5,
    "upvote_ratio": 0.61,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nnzk04",
    "title": "So,it's me or Airflow is kinda really hard ?",
    "content": "I'm DE intern and at our company we use dagster (i'm big fan) for orchestration. Recently, I started to get Airflow for my own since most of the jobs out there requires airflow and I'm kinda stuck. I mean, idk if it's just because I used dagster a lot in the last 6 months or the UI is really strange and not intuitive; or if the docker-compose is hard to setup. In your opinions, Airflow is a hard tool to masterize or am I being too stupid to understand ?\n\nAlso,  how do you guys initialize a project ? I saw a video with astro but I not sure if it's the standard way. I'd be happy if you could share your experience.",
    "author": "Morrgen",
    "timestamp": "2025-09-22T14:44:42",
    "url": "https://reddit.com/r/dataengineering/comments/1nnzk04/soits_me_or_airflow_is_kinda_really_hard/",
    "score": 95,
    "num_comments": 63,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1noglac",
    "title": "Need some genuine advice for a career path",
    "content": "Hi everyone,\n\nI’m a bit lost and hoping for advice from people who’ve been through similar situations.\n\nGraduated last year, worked 1 year as a frontend dev, then resigned. Right now I’m 2 months into a software developer trainee role. Most of what I do is around billing solutions basically connecting products, billing systems, payment gateways, and APIs.\n\n\nWhere I’m struggling:\n\n-I dont have a problem with my current work, but I find myself thinking sometimes if this kind of job would help me leverage my career and have a better salary in the next one or two years.\n\n-I’m interested in Cloud but I’m worried salaries for entry-level cloud roles might be lower, and I really need to save money right now.\n\n-I’ve thought about going into Full Stack Development, but most job postings ask for experience with CI/CD, containerization, and other tools I haven’t touched yet, which honestly feels overwhelming at this point.\n\nWhat I’ve done so far:\n\n-AWS Cloud Practitioner certified.(Wanna take this to the next lvl and add AWS SAA, but unsure if this is gonna be smart or not)\n-Built a few personal websites.\n-Revamping my portfolio.\n\n\nWhat I’m unsure about:\n\n-Should I stick to my current role for now and just see where it takes me?\n\n-Should I start focusing on cloud skills, even if that means a possible salary reset in the future?\n\n-or should I pivot toward full stack and slowly pick up DevOps-related tools along the way?\n\nI just don’t want to waste time going down the wrong path or put myself in a bad spot financially.\n\nAny advice would really mean a lot. ",
    "author": "Internal_Resort_4217",
    "timestamp": "2025-09-23T05:53:30",
    "url": "https://reddit.com/r/dataengineering/comments/1noglac/need_some_genuine_advice_for_a_career_path/",
    "score": 2,
    "num_comments": 3,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nnth0h",
    "title": "\"Design a Medallion architecture for 1TB/day of data with a 1hr SLA\". How would you answer to get the job?",
    "content": "from linkedisney",
    "author": "updated_at",
    "timestamp": "2025-09-22T10:52:23",
    "url": "https://reddit.com/r/dataengineering/comments/1nnth0h/design_a_medallion_architecture_for_1tbday_of/",
    "score": 112,
    "num_comments": 59,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1noatr8",
    "title": "What’s the hardest thing you’ve solved (or are struggling with) when building your own data pipelines/tools?",
    "content": "Hey folks,  \nRandom question for anyone who's built their own data pipelines or sync tools—what was the part that *really* made you want to bang your head on the wall?\n\nI'm asking because I'm a backend/data dev who went down the rabbit hole of building a “just works” sync tool for a non-profit (mostly SQL, Sheets, some cloud stuff). Didn’t plan to turn it into a project, but once you start, you kinda can't stop. \n\nAnyway, I hit every wall you can imagine—Google API scopes, scheduling, “why is my connector not working at 3am but fine at 3pm”, that sort of thing.\n\nCurious if others here have built their own tools, or just struggled with keeping data pipelines from turning into a pile of spaghetti?  \nBiggest headaches? Any tricks for onboarding or making it “just work”? Would honestly love to hear your stories (or, let's be real, war wounds).\n\nIf anyone wants to swap horror stories or lessons learned, I'm game. Not a promo post, just an engineer deep in the trenches.",
    "author": "SmundarBuddy",
    "timestamp": "2025-09-23T00:17:14",
    "url": "https://reddit.com/r/dataengineering/comments/1noatr8/whats_the_hardest_thing_youve_solved_or_are/",
    "score": 8,
    "num_comments": 19,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nnyn8w",
    "title": "Forget Indeed/LinkedIn, what are your favorite sites to find data engineering jobs?",
    "content": "LinkedIn is ok but has lots of reposted + promoted + fake jobs from staffing agencies, and Indeed is just really bad for tech jobs in general. I'm curious what everyone's favorite sites are for finding data engineering roles? I'm mainly interested in US and Canada jobs, ideally remote, but you can still share any sites you know that are global so that other people can benefit.\n\nedit - recapping the suggestions shared below: [Dice](https://www.dicecom/), [Meterwork](https://meterwork.com/), Twitter, [OuterJoin](https://outerjoinus/)",
    "author": "jjzwork",
    "timestamp": "2025-09-22T14:07:39",
    "url": "https://reddit.com/r/dataengineering/comments/1nnyn8w/forget_indeedlinkedin_what_are_your_favorite/",
    "score": 58,
    "num_comments": 25,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1noan8x",
    "title": "What is your approach for backfilling data?",
    "content": "What is your approach to backfilling data? Do you exclusively use date parameters in your pipelines? Or, do you have a more modular approach within your code that allows you to dynamically determine the `WHERE` clause for data reingestion?\n\nAlternatively, do you primarily rely on a script with date parameters and then create ad-hoc scripts for specific backfills, such as for a single customer?",
    "author": "TheOneWhoSendsLetter",
    "timestamp": "2025-09-23T00:05:17",
    "url": "https://reddit.com/r/dataengineering/comments/1noan8x/what_is_your_approach_for_backfilling_data/",
    "score": 8,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1noegss",
    "title": "Best partners for informatica Power center to cloud migration",
    "content": "We are exploring migration options for Informatica PowerCenter workloads to the cloud. Curious to hear from the community, who are the best partners or providers you have seen in this space? ",
    "author": "One_Veterinarian7053",
    "timestamp": "2025-09-23T04:10:26",
    "url": "https://reddit.com/r/dataengineering/comments/1noegss/best_partners_for_informatica_power_center_to/",
    "score": 3,
    "num_comments": 8,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1no8ukf",
    "title": "Large language model usecases",
    "content": "Hello,\n\nWe have a thirdparty LLM usecase in which the application is submitting queries to snowflake database and the few of the usecases , are using XL size warehouse but still running beyond 5minutes. The team is asking to use bigger warehouses(2XL) and the LLM suite has \\~5minutes time limit to provide the results back.\n\nSo wants to understand, In LLM-driven query environments like , where users may unknowingly ask very broad or complex questions (e.g., requesting large date ranges or detailed joins), the generated SQL can become resource-intensive and costly. Is there a recommended approach or best practice to sizing the warehouse in such use cases? Additionally, how do teams typically handle the risk of unpredictable compute consumption? \n\n",
    "author": "Upper-Lifeguard-8478",
    "timestamp": "2025-09-22T22:14:53",
    "url": "https://reddit.com/r/dataengineering/comments/1no8ukf/large_language_model_usecases/",
    "score": 9,
    "num_comments": 8,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1no4frv",
    "title": "Built a C++ chunker while working on something else, now open source",
    "content": "While building another project, I realized I needed a really fast way to chunk big texts. Wrote a quick C++ version, then thought, why not package it and share?\n\nRepo’s here: [https://github.com/Lumen-Labs/cpp-chunker](https://github.com/Lumen-Labs/cpp-chunker)\n\nIt’s small, but it does the job. Curious if anyone else finds it useful.",
    "author": "Odd-Stranger9424",
    "timestamp": "2025-09-22T18:25:36",
    "url": "https://reddit.com/r/dataengineering/comments/1no4frv/built_a_c_chunker_while_working_on_something_else/",
    "score": 9,
    "num_comments": 0,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nnx9bi",
    "title": "VectorLiteDB - a vector DB for local dev, like SQLite but for vectors",
    "content": " A simple, embedded vector database that stores everything in a single file, just like SQLite.\n\n\n\n⭐ [VectorLiteDB](https://github.com/vectorlitedb/vectorlitedb)\n\n\n\nFeedback on both the tool and the approach would be really helpful.\n\n* Is this something that would be useful\n* Use cases you’d try this for\n\n[https://github.com/vectorlitedb/vectorlitedb](https://github.com/vectorlitedb/vectorlitedb)",
    "author": "nagstler",
    "timestamp": "2025-09-22T13:14:20",
    "url": "https://reddit.com/r/dataengineering/comments/1nnx9bi/vectorlitedb_a_vector_db_for_local_dev_like/",
    "score": 21,
    "num_comments": 9,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nnhtxt",
    "title": "Why Don’t Data Engineers Unit Test Their Spark Jobs?",
    "content": "I've often wondered why so many Data Engineers (and companies) *don't unit/integration tes*t their Spark Jobs.\n\nIn my experience, the main reasons are:\n\n* Creating DataFrame fixtures (data and schemas) takes too much time .\n* Debugging jobs unit tests with multiple tables is complicated.\n* Boilerplate code is verbose and repetitive.\n\nTo address these pain points, I built [https://github.com/jpgerek/pybujia (opensource)](https://github.com/jpgerek/pybujia), a toolkit that:\n\n* Lets you define table fixtures using Markdown, making DataFrame creation, debugging and readability. much easier.\n* Generalizes the boilerplate to save setup time.\n* Fits for integrations tests (the whole spark job), not just unit tests.\n* Provides helpers for common Spark testing tasks.\n\nIt's made testing Spark jobs much easier for me, now I do *TDD*, and I hope it helps other Data Engineers as well.",
    "author": "jpgerek",
    "timestamp": "2025-09-22T02:28:29",
    "url": "https://reddit.com/r/dataengineering/comments/1nnhtxt/why_dont_data_engineers_unit_test_their_spark_jobs/",
    "score": 116,
    "num_comments": 103,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1no1he6",
    "title": "Handling File Precedence for Serverless ETL Pipeline",
    "content": "We're moving our ETL pipeline from Lambda and Step Functions to AWS Glue, however I'm having trouble figuring out how to handle file sequencing. We employ three Lambda functions to extract, transform, and load data in our current configuration. Step Functions manages all of this. The state machine takes all the S3 file paths that come from each Lambda and sends them to the load Lambda as a list. Each Transform Lambda can make one or more output files. The load Lambda understands exactly how to process the files since we control the order in that list and utilize environment variables to assist it understand the file roles. All of the files end up in the same S3 folder.  \nThe problem I'm having right now is that our new Glue task will produce a lot of files, and those files will need to be processed in a certain order. For instance, file1 has to be processed before file2. Right now, I'm using S3 event triggers to start the load Lambda, but S3 only fires one event per file, which messes up the order logic. To make things even worse, I can't change the load Lambda, and I want to maintain the system completely serverless and separate, which means that the Glue task shouldn't call any Lambdas directly.  \nI'm searching for suggestions on how to handle processing files in order in this kind of setup. When Glue sends many files to the same S3 folder, is there a clean, serverless technique to make sure they are in the right order?",
    "author": "dosa-palli-chutney",
    "timestamp": "2025-09-22T16:07:14",
    "url": "https://reddit.com/r/dataengineering/comments/1no1he6/handling_file_precedence_for_serverless_etl/",
    "score": 5,
    "num_comments": 5,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nnzt3r",
    "title": "Airbyte OSS - cannot create connection (not resolving schema)",
    "content": "I've deployed Airbyte OSS locally to evaluate it and see how it stacks up against something like Fivetran - if someone wanted to use an OSS data ingestion tool, alongside dbt Core for instance.\n\nI'm deploying this on my Windows 11 work laptop, which may not helps things but it is what it is. \n\nI've already got an OpenSSH / sFTP server on my laptop on which I've deployed some files for Airbyte to ingest into a local database.   Airbyte v0.30.1 is installed,  Docker Desktop is running and my local instance of Airbyte appears to be working fine.\n\nI've created the connections to the sFTP server and the local database, and these tested fine in the local Airbyte web UI.  In the logs and Event Viewer, I can also see the Airbyte account logging into the sFTP server without any problems.\n\nI get now stuck in creating the Airbyte Connection in the local web UI - after picking source and target, and sync mode,  it's not showing any schema whatsoever.  Even when I change the Airbyte file source to point to one specific file, it just isn't seeing showing a schema. \n\nI've checked the user account that logs into the sFTP server and it has all the privs it needs.  When I use the same account in WinSCP, I can connect just fine - and I can view, download, rename, delete, move, etc. any file on the sFTP server itself,  so I'm not sure if there's an issue with the sFTP user account privs?\n\nAny idea on why Airbyte cannot read the schema? I've been trying to look at logs in the Docker image but haven't found anything useful yet. \n\nIs there a way to more accurately debug this process somehow?",
    "author": "Unarmed_Random_Koala",
    "timestamp": "2025-09-22T14:55:10",
    "url": "https://reddit.com/r/dataengineering/comments/1nnzt3r/airbyte_oss_cannot_create_connection_not/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1no1qn7",
    "title": "Informatica to DBT migration inquiries",
    "content": "Hey guys! As you can read in the title I am a working on migrating/converting some Informatica mappings to dbt models. Have you ever done it?\n\nIt is kind of messy and confusing for me since I am a fresher/newbie and some mappings have many complex transformations. \n\nCould you give me any advice or any resources to look at to have a clearer idea of each transformation equivalent in SQL/dbt?\n\nThank you!",
    "author": "Think_Net7196",
    "timestamp": "2025-09-22T16:18:48",
    "url": "https://reddit.com/r/dataengineering/comments/1no1qn7/informatica_to_dbt_migration_inquiries/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nnt1ap",
    "title": "Advanced learning on AWS Redshift",
    "content": "Hello all,\n\nI would like to learn about AWS REDSHIFT. I have completed small projects on creating cluster and tables and reading/writing data from glue jobs. But I want to learn how redshift being used in industry. Are there any resource to help me learn that.",
    "author": "dosa-palli-chutney",
    "timestamp": "2025-09-22T10:36:08",
    "url": "https://reddit.com/r/dataengineering/comments/1nnt1ap/advanced_learning_on_aws_redshift/",
    "score": 8,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nog40w",
    "title": "Data Engineers: Which tool are you picking for pipelines in 2025 - Spark or dbt?",
    "content": "Data Engineers: Which tool are you picking for pipelines in 2025 - Spark or dbt? Share your hacks!\n\nHey r/dataengineering, I’m diving into the 2025 data scene and curious about your go-to tools for building pipelines. Spark’s power or dbt’s simplicity - what’s winning for you? Drop your favorite hacks (e.g., optimization tips, integrations) below!\n\n📊 Poll:\n\n1. Spark\n2. dbt\n3. Both\n4. Other (comment below)\n\nLooking forward to learning from your experience!",
    "author": "Weird_Mycologist_268",
    "timestamp": "2025-09-23T05:32:10",
    "url": "https://reddit.com/r/dataengineering/comments/1nog40w/data_engineers_which_tool_are_you_picking_for/",
    "score": 0,
    "num_comments": 17,
    "upvote_ratio": 0.21,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nnu2v7",
    "title": "Getting started with pipeline observability &amp; monitoring",
    "content": "Hello,\n\nI am ending my first DE project, using million song dataset and I am looking for good resources, courses about data observability and monitoring for pipelines.\n\nThanks for all resources!",
    "author": "AffectionateSeat4323",
    "timestamp": "2025-09-22T11:14:23",
    "url": "https://reddit.com/r/dataengineering/comments/1nnu2v7/getting_started_with_pipeline_observability/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nnoy2g",
    "title": "How to convert Oracle Db queries to MySQL.",
    "content": "I have a new project to rebuild few reports in Power BI which have been running in Oracle fusion.\nSo client gave the data as CSV files. I used python and ssms and setuped the base data.\n\nNow to create reports in power bi. I have to replicate the Oracle queries which they used in fusion to create reports into SQL to create a view and use it in power bi. \nI managed to recreate few using Gpt.\nBut when this parameter things come in this oracle query it's getting hard to convert.\n\nHave anyone done oracle fusion to power bi/sql migration. Or is there any specific tool by which I can easily convert the queries.\n\nThanks in advance.\n\nEdit.\nIt's not to MySql, want to convert query to MSSQL",
    "author": "tech-Brain",
    "timestamp": "2025-09-22T08:03:59",
    "url": "https://reddit.com/r/dataengineering/comments/1nnoy2g/how_to_convert_oracle_db_queries_to_mysql/",
    "score": 4,
    "num_comments": 6,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "DataEngineering",
    "post_id": "1nmw03j",
    "title": "Ok folks ... H1b visa's now cost 100k .. is the data engineering role affected?",
    "content": "Asking for a friend :) ",
    "author": "TheOverzealousEngie",
    "timestamp": "2025-09-21T09:10:09",
    "url": "https://reddit.com/r/dataengineering/comments/1nmw03j/ok_folks_h1b_visas_now_cost_100k_is_the_data/",
    "score": 133,
    "num_comments": 148,
    "upvote_ratio": 0.79,
    "is_original_content": false
  }
]