[
  {
    "subreddit": "Python",
    "post_id": "1occonw",
    "title": "T-Strings: Python's Fifth String Formatting Technique?",
    "content": "Every time I've talked about Python 3.14's new t-strings online, many folks have been confused about how t-strings are different from f-strings, why t-strings are useful, and whether t-strings are a replacement for f-strings.\n\n  \nI published [a short article (and video) on Python 3.14's new t-strings](https://pym.dev/t-strings-in-python/) that's meant to explain this.\n\nThe TL;DR:\n\n* Python has had 4 string formatting approaches before t-strings\n* T-strings are different because they *don't actually return strings*\n* T-strings are useful for library authors who need the disassembled parts of a string interpolation for the purpose of pre-processing interpolations\n* T-strings definitely do not replace f-strings: keep using f-strings until specific libraries tell you to use a t-string with one or more of their utilities\n\nWatch the video or read the article for a short demo and a library that uses them as well.\n\nIf you've been confusing about t-strings, I hope this explanation helps.",
    "author": "treyhunner",
    "timestamp": "2025-10-21T06:16:36",
    "url": "https://reddit.com/r/Python/comments/1occonw/tstrings_pythons_fifth_string_formatting_technique/",
    "score": 128,
    "num_comments": 45,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ocdw2j",
    "title": "GUI Toolkit Slint 1.14 released with universal transforms, asyncio and a unified text engine",
    "content": "We‚Äôre proud to release [\\#Slint](https://chat.slint.dev/public/channels/docs-internal#) 1.14 üíô with universal transforms üåÄ, [\\#Python](https://chat.slint.dev/public/channels/docs-internal#) asyncio üêç, and a unified text engine with fontique and parley üñãÔ∏è  \nRead more about it in the blog here üëâ [https://slint.dev/blog/slint-1.14-released](https://slint.dev/blog/slint-1.14-released)    \n  \n",
    "author": "slint-ui",
    "timestamp": "2025-10-21T07:05:25",
    "url": "https://reddit.com/r/Python/comments/1ocdw2j/gui_toolkit_slint_114_released_with_universal/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ochltx",
    "title": "IDS Project in Python",
    "content": "Hello everyone,\n\n\n\nI recently uploaded a repository to GitHub where I created an IDS in Python. I would appreciate any feedback and suggestions for improvement.\n\n[https://github.com/javisys/IDS-Python](https://github.com/javisys/IDS-Python)\n\nThank you very much, best regards.",
    "author": "Javi_16018",
    "timestamp": "2025-10-21T09:28:08",
    "url": "https://reddit.com/r/Python/comments/1ochltx/ids_project_in_python/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ocg3p5",
    "title": "Python Pest - A port of Rust's pest",
    "content": "I recently released Python Pest, a port of the Rust pest parsing library.\n\n# What My Project Does\n\nPython‚ÄØPest is a declarative PEG parser generator for Python, ported from Rust's Pest. You write grammars instead of hand-coding parsing logic, and it builds parse trees automatically.\n\nDefine a grammar using Pest version 2 syntax, like this:\n\n    jsonpath        = _{ SOI ~ jsonpath_query ~ EOI }\n    jsonpath_query  = _{ root_identifier ~ segments }\n    segments        = _{ (S ~ segment)* }\n    root_identifier = _{ \"$\" }\n    \n    segment = _{\n      | child_segment\n      | descendant_segment\n    }\n    \n    // snip\n\nAnd traverse parse trees using [structural pattern matching](https://peps.python.org/pep-0636/), like this:\n\n    def parse_segment(self, segment: Pair) -&gt; Segment:\n        match segment:\n            case Pair(Rule.CHILD_SEGMENT, [inner]):\n                return ChildSegment(segment, self.parse_segment_inner(inner))\n            case Pair(Rule.DESCENDANT_SEGMENT, [inner]):\n                return RecursiveDescentSegment(segment, self.parse_segment_inner(inner))\n            case Pair(Rule.NAME_SEGMENT, [inner]) | Pair(Rule.INDEX_SEGMENT, [inner]):\n                return ChildSegment(segment, [self.parse_selector(inner)])\n            case _:\n                raise JSONPathSyntaxError(\"expected a segment\", segment)\n\nSee [docs](https://jg-rp.github.io/python-pest/), [GitHub](https://github.com/jg-rp/python-pest) and [PyPi](https://pypi.org/project/python-pest/) for a complete example.\n\n# Target Audience\n\n* Python developers who need to parse custom languages, data formats, or DSLs.\n* Anyone interested in grammar-first design over hand-coded parsers.\n* Developers curious about leveraging Python's match/case for tree-walking.\n\n# Comparison\n\nParsimonious is another general purpose, pure Python parser package that reads parsing expression grammars. Python Pest differs in grammar syntax and subsequent tree traversal technique, preferring external iteration of parse trees instead of defining a visitor.\n\n# Feedback\n\nI'd appreciate any feedback, especially your thoughts on the trade-off between declarative grammars and performance in Python. Does the clarity and maintainability make up for slower execution compared to hand-tuned parsers?\n\nGitHub: [https://github.com/jg-rp/python-pest](https://github.com/jg-rp/python-pest)",
    "author": "Hefty-Pianist-1958",
    "timestamp": "2025-10-21T08:31:24",
    "url": "https://reddit.com/r/Python/comments/1ocg3p5/python_pest_a_port_of_rusts_pest/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oc4122",
    "title": "NGXSMK GameNet Optimizer: A Python-Powered, Privacy-First System and Network Optimization",
    "content": "I'm excited to share **NGXSMK GameNet Optimizer**, a comprehensive, open-source tool written primarily in **Python** designed to enhance system and network performance for gamers.\n\nWhile the primary use case is gaming, the core is a set of Python modules for process management, network analysis, and system configuration, making it a great example of Python for low-level system interaction on Windows/Linux.\n\n# What My Project Does\n\nNGXSMK GameNet Optimizer is a utility suite that addresses common performance bottlenecks by providing:\n\n* **Network Optimization:** Uses a Python module to analyze and test latency to various global servers (especially for games like League of Legends) and includes a traffic shaper to prioritize gaming packets (QoS).\n* **System Performance:** Manages system resources by setting high process priority for games, cleaning up unnecessary background applications, and optimizing RAM usage in real-time.\n* **System-Agnostic Core:** The majority of the logic is contained in cross-platform Python scripts (`main.py`, `modules/`), with platform-specific commands handled by batch/shell scripts (`run.bat`, `run.sh`).\n\nTarget Audience\n\nThis tool is primarily for **PC Gamers** who are performance-conscious and want a free, transparent alternative to commercial \"game booster\" software.\n\nFrom a development perspective, the **Target Audience** also includes **Python developers** interested in:\n\n* Python for **system programming** (e.g., process and memory management on Windows/Linux).\n* Building **cross-platform utility applications** with a Python backend.\n\nThis is meant to be a **production-ready utility** that is robust and reliable for daily use.\n\n# Comparison\n\nNGXSMK GameNet Optimizer differentiates itself from existing optimization software in two key areas:\n\n|| || |**Feature**|**NGXSMK GameNet Optimizer**|**Commercial Alternatives (e.g., Razer Cortex)| |Source Code**|**100% Open Source (MIT Licensed)|Closed Source| |Data/Telemetry**|**Privacy-First (No Telemetry, All Local)|Often collect usage data| |Customization**|Python-based modules are easily auditable and modifiable.|Configuration limited to the provided UI.| |**Core Function**|Focuses on Network Quality, FPS, and RAM.|Varies, often focuses heavily on simple process termination.|\n\nYou can find the full source code and installation steps on GitHub:\n\n[**GitHub Repository: toozuuu/ngxsmk-gamenet-optimizer**](https://github.com/toozuuu/ngxsmk-gamenet-optimizer)\n\n**Public Release:** [**https://github.com/toozuuu/ngxsmk-gamenet-optimizer/releases**](https://github.com/toozuuu/ngxsmk-gamenet-optimizer/releases)\n\nFeel free to check out the code and provide any feedback, particularly on the Python modules for system-level operations!",
    "author": "Forsaken_Lie_9989",
    "timestamp": "2025-10-20T21:56:16",
    "url": "https://reddit.com/r/Python/comments/1oc4122/ngxsmk_gamenet_optimizer_a_pythonpowered/",
    "score": 12,
    "num_comments": 3,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oborib",
    "title": "I built a Persistent KV Store in Pure Python",
    "content": "Hi everyone!\n\n I'm a final year CS student and I've been reading about data storage and storage engines. This is a passion project that I've been working on for the past few months. It is a lightweight, persistent key-value storage engine in Python, built from scratch to understand and implement the Log-Structured Merge-tree (LSM-tree) architecture. The project, which is fully open-source, is explicitly optimized for write-heavy workloads. \n\n\n\n# Core Architecture:\n\nThe engine implements the three fundamental LSM components: the **Write Ahead Log (WAL)** for durability, an in-memory **Memtable** (using `SortedDict` for sorted writes), and immutable persistent **SSTables (Sorted String Tables)**.\n\nSome features that I'm proud of:\n\n* **Async Compaction**: Merging and compaction are handled by a **separate background worker thread**. The process itself takes a hybrid approach.\n* **Client/Server Model**: The entire storage engine runs behind a **FastAPI server**. This allows multiple clients to connect via REST APIs or the included CLI tool.\n* **Efficient Range Queries**: Added full support for range queries from `start_key` to `end_key`. This is achieved via a memory-efficient **k-way merge iterator** that combines results from the Memtable and all SSTables. The FastAPI server delivers the results using a `StreamingResponse` to prevent memory exhaustion for large result sets.\n* **Bloom Filter**: Implemented a **Bloom Filter** for each SSTable to drastically reduce disk I/O by confirming that a key definitely does not exist before attempting a disk seek.\n* **Binary Storage**: SSTables now use **Msgpack binary format** instead of JSON for smaller file sizes and reduced CPU load during serialization/deserialization.\n\nMy favourite part of the project is that I actually got to see a practical implementation of [Merge Sorted Arrays - GeeksforGeeks](https://www.geeksforgeeks.org/dsa/merge-k-sorted-arrays/). This is a pretty popular interview question and to see DSA being actually implemented is a crazy moment.\n\n# Get Started\n\n    pip install lsm_storage_engine_key_value_store\n\n**Usage via CLI/Server:**\n\n1. **Terminal 1 (Server):** `lsm-server`\n2. **Terminal 2 (Client):** `lsm-cli` (Follow the CLI help for commands).\n\n# Looking for Feedback\n\nI'd love to hear your thoughts about this implementation and how I can make it better and what features I can add in later versions. Ideas and constructive criticism are always welcome. I'm also looking for contributors, if anyone is interested, please feel free to PM and we can discuss.\n\nRepo link: [Shashank1985/storage-engine](https://github.com/Shashank1985/storage-engine)  \nThanks!!",
    "author": "vollhard-natta",
    "timestamp": "2025-10-20T10:51:07",
    "url": "https://reddit.com/r/Python/comments/1oborib/i_built_a_persistent_kv_store_in_pure_python/",
    "score": 69,
    "num_comments": 11,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ocib9b",
    "title": "I created a Riot API library for python",
    "content": "Hello all,\n\nI've been working on a super simple api wrapper for league of legends and would love some feedback.\n\n[https://github.com/diodemusic/pyke](https://github.com/diodemusic/pyke)\n\nThanks :)",
    "author": "Electrical-Lab-5952",
    "timestamp": "2025-10-21T09:55:00",
    "url": "https://reddit.com/r/Python/comments/1ocib9b/i_created_a_riot_api_library_for_python/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ocgcqh",
    "title": "New UV Gitlab Component",
    "content": "I tried today to recreate a GitHub action which provides a python \\`uv setup as a GitLab CI component.\n\n# What this Component achieves\n\nWhile the documentation of UV already explains how to implement `uv` inside of GitLab CI, it still fills the `.gitlab-ci.yml` quite a bit.\n\nMy Component tries to minimize that, by also providing a lot of customizations.\n\n# Examples\n\nThe following example demonstrates how to implement the component on gitlab.com:\n\n    include:\n      - component: $CI_SERVER_FQDN/gitlab-uv-templates/python-uv-component/python-uv@1.0.0\n    \n    single-test:\n      extends: .python-uv-setup\n      stage: test\n      script:\n        - uv run python -c \"print('Hello UV!')\"\n\nThe next examples demonstrate how to achieve parallel matrix execution:\n\n    include:\n      - component: $CI_SERVER_FQDN/gitlab-uv-templates/python-uv-component/python-uv@1.0.0\n        inputs:\n          python_version: $PYTHON_V\n          uv_version: 0.9.4\n          base_layer: bookworm-slim\n    \n    matrix-test:\n      extends: .python-uv-setup\n      stage: test\n      parallel:\n        matrix:\n          - PYTHON_V: [\"3.12\", \"3.11\", \"3.10\"]\n      script:\n        - uv run python --version\"\n      variables:\n        PYTHON_V: $PYTHON_V\n\n# Comparison\n\nI am not aware of any public component which achieves similar as demonstrated above.\n\nI am quite happy about the current result, which I published via the GitLab CI/CD catalogue:\n\n[https://gitlab.com/explore/catalog/gitlab-uv-templates/python-uv-component](https://gitlab.com/explore/catalog/gitlab-uv-templates/python-uv-component)",
    "author": "MaKaNuReddit",
    "timestamp": "2025-10-21T08:41:07",
    "url": "https://reddit.com/r/Python/comments/1ocgcqh/new_uv_gitlab_component/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ocd9i4",
    "title": "Advice for a Javascript/Typescript dev getting into the python ecosystem",
    "content": "I'm a typescript dev that worked with frontend frameworks and nodejs for the last 10 years.\n\nI just joined a startup and I'm required to build a serverless rest api with a python based stack.\n\nThe problem is that I have around a few days to figure out what's considered industry standard currently for the python ecosystem, and I can't afford to take any wrong turns here.\n\nOf course the particularities of the project might affect your answer to some degree and I'm aware of that, but for the sake of trying to point me to the right direction let's try to make the best out of this.\n\nI would make some typescript analogies in order for you to better understand what I'm aiming at with the stack.\n\n1.ORM - drizzle (will use postgres)\n2.Deployment - vercel/fallback to aws lambda\n3.Package manager - pnpm\n4.Types - typescript\n\nThe most uncertainities I have are about the platform where I have to deploy this(I really want something that is serverless and has good DX), vercel is such a no brainer rn for typescript projects, and I wonder if I have similar no brainers in python as well.\n\nI have read about modal for deploying FastAPI, but again I'm not sure.\n\nReally appreciate anyone taking time to answer this.\n",
    "author": "Lupexlol",
    "timestamp": "2025-10-21T06:40:36",
    "url": "https://reddit.com/r/Python/comments/1ocd9i4/advice_for_a_javascripttypescript_dev_getting/",
    "score": 1,
    "num_comments": 9,
    "upvote_ratio": 0.54,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ocq03z",
    "title": "Hi introducing python, CLI Tool called evilwaf most powerful firewall bypass V2.2 was released",
    "content": "\n\n\nNow evilwaf supports more than 11 firewall bypass techniques includes \n\nCritical risk: Direct Exploitation\n  ‚Ä¢ HTTP Request Smuggling\n  ‚Ä¢JWT Algorithm Confusion\n  ‚Ä¢HTTP/2 Stream Multiplexing\n  ‚Ä¢WebAssembly Memory Corruption\n  ‚Ä¢cache poisoning\n  ‚Ä¢web cache poisoning\n\n  High risk: Potential Exploitation\n  ‚Ä¢SSTI Polyglot Payloads\n  ‚Ä¢gRPC/Protobuf Bypass\n  ‚Ä¢GraphQL Query Batching\n  ¬∞ML WAF Evasion\n\n  Medium risk: Information Gathering\n  ¬∞ Subdomain Discovery\n  ¬∞ DNS History Bypass\n  ¬∞ Header Manipulation\n  ¬∞ Advanced Protocol Attacks\n\nFor more info visit GitHub repo: https://github.com/matrixleons/evilwaf",
    "author": "Tricky-Frosting9047",
    "timestamp": "2025-10-21T14:42:39",
    "url": "https://reddit.com/r/Python/comments/1ocq03z/hi_introducing_python_cli_tool_called_evilwaf/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.22,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obmzbm",
    "title": "func-to-web is now much better ‚Äì Thanks for the feedback!",
    "content": "15 days ago I shared func-to-web here and got amazing feedback (150+ upvotes, thank you!). Since then, I've been working hard on the suggestions and added some major features.\n\n**What it does (quick reminder):**\nTurn any Python function into a web UI with zero boilerplate:\n\n```python\nfrom func_to_web import run\n\ndef divide(a: int, b: int):\n    return a / b\n\nrun(divide)  # Web form at localhost:8000\n```\n\n**Major updates since v0.1:**\n\n**Dynamic Lists** ‚Äì Add/remove items with advanced validation:\n```python\ndef process_data(\n    # Dynamic lists with add/remove buttons\n    images: list[ImageFile],                      # Multiple file uploads\n    \n    # Dual validation: list size AND individual items\n    scores: Annotated[\n        list[Annotated[int, Field(ge=0, le=100)]], \n        Field(min_length=3, max_length=10)\n    ],  # 3-10 items required, each 0-100\n    \n    # Optional fields with toggle switches\n    notes: str | None = None,                     # Optional text\n    tags: list[str] | None = None                 # Optional list\n):\n    return FileResponse(generate_pdf(), \"report.pdf\")  # Auto-download\n```\n\n**High-Performance File Handling** ‚Äì Optimized streaming for large files:\n- **Upload**: Real-time progress bars, 8MB chunks, handles GB+ files\n- **Download**: Return `FileResponse(data, filename)` for auto-downloads\n- **Performance**: ~237 MB/s localhost, ~115 MB/s over Gigabit Ethernet\n- **Memory efficient**: Constant usage regardless of file size\n- **Any format**: PDF, Excel, ZIP, images, binary data\n\n**Optional Fields** ‚Äì `Type | None` creates toggle switches:\n- Fields with defaults start enabled, without defaults start disabled\n- Explicit control: `Type | OptionalEnabled/OptionalDisabled`\n- Works with all types, constraints, and lists\n\n**Dynamic Dropdowns** ‚Äì Runtime-generated options:\n```python\ndef get_themes(): return fetch_from_database()\n\ndef configure(theme: Literal[get_themes]): pass  # Fresh options each request\n```\n\n**Rich Output Support**:\n- **PIL Images**: Auto-displayed in browser\n- **Matplotlib plots**: Rendered as PNG\n- **File downloads**: Single or multiple files with streaming\n- **JSON/text**: Formatted with copy-to-clipboard\n\n**UX Improvements**:\n- Dark mode with theme persistence\n- Keyboard shortcuts (Ctrl+Enter to submit)\n- Auto-focus first field\n- Toast notifications\n- Upload progress with speed indicators\n\n**Current stats:**\n- 180+ GitHub stars (The chinese community is sharing it too!)\n- 454 unit tests\n- Published on PyPI: `pip install func-to-web`\n- 20+ runnable examples\n- Used daily for internal tools at multiple companies\n\n**Other improvements:**\n- **Modular architecture**: Code separated by responsibilities (analysis, validation, form building...)\n- **Comprehensive documentation**: Every function and class documented\n- **Detailed changelog**: Track all improvements and breaking changes\n\nI've tried to make this as professional and production-ready as possible while keeping the simple API.\n\nStill focused on internal tools and rapid prototyping, not replacing proper web frameworks.\n\nGitHub: https://github.com/offerrall/FuncToWeb\n\nThe community feedback really shaped these improvements. Thank you again! Keep the suggestions coming.",
    "author": "drboom9",
    "timestamp": "2025-10-20T09:27:41",
    "url": "https://reddit.com/r/Python/comments/1obmzbm/functoweb_is_now_much_better_thanks_for_the/",
    "score": 22,
    "num_comments": 0,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ob3na1",
    "title": "I built a tool that tells you how hard a website is to scrape",
    "content": "# UPDATE:\n\nWebsite is now live!\n\nTry it now: [https://www.caniscrape.org](https://www.caniscrape.org/)\n\n\\- No installation required\n\n\\- Instant analysis\n\n\\- Same comprehensive checks as the CLI\n\nNOTE:  \nI haven't added the flag capabilities yet so its just the default scan. Its also still one link at a time, so all the great ideas I've received for the website will come soon (I'm gonna keep working on it). It'll take about 1-3 days but ill make it a lot better for the V1.0.0 release.\n\nCLI still available on GitHub for those who prefer it.\n\n\n\nHi everyone,  \nI made a Python package called **caniscrape** that analyzes any website's anti-bot protections before you start scraping.\n\nIt tells you what you're up against (Cloudflare, rate limits, JavaScript rendering, CAPTCHAs, TLS fingerprinting, honeypots) and gives you a difficulty score + specific recommendations.\n\n# What My Project Does\n\ncaniscrape checks a website for common anti-bot mechanisms and reports:\n\n* A difficulty score (0‚Äì10)\n* Which protections are active (e.g., Cloudflare, Akamai, hCaptcha, etc.)\n* What tools you‚Äôll likely need (headless browsers, proxies, CAPTCHA solvers, etc.)\n* Whether using a scraping API might be better\n\nThis helps you decide the right scraping approach before you waste time building a bot that keeps getting blocked.\n\n# Target Audience\n\n* Web scrapers, data engineers, and researchers who deal with protected or dynamic websites\n* Developers who want to test bot-detection systems or analyze site defenses\n* Hobbyists learning about anti-bot tech and detection methods\n\nIt‚Äôs not a bypassing or cracking tool ‚Äî it‚Äôs for diagnostics and awareness.\n\n# Comparison\n\nUnlike tools like WAFW00F or WhatWaf, which only detect web application firewalls,  \ncaniscrape runs multi-layered tests:\n\n* Simulates browser and bot requests (via Playwright)\n* Detects rate limits, JavaScript challenges, and honeypot traps\n* Scores site difficulty based on detection layers\n* Suggests scraping strategies or alternative services\n\nSo it‚Äôs more of a pre-scrape analysis toolkit, not just a WAF detector.\n\n# Installation\n\n    pip install caniscrape\n\nQuick setup (required):\n\n    playwright install chromium  # Download browser\n    pipx install wafw00f         # WAF detection\n\n# Example Usage\n\n    caniscrape https://example.com\n\nOutput includes:\n\n* Difficulty score (0‚Äì10)\n* Active protections\n* Recommended tools/approach\n\n# ADVICE:\n\nResults can vary between runs because bot protections adapt dynamically.  \nSome heavy-protection sites (like Amazon) may produce these varied results. Of course, this will improve over time, but running the command multiple times can mitigate this.\n\n# GitHub\n\n[https://github.com/ZA1815/caniscrape](https://github.com/ZA1815/caniscrape)",
    "author": "CrroakTTV",
    "timestamp": "2025-10-19T15:50:55",
    "url": "https://reddit.com/r/Python/comments/1ob3na1/i_built_a_tool_that_tells_you_how_hard_a_website/",
    "score": 479,
    "num_comments": 38,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obrz11",
    "title": "Assembly-to-Minecraft-Command-Block-Compiler (Python) ‚Äî updated ‚Äî testers &amp; contributors wanted",
    "content": "¬†I updated a small Python compiler that converts an assembly-like language into Minecraft command-block command sequences. Looking for testers, feedback, and contributors. Repo:¬†[https://github.com/Bowser04/Assembly-to-Minecraft-Command-Block-Compiler](https://github.com/Bowser04/Assembly-to-Minecraft-Command-Block-Compiler)\n\nWhat My Project Does:\n\n* Parses a tiny assembly-style language (labels, arithmetic, branches, simple I/O) and emits Minecraft command sequences tailored for command blocks.\n* Produces low-level, inspectable output so you can see how program logic maps to in-game command-block logic.\n* Implemented in Python for readability and easy contribution.\n\nTarget Audience:\n\n* Minecraft command-block creators who want to run low-level programs without mods.\n* Hobbyist compiler writers and learners looking for a compact Python codegen example.\n* Contributors interested in parsing, code generation, testing strategies, or command optimization.\n* This is an educational/hobby tool for small demos and experiments ‚Äî not a production compiler for large-scale programs.\n\nComparison (how it differs from alternatives):\n\n* Assembly-focused: unlike high-level language‚ÜíMinecraft tools, it targets an assembly-like input so outputs are low-level and easy to debug in command blocks.\n* Python-first and lightweight: prioritizes clarity and contributor-friendliness over performance.\n* Command-block oriented: designed to work with vanilla in-game command blocks (does not target datapacks or mods).\n\nHow to help:\n\n* Test: run examples, try outputs in a world, and note Minecraft version and exact steps when something fails.\n* Report: open issues with minimal reproduction files and steps.\n* Contribute: PRs welcome for bug fixes, examples, optimizations, docs, or tests ‚Äî look for good-first-issue.",
    "author": "bowser04410",
    "timestamp": "2025-10-20T12:57:47",
    "url": "https://reddit.com/r/Python/comments/1obrz11/assemblytominecraftcommandblockcompiler_python/",
    "score": 5,
    "num_comments": 2,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obi9f6",
    "title": "Access computed Excel values made easy using calc-workbook library",
    "content": "`calc-workbook` is an easy-to-use Python library that lets you access *computed Excel values* directly from Python. It loads Excel files, evaluates all formulas using the `formulas` engine, and provides a clean, minimal API to read the computed results from each sheet ‚Äî no Excel installation required.\n\n# What My Project Does\n\nThis project solves a common frustration when working with Excel files in Python: most libraries can read or write workbooks, but they can‚Äôt compute formulas. `calc-workbook` bridges that gap. You load an Excel file, it computes all the formulas using the `formulas` package, and you can instantly access the computed cell values ‚Äî just like Excel would show them. Everything runs natively in Python, making it platform-independent and ideal for Linux users who want full Excel compatibility without Excel itself.\n\n# Target Audience\n\nFor Python developers, data analysts, or automation engineers who work with Excel files and want to access real formula results (not just static values) without relying on Excel or heavy dependencies.\n\n# Comparison\n\n* **openpyxl** and **pandas** can read and write Excel files but do not calculate formulas.\n* **xlwings** requires Excel to compute formulas and is Windows/macOS only.\n* **calc-workbook** computes formulas natively in Python using the `formulas` engine and gives you the results in one simple call.\n\n# Installation\n\n    pip install calc-workbook\n\n# Example\n\n    from calc_workbook import CalcWorkbook\n    \n    wb = CalcWorkbook.load(\"example.xlsx\")\n    print(wb.get_sheet_names())           # ['sheet1']\n    \n    sheet = wb.get_sheet(\"sheet1\")        # or get_sheet() to get the first sheet\n    print(\"A1:\", sheet.cell(\"A1\"))        # 10\n    print(\"A2:\", sheet.cell(\"A2\"))        # 20\n    print(\"A3:\", sheet.cell(\"A3\"))        # 200\n\n# Example Excel file:\n\n|**A**|**B**|\n|:-|:-|\n|**1**|10|\n|**2**|20|\n|**3**|`=A1+A2`|\n\n# GitHub\n\n[https://github.com/a-bentofreire/calc-workbook](https://github.com/a-bentofreire/calc-workbook)",
    "author": "abentofreire",
    "timestamp": "2025-10-20T05:55:00",
    "url": "https://reddit.com/r/Python/comments/1obi9f6/access_computed_excel_values_made_easy_using/",
    "score": 21,
    "num_comments": 10,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obqtmz",
    "title": "Has any library emerged as the replacement for Poliastro?",
    "content": "I'm trying to develop some code that works with orbital dynamics, and it looks like the go-to is somehow still Poliastro, and at this point it's a no-go. Even if you restrict yourself to 3.11 you *also* have to go back to pip &lt;24.1 because of how some package requirements are written. I've looked around and can't find any other orbital dynamics libraries that are more than personal projects. Is the field just dead in python?",
    "author": "KerPop42",
    "timestamp": "2025-10-20T12:15:23",
    "url": "https://reddit.com/r/Python/comments/1obqtmz/has_any_library_emerged_as_the_replacement_for/",
    "score": 4,
    "num_comments": 5,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obprx2",
    "title": "Building an open-source observability tool for multi-agent systems - looking for feedback",
    "content": "I've been building multi-agent workflows with LangChain and got tired of debugging them with scattered console.log statements, so I built an open-source observability tool.   \n  \n**What it does**:  \n\\- Tracks information flow between agents   \n\\- Shows which tools are being called with what parameters   \n\\- Monitors how prompt changes affect agent behavior   \n\\- Works in both development and production   \n  \n**The gap I'm trying to fill**: Existing tools (LangSmith, LangFuse, AgentOps) are great at LLM observability (tokens, costs, latency), but I feel like they don't help much with multi-agent coordination. They show you **what** happened but not **why** agents failed to coordinate.   \n  \n  \n**Looking for feedback**:  \n1. Have you built multi-agent systems? What do you use for debugging?   \n2. Does this solve a real problem or am I overengineering?   \n3. What features would actually make this useful for you? Still early days, but happy to share the repo if folks are interested.",
    "author": "Standard_Career_8603",
    "timestamp": "2025-10-20T11:35:45",
    "url": "https://reddit.com/r/Python/comments/1obprx2/building_an_opensource_observability_tool_for/",
    "score": 3,
    "num_comments": 9,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1objndl",
    "title": "temporals - periods support for the core datetime library",
    "content": "Hi all!\n\n\nNearly a year ago (apparently, just a day shy of a whole year!), I shared the first iteration of my Python library with you all; now, a year later, I'm hoping to bring you an improved version of it. :)\n\n\n# What Does It Do\n\n\n**temporals** aims to provide a minimalistic utility layer on top of the Python standard library's `datetime` package in regards to working with `time`, `date` and `datetime` periods.\n\n\nThe library offers four different flavours of periods:\n\n* `TimePeriod`\n* `DatePeriod`\n* `WallClockPeriod`\n* `AbsolutePeriod`\n\n\nThe separation between a wall clock and an absolute period replaces the original DatetimePeriod with more concrete types as well as support for DST time changes and/or leap years. \n\n\nThis iteration also comes with more interfaces which should allow you to further extend the library to match your own needs, in case the current implementations aren't satisfactory.\n\n\n# Examples, Documentation, Links\n\n\nMy original post contains a bit more information on available methods as well as comparison to other libraries, I wanted to save you from being blasted with a wall of text, but if you're curious, feel free to have a look here - https://old.reddit.com/r/Python/comments/1g8nu9s/temporals_a_time_date_and_datetime_periods_support/\n\n\nIn-depth documentation and examples is available on the Wiki page in Github - https://github.com/dimitarOnGithub/temporals/wiki\n\n\nPyPi page - https://pypi.org/project/temporals/\n\n\nSource Code - https://github.com/dimitarOnGithub/temporals\n\n\n\n# Notes\n\n\n* Any feedback and criticism is always more than welcome and will be greatly appreciated! Thank you for taking the time and have a fantastic day!",
    "author": "winterchillz",
    "timestamp": "2025-10-20T06:56:56",
    "url": "https://reddit.com/r/Python/comments/1objndl/temporals_periods_support_for_the_core_datetime/",
    "score": 7,
    "num_comments": 4,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1occzel",
    "title": "NamedTuples are a PITA",
    "content": "I've also created a thread for this on Python forum - [see here](https://discuss.python.org/t/allow-to-pass-defaults-when-defining-namedtuple-dynamically/104484).\n\nTL;DR - When defining NamedTuples dynamically, there should be a single interface that'd allow to pass all 3 - field names, annotations, and defaults.\n\nI needed to convert to convert normal Python classes into NamedTuples. (see final implementation [here](https://github.com/django-components/django-components/pull/1466#discussion_r2447261223))\n\n‚ùå For normal classes, you could simply make a new class that subclasses from both.\n\n    class X(MyClass, NamedTuple):\n        pass\n\nBut NamedTuples don't support that.\n\n‚ùå And you can't further subclass the subclass of `NamedTuples`:\n\n    class Another(NamedTuple):\n        x: int = 1\n    \n    class X(Another):\n        y: str\n\n‚ùå When using `typing.NamedTuple` as a function, you can't pass in defaults:\n\n    my_class = typing.NamedTuple(\"MyClass\", [(\"x\", int), (\"y\", str)])\n\nI tried setting the defaults ([`_field_defaults`](https://docs.python.org/3/library/collections.html#collections.somenamedtuple._field_defaults)) manually, but Python wasn't picking that up.\n\n‚ùå One option was to define the NamedTuple with a class syntax as a string, and then evaluate that string. But that had 2 problems - 1) security risk, and 2) we'd need to import all the types used in annotations:\n\n    my_cls_str = \"\"\"\n    from typing import NamedTuple\n    \n    from path.to.custom import CustomClass\n    \n    class MyClass(NamedTuple):\n        x: int\n        y: str\n        z: CustomClass\n    \"\"\"\n    my_cls = eval(my_cls_str, {}, {})\n\n‚úÖ Lastly I managed to get it working using `collections.namedtuple`. This function doesn't define the field annotations, but it is able to handle defaults. Just one annoying thing was how it sets defaults the same way as Python functions do - it assigns the defaults from the back of the function signature.\n\nSo if I have NamedTuple with 3 fields - `x`, `y`, and `z`, and I set defaults to `[\"hello\", 123]`:\n\n    my_cls = namedtuple(\"MyClass\", [\"x\", \"y\", \"z\"], defaults=[\"hello\", 123])\n\nthen this is the same as writing:\n\n    class MyClass(NamedTuple):\n        x: int\n        y: str = \"hello\"\n        z: int = 123\n\nOne caveat is that `collections.namedtuple()` doesn't set the annotations, so I had also set `__annotations__` at the end.\n\nOne thing I didn't know - when defining `NamedTuple`, the fields it recognizes are only those that are annotated. If a field is NOT annotated, it's treated as a CLASS VARIABLE:\n\n    class MyClass(NamedTuple):\n        x: int            # Required field\n        y: str = \"hello\"  # Optional field (with default)\n        z = 123           # Class var!\n\nSo at the end I also assigned the class variables to the new namedtuple class.\n\nSurprisingly LLMs were really bad at solving this, so I'm hoping this comment gets into the next training set.",
    "author": "JuroOravec",
    "timestamp": "2025-10-21T06:28:58",
    "url": "https://reddit.com/r/Python/comments/1occzel/namedtuples_are_a_pita/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oc9sm4",
    "title": "Sprechstimme python library",
    "content": "Hey guys! I just made a library called sprechstimme which everyone should definitely download. **no pressure**. You can make synthesisers and music so you could just try‚Ä¶ ",
    "author": "Alive-Call-3453",
    "timestamp": "2025-10-21T03:56:03",
    "url": "https://reddit.com/r/Python/comments/1oc9sm4/sprechstimme_python_library/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oc59ki",
    "title": "We are automating the mobile apps via our agent",
    "content": "# What My Project Does\n\nMy project is called Droidrun, it is first native mobile AI agent. It can:\n\n* Automates Android apps through real user interactions (click, swipe, type, scroll)\n* Connects to real Android devices or emulators via ADB\n* Accepts natural language or JSON instructions\n* Runs via CLI or Python API\n\nYou can automate workflows like:\n\n* Open WhatsApp ‚Üí tap Login ‚Üí enter number ‚Üí check for code\n* Scroll through a feed and capture screenshots\n* Simulate checkout flows in test builds\n\n# Target Audience\n\nThis will help developers, QA engineers to test apps automatically. \n\n# Comparison\n\nWe live our digital lives through mobile apps, yet for AI and automation, this vibrant ecosystem often remains a locked garden. Unlike the relatively open structure of the web, comprehensive APIs for mobile apps are rare, leaving countless essential workflows and valuable data trapped behind native user interfaces designed solely for human taps and swipes.\n\n# Open Source &amp; Free Credits\n\nDroidrun is open source and we are continously improving its speed and functionality. Make sure to can try it, test it, and modify it.   \nHere is more about Droidrun: [https://www.droidrun.ai/](https://www.droidrun.ai/)  \nGithub: [https://github.com/droidrun/droidrun](https://github.com/droidrun/droidrun)  \nDiscord: [https://discord.com/invite/ZZbKEZZkwK](https://discord.com/invite/ZZbKEZZkwK)\n\nDM me if you have any questions, I would be happy to answer.",
    "author": "ya_Priya",
    "timestamp": "2025-10-20T23:09:02",
    "url": "https://reddit.com/r/Python/comments/1oc59ki/we_are_automating_the_mobile_apps_via_our_agent/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obktic",
    "title": "Anything funny and engaging for python devs",
    "content": "Hi everyone. \n\nSo every day I have to travel around 4 hours (2-2) to reach my job. \n\nIn that spare time I get really bored. I waste so much crucial time on YouTube music and other non sensical social media stuff.\n\nI have tried watching YouTube tutorial, but the only problem is that they are long and thus get boring. One advice that my boss had once given me when I was recording video tutorial for our staff ( our staff is not that tech friendly so we have to actually teach them about excel, google workspace and other kind of very common stuff) is that it shouldn't be longer then 2 minutes, else it start to become boring. \n\nAs I travel through underground metro rail, and internet is not stable there. \n\nI had heard about devdocs and it is good. \n\nSo, is there any such android app for developers which is engaging and fun. \n\nEngaging podcast\nInteresting facts\nSmall tutorials \nQuizzes\nDocs to read ( with big fonts ) \n\nI love solving those leetcode problems but the thing is they don't have any mobile app. \n\nIt should have the facility to save offline content. \n\nTill now this is what I have tried:\n1. YouTube ( long tutorials become boring ) \n2. Reddit ( doesn't work without internet, less content) \n3. Discord ( doesn't work without internet) \n4. PDFs ( small fonts not that mobile friendly, I have to scroll both horizontal and vertical) \n\nIf I am Posting it in wrong forum then kindly let me know I will delete it. \n\nI and open to any sort of suggestions/ feedback / criticism. \n\nSorry if I have asked too much. \n\nRight now I work as a django dev",
    "author": "virtualshivam",
    "timestamp": "2025-10-20T07:46:45",
    "url": "https://reddit.com/r/Python/comments/1obktic/anything_funny_and_engaging_for_python_devs/",
    "score": 4,
    "num_comments": 5,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obo3rg",
    "title": "Kryypto an open source python text editor.",
    "content": "Kryypto A lightweight, fully keyboard-supported python text editor with deep customization and GitHub integration.\n\n* Lightweight ‚Äì minimal overhead\n* Full Keyboard Support ‚Äì no need for the mouse, every feature is accessible via hotkeys\n* Discord presence\n* Live MarkDown Preview\n* Session Restore\n* Custom Styling\n   * `config\\configuration.cfg` for editor settings\n   * CSS for theme and style customization\n* Editing Tools\n   * Find text in file\n   * Jump to line\n   * Adjustable cursor (color &amp; width)\n   * Configurable animations (types &amp; duration)\n* Git &amp; GitHub Integration\n   * View total commits\n   * See last commit message &amp; date\n   * Track file changes directly inside the editor\n* Productivity Features\n   * Autocompleter\n   * Builtin Terminal\n   * Docstring panel (hover to see function/class docstring)\n   * Tab-based file switching\n   * Bookmarking lines\n   * Custom title bar\n* Syntax Highlighting for\n   * Python\n   * CSS\n   * JSON\n   * Config files\n   * Markdown\n\nTarget Audience\n\n* Developers who prefer keyboard-driven workflows (no mouse required)\n* Users looking for a lightweight alternative to heavier IDEs\n* People who want to customize their editor with CSS and configuration settings\n* Anyone experimenting with Python-based editors or open-source text editing tools\n\n# Comparison:\n\n* Lightweight ‚Äì minimal overhead, focused on speed\n* Highly customizable ‚Äì styling via CSS and config files\n* Keyboard-centric ‚Äì designed to be fully usable without a mouse\n\n  \ngithub repo: [https://github.com/NaturalCapsule/Kryypto](https://github.com/NaturalCapsule/Kryypto)\n\n  \nwebsite: [https://naturalcapsule.github.io/Kryypto/](https://naturalcapsule.github.io/Kryypto/)",
    "author": "SxxVe",
    "timestamp": "2025-10-20T10:21:30",
    "url": "https://reddit.com/r/Python/comments/1obo3rg/kryypto_an_open_source_python_text_editor/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ob2vp3",
    "title": "friendly PyTorch book ‚Äî here‚Äôs what I learned about explaining machine learning simply üëá",
    "content": "Hey everyone,\n\nI recently published Tabular Machine Learning with PyTorch: Made Easy for Beginners, and while writing it, I realized something interesting ‚Äî most people don‚Äôt struggle with code, they struggle with understanding what the model is doing underneath.\n\nSo in the book, I focused on:\n\t‚Ä¢\tMaking tabular ML (the kind that powers loan approvals, churn prediction, etc.) actually intuitive.\n\t‚Ä¢\tShowing how neural networks think step-by-step ‚Äî from raw data to predictions.\n\t‚Ä¢\tExplaining why we normalize, what layers really do, and how to debug small models before touching big ones.\n\nIt‚Äôs not a dense textbook ‚Äî more like a hands-on guide for people who want to ‚Äúget it‚Äù before moving to CNNs or Transformers.\n\nI‚Äôd love your feedback or suggestions:\nüëâ What part of ML do you wish was explained more clearly?\n\nIf anyone‚Äôs curious, here‚Äôs the Amazon link: https://www.amazon.com/dp/B0FV76J3BZ\n\nThanks for reading ‚Äî I‚Äôm here to learn and discuss with anyone building their ML foundation too.\n\n#MachineLearning #PyTorch #DeepLearning\n",
    "author": "disciplemarc",
    "timestamp": "2025-10-19T15:17:08",
    "url": "https://reddit.com/r/Python/comments/1ob2vp3/friendly_pytorch_book_heres_what_i_learned_about/",
    "score": 31,
    "num_comments": 7,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ob3xmq",
    "title": "Production-ready FastAPI template with CI/CD and Docker releases",
    "content": "**What My Project Does**\n\nThis is a starter template for FastAPI applications that comes with production-friendly defaults:\n\nContinuous Integration on every push (tests, linting, CodeQL security scan)\n\nAutomated releases on tag push: builds a Docker image, runs a health check, pushes to GHCR, and creates a GitHub Release\n\nDependabot integration for dependency upkeep\n\nOptional features (Postgres integration tests and Sentry release) that activate when you add secrets, but the template works fine with no secrets out of the box\n\n**Target Audience**\n\nThis is meant for developers who want to start a new FastAPI service with deployment and release hygiene already set up. It works both for learners (since it runs green with no configuration) and for teams who want a reproducible release pipeline from day one.\n\n**Comparison**\n\nThere are cookiecutter templates and boilerplates for FastAPI, but most focus on project structure or async patterns. This one focuses on shipping: tag-driven releases, GHCR publishing, CI/CD pipelines, and optional integrations. It‚Äôs not trying to reinvent frameworks, just remove the boilerplate around DevOps setup.\n\nRepo: [https://github.com/ArmanShirzad/fastapi-production-template](https://github.com/ArmanShirzad/fastapi-production-template)",
    "author": "Armanshirzad",
    "timestamp": "2025-10-19T16:03:43",
    "url": "https://reddit.com/r/Python/comments/1ob3xmq/productionready_fastapi_template_with_cicd_and/",
    "score": 19,
    "num_comments": 0,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ob0jk7",
    "title": "üß™ Promethium ‚Äî The Offline Chemistry Toolkit for Python",
    "content": "# What My Project Does\n\n**Promethium** is your go-to **periodic table and chemistry toolkit for Python,** designed for scientists, students, and developers who want powerful chemistry features without external dependencies.\n\nIt works **100% offline**, with all elements and reaction data bundled inside the library, making it fast, reliable, and perfect for classrooms, research, or automation scripts where internet access isn‚Äôt guaranteed.\n\n# Target Audience\n\nPromethium is ideal for:\n\n* Chemistry students and educators\n* Scientific software developers\n* Automation and data science enthusiasts who need chemistry computation in Python\n\n# Comparison¬†\n\nWhile **Mendeleev** is a great reference library for elemental data, **Promethium** takes it further by offering **offline data access** *and* a **built-in chemical reaction balancer**, all wrapped in a more lightweight, performance-oriented design. Mendeleev still works just fine for elemental purposes.\n\n# GitHub\n\n[https://github.com/rohankishore/Promethium](https://github.com/rohankishore/Promethium)",
    "author": "Specialist-Arachnid6",
    "timestamp": "2025-10-19T13:42:02",
    "url": "https://reddit.com/r/Python/comments/1ob0jk7/promethium_the_offline_chemistry_toolkit_for/",
    "score": 23,
    "num_comments": 3,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obzlam",
    "title": "About Me (and the order i code in)",
    "content": "hi, i recently started python, and i am really happy, i enjoy it very much and it has become a hobby,\n\nthe order i code in: 1: imports 2 variables: 3: normal code (print, lists etc) 4: if/else statements. (i put notes at the tops of each section.)",
    "author": "Consistent_Hall631",
    "timestamp": "2025-10-20T18:13:32",
    "url": "https://reddit.com/r/Python/comments/1obzlam/about_me_and_the_order_i_code_in/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.28,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ob573h",
    "title": "Monday Daily Thread: Project ideas!",
    "content": "# Weekly Thread: Project Ideas üí°\n\nWelcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.\n\n## How it Works:\n\n1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.\n2. **Build &amp; Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.\n3. **Explore**: Looking for ideas? Check out Al Sweigart's [\"The Big Book of Small Python Projects\"](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.\n\n## Guidelines:\n\n* Clearly state the difficulty level.\n* Provide a brief description and, if possible, outline the tech stack.\n* Feel free to link to tutorials or resources that might help.\n\n# Example Submissions:\n\n## Project Idea: Chatbot\n\n**Difficulty**: Intermediate\n\n**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar \n\n**Description**: Create a chatbot that can answer FAQs for a website.\n\n**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)\n\n# Project Idea: Weather Dashboard\n\n**Difficulty**: Beginner\n\n**Tech Stack**: HTML, CSS, JavaScript, API\n\n**Description**: Build a dashboard that displays real-time weather information using a weather API.\n\n**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)\n\n## Project Idea: File Organizer\n\n**Difficulty**: Beginner\n\n**Tech Stack**: Python, File I/O\n\n**Description**: Create a script that organizes files in a directory into sub-folders based on file type.\n\n**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)\n\nLet's help each other grow. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-19T17:00:32",
    "url": "https://reddit.com/r/Python/comments/1ob573h/monday_daily_thread_project_ideas/",
    "score": 11,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obgm3n",
    "title": "Up-to-date syntax highlighting for Vim?",
    "content": "All the Python syntax plugins I can find were abandoned 4+ years ago.\n\nLast commit age for a handful of plugins I've found:\n\n* [https://github.com/vim-python/python-syntax:](https://github.com/vim-python/python-syntax:) 5 years\n* [https://github.com/numirias/semshi:](https://github.com/numirias/semshi:) 4 years\n* [https://github.com/kh3phr3n/python-syntax:](https://github.com/kh3phr3n/python-syntax:) 4 years\n\nThere's a bunch of new syntax that's popped up since then, and I'm surprised that there's no actively maintained plugin for Python syntax highlighting in Vim. Am I missing something?",
    "author": "tapered_elephant",
    "timestamp": "2025-10-20T04:33:27",
    "url": "https://reddit.com/r/Python/comments/1obgm3n/uptodate_syntax_highlighting_for_vim/",
    "score": 1,
    "num_comments": 4,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oavf1l",
    "title": "[Fun project] UV scripts, but for functions.",
    "content": "**What My Project Does**\n\nI recently created [**uv-func**](https://github.com/Roulbac/uv-func), a small tool that brings the dependency-isolation concept of tools like *uv scripts* down to the level of individual Python functions. Instead of managing dependencies per module or script, uv-func lets you run discrete functions in a contained environment so they can run, communicate with each other, and manage their dependencies cleanly and separately.\n\n**Target Audience**\n\n* Python developers working with scripts or functions that need to be isolated or decoupled in terms of dependencies.\n* Hobbyists or maintainers who appreciate minimal tooling (uv-func has only three dependencies: cloudpickle, portalocker and rich).\n\nNote: This isn‚Äôt a full framework for large applications ‚Äî it‚Äôs intended to be lightweight and easy to embed or integrate as needed.\n\n**Comparison**\n\nThere are other tools that handle dependency isolation or function-level execution (for example, using containers, virtual environments per script, or Function-as-a-Service frameworks like Ray, etc...). \n\nWhat sets uv-func apart in my opinion:\n\n1. Minimal footprint: only three external dependencies.\n2. Focused on the function-level rather than full modules or services.\n3. Lightweight and easy to drop into existing Python codebases without heavy platform or infrastructure requirements.\n\nI see many AWS lambdas using requirements.txt then needing to run \\`pip install\\` somewhere in their app or infra code, and one example that comes immediately to mind is to use \\`uv-func\\` instead of \\`requirements.txt\\` for something like that (or even just uv scripts if function-level granularity isn't needed).\n\nI‚Äôd love to hear your thoughts, thanks!\n\n",
    "author": "DifficultDifficulty",
    "timestamp": "2025-10-19T10:23:20",
    "url": "https://reddit.com/r/Python/comments/1oavf1l/fun_project_uv_scripts_but_for_functions/",
    "score": 22,
    "num_comments": 5,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ob1kxr",
    "title": "gs-batch-pdf v0.6.0: Parallel PDF processing with Ghostscript",
    "content": "As a structural engineer I have to deal with lots of pdfs and Public Administration strict, sometimes ridiculous, size requirements. I don't like to use online tools, but instead I prefer a nifty cli like Ghostscript (`gs`). The only problem is that `gs` syntax could be quite criptic sometimes, and I always need to search online for it because I would forget it. So I built a wrapper for it.\n\n# What My Project Does\n\ngs-batch-pdf is a CLI tool that batch-processes multiple PDF files simultaneously using Ghostscript. It handles compression (5 quality levels), PDF/A conversion (PDF/A-1/2/3), and custom Ghostscript operations with multi-threaded execution. Features include automatic file size comparison (keeps smaller file by default), recursive directory processing, flexible output naming with prefixes/suffixes, and configurable error handling modes (prompt/skip/abort).\n\nInstallation: `pipx install gs-batch-pdf`\n\nQuick example:\n\n    # Compress all PDFs in docs/ recursively, attach prefix to output\n    gsb ./docs/ -r --compress --prefix compressed_\n    \n    # Compress + convert to PDF/A inplace\n    gsb *.pdf --compress --pdfa --force\n\n# Target Audience\n\nFor users who regularly process multiple PDFs (archiving, compliance, file size reduction). Requires Ghostscript installed as a system dependency. Tested on Windows, Linux with Python 3.12+ (macOS user, tell me). Particularly useful for:\n\n* Batch compress multiple files\n* Batch conversion to PDF/A standard (2 recommended)\n* Automated document processing pipelines\n\n# Comparison\n\nUnlike running Ghostscript directly (which processes one file at a time), **gs-batch-pdf adds parallel execution**, progress tracking, and smart file management. Compared to Python PDF libraries (pypdf, PyPDF2), this leverages Ghostscript's robust compression/conversion capabilities rather than pure-Python implementations. Unlike pdftk (focused on splitting/merging), this specializes in compression and standards compliance.\n\n**Unlike online tools, all processing happens locally with no privacy concerns.**\n\nGitHub: [https://github.com/kompre/gs-batch](https://github.com/kompre/gs-batch)\n\nPyPI: [https://pypi.org/project/gs-batch-pdf/](https://pypi.org/project/gs-batch-pdf/)",
    "author": "komprexior",
    "timestamp": "2025-10-19T14:23:16",
    "url": "https://reddit.com/r/Python/comments/1ob1kxr/gsbatchpdf_v060_parallel_pdf_processing_with/",
    "score": 10,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oborpj",
    "title": "I made a Python bot that turns your text &amp; images into diagrams right in Telegram.",
    "content": "https://i.imgur.com/O1R7s3X.gif\n\n**(sample)**\n\n---\n\nHey everyone!\n\nLike many of you, I often need to quickly visualize an idea ‚Äì sketch out a project structure, a mind map, or just explain a concept. Every time, I had to open heavy editors like Miro or Figma, which felt like overkill.\n\nSo, I decided to build a tool that lives right inside the app I use for communication all day: Telegram.\n\nI'm excited to share my side project: **Diagrammer Bot**. It's a simple yet powerful bot in Python that lets you create diagrams on the fly.\n\n**Here are the key features:**\n*   **Text &amp; Image Nodes:** You can create blocks not just from text, but from any image you send.\n*   **Full Editing:** Create, connect, edit, and delete both nodes and edges.\n*   **Project System:** Save your diagrams with custom names, load them later, or start new ones.\n*   **Themes &amp; Export:** Switch between a sleek dark mode and a clean light mode. Export your final diagram as a high-quality PNG.\n*   **Open-Source:** The entire project is available on GitHub!\n\n**Tech Stack:** Python, `python-telegram-bot`, `Graphviz` for rendering, and `Pillow` for watermarking.\n\nI would be incredibly grateful for any feedback, feature ideas, or bug reports. And of course, a star ‚≠ê on GitHub would make my day!\n\n*   **Try the bot here:** [https://t.me/diagrammer_robot](https://t.me/diagrammer_robot)\n*   **Check out the code on GitHub:** [https://github.com/Lixher/Diagrammer-bot](https://github.com/Lixher/Diagrammer-Bot)",
    "author": "No_Mongoose_8139",
    "timestamp": "2025-10-20T10:51:22",
    "url": "https://reddit.com/r/Python/comments/1oborpj/i_made_a_python_bot_that_turns_your_text_images/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obo0ze",
    "title": "Forgetting Python",
    "content": "I started python when i was 9th grade through udemy lectures, i watched a lot of them but didnt solve problems after that i took 2-3 gap for preping for college exams , now when i come back to python it feels i have lost my level and my touch i feel like fkn loser , all those hrs spent in 8th grade for nothing , i forgot a lot , is it common or just me???",
    "author": "Ashamed-Society-2875",
    "timestamp": "2025-10-20T10:17:52",
    "url": "https://reddit.com/r/Python/comments/1obo0ze/forgetting_python/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.15,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obl20g",
    "title": "Refurbished De-Googled Smartphones (LineageOS)",
    "content": "Salut,\n\nOn a de la chance d'avoir une tonne de distributions Linux pour nos PC ador√©s (respectueux de la vie priv√©e et s√©curis√©s -&gt; merci l'open source).\n\nPar contre, c√¥t√© t√©l√©phone, on est toujours pist√©s par nos GAFAM pr√©f√©r√©s, Apple et Google.\n\nChez Apple, avec iOS, c'est \"LES ROIS DU VERROUILLAGE\", impossible de faire quoi que ce soit.\n\nPar contre, c√¥t√© Android, on a la possibilit√© de \"d√©-googliser\" certains smartphones.\n\nNotre objectif :\n\nOn pr√©voit d'acheter des smartphones reconditionn√©s (genre Google Pixel, d'autres ?) et d'installer LineageOS dessus, puis de les revendre pr√™ts √† l'emploi.\n\nVous pensez que c'est une bonne id√©e ? √áa vous int√©resserait ? √áa int√©resserait vos potes ?\n\nMerci les amis.",
    "author": "talithaka",
    "timestamp": "2025-10-20T07:57:40",
    "url": "https://reddit.com/r/Python/comments/1obl20g/refurbished_degoogled_smartphones_lineageos/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.1,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oalj6u",
    "title": "For those who miss terminal animations...",
    "content": "Just for ease, the repo is also posted up here. \n\nhttps://github.com/DaSettingsPNGN/PNGN-Terminal-Animator\n\nWhat my project does: animates text in Discord to look like a terminal output!\n\nTarget audience: other nostalgic gamers who enjoy Fallout and Pok√©mon, and who are interested in animation in Discord using Python.\n\nComparison: to my knowledge, there's not another Discord bot that generates on-demand custom responses, animated in a terminal style, and uploaded to Discord as a 60 frame, 5 second 12 FPS GIF. I do this to respect Discord rate limits. It only counts as one message. I also use neon as the human eye has a neon reaction biologically similar to a phosphor glow. The colors persist longer with higher saturation on the human retina, and we interpolate to smooth the motion. \n\nI'm new to Python, but I absolutely love it. I designed an animated Discord bot that has Pok√©mon/Fallout style creatures. I was thinking of adding battling, but for now it is more an interactive guide. \n\nI used accurate visual width calculations to get the text art wrapping correct. Rendered and then scaled so it fits any device. And then vectorized the rendering. Visual width is expensive, but it lines up in nice columns allowing vectorized rendering. \n\nI wanted to see what you all thought, so here is the repo! It has everything you should need to get your own terminal animations going. It includes my visual width file, my scaling file, and also an example animation of my logo that demonstrates how to use the width calculations. That is the trickiest part. Once you have that down you're solid. \n\nhttps://github.com/DaSettingsPNGN/PNGN-Terminal-Animator\n\nNote: I included the custom emojis for the renderer. They work fairly well but not perfectly quite yet. The double cell size is hard to handle with visual width calculations. I will be working on it!\n\nPlease take a look and give me feedback! I will attach animated GIFs to the repo that are outputted from my bot! There is an example logo renderer too to get you started. \n\nThank you!",
    "author": "DaSettingsPNGN",
    "timestamp": "2025-10-19T02:42:14",
    "url": "https://reddit.com/r/Python/comments/1oalj6u/for_those_who_miss_terminal_animations/",
    "score": 19,
    "num_comments": 10,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oar9y1",
    "title": "CTkSidebar: a customizable sidebar navigation control for CustomTkinter",
    "content": "Hi everyone.\n\nI'm sharing a new package I've been working on: **ctk-sidebar**. It's a customizable control for CustomTkinter that adds sidebar navigation to your Python GUI app.\n\nProject link and screenshots: [https://github.com/anthony-bernaert/ctk-sidebar](https://github.com/anthony-bernaert/ctk-sidebar)\n\n# What My Project Does\n\n* Adds a sidebar to your CustomTkinter app\n* Handles navigation: each menu item gets a separate view where you can add your controls\n* Easy to use\n* Customizable styling\n* Supports hierarchical navigation (tree structure) with collapsible submenus\n* Optional, automatic colorization of menu icons\n\n# Target Audience\n\nEveryone who wants to include multiple UI panes inside the same window, and wants an easy, modern-looking solution.\n\n# Comparison\n\nCustomTkinter already features a tab view control to switch between multiple views, but a sidebar is better suited for more complex types of navigation, or to navigate between more unrelated sections. Except for some code snippets, I didn't find any existing package that implemented this in CustomTkinter yet.\n\n",
    "author": "abernaert",
    "timestamp": "2025-10-19T07:38:38",
    "url": "https://reddit.com/r/Python/comments/1oar9y1/ctksidebar_a_customizable_sidebar_navigation/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oah08y",
    "title": "Trio - Should I move to a more popular async framework?",
    "content": "I'm new-ish to python but come from a systems and embedded programming background and want to use python and pytest to automate testing with IoT devices through BLE, serial or other transports in the future. I started prototyping with Trio as that was the library I saw being used in a reference pytest plugin, I checked out Trio and was very pleased on the emphasis of the concept of structured concurrency  (Swift has this concept with the same name in-grained so I knew what it meant and love it) and started writing a prototype to get something working. \n\nIt was quick for me to notice the lack of  IO library that support Trio natively and this was bummer at first but no big deal as I could manage to find a small wrapper library for serial communication with Trio and wrote my own. However now that I'm having to prototype the BLE side of things I've found zero library, examples or material that uses Trio. Bleak which is the prime library I see pop-up when I look for BLE and python is written with the asyncio backend. I haven't done a lot of research into asyncio or anyio but now I'm thinking if I should switch to one of these (preferably anyio as it's the middle ground) and have to refactor while it is still early.\n\nSo wanted to ask if I would be losing  much by not going with Trio instead of one of the other libraries? I would hate to lose Tasks and TaskGroups (Nurseries in Trio) as well as Channels and Events but I think Anyio has them too although the implementation might be different. I also like Trio's support for sockets, subprocess and other low level APIs out of the box. Would appreciate any feedback on your experience using Trio or the other async libraries for similar use cases as mine. ",
    "author": "IncreaseMelodic9809",
    "timestamp": "2025-10-18T21:56:38",
    "url": "https://reddit.com/r/Python/comments/1oah08y/trio_should_i_move_to_a_more_popular_async/",
    "score": 24,
    "num_comments": 27,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oa9e3c",
    "title": "Showcase: I wrote a GitHub Action to Summarize uv.lock Changes",
    "content": "**What My Project Does**\n\nI have been loving everything about uv but reviewing changes as git diffs is always a chore.  \nI wrote this action to summarize the changes and provide a simple report via PR comment.\n\n**Target Audience**\n\nThis is intended for anyone building or maintaining Python projects with uv in Github.\n\n**Comparison**  \nI could not find any other similar actions out there.\n\nURL: [https://github.com/mw-root/uv-lock-report](https://github.com/mw-root/uv-lock-report)\n\nExample PR Comments:\n[https://github.com/mw-root/uv-lock-report/raw/main/images/uv-lock-report-simple-comment.png](https://github.com/mw-root/uv-lock-report/raw/main/images/uv-lock-report-simple-comment.png)\n\n[https://github.com/mw-root/uv-lock-report/raw/main/images/uv-lock-report-table-comment.png](https://github.com/mw-root/uv-lock-report/raw/main/images/uv-lock-report-table-comment.png)",
    "author": "burlyginger",
    "timestamp": "2025-10-18T15:29:43",
    "url": "https://reddit.com/r/Python/comments/1oa9e3c/showcase_i_wrote_a_github_action_to_summarize/",
    "score": 55,
    "num_comments": 16,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oa4r54",
    "title": "Saving Memory with Polars (over Pandas)",
    "content": "You can save some memory by moving to Polars from Pandas but watch out for a subtle difference in the quantile's different default interpolation methods.  \n  \nRead more here:  \n[https://wedgworth.dev/polars-vs-pandas-quantile-method/](https://wedgworth.dev/polars-vs-pandas-quantile-method/)\n\nAre there any other major differences between Polars and Pandas that could sneak up on you like this?",
    "author": "paltman94",
    "timestamp": "2025-10-18T12:23:02",
    "url": "https://reddit.com/r/Python/comments/1oa4r54/saving_memory_with_polars_over_pandas/",
    "score": 99,
    "num_comments": 34,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1obcfl4",
    "title": "What could be tips and tricks to get ranked in top 10 in Luna Prompts?",
    "content": "Hey everyone,  \nI‚Äôve been participating in the Luna Prompts contests for the past few weeks, but I can‚Äôt seem to break into the top 10 on the leaderboard. From what I understand, the ranking depends on token size and the number of test cases passed, but even getting all the test cases to pass feels tricky.\n\nIf anyone has figured out what really helps improve the score or what I might be missing, I‚Äôd love some advice.  \nHere‚Äôs the contest link if you want to check it out:¬†[https://lunaprompts.com/contests](https://lunaprompts.com/contests)",
    "author": "Ok-Bonus9520",
    "timestamp": "2025-10-19T23:14:31",
    "url": "https://reddit.com/r/Python/comments/1obcfl4/what_could_be_tips_and_tricks_to_get_ranked_in/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.24,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9zow6",
    "title": "I've written an article series about SQLAlchemy, hopefully it can benefit some of you",
    "content": "You can read it here [https://fullstack.rocks/article/sqlalchemy/brewing\\_with\\_sqlalchemy](https://fullstack.rocks/article/sqlalchemy/brewing_with_sqlalchemy)  \nI'm really attempting to go deep into the framework with this one. Obviously, the first few articles are not going to provide too many new insights to experienced SQLAlchemy users, but I'm also going into some advanced topics, such as:\n\n* Custom data types\n* Polymorphic tables\n* Hybrid declarative approach (next week)\n* JSON and JSONb (week after that)\n\nIn the coming weeks, I'll be continuing to add articles to this series, so if you see anything that is missing that might benefit other developers (or yourself), let me know.",
    "author": "Moon_Walking_Ape",
    "timestamp": "2025-10-18T09:04:31",
    "url": "https://reddit.com/r/Python/comments/1o9zow6/ive_written_an_article_series_about_sqlalchemy/",
    "score": 140,
    "num_comments": 21,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oaaddg",
    "title": "Google Tasks TUI",
    "content": "What My Project Does:\n\nThis project is a TUI(terminal user interface) that hooks up with the Google Tasks Api, allowing you to edit and view your tasks straight from your terminal.\n\nTarget Audience:\n\nThis is just a toy project and for everyone. It is also open source so feel free to make any contributions.\n\nComparison:\n\nI'm sure there are other TUIs out there similar to this that allows you to keep track of your tasks/notes. I guess this application is nice because it hooks up with your Google Tasks which allows for cross platform use. \n\nSource:\n\n[https://github.com/huiiy/GTask](https://github.com/huiiy/GTask)",
    "author": "Cow-Primary",
    "timestamp": "2025-10-18T16:13:30",
    "url": "https://reddit.com/r/Python/comments/1oaaddg/google_tasks_tui/",
    "score": 31,
    "num_comments": 9,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oa8cwj",
    "title": "New package: gnosis-dispatch",
    "content": "I created the [gnosis-dispatch](https://pypi.org/project/gnosis-dispatch/) package in large part to \"scratch an itch\" that followed Brett Slatkin's excellent PyCon US 2025 presentation, [The Zen of Polymorphism](https://us.pycon.org/2025/schedule/presentation/99) (a number of months ago).\n\nI think that [Multiple and Predicative Dispatch](https://gnosis-dispatch.readthedocs.io/en/latest/) is often a more elegant and more straightforwardly extensible way of structuring programs than is Protocol inheritance, OOP in general, the Registration Pattern, or other existing approaches to extensibility of related capabilities.\n\nI gave a talk on this package, but also on the concepts that underlay it at [PyCon Africa 2025](https://za.pycon.org/talks/303-multiple-and-predicative-dispatch/) that was extraordinarily well received, with questions running long over the scheduled time.\n\nFollowing my trip to Johannesburg, I finalized a few API details, added tests, and created the RtD pages for this module. All of which makes me comfortable calling it 1.0 now.\n\nI'd love for folks to try it out, give me feedback, report bugs, build large projects using the framework, etc.\n\nA quick `uv add gnosis-dispatch` or `uv pip install gnosis-dispatch` will get you started (or whatever people who don't use `uv` do to install software :-)).",
    "author": "DavidMertz",
    "timestamp": "2025-10-18T14:45:32",
    "url": "https://reddit.com/r/Python/comments/1oa8cwj/new_package_gnosisdispatch/",
    "score": 18,
    "num_comments": 3,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9tvtc",
    "title": "Which language is similar to Python?",
    "content": "I‚Äôve been using Python for almost 5 years now. \nFor work and for personal projects.\n\nRecently I thought about expanding programming skills and trying new language.\n\nWhich language would you recommend (for backend, APIs, simple UI)? Did you have experience switching from Python to another language and how it turned out?",
    "author": "iglebov",
    "timestamp": "2025-10-18T05:00:19",
    "url": "https://reddit.com/r/Python/comments/1o9tvtc/which_language_is_similar_to_python/",
    "score": 114,
    "num_comments": 235,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oayl0a",
    "title": "URL Shortener with FastAPI - Deployed to Leapcell",
    "content": "What My Project Does \nWorking with Django in real life for years, I wanted to try something new.\nThis project became my hands-on introduction to FastAPI and helped me get started with it.\n\nMiniurl a simple and efficient URL shortener.\n\nTarget Audience \nThis project is designed for anyone who frequently shares links online‚Äîsocial media users\n\nComparison \nUnlike larger URL shortener services, miniurl is open-source, lightweight, and free of complex tracking or advertising.\n\nURLs\nDocumentation and Github repo: https://github.com/tsaklidis/miniurl.gr\n\nLive version:\nhttps://miniurl.gr",
    "author": "steftsak",
    "timestamp": "2025-10-19T12:25:28",
    "url": "https://reddit.com/r/Python/comments/1oayl0a/url_shortener_with_fastapi_deployed_to_leapcell/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oad5wv",
    "title": "Skylos- Expanded capabilities",
    "content": "Hello Everyone. Skylos is a static analyzer that finds dead code (unused functions, imports, classes,  vars). It runs locally and has a CI/CD hook . Under the hood, Skylos uses AST with framework/test awareness, confidence scoring, and LibCST edits to flush out any dead code. We have expanded its capabilities to also detect the most common security flaws that is output by an AI model, aka to catch vibe coding vulnerabilities. \n\n  \nThe system is not perfect and we are constantly refining it. We have also included a VSC extension that you can use by searching for \\`Skylos\\` in the extension marketplace. Or you can download it via \n\n    pip install skylos==2.4.0\n\nTo use skylos with the security enhancement, run \n\n    skylos /path/to/your/folder --danger\n\n# Target audience:\n\nAnyone and everyone who uses python. Currently it's only for python. \n\n  \nWe are looking for feedback and contributors. If you have any feedback or will like to contribute, feel free to reach out to me over here. Please leave a star if you find it useful and share it. \n\n  \nI apologise if I disappear for a wk or two and have 0 updates to the repo, because I'm in the midst of writing my research paper. Once it's done i'll focus more on building this to its full potential. \n\n  \nThis is the link to the repo. [https://github.com/duriantaco/skylos](https://github.com/duriantaco/skylos)",
    "author": "papersashimi",
    "timestamp": "2025-10-18T18:28:51",
    "url": "https://reddit.com/r/Python/comments/1oad5wv/skylos_expanded_capabilities/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oaski5",
    "title": "What should be the design and functionality of an agent framework like Langchain?",
    "content": "I would like to study and deepen my knowledge on how to build a framework, how it should be designed and so on.\nI tried searching on Google but couldn't find anything satisfactory.\nIs there any discussion, paper or book where it is possible to delve into this topic professionally?",
    "author": "Warm_Interaction_375",
    "timestamp": "2025-10-19T08:31:11",
    "url": "https://reddit.com/r/Python/comments/1oaski5/what_should_be_the_design_and_functionality_of_an/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9wltp",
    "title": "üöÄ Shipped My First PyPI Package ‚Äî httpmorph, a C-backed ‚Äúbrowser-like‚Äù HTTP client for Python",
    "content": "Hey¬†r/Python¬†üëã\n\n\nJust published my first package to PyPI and wanted to share what I learned along the way.It‚Äôs called¬†httpmorph¬†‚Äî a¬†requests-compatible HTTP client built with a native C extension for more realistic network behavior.\n\nüß© What My Project Does\n\n\nhttpmorph¬†is a Python HTTP library written in¬†C¬†with¬†Python bindings.It reimplements parts of the HTTP and TLS layers using¬†BoringSSL¬†to more closely resemble modern browser-style connections (e.g., ALPN, cipher order, TLS 1.3 support).\nYou can use it just like¬†requests:\n\nimport httpmorph\n\nr = httpmorph.get(\"&lt;the_url&gt;\")\n\nprint(r.status_code)\n\nIt‚Äôs designed to help developers explore and understand how small transport-layer differences affect responses from servers and APIs.\n\nüéØ Target Audience\n\n\nThis project is meant for:\n* Developers curious about¬†C extensions and networking internals\n* Students or hobbyists learning how¬†HTTP/TLS clients¬†are built\n* Researchers exploring¬†protocol-level differences¬†across clients\nIt‚Äôs a learning-oriented tool ‚Äî not production-ready yet, but functional enough for experiments and debugging.\n\n‚öñÔ∏è Comparison\n\n\nCompared to existing libraries like¬†requests,¬†httpx, or¬†aiohttp:\n* Those depend on¬†OpenSSL, while¬†httpmorph¬†uses¬†BoringSSL, offering slightly different protocol negotiation flows.\n* It‚Äôs fully synchronous for now (like¬†requests), but the goal is transparency and low-level visibility into the connection process.\n* No dependencies ‚Äî it builds natively with a single¬†pip install.\n\nüß† Why I Built It\n\n\nI wanted to stop overthinking and finally learn how C extensions work.After a few long nights and 2000+ GitHub Actions minutes testing on Linux, Windows, and macOS (Python 3.8‚Äì3.14), it finally compiled cleanly across all platforms.\n\nüîó Links\n\n* PyPI ‚Üí https://pypi.org/project/httpmorph\n\n* GitHub ‚Üí¬†https://github.com/arman-bd/httpmorph\n\n\nüí¨ Feedback Welcome\n\n\nWould love your feedback on:\n* Code structure or API design improvements\n* Packaging/build tips for cross-platform C extensions\n* Anything confusing about the usage or docs\n\nI‚Äôm mainly here to learn ‚Äî any insights are super appreciated üôè\n",
    "author": "armanfixing",
    "timestamp": "2025-10-18T07:00:56",
    "url": "https://reddit.com/r/Python/comments/1o9wltp/shipped_my_first_pypi_package_httpmorph_a_cbacked/",
    "score": 25,
    "num_comments": 7,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9gyxa",
    "title": "Rant: Python imports are convoluted and easy to get wrong",
    "content": "Inspired by the famous \"module 'matplotlib' has no attribute 'pyplot'\" error, but let's consider another example: numpy.\n\nThis works:\n\n    from numpy import ma, ndindex, typing\n    ma.getmask\n    ndindex.ndincr\n    typing.NDArray\n\nBut this doesn't:\n\n    import numpy\n    numpy.ma.getmask\n    numpy.ndindex.ndincr\n    numpy.typing.NDArray  # AttributeError\n\nAnd this doesn't:\n\n    import numpy.ma, numpy.typing\n    numpy.ma.getmask\n    numpy.typing.NDArray\n    import numpy.ndindex  # ModuleNotFoundError\n\nAnd this doesn't either:\n\n    from numpy.ma import getmask\n    from numpy.typing import NDArray\n    from numpy.ndindex import ndincr  # ModuleNotFoundError\n\nThere are explanations behind this (`numpy.ndindex` is not a module, `numpy.typing` has never been imported so the attribute doesn't exist yet, `numpy.ma` is a module and has been imported by numpy's `__init__.py` so everything works), but they don't convince me. I see no reason why `import A.B` should only work when B is a module. And I see no reason why using a not-yet-imported submodule shouldn't just import it implicitly, clearly you were going to import it anyway. All those subtle inconsistencies where you can't be sure whether something works until you try are annoying. Rant over.\n\n**Edit**: as some users have noted, the AttributeError is gone in modern numpy (2.x and later). To achieve that, the numpy devs [implemented](https://github.com/numpy/numpy/blob/0066c73f573daafa01cbb975fde7f21d2b045ccb/numpy/__init__.py#L626) lazy loading of modules themselves. Keep that in mind if you want to try it for yourselves.",
    "author": "zabolekar",
    "timestamp": "2025-10-17T16:48:43",
    "url": "https://reddit.com/r/Python/comments/1o9gyxa/rant_python_imports_are_convoluted_and_easy_to/",
    "score": 146,
    "num_comments": 65,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oav08x",
    "title": "–ò—â—É —á–µ–ª–æ–≤–µ–∫–∞ —Å –∫–æ—Ç–æ—Ä—ã–º –º–æ–∂–Ω–æ –æ–∫—É–Ω—É—Ç—å—Å—è –≤ IT, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ Python.",
    "content": "–ú–Ω–µ 16 –ª–µ—Ç, —Ö–æ—á—É –∑–∞–Ω–∏–º–∞—Ç—å—Å—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º –Ω–∞ Python. –ò—â—É —á–µ–ª–æ–≤–µ–∫–∞ –∫—Ç–æ –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω –≤ —ç—Ç–æ–º –∏ —Ö–æ—á–µ—Ç –Ω–∞—á–∞—Ç—å –≤–º–µ—Å—Ç–µ. –ó–∞—Ä–∞–Ω–µ–µ –≤—Å–µ–º —Å–ø–∞—Å–∏–±–æ!",
    "author": "fresco_03",
    "timestamp": "2025-10-19T10:07:07",
    "url": "https://reddit.com/r/Python/comments/1oav08x/–∏—â—É_—á–µ–ª–æ–≤–µ–∫–∞_—Å_–∫–æ—Ç–æ—Ä—ã–º_–º–æ–∂–Ω–æ_–æ–∫—É–Ω—É—Ç—å—Å—è_–≤_it/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.19,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oarf25",
    "title": "EEG WIP. Can you do better than Claude?",
    "content": "Working on an EEG device that reads brainwaves- and does stuff with them after initial tests.\n\nClaude made this initial code. I would have tested it myself if I had everything for my device. See if you can't make something better!\n\nThe final AI I am working on- Idas- will be under GPL 3, using Python 3.12.\n\nimport torch\nimport pyeeg\nimport queue\n\nsignal_queue = queue.Queue()\n\nwhile True:\neeg_data = read.EEG()\ntensor = torch.tensor(eeg_data)\nsignal_queue.put(tensor)\n# Other processes consume from queue\n\nGPL 3 link and Ko-Fi page:\nhttps://ko-fi.com/nerdzmasterz",
    "author": "Member9999",
    "timestamp": "2025-10-19T07:44:29",
    "url": "https://reddit.com/r/Python/comments/1oarf25/eeg_wip_can_you_do_better_than_claude/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.12,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9s17w",
    "title": "Passive SDR Radar with KrakenSDR: DVB-T2 for Drone/Target Detection",
    "content": "Hello everyone,\n\nI'm starting a new open-source project aimed at developing a fully functional **Passive SDR Radar (PCL)** system using the **KrakenSDR** platform. The primary goal is to effectively detect and track **dynamic aerial targets** (like drones and aircraft) by processing existing broadcast signals, specifically **DVB-T2**, in the 514 MHz range .\n\nWe are currently in the architecture and initial development phase, and I welcome any feedback, expertise, and collaboration from the KrakenSDR community, especially regarding signal processing and phase calibration.\n\n\n\n# Project Overview &amp; Goals\n\n\n\nThis system operates entirely passively, making it robust against electronic countermeasures (ECM).\n\n* **Hardware:** KrakenSDR (5 channels), 5 x Yagi-Uda Antennas (1 Reference + 4 Surveillance for a phased array setup), Raspberry Pi 5.\n* **Illuminator:** DVB-T2 broadcast signals (around **514 MHz**).\n* **Target:** Drones, aircraft, and missiles.\n\n\n\n# Core Processing Pipeline\n\n\n\nThe project focuses heavily on signal processing to separate moving targets from static ground reflections (clutter). Our pipeline involves these key steps:\n\n1. **IQ Data Acquisition:** Capture raw data from the 5 KrakenSDR channels.\n2. **Calibration:** Synchronization and phase calibration (a critical challenge with non-coherent sources).\n3. **CAF Calculation:** Generate the **Cross Ambiguity Function (CAF)**, which creates a `delay √ó doppler` map (our radar frame).\n4. **Clutter Suppression:** Apply **MTI (Moving Target Indication)** or FIR High-Pass filters along the time axis to suppress stationary echoes (zero-Doppler).\n5. **Detection:** Use 2D **CFAR (Constant False Alarm Rate)** to extract targets from the filtered CAF maps.\n6. **Tracking:** Implement a **Kalman Filter** combined with the **Hungarian Algorithm** for robust data association and continuous tracking of targets (creating unique IDs and time series data).\n\n\n\n# Current Focus &amp; Challenges\n\n\n\nWe are seeking advice and discussion on the following technical points:\n\n1. **Phase Synchronization:** Best practices for achieving precise phase synchronization between the four surveillance channels on the KrakenSDR using an external clock or through software compensation, especially for non-coherent DVB-T2 signals.\n2. **CAF Optimization:** Techniques to optimize the computation time of the CAF on resource-limited devices like the Raspberry Pi 5.\n3. **MTI/Clutter Filtering:** Experience with adaptive clutter suppression algorithms (beyond simple MTI) for PCL systems utilizing OFDM signals like DVB-T2.\n\n\n\n# Repository and Collaboration\n\n\n\nThe project structure is available on GitHub. We are organizing the code into logical folders (`src/`, `config/`, `systemd/`) and are documenting the technical specifications in the `docs/` folder.\n\n**GitHub Repository:** [`https://github.com/Stanislav-sipiko/passive-sdr-radar`](https://github.com/Stanislav-sipiko/passive-sdr-radar)\n\nFeel free to check out the repo, submit issues, or share your knowledge here!\n\nThanks in advance for your input!",
    "author": "After-Ad-8431",
    "timestamp": "2025-10-18T03:12:58",
    "url": "https://reddit.com/r/Python/comments/1o9s17w/passive_sdr_radar_with_krakensdr_dvbt2_for/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9ar5b",
    "title": "De-emojifying scripts - setting yourself apart from LLMs",
    "content": "I am wondering if anyone else has had to actively try to set themselves apart from LLMs. That is, to convince others that you made something with blood, sweat and tears rather than clanker oil.\n\nFor context, I'm the maintainer of [Spectre](https://github.com/jcfitzpatrick12/spectre) ([https://github.com/jcfitzpatrick12/spectre](https://github.com/jcfitzpatrick12/spectre)), a Python program for recording radio spectrograms from software-defined radios. A long while ago, I wrote a setup script - it's the first thing a user runs to install the progam. That script printed text to the terminal indicating progress, and that text included emoji's ‚úîÔ∏è\n\nCertainly! Here‚Äôs a way to finish your post with a closing sentiment that emphasizes your personal touch and experience:\n\nMarkdown\n\n    I guess what I'm getting at is, sometimes the little details‚Äîlike a hand-picked emoji or a carefully-worded progress message‚Äîcan be a subtle but honest sign that there's a real person behind the code. In a world where so much content is generated, maybe those small human touches are more important than ever.\n    \n    Has anyone else felt the need to leave these kinds of fingerprints in their work?\n    ",
    "author": "jcfitzpatrick12",
    "timestamp": "2025-10-17T12:28:48",
    "url": "https://reddit.com/r/Python/comments/1o9ar5b/deemojifying_scripts_setting_yourself_apart_from/",
    "score": 93,
    "num_comments": 33,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oael9w",
    "title": "I am not able to start with GUI in Python.",
    "content": "Hi, i recently completed my CS50's Introduction to programming with Python Course, and was planning to start on GUIs to build better desktop apps for me or my friends... But Can't really Figure out where to start with GUI, There are dozens of different ways to learn it and create decent apps but I which one should i start with? Would love to know your experiences and opinions as well.",
    "author": "ComplaintGlass2005",
    "timestamp": "2025-10-18T19:42:38",
    "url": "https://reddit.com/r/Python/comments/1oael9w/i_am_not_able_to_start_with_gui_in_python/",
    "score": 0,
    "num_comments": 9,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9756d",
    "title": "I was tired of writing CREATE TABLE statements for my Pydantic models, so I built PydSQL to automate",
    "content": "Hey,\n\nI'd like to share a project I built to streamline a common task in my workflow. I've structured this post to follow the showcase rules.\n\n**What My Project Does:**\n\n**PydSQL** is a lightweight,  no dependencies besides Pydantic utility that converts Pydantic models directly into SQL `CREATE TABLE` statements.\n\nThe goal is to eliminate the manual, error-prone process of keeping SQL schemas synchronized with your Python data models.\n\nFor example, you write this Pydantic model:\n\nPython\n\n    from pydantic import BaseModel\n    from datetime import date\n    \n    class Product(BaseModel):\n        product_id: int\n        name: str\n        price: float\n        launch_date: date\n        is_available: bool\n\nAnd PydSQL instantly generates the corresponding SQL:\n\nSQL\n\n    CREATE TABLE product (\n        product_id INTEGER,\n        name TEXT,\n        price REAL,\n        launch_date DATE,\n        is_available BOOLEAN\n    );\n\nIt does one thing and aims to do it well, without adding the complexity of a full database toolkit.\n\n**Target Audience:**\n\nThe target audience is **Python developers who prefer writing raw SQL or use lightweight database libraries** (like `sqlite3`, `psycopg2`, etc.) instead of a full ORM.\n\nIt's intended for small to medium-sized projects where a tool like SQLAlchemy or Django's ORM might feel like overkill, but you still want the benefit of automated schema generation from a single source of truth (your Pydantic model). It is meant for practical development workflows, not just as a toy project.\n\n**Comparison**\n\n* **vs. Manual SQL:** PydSQL is a direct replacement for manually writing and updating `.sql` files. It reduces boilerplate, prevents typos, and ensures your database schema never drifts from your application's data models.\n* **vs. ORMs (SQLAlchemy, Django ORM):** PydSQL is **not an ORM**. It doesn't handle database connections, sessions, or query building. This makes it far more lightweight and simpler. It's for developers who want to write their own SQL queries but just want to automate the table creation part.\n* **vs. SQLModel:** While SQLModel also uses Pydantic, it is a full ORM built on top of SQLAlchemy. PydSQL is different because it has no opinion on how you interact with your database it only generates the `CREATE` statement.\n\n**Links**\n\n* **Source Code (GitHub):**[https://github.com/pranavkp71/PydSQL](https://github.com/pranavkp71/PydSQL)\n* **PyPI:** `pip install pydsql`\n\nThe project is very new, and I'm actively looking for feedback, feature requests, and contributors. Thanks for checking it out!",
    "author": "MainWild1290",
    "timestamp": "2025-10-17T10:11:04",
    "url": "https://reddit.com/r/Python/comments/1o9756d/i_was_tired_of_writing_create_table_statements/",
    "score": 60,
    "num_comments": 43,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o98n90",
    "title": "If starting from scratch, what would you change in Python. And bringing back an old discussion.",
    "content": " I know that it's a old discussion on the community, the trade of between simplicity and \"magic\" was a great topic about 10 years ago. Recently I was making a Flask project, using some extensions, and I stop to think about the usage pattern of this library. Like you can create your app in some function scope, and use current\\_app to retrieve it when inside a app context, like a route. But extensions like socketio you most likely will create a \"global\" instance, pass the app as parameter, so you can import and use it's decorators etc. I get why in practice you will most likely follow.\n\n  \n What got me thinking was the decisions behind the design to making it this way. Like, flask app you handle in one way, extensions in other, you can create and register multiples apps in the same instance of the extension, one can be retrieved with the proxy like current\\_app, other don't (again I understand that one will be used only in app context and the other at function definition time). Maybe something like you accessing the instances of the extensions directly from app object, and making something like route declaration, o things that depends on the instance of the extension being declared at runtime, inside some app context. Maybe this will actually make things more complex? Maybe.\n\n I'm not saying that is wrong, or that my solution is better, or even that I have a good/working solution, I'm just have a strange fell about it. Mainly after I started programming in low level lang like C++ and Go, that has more strict rules, that makes things more complex to implement, but more coherent. But I know too that a lot of things in programming goes as it was implemented initially and for the sake of just make things works you keep then as it is and go along, or you just follow the conventions to make things easier (e.g. banks system still being in Cobol).\n\n  \n Don't get me wrong, I love this language and it's still my most used one, but in this specific case it bothers me a little, about the abstraction level (I know, I know, it's a Python programmer talking about abstraction, only a Js could me more hypocritical). And as I said before, I know it's a old question that was exhausted years ago. So my question for you guys is, to what point is worth trading convenience with abstraction? And if we would start everything from scratch, what would you change in Python or in some specific library?",
    "author": "FenomoJs",
    "timestamp": "2025-10-17T11:08:02",
    "url": "https://reddit.com/r/Python/comments/1o98n90/if_starting_from_scratch_what_would_you_change_in/",
    "score": 44,
    "num_comments": 238,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9axeo",
    "title": "Pygls v2.0.0 released: a library for building custom LSP servers",
    "content": "We've just released [v2.0.0 of pygls](https://pypi.org/project/pygls/2.0.0/), the library to help you build your own LSP servers: https://github.com/openlawlibrary/pygls\n\nIt's the first major rewrite since its inception 7 years ago. The pre-release has been available for over a year, so this is already well used and tested code.\n\nIf you write Python in VSCode it's likely you're already using [a _pygls_-based LSP server implementation](https://github.com/openlawlibrary/pygls/blob/main/Implementations.md) as we work with Microsoft to support their [lsprotocol](https://github.com/microsoft/lsprotocol) typing library.",
    "author": "tombh",
    "timestamp": "2025-10-17T12:35:36",
    "url": "https://reddit.com/r/Python/comments/1o9axeo/pygls_v200_released_a_library_for_building_custom/",
    "score": 20,
    "num_comments": 5,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9dey5",
    "title": "I just released PyPIPlus.com 2.0 offline-ready package bundles, reverse deps, license data, and more",
    "content": "Hey everyone,\n\nI‚Äôve pushed a major update to PyPIPlus.com my tool for exploring Python package dependencies in a faster, cleaner way.\n\nSince the first release, I‚Äôve added a ton of improvements based on feedback:   \n‚Ä¢\tOffline Bundler: Generate a complete, ready-to-install package bundle with all wheels, licenses, and an installer script   \n‚Ä¢\tAutomatic Compatibility Resolver: Checks Python version, OS, and ABI for all dependencies   \n‚Ä¢\tExpanded Dependency Data: Licensing, size, compatibility, and version details for every sub-dependency ‚Ä¢\tDependents View: See which packages rely on a given project   \n‚Ä¢\tHealth Metrics &amp; Score: Quick overview of package quality and metadata completeness   \n‚Ä¢\tDirect Links: Access project homepages, documentation, and repositories instantly ‚Ä¢  \n\tImproved UI: Expanded view, better mobile layout, faster load times   \n‚Ä¢\tDedicated Support Email: For feedback, suggestions, or bug reports\n\nIt‚Äôs now a much more complete tool for developers working with isolated or enterprise environments or anyone who just wants deeper visibility into what they‚Äôre installing.\n\nWould love your thoughts, ideas, or feedback on what to improve next.\n\nüëâ [https://pypiplus.com](https://pypiplus.com)\n\nIf you missed it, here‚Äôs the original post: [https://www.reddit.com/r/Python/s/BvvxXrTV8t](https://www.reddit.com/r/Python/s/BvvxXrTV8t)",
    "author": "RoyalW1zard",
    "timestamp": "2025-10-17T14:13:00",
    "url": "https://reddit.com/r/Python/comments/1o9dey5/i_just_released_pypipluscom_20_offlineready/",
    "score": 10,
    "num_comments": 11,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o90fan",
    "title": "Free-threaded Python on GitHub Actions",
    "content": "[https://hugovk.dev/blog/2025/free-threaded-python-on-github-actions/](https://hugovk.dev/blog/2025/free-threaded-python-on-github-actions/)",
    "author": "pauloxnet",
    "timestamp": "2025-10-17T05:50:14",
    "url": "https://reddit.com/r/Python/comments/1o90fan/freethreaded_python_on_github_actions/",
    "score": 45,
    "num_comments": 4,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9frjw",
    "title": "Platform differences Windows &lt;-&gt; MacOS",
    "content": "Context: scans of documents, python environment, running configuration-file-based OCR against said scans. Configuration options include certain things for x- and y-thresholds on joining data in lines, etc. Using Regular Expressions to pull structured data from different parts of the document. Documents are PDFs and PNGs of structured, form-based documents.\n\nI built a config for a new client yesterday that worked picture perfect, basically first time and for a number of documents I ran as a test suite. Very little tweaking and special configs. It was straight forward and was probably the first time this system didn't feel overtaxed. (don't get me started on the overall design of it)\n\nCoworker ran the same setup, and it failed. Built on the same version of Python, all from the same requirements list, etc. Literally the only difference is I'm running on MacOS and he's running Windows 11. Same code base, pulled from same repository. Same config file. Same same all around. \n\nHe had to adjust one setting to get it to work at all, and I'm still not sure the whole thing worked as expected. Mine did, repeatedly, on multiple documents. \n\n  \nAs this will eventually be running on a container in some silly google environment which is probably running some version of \\*nix OS, I'd say my Mac is closer to the \"real deal\" than his windows machine; gun to my head, I'm saying if it works on mine and not on his, his is the bigger problem. \n\nAnyone aware of such differences on disparate platforms?",
    "author": "Any_Peace_4161",
    "timestamp": "2025-10-17T15:53:39",
    "url": "https://reddit.com/r/Python/comments/1o9frjw/platform_differences_windows_macos/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9h81g",
    "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread",
    "content": "# Weekly Thread: Resource Request and Sharing üìö\n\nStumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!\n\n## How it Works:\n\n1. **Request**: Can't find a resource on a particular topic? Ask here!\n2. **Share**: Found something useful? Share it with the community.\n3. **Review**: Give or get opinions on Python resources you've used.\n\n## Guidelines:\n\n* Please include the type of resource (e.g., book, video, article) and the topic.\n* Always be respectful when reviewing someone else's shared resource.\n\n## Example Shares:\n\n1. **Book**: [\"Fluent Python\"](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \\- Great for understanding Pythonic idioms.\n2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \\- Excellent overview of Python's built-in data structures.\n3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \\- A deep dive into decorators.\n\n## Example Requests:\n\n1. **Looking for**: Video tutorials on web scraping with Python.\n2. **Need**: Book recommendations for Python machine learning.\n\nShare the knowledge, enrich the community. Happy learning! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-17T17:00:30",
    "url": "https://reddit.com/r/Python/comments/1o9h81g/saturday_daily_thread_resource_request_and/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8ors4",
    "title": "TOML is great, and after diving deep into designing a config format, here's why I think that's true",
    "content": "Developers have strong opinions about configuration formats. YAML advocates appreciate the clean look and minimal syntax. JSON supporters like the explicit structure and universal tooling. INI users value simplicity. Each choice involves tradeoffs, and those tradeoffs matter when you're configuring something that needs to be both human-readable and machine-reliable. This is why I settled on TOML. \n\n[https://agent-ci.com/blog/2025/10/15/object-oriented-configuration-why-toml-is-the-only-choice](https://agent-ci.com/blog/2025/10/15/object-oriented-configuration-why-toml-is-the-only-choice)",
    "author": "tcdent",
    "timestamp": "2025-10-16T18:52:41",
    "url": "https://reddit.com/r/Python/comments/1o8ors4/toml_is_great_and_after_diving_deep_into/",
    "score": 169,
    "num_comments": 82,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9bqks",
    "title": "Resources from Intermediate - Advanced for decently experienced dev to upskill?",
    "content": "Hey guys A bit of a background - I have a bachelors in CS (just finished) and quite a bit of \"experience\" - since I started working basically full time after my sophomore year of uni at an AI startup based in SF. Since then I have graduated, switched jobs to a different startup in SF that values me more. I also do some part time research in AI, have a research paper - and a couple more on the way - beside my day job. However the problem is - I dont think in the past 1-2 years or so - I haven't really made my skills more robust. So here I am looking for resources on how to learn some of the more intermediate concepts in Python specifically - as that is the language that I use the most often. A bit of background of my familiarity with programming - have done a decent bit of C - in undergrad - dealt with some networking and OS-level code in C (sockets, raw sockets, implementing file transfer protocols from RFCs etc). For Python - obviously know the basic stuff, but a lot of the nice-to-haves that I dont understand. Like yeah I'm very familiar with the raw types and basic concepts like dicts, lists, mutability etc, have extensively used Flask, and also built \"production apps\". But I find that I lack for example proper understanding of when/where would I need to use stuff like dataclasses, or other niceties of python. Due to my day job - which usually involves \"shipping quickly\" - I find that I dont really follow the best practices/probably dont really write \"clean\" code. Part of it is also just some practice that comes from the jupyter notebook type of prototyping because I do quite a bit of ML research and the code that you write there isnt really ever \"clean\" or prod grade What are some intermediate level books to learn from/learn design patterns and OOP applications from? For example - when would I need to build abstractions when building CRUD apps/ when to just let it be? I'm looking for stuff like the interpreter book in Go but for my usecase.\n\nGave that example because I really want a resource to \"do\" stuff instead of just read/have small exercises at the end to solve - I dont really feel I learn much from that.\n\nMaybe also stuff like \"practical version\" of the Designing Data intensive applications or similar books.\n\n  \nTLDR:\n\nDecently experienced in terms of just programming - looking for stuff that is like \"The Interpreter book using Go\" but for Python + Design pattern related stuff. Any suggestions?",
    "author": "DarthLoki79",
    "timestamp": "2025-10-17T13:07:39",
    "url": "https://reddit.com/r/Python/comments/1o9bqks/resources_from_intermediate_advanced_for_decently/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9ldg2",
    "title": "Private Package Hosting + Vetted Packatges / Security Auditing",
    "content": "[I've previously asked about package hosting before](https://www.reddit.com/r/Python/comments/17tg1lk/third_party_private_package_hosting/), but with the fairly constant stream of supply chain attacks ocurring it's clear to me that having a \"vetted\" PyPI mirror is needed on top of any private package hosting.\n\nThis isn't a particularly poignant realisation, but good solutions that are suitable for for small organisations / security teams seem few and far between.\n\nFrom my point of view ^(feel free to argue with me on this) an ideal solution would meet the following:\n\n* Hosted (i.e. SaaS)\n* Must be able to have both private packages and mirrored packages in the same index.\n* Packages mirrored from PyPI should be vetted in a no-touch / low-touch way. As a solo security person I don't have the time or skills to vett every package and version and built artifact.\n* Pricing should be usage based - preferably with fine-grained pay-as-you-go metering. Many that do price on usage tend to be course grained on pre-selected amounts rather than metered. Pricing should absolutely not be priced on number of users.\n\nSo far I've not found anything that suits - so please provide your recommendations / reviews if you have any.\n\nHere's things I've looked at so far:\n\n* [**Inedo ProGet**](https://inedo.com/proget/features) \\- mostly self-hosted, very coarse grained pricing.\n* [**ActiveState**](https://www.activestate.com/platform/supported-languages/python/) \\- appears to mostly be container based, doesn't look like standard private respository hosting.\n* [**Cloudsmith**](https://cloudsmith.com/product/formats/python-repository) \\- looks like the cloest thing, their minimum pricing is still a lot for tiny teams / organisations.\n* [**JFrog**](https://jfrog.com/pricing/) \\- Epensive coarse grained pricing\n* [**Sonatype (Nexus / Firewall)**](https://www.sonatype.com/products/pricing) \\- expensive per user based pricing. Self hosted Nexus is a lot of manual work.\n\nFinally, I'm aware that there are CI/CD based solutions for this, but really want to push it at the repository level because generally speaking they also give access to things like centralised reporting and SBOMs.",
    "author": "nicholashairs",
    "timestamp": "2025-10-17T20:28:57",
    "url": "https://reddit.com/r/Python/comments/1o9ldg2/private_package_hosting_vetted_packatges_security/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oa0gx2",
    "title": "Built a PyTorch system that trains ML models 11√ó faster with 90% energy savings [850 lines, open sou",
    "content": "    Hey r/Python! Wanted to share a PyTorch project I just open-sourced.\n    \n    \n    **What it does:**\n    Trains deep learning models by automatically selecting only the most important 10% of training samples each epoch. Results in 11√ó speedup and 90% energy savings.\n    \n    \n    **Tech Stack:**\n    - Python 3.8+\n    - PyTorch 2.0+\n    - NumPy, Matplotlib\n    - Control theory (PI controller)\n    \n    \n    **Results:**\n    - CIFAR-10: 61% accuracy in 10.5 minutes (vs 120 min baseline)\n    - Energy savings: 89.6%\n    - Production-ready (850 lines, fully documented)\n    \n    \n    **Python Highlights:**\n    - Clean OOP design (SundewAlgorithm, AdaptiveSparseTrainer classes)\n    - Type hints throughout\n    - Comprehensive docstrings\n    - Dataclasses for config\n    - Context managers for resource management\n    \n    \n    **Interesting Python Patterns Used:**\n    ```python\n    @dataclass\n    class SundewConfig:\n    ¬† ¬† activation_threshold: float = 0.7\n    ¬† ¬† target_activation_rate: float = 0.06\n    ¬† ¬† # ... (clean config pattern)\n    \n    \n    class SundewAlgorithm:\n    ¬† ¬† def __init__(self, config: SundewConfig):\n    ¬† ¬† ¬† ¬† self.threshold = config.activation_threshold\n    ¬† ¬† ¬† ¬† self.activation_rate_ema = config.target_activation_rate\n    ¬† ¬† ¬† ¬† # ... (EMA smoothing for control)\n    \n    \n    ¬† ¬† def process_batch(self, significance: np.ndarray) -&gt; np.ndarray:\n    ¬† ¬† ¬† ¬† # Vectorized gating (50,000√ó faster than loops)\n    ¬† ¬† ¬† ¬† return significance &gt; self.threshold\n    ```\n    \n    \n    **GitHub:**\n    https://github.com/oluwafemidiakhoa/adaptive-sparse-training\n    \n    \n    **Good for Python devs interested in:**\n    - ML engineering practices\n    - Control systems in Python\n    - GPU optimization\n    - Production ML code\n    \n    \n    Let me know if you have questions about the implementation!",
    "author": "Klutzy-Aardvark4361",
    "timestamp": "2025-10-18T09:36:20",
    "url": "https://reddit.com/r/Python/comments/1oa0gx2/built_a_pytorch_system_that_trains_ml_models_11/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.26,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9v5sq",
    "title": "Realistically speaking, what can you do with Python besides web backends and ML/DS ?",
    "content": "Hello there!  \nI am working in web development for three years and I've got a strong glimpse at most of the programming languages out there. I know CSharp, Python and JavaScript and I've toyed with many others too. My main question is what can you actually build with Python more than app backends or software for machine learning and data science?\n\nThere's like lots of libraries designed for making desktop applications or games in Python or physics simulations and much more. But I am pretty sure I've never seen and used an app that is entirely written in Python. At best, I've seen some internal dashboards or tools made at my workplace to monitor our project's parameters and stuff.\n\nThere seems to be lots of potential for Python with all these frameworks and libaries supported by so many people. Yet, I can't find an application that is successful and destined for the normal user like a drawing program, a game or an communication app. I know that Python is pretty slow, sometimes dozens of times slower than CSharp/Java. But there are JIT compilers available, an official one is right now in development.\n\nPersonally, I enjoy writing Python much more because of it's more functional approach. Sending an input string through sockets in Java is as complicated as instantiating a Socket, a DataInputStream, a DataOutputStream, a Scanner and some more objects I don't remember the name of. In Python it's as easy as passing a string through functions. Java likes to hide primitives behind class walls while Python embraces their use.\n\nSo, realistically speaking, what makes Python so unpopular for real, native app development compared to other languages? Given the fact that now the performance gap is closing and hardware is faster?\n\nThanks!",
    "author": "yughiro_destroyer",
    "timestamp": "2025-10-18T06:00:23",
    "url": "https://reddit.com/r/Python/comments/1o9v5sq/realistically_speaking_what_can_you_do_with/",
    "score": 0,
    "num_comments": 32,
    "upvote_ratio": 0.32,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9bk6z",
    "title": "React Native with Python Backend Developer",
    "content": "My company has a react native app close to being finished but we need to make a decision on the backend. We have a cms that manages the feed for our content that‚Äôs built in Python and we were thinking of using Python for the backend. We need to hire a developer to do the back end of the app and connect our subscription management software. The app is fitness related and in the future will have device data and gamification. We also may do some algorithms for displaying content etc so possible machine learning or AI. \n\nIs it better to find someone that can do react native and python or two specialists? Does choosing this stack make it harder to find developers in the future? ",
    "author": "Ok-Mechanic940",
    "timestamp": "2025-10-17T13:00:39",
    "url": "https://reddit.com/r/Python/comments/1o9bk6z/react_native_with_python_backend_developer/",
    "score": 3,
    "num_comments": 4,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1oa3n4x",
    "title": "Hot take: list comprehensions are almost always a bad idea",
    "content": "simply annoyed by the amount of long list comprehensions i find in codebases, what the hell happened to readability, sure, its convenient, but come back to it a month later and you'll hate yourself as well as every other dev that had to read your code.   \n  \nSTOP using list comprehensions if you have more than 1 for loop in it and more than 1 conditional üôè",
    "author": "Quirky_Decision_2827",
    "timestamp": "2025-10-18T11:40:49",
    "url": "https://reddit.com/r/Python/comments/1oa3n4x/hot_take_list_comprehensions_are_almost_always_a/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.28,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9m98i",
    "title": "Am I allowed to ask whether anyone has PandasGUI working with 3.14 here?",
    "content": "LearnPython seems an odd subreddit to ask that question - I'm hoping a power user might see this post and let me know the dependencies external to Python (VS interpreters etc).  Depending upon where you look, the responses vary wildly.",
    "author": "myposttracker2",
    "timestamp": "2025-10-17T21:16:47",
    "url": "https://reddit.com/r/Python/comments/1o9m98i/am_i_allowed_to_ask_whether_anyone_has_pandasgui/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.47,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8un9r",
    "title": "Sanguine ‚Äî Local Semantic Code Search, No Cloud, No APIs",
    "content": "What My Project Does:\nSanguine is a CLI tool that indexes your code across multiple repos and languages using Tree-sitter. It allows you to search for code by meaning, not just keywords. For example:\n\n&gt; sanguine search \"parse http headers\"\nwill find the actual functions that perform that task. It integrates with Git (optional post-commit hook) to keep the index up to date. Everything runs locally ‚Äî no servers, no APIs, no telemetry.\n\n\nLink:\nhttps://github.com/n1teshy/sanguine\n\nWould love your feedback.",
    "author": "Specialist_Ruin_9333",
    "timestamp": "2025-10-17T00:13:40",
    "url": "https://reddit.com/r/Python/comments/1o8un9r/sanguine_local_semantic_code_search_no_cloud_no/",
    "score": 12,
    "num_comments": 2,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9o3f5",
    "title": "Guido knew better than his boss",
    "content": "Looking into the history it appears Guido built Python as a project to just help him in his real job.\n\nIt turned out that Python was a more important product than what he was paid to actually do.\n\nI see that as almost a comfort to me that perhaps the work I am assigned is not the work I should be.\n\nAnyone else relate?",
    "author": "Jealous_Lock_393",
    "timestamp": "2025-10-17T23:04:05",
    "url": "https://reddit.com/r/Python/comments/1o9o3f5/guido_knew_better_than_his_boss/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9oqao",
    "title": "Need a function to get the average of two colours",
    "content": "Hi I am building a program that scans along a line and checks the change in colour.\n\nIs there an easy way to get the average of two colours? E.g. with 0,0,0 and 255,255,255 the average is 128,128,128",
    "author": "Jealous_Lock_393",
    "timestamp": "2025-10-17T23:42:52",
    "url": "https://reddit.com/r/Python/comments/1o9oqao/need_a_function_to_get_the_average_of_two_colours/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8fz6j",
    "title": "I built a VS Code extension for uv integration and PEP 723 scripts",
    "content": "Hey folks! I've been working on a VS Code extension that brings [uv](https://docs.astral.sh/uv/) integration and [PEP 723](https://peps.python.org/pep-0723/) support directly into your editor ‚Äî making Python script development way more powerful.\n\nThe extension lets you manage packages, run scripts, and handle dependencies without ever leaving VS Code or switching to the terminal. Plus, with PEP 723 support, your scripts become truly portable and shareable.\n\nHere's what a PEP 723 script looks like:\n\n```python\n# /// script\n# requires-python = \"&gt;=3.9\"\n# dependencies = [\n#     \"cowsay\"\n# ]\n# ///\nimport cowsay\n\ncowsay.cow(\"Hello World!\")\n```\n\nYou can copy this script anywhere, share it with anyone, and it'll just work ‚Äî no setup instructions needed.\n\n## What My Project Does\n\nMy extension provides:\n* `uv` integration built directly into VS Code\n* Add, remove, and update packages without touching the terminal\n* Automatic PEP 723 script metadata detection and management\n* Complete LSP support (autocomplete, type checking, go-to-definition) for scripts\n* One-click run and debug for scripts\n* Smart virtual environment handling behind the scenes\n\nBasically, you get the speed and power of `uv` with the convenience of never leaving your editor, plus a better way to write and share self-contained Python scripts.\n\n## Target Audience\n\nThis is mainly aimed at:\n* Python developers who want faster package management in their workflow\n* People who love quick scripts and prototypes without the setup overhead\n* Developers who want to share scripts that \"just work\" for anyone\n\nI've been using it daily for my own work and would love to hear your feedback! If you find it useful, a GitHub star would mean a lot ‚≠ê And if you have ideas for improvements or want to contribute, PRs are super welcome! üôå\n\n‚≠ê GitHub: https://github.com/karpetrosyan/uv-vscode\n\nüì¶ Marketplace: https://marketplace.visualstudio.com/items?itemName=KarPetrosyan.uv-vscode",
    "author": "karosis88",
    "timestamp": "2025-10-16T12:37:29",
    "url": "https://reddit.com/r/Python/comments/1o8fz6j/i_built_a_vs_code_extension_for_uv_integration/",
    "score": 61,
    "num_comments": 9,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9o13e",
    "title": "Python question about dictionaries",
    "content": "In Python if you have a dictionary k={} and you do del k['s'] it raises an exception.\n\nWhy is it designed like this?\n\nI feel like there should be some kind of \"ignore if already deleted\" option.",
    "author": "Jealous_Lock_393",
    "timestamp": "2025-10-17T23:00:21",
    "url": "https://reddit.com/r/Python/comments/1o9o13e/python_question_about_dictionaries/",
    "score": 0,
    "num_comments": 41,
    "upvote_ratio": 0.32,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9ctmg",
    "title": "[Project] git2mind ‚Äî Summarize your repo for AI models in seconds",
    "content": "Hi folks! Ever tried feeding a large codebase to an LLM, only to hit the context window limit? Zipping it or copying files is a pain, and Repomix just bundled the whole project instead of giving a clean summary.\n\n### What my project does?\n**git2mind** solves this: it‚Äôs a CLI tool that generates a clean Markdown or JSON summary of your repo. Think of it as a ‚ÄúTL;DR‚Äù for your codebase. \n\nIt generates a general summary by extracting the names of classes and functions, without including the actual code. As long as the variable, class, and function names are meaningful, the AI can easily understand their purpose.\n### Target audience\nPython developers who want brief summary of their project for onboarding and documentation generation.\n### Comparison\nThe tool is similiar to Repomix but doesn't include the source code in the output. I tried to feed Repomix output to local models on Ollama and models couldn't read majority of the file because the file was too large.\n\n### Installation\n**Source code:** [https://github.com/yegekucuk/git2mind](https://github.com/yegekucuk/git2mind)\nThe project is on PyPi so you can install with pip. The README file is fairly detailed and easy to read, you can find the flags, usage tips and examples there. You can install and try the tool as easy as this:\n```sh\n# Install\npip install git2mind\n# Run (Generate summary of current directory)\ng2m .\n```\n\nFor now, the project really summarizes only Python projects. Currently, git2mind parses Python, Markdown, and Dockerfile files. But I plan to add parsers for many other programming languages. ",
    "author": "ResearchSwimming6553",
    "timestamp": "2025-10-17T13:49:46",
    "url": "https://reddit.com/r/Python/comments/1o9ctmg/project_git2mind_summarize_your_repo_for_ai/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.46,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o907tp",
    "title": "Talk Python Podcast Ep 523 ‚Äì Pyrefly: Fast, IDE-Friendly Typing for Python",
    "content": "https://talkpython.fm/episodes/show/523/pyrefly-fast-ide-friendly-typing-for-python\n\nTopics covered in this episode:\n- Pyrefly = fast type checker plus full IDE language server, built in Rust\n- Why speed matters: IDE feel and developer flow\n- Designed as a language server from the ground up\n- Installation is a single click in editors and simple on the CLI\n- Inference first, even for lightly typed code\n- Inlay hints in the editor and a one shot CLI to add annotations\n- Pragmatic adoption with migration and suppression scripts\n- Open source from day one with weekly releases and community input\n- Real world anchor: Instagram scale and deep dependency graphs\n- Ecosystem alignment rather than ‚Äúthe one true checker‚Äù\n- Comparing to ty (Astral)\n- Typing helps AI workflows and code mods\n- Use today for IDE; adopt type checking as it stabilizes\n\n(Disclaimer: I'm a maintainer for Pyrefly, happy to answer further questions in the comments)\n",
    "author": "BeamMeUpBiscotti",
    "timestamp": "2025-10-17T05:41:00",
    "url": "https://reddit.com/r/Python/comments/1o907tp/talk_python_podcast_ep_523_pyrefly_fast/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9i7p0",
    "title": "Uber Eats Account Generator Showcase, and ethical concerns?",
    "content": "Hey yall, I wanted to discuss the ethical concerns about this new project I did. This area in python on web scraping &amp; automation has pretty divided opinions based on what im seeing so far, so im looking to get your guys insight on things. \n\n  \nSo I got into automation not too long ago, there was this guy in a small community im in asking for help on this project he was doing related to uber, so I tried helping but didn't really have the answers to his questions. His solution required mobile requests, so I started to do more research on it. I hit a hard block for around a week, as there are BARLEY any resources on youtube or online in general. Most the guides are very simple and just scratch the surface. I had to do a lot of trial and error and finally got a medium understanding on this area of automation. After spending a long time purely on research and starting to build the project, I finished the prototype if you would call it that in around a month or so working almost every day. In the middle of this, I asked others for help in different web scraping communities, and I had quite a few chats on the ethics of this project. So, as any normal person would do, I tried looking for anything related to any developer or technical support team I could report this issue to. There was no reliable places I could email or submit a form, and reliable in the sense that they actually listen and attempt to do anything about this problem. I talked with their normal support team, and they kept telling me things like 'I will escalate your case sir' which pissed me off, because I know damn well they ghosted me each time. So my opinion on this topic is that it should be allowed to do research and have practice and open sourced material for learning, and companies should have a dedicated(and actually helpful) support team for developers and people who actually know their stuff. These projects help out the companies security a lot as well. However, the other opinion I heard was that the user experience would go down when companies add more security, such as captcha and stuff. But cmon, is the user experience really that important to where we sacrifice security?? So honestly would want your thoughts on this, and see other perspectives on this, especially in an era where bots are becoming really advanced.\n\n  \nNow heres the brief description overview/showcase of my project:\n\n* Automatically generates uber eats accounts all using their mobile api\n* To make this, I used a jailbroken iphone(to bypass ssl pinning) and mitmproxy to capture the network requests of their api\n* Built it out using python curl\\_cffi library to make requests, useful for spoofing the tls handshake to make the requests look more authentic\n* Options to use catch-all domains with googles imap, or a list of hotmail accounts, to generate mass amounts in batch.\n* Auto gets the OTP code on signup from either hotmail or google imap\n*  And a couple other stuff like proxy support, multi imap domain support, and spoofed device data and signature to avoid spam looking account generations.\n\n  \nIf anyone would like to check it out, its open-sourced on github here: [https://github.com/yubunus/Uber-Eats-Account-Generator](https://github.com/yubunus/Uber-Eats-Account-Generator)\n\n  \nHonestly the learning curve on this was brutal, im thinking of maybe making my own youtube video to guide beginners, with something thats actually a bit more advanced and not some basic api requests like most youtube videos I watched during my research. Let me know if thats something yall would be interested in. But do you guys think there should be more educational resources covering this?",
    "author": "TheCompMann",
    "timestamp": "2025-10-17T17:48:46",
    "url": "https://reddit.com/r/Python/comments/1o9i7p0/uber_eats_account_generator_showcase_and_ethical/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8pn4t",
    "title": "Made A Video Media Player that Plays Multi-Track Audio with Python",
    "content": "# Crusty Media Player\n\nI made a media player that was built to be able to take Multi-Track Video Files (ex: If you clip Recordings with separate Audio Tracks like System Audio and Microphone Audio) and give you the ability to play them back with both tracks synced without the use of an external editing software like Premiere Pro! And it's Open Source!\n\n# What This Project Does.\n\nIt utilizes¬†[ffmpeg](https://ffmpeg.org/)¬†bundled in to rip apart audio tracks from multi-tracked video media and¬†[PyQt6](https://www.pythonguis.com/tutorials/pyqt6-widgets)¬†to build the application and display video media.\n\n[GitHub](https://github.com/CrustyMonk/Crusty-Media-Player)¬†**&lt;---- Repo Here**\n\n[Crusty Media Player v0.1.1](https://github.com/CrustyMonk/Crusty-Media-Player/releases/tag/MediaPlayer)¬†**&lt;---- First Downloadable Release Here**\n\n# Why Did I Make This?\n\nIt's simple really lol. I like clipping funny and cool parts of when my friends and I play video games and such. I also like sometimes editing the videos as a hobby! To make the video editing simpler I have my recording settings set to record two tracks of audio, my system audio, and my microphone audio separate. The problem lies in that, if I ever want to just pull up a clip to show a friend or something, with any other media player I've used I am only able to select one track or the other! I have to open Premiere pro with my game running (Making my machine use a lot of resources!) and drag the clip into Premiere. This solves that problem by being able to just open the file with the low resource app and watch the clip with all the audio goods!\n\n# Target Audience?\n\nIf you really have that niche issue that I have, then Crusty Media Player might be perfect for you! I just have the .exe pinned to my task bar so I can run it whenever I get the urge to show off or even just view a clip!\n\n# Quick Start\n\n0. Download the packaged zip folder containing the .exe and bundled packages from the¬†[Downloadable Release](https://github.com/CrustyMonk/Crusty-Media-Player/releases/tag/MediaPlayer)\n\n1. Extract zip folder contents to desired location\n2. Run the¬†**Crusty\\_Media\\_Player.exe**\n3. If prompted with \"Windows protected your PC\" Pop-up, just click \"More Info\" and then \"Run Anyway\"\n4. Open Video Files that contain up to two tracks of audio (i.e. System and Microphone Audio)\n5. Watch the media all in sync! (Without the use of an editing software!)\n\nI would really appreciate any constructive criticism and any suggestions on things that I could add it for ease of use in future releases as well!\n\n# Comparison\n\nMedia Players like VLC and such also play video files from your computer. When using these tools though, you are always unable to play both audio tracks for multi-tracked videos simultaneously! Crusty Media Player fixes this problem, making you able to view multi-track audio media with both tracks simultaneously without the use of any resource heavy editing software like Premiere Pro or Filmora.\n\n# TLDR\n\n**Crusty Media Player**¬†is a media player that was built to be able to take Multi-Track Video Files (ex: If you clip Recordings with separate Audio Tracks like System Audio and Microphone Audio) and give you the ability to play them back with both tracks synced without the use of an external editing software like Premiere Pro!",
    "author": "Crusty_Monk",
    "timestamp": "2025-10-16T19:33:48",
    "url": "https://reddit.com/r/Python/comments/1o8pn4t/made_a_video_media_player_that_plays_multitrack/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8sxlx",
    "title": "How to profile django backend + celery worker app?",
    "content": "I'm working on a decently-sized codebase at my company that runs off a Django backend with Celery workers for computing our workflows. It's getting to the point where we seriously need to start profiling and adding optimizations, and I'm not sure of what tooling exists for this kind of stack. I'm used to compiled languages where it is much more straight-forward. We do not have proper tracing spans or anything of the sort. What's a good solution to profiling this sort of application? The compute-heavy stuff runs on Celery so I was considering just writing a script that launches Django + Celery in subprocesses then attaches pyspy to them and dumps flamegraph/speedscope data after executing calculation commands in a third process. All help is appreciated.",
    "author": "auric_gremlin",
    "timestamp": "2025-10-16T22:27:27",
    "url": "https://reddit.com/r/Python/comments/1o8sxlx/how_to_profile_django_backend_celery_worker_app/",
    "score": 1,
    "num_comments": 9,
    "upvote_ratio": 0.55,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8gd22",
    "title": "Python as a Configuration Language Using Starlark",
    "content": "I wrote an [article](https://openrun.dev/blog/starlark/) about how Pythonic syntax (using Starlark) helps avoids many of the configuration related challenges seen with YAML and other such languages. Let me know any feedback.",
    "author": "avkijay",
    "timestamp": "2025-10-16T12:52:05",
    "url": "https://reddit.com/r/Python/comments/1o8gd22/python_as_a_configuration_language_using_starlark/",
    "score": 17,
    "num_comments": 2,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o9a3f0",
    "title": "invert PDF colors",
    "content": "`import subprocess`\n\n`import sys`\n\n`import os`\n\n\n\n`try:`\n\n`import fitz`\n\n`except ImportError:`\n\n`subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"PyMuPDF\"])`\n\n`import fitz`\n\n\n\n`try:`\n\n`import tkinter`\n\n`except ImportError:`\n\n`subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tk\"])`\n\n`import tkinter`\n\n`from tkinter.filedialog import askopenfilename`\n\n\n\n`from PIL import Image, ImageOps`\n\n`try:`\n\n`from PIL import Image`\n\n`except ImportError:`\n\n`subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow\"])`\n\n`from PIL import Image, ImageOps`\n\n\n\n`root = tkinter.Tk()`\n\n`root.withdraw()`\n\n`input_path = askopenfilename(title=\"Select PDF\", filetypes=[(\"PDF files\", \"*.pdf\")])`\n\n`if not input_path:`\n\n`print(\"No file selected\")`\n\n`exit()`\n\n\n\n`dir_name, base_name = os.path.split(input_path)`\n\n`name, _ = os.path.splitext(base_name)`\n\n`output_path = os.path.join(dir_name, f\"{name}_inverted.pdf\")`\n\n\n\n`zoom = 4.0  # 4x resolution`\n\n`mat = fitz.Matrix(zoom, zoom)`\n\n\n\n`doc = fitz.open(input_path)`\n\n`images = []`\n\n\n\n`for page in doc:`\n\n`pix = page.get_pixmap(matrix=mat, alpha=False)`\n\n`img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)`\n\n`img = ImageOps.invert(img)`\n\n`images.append(img.convert(\"RGB\"))`\n\n\n\n`if images:`\n\n`images[0].save(output_path, save_all=True, append_images=images[1:])`\n\n`print(f\"inverted PDF saved to: {output_path}\")`\n\n`else:`\n\n`print(\"No pages found in PDF\")`\n\n",
    "author": "DerrickBagels",
    "timestamp": "2025-10-17T12:03:35",
    "url": "https://reddit.com/r/Python/comments/1o9a3f0/invert_pdf_colors/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.18,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8meso",
    "title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays",
    "content": "# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è\n\nWelcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!\n\n## How it Works:\n\n1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.\n2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.\n3. **News &amp; Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.\n\n## Guidelines:\n\n* All topics should be related to Python or the /r/python community.\n* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).\n\n## Example Topics:\n\n1. **New Python Release**: What do you think about the new features in Python 3.11?\n2. **Community Events**: Any Python meetups or webinars coming up?\n3. **Learning Resources**: Found a great Python tutorial? Share it here!\n4. **Job Market**: How has Python impacted your career?\n5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!\n6. **Community Ideas**: Something you'd like to see us do? tell us.\n\nLet's keep the conversation going. Happy discussing! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-16T17:00:42",
    "url": "https://reddit.com/r/Python/comments/1o8meso/friday_daily_thread_rpython_meta_and_freetalk/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o80g8o",
    "title": "[Project] doespythonhaveit: a semantic search engine for Python libraries",
    "content": "Hey folks! I've been working on an open-source project called **doespythonhaveit**, a **semantic search engine for Python libraries** powered by `FastAPI` and `sentence-transformers`.\n\nBasically, you can type something like:\n\n&gt;\"machine learning time series\"\n\nand it'll (hopefully) suggest things like scikit-learn or darts.\n\nThe goal is to make discovering Python libraries faster, smarter, and a little less about keyword guessing.\n\nIt's not live *yet* (hosting the model costs a bit), but you can **try it locally**, setup instructions are in the repos:\n\n* API: [github.com/hasboi/doespythonhaveit-api](https://github.com/hasboi/doespythonhaveit-api)\n* Web: [github.com/hasboi/doespythonhaveit-web](https://github.com/hasboi/doespythonhaveit-web)\n\n---\n\n# What My Project Does\n\n**doespythonhaveit** lets you search Python libraries *by meaning*, not by exact keywords. Instead of googling *\"python library for handling CSVs elegantly\"* and clicking through five Stack Overflow posts, you can just search that sentence directly ‚Äî and it'll understand what you mean using embeddings.\n\nI am also planning a **terminal version**, so you can type something like:\n\n    dphi &lt;query&gt; &lt;flags&gt;\n\nand it will suggest relevant libraries **without leaving your code editor or terminal**, basically a semantic library search right where you write code.\n\n---\n\n# Target Audience\n\nMainly aimed at:\n\n* **Developers** who are tired of remembering exact library names\n* **Beginners** who want to discover tools without knowing where to start\n* **Open-source enthusiasts** who love browsing cool Python projects\n\nRight now it's mostly a **toy project / prototype**, but I‚Äôm hoping to make it stable enough for production someday.\n\n---\n\n# Comparison\n\nIt's kinda like if pypi.org and Google had a baby, but that baby actually understands what you're looking for. Unlike traditional search (which relies on exact matches), this one uses semantic similarity. So searching \"plotting dataframes nicely\" might bring up seaborn or plotly, even if you never mention the words \"plot\" or \"graph.\"\n\nIf you'd like to support deployment and hosting, you can sponsor me via [GitHub Sponsors](https://github.com/sponsors/hasboi) or [Ko-fi](https://ko-fi.com/hasboi).\n\nAlso, contributions are super welcome! üôå I am looking for:\n\n* More Python libraries to add to the dataset\n* Help cleaning and improving the dataset, so results are more accurate and relevant\n* Ideas for improving the search algorithm\n\nEverything else (tech details, install guide, roadmap, etc.) is in the repos. Would love your feedback, PRs, or just general thoughts! üí¨",
    "author": "OpportunityAway4972",
    "timestamp": "2025-10-16T01:13:15",
    "url": "https://reddit.com/r/Python/comments/1o80g8o/project_doespythonhaveit_a_semantic_search_engine/",
    "score": 52,
    "num_comments": 7,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8yhuk",
    "title": "Turn on Wi-Fi via browser in 7 lines?",
    "content": "**What My Project Does**\n\nThe [mininterface](https://github.com/CZ-NIC/mininterface) project creates dialogs that work everywhere, ex. in the browser.\n\nHere is the app that checks the Wi-Fi status and then turns it on/off. By default, it raises a desktop window with the confirmation dialog. See it here: [https://imgur.com/a/20476ZN](https://imgur.com/a/20476ZN)\n\n    #!/usr/bin/env python3\n    from subprocess import check_output, Popen\n    from mininterface import run\n    \n    cmd = \"nmcli\", \"radio\", \"wifi\"\n    state = check_output(cmd, text=True).strip() # -&gt; 'enabled' / 'disabled'\n    \n    m = run() # shows the dialog\n    if m.confirm(f\"The wifi is {state}. Turn it on?\"):\n        Popen(cmd + (\"on\",))\n    else:\n        Popen(cmd + (\"off\",))#!/usr/bin/env python3\n\nHowever when you put the `interface=\"web\"` parameter in the `run` function or when use launch the file with the `MININTERFACE_INTERFACE=web`environment variable set like this:\n\n`$ MININTERFACE_INTERFACE=web ./wifi.py`\n\nit starts listening on the HTTP port 64646. That way, you can turn on/off the Wi-Fi status (or do anything else, it's up to you to imagine all the possibilities) remotely.\n\n**Target Audience**\n\nEven though opening a port needs a security measures that I won't enlist here, and thus the functionality I recommend for geeks, the library is ready for the production to handle all the system utility dialogs.\n\n**Comparison**\n\nThere is none alternative to [https://github.com/CZ-NIC/mininterface](https://github.com/CZ-NIC/mininterface) that creates dialogs as a desktop application, as a terminal application, or as a web application at once.\n\nHowever, you may do similar behaviour with these utilies:\n\n\\* Zenity ‚Äì just bash dialogs  \n\\* notify-send ‚Äì small utility to print out a notification in the desktop  \n\\* [Gooey](https://github.com/chriskiehl/Gooey) ‚Äì turn python script into a GUI application ‚Äì needs to work with ArgumentParser",
    "author": "the-e2rd",
    "timestamp": "2025-10-17T04:15:01",
    "url": "https://reddit.com/r/Python/comments/1o8yhuk/turn_on_wifi_via_browser_in_7_lines/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o97fxh",
    "title": "I built a tool to run Python in a full Linux environment, instantly, in your browser.",
    "content": "I've been building a tool called Stacknow. It's a full Linux environment that boots instantly in a browser tab using WebAssembly. This isn't a remote VM‚Äîall the code runs locally on your machine, completely sandboxed from your file system.\n\nHere's what it's for:\n\nInstantly test libraries:¬†pip install¬†anything without touching your local setup.\n\nZero-setup scripting:¬†Just open a tab and start writing Python.\n\nSafe execution:¬†The browser's sandbox means it's totally isolated from your machine.\n\nShareable environments:¬†Send a single link to a working, reproducible setup.\n\nI'd love for you to try it out and let me know what you think. It's still early, so any feedback is super valuable.\n\nYou can find it here:¬†[https://console.stacknow.io/](https://console.stacknow.io/)",
    "author": "Substantial-Stage459",
    "timestamp": "2025-10-17T10:22:22",
    "url": "https://reddit.com/r/Python/comments/1o97fxh/i_built_a_tool_to_run_python_in_a_full_linux/",
    "score": 0,
    "num_comments": 15,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o86n2t",
    "title": "InfoLens - A python based GUI dashboard",
    "content": "Hello everyone!\n\nI‚Äôve been working on a Python project called **InfoLens,** a **CustomTkinter**\\-based **GUI dashboard** that fetches and displays personalized information across multiple genres ‚Äî **news, finance,** and **weather** ‚Äî all in one place.\n\n# What My Project Does:\n\nIt pulls live data from credible sources like:\n\nüß™**ScienceDaily** ‚Äì for science and innovation headlines\n\nüí∞**Economic Times &amp; Yahoo Finance API** ‚Äì for real-time stock data and trends\n\nüå§Ô∏è[**wttr.in**](http://wttr.in) **API** ‚Äì for location-based weather updates\n\n# Purpose:\n\nWe live in a world where **information surrounds us everywhere.** In fact, the average person in 2025 processes about **75-80 GB of information per day** up from 34 GB in 2008 and 63 GB in 2012. That includes all the **ads**, **unnecessary clutter** that one doesn't even need. However, studies have shown **color-coded dashboards** improved **visual search performance** and **recall**, enhancing both comprehension and memory; exactly **what InfoLens does!**\n\n# üîßBuilt with:\n\nPython\n\nCustomTkinter for the GUI\n\nWeb scraping (BeautifulSoup, requests)\n\nAPIs (yfinance, [wttr.in](http://wttr.in), etc.)\n\n# Target Audience:\n\nCurrently this is a side project, but meant for all python enthusiasts who are eager to provide their invaluable experience in this app.\n\n# Comparison:\n\nAs a GUI dashboard, InfoLens focuses highly on **data readability.** While other tools like Perplexity exist, InfoLens is **unique in the problem solving sense**, using web scraping to **remove  clutter** such as ads and provides you only what you need. Its still in its **budding phase** as it started out as just a science exhibition project, and **further refinements** in quality and user access and make it highly efficient.\n\n# I‚Äôd love your feedback on:\n\nUI/UX ‚Äì is the layout intuitive or could it be cleaner?\n\nPerformance or usability improvements\n\nFeature ideas (e.g., more data sources, customization, alerts, etc.)\n\n**GitHub Repo:** [https://github.com/WaveInCode/InfoLens.git](https://github.com/WaveInCode/InfoLens.git)\n\nIf you try it out, please let me know what you think! All feedback ‚Äî big or small ‚Äî will help shape future versions of InfoLens. Thanks in advance for checking it out! üöÄ",
    "author": "Cool-Worry-8045",
    "timestamp": "2025-10-16T06:48:27",
    "url": "https://reddit.com/r/Python/comments/1o86n2t/infolens_a_python_based_gui_dashboard/",
    "score": 7,
    "num_comments": 7,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o86j5t",
    "title": "Automating your heating with Octopus Energy AGILE tariff",
    "content": "Hi all, I've just made a Python tutorial for how you can automate your electric heaters during the Agile Energy Plunge Pricing, in the UK.\n\nEffectively, we're automatically switching on our smart plugs (electric radiators), when the price of electricity is negative. This results in consistent credit back every time there's an Octopus Energy Plunge Pricing, plus a nice warm home.\n\nYou just need Tapo smart plugs and a Raspberry pi.\n\n[https://youtu.be/ch-9DpZL6Vg](https://youtu.be/ch-9DpZL6Vg)\n\ncode:\n\n[https://github.com/yojoebosolo/AutoHeating/](https://github.com/yojoebosolo/AutoHeating/)\n\nHope it's helpful to some of you.",
    "author": "yojoebosolo",
    "timestamp": "2025-10-16T06:44:02",
    "url": "https://reddit.com/r/Python/comments/1o86j5t/automating_your_heating_with_octopus_energy_agile/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8wuhr",
    "title": "What is the easiest neural network project to someone who is just starting with AI/ML and python",
    "content": "Is it easier to work with datasheets? like predicting the probability of someone having diabetes using pima Indians Diabetes Database? Or is images or something else easier ",
    "author": "tomuchto1",
    "timestamp": "2025-10-17T02:36:47",
    "url": "https://reddit.com/r/Python/comments/1o8wuhr/what_is_the_easiest_neural_network_project_to/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.32,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8yfb5",
    "title": "Made an encryption tool in Python (and use of some C)",
    "content": "\n\n# PyLI\n\nMade a standalone GUI app that encrypts files locally, no middle-man interaction.\n\nUses **AES-256-GCM** or **ChaCha20-Poly1305** for encryption and **Argon2ID** (or **PBKDF2** as fallback) for key derivation. Works offline, open source (MIT);\n\n\\~40MB standalone.\n\n# Source code\n\n[**GitHub**](https://github.com/Commonwealthrocks/PyLI) **&lt;-- here!**\n\nMore can be seen on my repo's README file, I recommend reading it before trying the app.\n\n# What my project does?\n\nEncrypts files using **AES-256-GCM (AEAD)** or **ChaCha20-Poly1305** locally on your PC / machine; uses **Argon2ID** as said earlier of **PBDKF2** for KDF.\n\n  \nAll cryptowork is tweakable in the settings of the app.\n\n# QUICK START\n\n0. Install the .exe (or source) from the dist folder / releases tab for the full source code.\n\n1. Run the app\n\n2. Select file(s) or a folder; folders only work with drag n' drop\n\n3. Choose a password, any kind for a simple test really\n\n4. Hit encrypt / decrypt\n\nIt is recommended to also check out the apps settings tab, especially for archive mode and the crypto tweaks.\n\n# FEATURES (as said earlier)\n\n\\- **AES-256-GCM or ChaCha20-Poly1305** encryption\n\n\\- Archive mode (encrypt multiple files into one; basically knockoff .zip files)\n\n\\- Optional compression\n\n\\- Optional error correction (Reedsolo)\n\n\\- Works completely offline\n\n# COMPARISON\n\nTools like **WinRAR** or **7-zip** MIGHT do similar but they are compression focused; **PyLI** is dedicated to security / encryption. More dedicated tools for this stuff like **VeraCrypt** is for whole disks, overkill for regular files or **AxCrypt** which is also based on security. But they use **AES-128** for the free tier and their docs about the core crypto itself is vague.\n\n# Target audience\n\n**PyLI** is MOSTLY meant for power users, or users who want control over their settings without going through the pain that is trying to use **GPG** or **PGP**.\n\n# TL--DR\n\n**PyLI** as a whole can be seen as \"joke\" software, but from what it offers; you can decide that.\n\nThe code is not professionally audited or reviewed, but is open source for the community. Feel free to leave any feedback!",
    "author": "CommonWealthHimself",
    "timestamp": "2025-10-17T04:11:04",
    "url": "https://reddit.com/r/Python/comments/1o8yfb5/made_an_encryption_tool_in_python_and_use_of_some/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.28,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8uh8d",
    "title": "gitfluff: Commit Message Linter (Conventional Commits + AI signature cleanup)",
    "content": "Hey Peeps,\n\nI'm pleased to show case a new small and very fast commit message linter and autofixer tool [gitfluff](https://github.com/Goldziher/gitfluff).\n\n## What My Project Does\n\nClaude Code kept injecting \"ü§ñ Co-Authored-By\" trailers into commits. You can disable it now in local settings, but I needed team-wide enforcement across multiple repos and multiple languages. Plus I wanted strict Conventional Commits validation without cobbling together multiple tools.\n\n## What it does\n\n- **Enforces Conventional Commits 1.0.0** (type, scope, breaking changes, footers) with full spec compliance.\n- **Strips AI signatures** automatically (configurable patterns)\n- **Validates or rewrites** messages in place with `--write`\n- **Zero config** to start, optional `.gitfluff.toml` for custom rules which allow you to do whatever you want basically. \n\n## Install &amp; Use\n\nThe tool is written and rust and is compiled to multiple platforms. You can install it directly via cargo:\n\n```bash\ncargo install gitfluff\n```\n\nOr using homebrew:\n\n```bash\nbrew install goldziher/tap/gitfluff\n```\n\nOr via NPM:\n\n```bash\nnpm install -g gitfluff\n```\n\nOr via PIP:\n\n```bash\npip install gitfluff\n```\n\nYou can then install it as a commit message hook:\n\n```bash\ngitfluff hook install commit-msg --write\n```\n\nAlternatively you can install it as a hook for `pre-commit` (or `prek`) by adding the following to you `.pre-commit-config`:\n\n```yaml\nrepos:\n  - repo: https://github.com/Goldziher/gitfluff\n    rev: v0.2.0\n    hooks:\n      - id: gitfluff-lint\n        name: gitfluff (lint)\n        entry: gitfluff lint --from-file\n        language: system\n        stages: [commit-msg]\n        args: [\"{commit_msg_file}\"]\n\n      # or using the autofix hook:\n\n      # - id: gitfluff-write\n      #  name: gitfluff (lint + write)\n      #  entry: gitfluff lint --from-file\n      #  language: system\n      #  stages: [commit-msg]\n      #  args: [\"{commit_msg_file}\", \"--write\"]\n```\n\nAnd then run `pre-commit install --hook-type commit-msg`, which will install the hook correctly. \n\nYou can also integrate it into `lefthook` or `husky` using `npx` or `uvx` commands!\n\nMain workflow: add to pre-commit config, forget about it. Devs commit normally, hook validates/cleans messages before they hit history.\n\n## Target Audience\n\nTeams enforcing commit conventions across polyglot projects. Devs using AI coding assistants who want clean commit history. Anyone who needs Conventional Commits validation without JavaScript dependencies.\n\n## Comparison\n\n- **commitlint** (Node ecosystem, requires separate config for cleanups)\n- **cocogitto** (Rust, focused on semver release workflows)\n- **gitlint** (Python, extensible but requires custom plugins for AI signatures)\n\nAnd many other tools of course, I cant claim this is original. The main difference is that `gitfluff` combines validation + pattern cleanup in one binary with prebuilt distributions for all major platforms.\n\nAs usual, if you like the tool, star [github.com/Goldziher/gitfluff](https://github.com/Goldziher/gitfluff).",
    "author": "Goldziher",
    "timestamp": "2025-10-17T00:02:43",
    "url": "https://reddit.com/r/Python/comments/1o8uh8d/gitfluff_commit_message_linter_conventional/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o7bat4",
    "title": "Zuban - A Python Language Server / Typechecker - Beta Release",
    "content": "I have just created a Beta Release for Zuban.\n\nZuban now supports all key features of a Python Language Server ‚Äî including completions, rename, and type checking ‚Äî with auto-imports coming soon.\n\nZuban is a high-performance Python Language Server and type checker implemented in Rust, by the author of Jedi. Zuban is **20‚Äì200√ó faster than Mypy**, while using roughly half the memory and CPU compared to Ty and Pyrefly. It offers both a PyRight-like mode and a Mypy-compatible mode, which behaves just like Mypy;  \nsupporting the same config files, command-line flags, and error messages.\n\nYou can find the source code [here](https://github.com/zubanls/zuban/).  \nDifferent Python type checkers are compared [here](https://htmlpreview.github.io/?https://github.com/python/typing/blob/main/conformance/results/results.html).\n\nThe Zuban type checker is now in a very stable state, with many issues resolved and only a few remaining. The next planned features include dedicated support for Django and Pytest.\n\n### Support\n\nIf you have a large Mypy codebase that needs significant bug fixing, I‚Äôd be happy to help.",
    "author": "zubanls",
    "timestamp": "2025-10-15T06:40:24",
    "url": "https://reddit.com/r/Python/comments/1o7bat4/zuban_a_python_language_server_typechecker_beta/",
    "score": 127,
    "num_comments": 42,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o77mip",
    "title": "Recommending `prek` - the necessary Rust rewrite of `pre-commit`",
    "content": "Hi peeps,\n\nI wanna recommend to all of you the tool [prek](https://github.com/j178/prek) to you. This is a Rust rewrite of the established Python tool [pre-commit](https://pre-commit.com), which is widely used. Pre-commit is a great tool but it suffers from several limitations:\n\n1. Its pretty slow (although its surprisingly fast for being written in Python)\n2. The maintainer ([asottile](https://github.com/asottile)) made it very clear that he is not willing to introduce monorepo support or any other advanced features (e.g. parallelization) asked over the years\n\nI was following this project from its inception (whats now called Prek) and it evolved both very fast and very well. I am now using it across multiple project, e.g. in [Kreuzberg](https://github.com/Goldziher/kreuzberg), both locally and in CI and it does bring in an at least x10 speed improvement (linting and autoupdate commands!)\n\nSo, I warmly recommend this tool, and do show your support for Prek by giving it a star!",
    "author": "Goldziher",
    "timestamp": "2025-10-15T03:46:11",
    "url": "https://reddit.com/r/Python/comments/1o77mip/recommending_prek_the_necessary_rust_rewrite_of/",
    "score": 212,
    "num_comments": 105,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8niqf",
    "title": "New to Coding in Python",
    "content": "I don't have a question related to Python. I just wanted to say that I'm new to python and I'm just now finding out there is a function called \"cumsum.\" As far as I'm concerned, python is now a 10/10 coding language.",
    "author": "YaBoi843",
    "timestamp": "2025-10-16T17:52:58",
    "url": "https://reddit.com/r/Python/comments/1o8niqf/new_to_coding_in_python/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.15,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8apus",
    "title": "[Project] mini language based on Python: Montyp",
    "content": "I thought it would be fun to base a mini language on python.\n\nThe result is less than stellar after a lot of work, there is basically not much, but anyway...I just wanted to do something funny.\n\nIf anyone wants to look around and contribute, or give advice, I would honored...\n\nI wanted to call it Monthy or Monty to continue the reference on Monty Python but it is apparently already taken...\n\nAnyway... I wanted to sort of make it even more human readable than python, and also (I know that this is crazy and impossible, but indentation made me a bit crazy at first I was always having indentation errors) indentation free, case insensitive keywords and various other things.\n\nI know all of this may be stupid.\n\nBut anyway....here we are, this is the github¬†[repo](https://github.com/SimonGPrs/montyp_language).\n\n  \nI also tried to compile Montyp in Montyp but this has so far failed and failed and failed and failed forever. The file is nonetheless on Github.\n\n  \nIf anyone has any advice, great...\n\n\n\n\\--------------------------------------------------------------------------------------------------------------------  \n**What Montyp Does**\n\nAs said, the idea of Montyp was to have mega simple programming language that compiles to Python but removes the pain points that frustrate beginners related to strict indentation and other points. The idea would be to approach even more plain English.\n\nInstead of writing:\n\n    if score &gt;= 10:\n        print(f\"Score: {score}\")\n\nYou write:\n\n    if score is at least 10 ¬†¬†¬† say: Score {score} end\n\n**Target Audience**\n\nCurious people,\n\nAdvanced developers that would be crazy enough to play around this \"toy\" language\n\n**Comparison**\n\nI am not aware of other languages based on Python",
    "author": "Whole-Ad7298",
    "timestamp": "2025-10-16T09:24:08",
    "url": "https://reddit.com/r/Python/comments/1o8apus/project_mini_language_based_on_python_montyp/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8e955",
    "title": "searching for job by preparing my own",
    "content": "hi,am 35 years old with no prior experience in IT,now am preparing myself as python developer or related jobs.I learnt Python,Numpy,Pandas,Mattplotlib,[SQL.Am](http://SQL.Am) still in a process of learning. To get a job now may i know the path to proceed forward other than applying online? Any other guys who passed through the same path? Any other inputs plz.",
    "author": "Zestyclose_Block5381",
    "timestamp": "2025-10-16T11:32:54",
    "url": "https://reddit.com/r/Python/comments/1o8e955/searching_for_job_by_preparing_my_own/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o8a2bi",
    "title": "Quickest way to build a custom AI chatbot to query your python project",
    "content": "Hi Community,\n\nI‚Äôve been working on a side project to make it easier for Python developers to understand, explore, and interact with their own codebases ‚Äî using AI.\n\n# What My Project Does\n\nThe tool indexes your code and creates a chatbot that acts like a personal coding assistant for your project.  \nYou can ask it things like:\n\n* Generate code base on these functional requirements and my current code context\n* Explain a specific API call\n* Create a flowchart of this API call\n\nIt‚Äôs designed to help you navigate large projects faster and automate documentation and comprehension tasks.\n\n# Quickstart\n\nWe‚Äôve got a hosted version you can try:\n\nüëâ [https://firstmate.io/](https://firstmate.io/)  \nüëâ [https://console.firstmate.io/](https://console.firstmate.io/)\n\nJust connect your repo (GitHub or local) ‚Äî the chatbot will automatically build itself.\n\n# Target Audience\n\n* Python developers\n\n# Comparison\n\n* We are faster than index your code &amp; build your own chatbot\n* Unlike GitHub's Semantic, we generate a system of tools for you\n* From my test, we work better than Github's copilot search. I might be biased. Let me know if you think otherwise üôè\n\n# Features\n\n* Supports Python projects\n* Understands code structure, dependencies, and flow\n* Lets you query or modify code directly\n* Works on both private and open-source repos\n\nOur Github: [https://github.com/firstmatecloud](https://github.com/firstmatecloud)\n\nIf it‚Äôs useful, a ‚≠ê on the repo and comments here really help prioritize the roadmap. üôè",
    "author": "Ill_Ad4125",
    "timestamp": "2025-10-16T08:59:55",
    "url": "https://reddit.com/r/Python/comments/1o8a2bi/quickest_way_to_build_a_custom_ai_chatbot_to/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o71ejn",
    "title": "GIL free and thread safety",
    "content": "For Python 3.14 free GIL version to be usable, shouldn't also Python libraries be re-written to become thread safe? (or the underlying C infrastructure)",
    "author": "Active-Fuel-49",
    "timestamp": "2025-10-14T21:20:34",
    "url": "https://reddit.com/r/Python/comments/1o71ejn/gil_free_and_thread_safety/",
    "score": 96,
    "num_comments": 25,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o7r7dw",
    "title": "blank-line-after-blocks, a formatter to improve readability and prevent errors",
    "content": "I recently developed [blank-line-after-blocks](https://github.com/jsh9/blank-line-after-blocks), a Python auto-formatter to improve code readability and prevent human errors.\n\n# What My Project Does\n\nIt adds a blank line after if/for/while/with/try blocks. See the example below (the lines with `+` sign are added by this formatter.\n\n      if condition:\n          do_something()\n    +\n      next_statement()  if condition:\n          do_something()\n    +\n      next_statement()\n\n**Why is it s a good idea to add a blank line after blocks?**\n\nThis can improve readability:\n\n* A blank line sends a visual cue that a block ends here\n* A blank line makes it easier to distinguish `if` and `if/else` blocks. Look at this example\n\nHard to distinguish:\n\n    if a &gt; 2:\n        print(a)\n    if b &lt; 3:\n        print(b)\n    else:\n        print('1')\n\nEasier to distinguish\n\n    if a &gt; 2:\n        print(a)\n    \n    if b &lt; 3:\n        print(b)\n    else:\n        print('1')\n\nHaving a blank line after blocks can also reduce the chance of human errors. Sometimes we accidentally hit \"Tab\" or \"Backspace\" on our keyboards. This could introduce costly errors in Python, because Python relies on indentation as syntax cues.\n\nHere is an example:\n\n    raw_result = 0\n    for i in range(10):\n        raw_result += i\n    final_result = my_func(raw_result)\n\nIf we accidentally hit \"Tab\" on the last line, the code becomes:\n\n    raw_result = 0\n    for i in range(10):\n        raw_result += i\n        final_result = my_func(raw_result)\n\nwhich will yield a completely different result. **This error is very difficult to find, thus a costly error.**\n\nBut if we add a blank line after the block,\n\n    raw_result = 0\n    for i in range(10):\n        raw_result += i\n    \n        final_result = my_func(raw_result)\n\nIt would be ***slightly*** easier to find out the error.\n\n# Target Audience\n\nAnyone who writes Python code. But this is especially helpful for production-level code, because reducing diffs and reducing human errors can be valuable.\n\n# Comparison with Alternatives\n\nAs far as I know, there are no alternatives. No existing Python formatter does this.  \n  \n",
    "author": "Linter-Method-589",
    "timestamp": "2025-10-15T16:50:41",
    "url": "https://reddit.com/r/Python/comments/1o7r7dw/blanklineafterblocks_a_formatter_to_improve/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.58,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o7u9q1",
    "title": "Interactive HMTL",
    "content": "Hi guys\n\nI‚Äôm creating an interactive HTML page to study graphs. The idea is to create an interface where the user can click on each node and see information about it. Another feature is to display the graph legend in a pop-up window.\nI‚Äôm using NetworkX to create the graph and Bokeh to generate the HTML. Do you know if it‚Äôs possible to create a professional interface using Bokeh or another Python library? \nI create a page but seems so simple :(",
    "author": "gps100",
    "timestamp": "2025-10-15T19:13:34",
    "url": "https://reddit.com/r/Python/comments/1o7u9q1/interactive_hmtl/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6u9cg",
    "title": "Gave up on C++ and just went with Python",
    "content": "I was super hesitant on going with python, since it felt like I wasn't gonna learn alot if I just go with python... which everyone in ProgrammingHumor was dissing on... then I started automating stuff... and Python just makes everything so smooth.... then I learned about the wonders of Cython... now I'm high on Cython..\n\nHow do you all speed up your python project?",
    "author": "Gazuroth",
    "timestamp": "2025-10-14T15:44:45",
    "url": "https://reddit.com/r/Python/comments/1o6u9cg/gave_up_on_c_and_just_went_with_python/",
    "score": 133,
    "num_comments": 76,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o7k3y6",
    "title": "Completely rewrote Buridan UI",
    "content": "Hey everyone, so today I decided to rewrite my ui lib from scratch and implemented a new site architecture. It's not perfect nor is it the last iteration, but I really liked the results and so I deccided to share it here!\n\n**What My Project Does**\n\nBuridan UI is a component library for Reflex that you copy and paste directly into your project instead of installing as a package. It provides:\n\n* Wrapped React components (CountUp, Icons, Spinner, Typed effects, etc.)\n* Pre-built UI patterns and layouts\n* Chart components and data visualizations\n* JavaScript integrations ready to use\n* Multiple theming options (Hematite, Feyrouz, Yaqout, Zumurrud, Kahraman, Amethyst)\n\nNew features in this rewrite:\n\n* **Markdown files static serve** \\- you can view the content as markdown\n* **AI assistant integration** \\- Click to open ChatGPT or Claude with pre-filled prompts about the component or page that can be easily scrapped in markdown\n* **SPA architecture** \\- Completely rebuilt for smoother navigation and better performance\n* **Cleaner codebase** \\- Rewrote everything from scratch with lessons learned from v1\n\n**Target Audience**\n\nThis is built for any Reflex developer, the copy-paste approach means you can use it in serious projects without worrying about the library being abandoned or breaking changes in updates.\n\n**Comparison**\n\nIt's heavily inspired theme from shadcn but its also heavily tailored for the reflex ecosystem, specifically where we wrap react and include JS integration documentation\n\n\n\nYou can check it out here: [Buridan UI](https://buridan-ui.reflex.run/docs/getting-started/introduction)  \nThe repo (it's open soruce!): [https://github.com/buridan-ui/ui](https://github.com/buridan-ui/ui)  \n\n\nFeedback is always welcome! \n\n",
    "author": "Wonderful-Today-497",
    "timestamp": "2025-10-15T12:08:00",
    "url": "https://reddit.com/r/Python/comments/1o7k3y6/completely_rewrote_buridan_ui/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o77t5h",
    "title": "Built a Tool to Sync GitHub Issues to Linear ‚Äì Feedback Welcome!",
    "content": "Hey everyone,\n\n**Target Audience**: Useful for technical support engineers, dev leads, or anyone managing projects via GitHub and Linear.\n\n**What my project does**  \nI‚Äôve built a tool that automatically syncs GitHub issues into Linear tickets. The idea is to reduce the manual overhead of copy-pasting or re-creating issues across platforms, especially when you're using GitHub for external collaboration (e.g., open source, customer bug reports) and Linear for internal planning and prioritization.\n\nYou can find it here:  \nüîó [https://github.com/olaaustine/github-issues-linear](https://github.com/olaaustine/github-issues-linear)\n\nThe README is fairly detailed and should help you get it running quickly ‚Äî it's currently packaged as a customizable Docker container, so setup should be straightforward if you‚Äôre familiar with containers.\n\nüß™ **Status:**  \nThe project is still in early development, so it‚Äôs very much a WIP. But it works, and I‚Äôm actively iterating on it. The goal is to make it reliable enough for daily use and eventually extend support to other issue trackers beyond Linear.\n\nI‚Äôd really appreciate any thoughts or ideas ‚Äì even if it‚Äôs just a quick reaction. Thanks!",
    "author": "Virtual_Initiative67",
    "timestamp": "2025-10-15T03:56:37",
    "url": "https://reddit.com/r/Python/comments/1o77t5h/built_a_tool_to_sync_github_issues_to_linear/",
    "score": 14,
    "num_comments": 6,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o7rffb",
    "title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",
    "content": "# Weekly Thread: Professional Use, Jobs, and Education üè¢\n\nWelcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.\n\n---\n\n## How it Works:\n\n1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.\n2. **Education Q&amp;A**: Ask or answer questions about Python courses, certifications, and educational resources.\n3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.\n\n---\n\n## Guidelines:\n\n- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.\n- Keep discussions relevant to Python in the professional and educational context.\n  \n---\n\n## Example Topics:\n\n1. **Career Paths**: What kinds of roles are out there for Python developers?\n2. **Certifications**: Are Python certifications worth it?\n3. **Course Recommendations**: Any good advanced Python courses to recommend?\n4. **Workplace Tools**: What Python libraries are indispensable in your professional work?\n5. **Interview Tips**: What types of Python questions are commonly asked in interviews?\n\n---\n\nLet's help each other grow in our careers and education. Happy discussing! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-15T17:00:29",
    "url": "https://reddit.com/r/Python/comments/1o7rffb/thursday_daily_thread_python_careers_courses_and/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o7fcvo",
    "title": "I built a classic \"Crack the Code\" console game in Python: Digit Detective üïµÔ∏è‚Äç‚ôÄÔ∏è",
    "content": "Hello everyone! I'm sharing my completed project: Digit Detective, a pure Python console game.\n\nMy goal was to create a clean, working implementation of a code-breaking puzzle game, focusing on clean structure and good input validation.\n\n\n\n# üîç What My Project Does (The Game and Code)\n\n\n\nDigit Detective is a command-line utility where you try to crack a secret 4-digit numeric code in 8 attempts.\n\n* Gameplay: The game gives you instant, clear textual feedback after each guess, indicating how many digits are:\n   1. Correct and in the Right Position.\n   2. Correct but in the Wrong Position.\n* Code Focus: The project demonstrates basic Object-Oriented Programming (OOP), robust input validation to prevent non-numeric guesses, and clear separation of game logic. It's a single, runnable Python file.\n\n\n\n# üéØ Target Audience\n\n\n\nWhile anyone can play, the project is structured to benefit specific audiences:\n\n* Python Beginners/Learners: The code is straightforward. It's an excellent, simple project to read, clone, and understand basic game loop structure and logic implementation.\n* Fans of Mastermind: If you enjoy classic code-breaking puzzles, this offers a fast, clean, terminal-based version.\n\n\n\n# üÜö Comparison:\n\n\n\nThis project is inspired by the logic of Mastermind, but adapted for the modern terminal environment. Unlike the classic board game:\n\n* It deals exclusively with a 4-digit numeric code (0-9) instead of colored pegs, simplifying input.\n* It provides instant, unambiguous textual hints instead of relying on manually tracking black and white pegs.\n* The entire experience is self-contained in a single, accessible Python script, emphasizing a focus on logic and code execution over complex UI.\n\nFeel free to check out the [digit-detective.py](http://digit-detective.py) file. I‚Äôd appreciate any feedback on the Python logic, structure, or best practices!\n\nGitHub Link:[https://github.com/itsleenzy/digit-detective](https://github.com/itsleenzy/digit-detective)",
    "author": "leenzy-leen",
    "timestamp": "2025-10-15T09:13:01",
    "url": "https://reddit.com/r/Python/comments/1o7fcvo/i_built_a_classic_crack_the_code_console_game_in/",
    "score": 4,
    "num_comments": 3,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o7nwyp",
    "title": "OpenJlang BetaV0.1 \"Verna\" is here!",
    "content": "The open source programming language oJl releases its first public version, find out more about the project on the website: https://ojlang.github.io/ojl/index.html See the oJl page on GitHub: https://github.com/ojlang",
    "author": "Jaozerakkj",
    "timestamp": "2025-10-15T14:33:01",
    "url": "https://reddit.com/r/Python/comments/1o7nwyp/openjlang_betav01_verna_is_here/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o7dqre",
    "title": "Nyxelf: An Unreliable Dynamic Analysis Toolkit.",
    "content": "A while back I made [Nyxelf](https://github.com/m3rcurylake/Nyxelf/), a static and dynamic analysis toolkit. Well, in a sentence, it got overhauled to a great extent. \n\nWhat it does :\nIt provides a professional report of the executable, I replaced the simple strace dynamic analysis system with BPFtrace, Valgrind and tcpdump running on a minimal buildroot image, tracing dynamic and memory activity, along with capturing network packets, which is further enhanced with ai-assisted summerisation of the dynamic analysis. I used pyelftools, capstone etc for static analysis, which detects symbols, functions, sections, headers, .rodata variables etc. Finally it disassembles the binary to readable C and x64 intel Assembly with capstone, r2pipe and angr. And this entire thing is presented on the screen with pywebview with a cool one-dark theme.\n\nTarget Audience:\nDirect audience are security enthusiasts, professionals, reverse engineers, anyone wanting a quick scan on a suspicious file. According to me it can be used by both offensive and defensive teams.\n\nComparisons : \nIt is not very hard to find comparisons or inspirations in this area. I was using ghidra on almost a regular basis, so I just wanted something similar but minimal with power of dynamic analysis and ai overview, though it comes nowhere near the power of ghidra. A direct comparison would be [LiSa](https://github.com/danielpoliakov/lisa) by danielpoliakov which uses SystemTap instead of eBPF for tracing. \n\nCritisisms are welcome. Suggestions are highly appreciated. Thanks for checking this out.",
    "author": "neptunym",
    "timestamp": "2025-10-15T08:13:30",
    "url": "https://reddit.com/r/Python/comments/1o7dqre/nyxelf_an_unreliable_dynamic_analysis_toolkit/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6jivj",
    "title": "Python 3.15 Alpha Released",
    "content": "[https://docs.python.org/3.15/whatsnew/3.15.html](https://docs.python.org/3.15/whatsnew/3.15.html)\n\n  \nSummary ‚Äì Release highlights\n\n* [**PEP 799**](https://peps.python.org/pep-0799/):¬†[A dedicated profiling package for organizing Python profiling tools](https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-sampling-profiler)\n* [**PEP 686**](https://peps.python.org/pep-0686/):¬†[Python now uses UTF-8 as the default encoding](https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-utf8-default)\n* [**PEP 782**](https://peps.python.org/pep-0782/):¬†[A new PyBytesWriter C API to create a Python bytes object](https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-pep782)\n* [Improved error messages](https://docs.python.org/3.15/whatsnew/3.15.html#whatsnew315-improved-error-messages)\n\n",
    "author": "miabajic",
    "timestamp": "2025-10-14T09:01:22",
    "url": "https://reddit.com/r/Python/comments/1o6jivj/python_315_alpha_released/",
    "score": 188,
    "num_comments": 35,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6v4fb",
    "title": "I wrote a short tutorial on how to kill the GIL in Python 3.14",
    "content": "Hey friends, for those who have heard about the new free-threading build but haven't had a chance to try it out, I wrote this tutorial that comes with a benchmark: [https://www.neelsomaniblog.com/p/killing-the-gil-how-to-use-python](https://www.neelsomaniblog.com/p/killing-the-gil-how-to-use-python)\n\nFeel free to ask me any questions and appreciate any feedback!",
    "author": "nsomani",
    "timestamp": "2025-10-14T16:21:13",
    "url": "https://reddit.com/r/Python/comments/1o6v4fb/i_wrote_a_short_tutorial_on_how_to_kill_the_gil/",
    "score": 48,
    "num_comments": 12,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o7paue",
    "title": "Getting back into Python",
    "content": "I‚Äôm a perpetual Python beginner since I don‚Äôt have a chance to use it very often. Can anyone recommend any resources/ tutorials/ short courses for me to get up to speed fast? Thanks!",
    "author": "dedenorio",
    "timestamp": "2025-10-15T15:30:44",
    "url": "https://reddit.com/r/Python/comments/1o7paue/getting_back_into_python/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o74arl",
    "title": "I built a modern async Python SDK for Expo Push Notifications (with full type hints!)",
    "content": "I've been working with Expo push notifications in Python and got frustrated with the limitations of existing SDKs - no async support, limited type safety, and missing modern features. So I built \\*\\*async-expo-push-notifications\\*\\*.\n\n\n\n\\## What My Project Does\n\n\n\nA Python SDK for sending push notifications through Expo's push notification service. It provides both async and sync interfaces for sending notifications to mobile apps built with Expo/React Native. The library handles message validation, batching, error handling, and provides full type safety with Pydantic models.\n\n\n\n\\## Target Audience\n\n\n\n\\*\\*Production-ready\\*\\* for developers building:\n\n\\- FastAPI/Django/ETC backends that need async push notifications\n\n\\- Python servers communicating with Expo/React Native mobile apps\n\n\\- Applications requiring type-safe, testable notification systems\n\n\\- High-performance apps sending concurrent notifications\n\n\n\nRequires Python 3.8+ and works with any modern Python web framework.\n\n\n\n\\## Comparison\n\n\n\nCompared to the existing \\[expo-server-sdk-python\\](https://github.com/expo-community/expo-server-sdk-python):\n\n\n\n| Feature | async-expo-push-notifications | expo-server-sdk-python |\n\n|---------|------------------------------|------------------------|\n\n| Async/await support | ‚úÖ Full async | ‚ùå Sync only |\n\n| Type hints | ‚úÖ Complete | ‚ö†Ô∏è Partial |\n\n| Pydantic models | ‚úÖ Type-safe validation | ‚ùå Named tuples |\n\n| Dependency injection | ‚úÖ Testable | ‚ùå No |\n\n| Rich content (images) | ‚úÖ Supported | ‚ùå No |\n\n| Backward compatible | ‚úÖ Drop-in replacement | - |\n\n\n\nThe official SDK is great for synchronous use cases, but lacks modern Python features. This SDK provides the same API while adding async support, full type safety, and better testability - perfect for modern async Python applications.\n\n\n\n\\## Quick Example\n\n\n\n\\`\\`\\`python\n\nimport asyncio\n\nfrom exponent\\_server\\_sdk import AsyncPushClient, PushMessage\n\n\n\nasync def send\\_notification():\n\nasync with AsyncPushClient() as client:\n\nmessage = PushMessage(\n\nto=\"ExponentPushToken\\[xxxxxxxxxxxxxxxxxxxxxx\\]\",\n\ntitle=\"Hello\",\n\nbody=\"World!\",\n\ndata={\"extra\": \"data\"}\n\n)\n\nticket = await client.publish(message)\n\nticket.validate\\_response()\n\n\n\nasyncio.run(send\\_notification())\n\n\\`\\`\\`\n\n\n\n\\*\\*Installation:\\*\\*\n\n\\`\\`\\`bash\n\npip install async-expo-push-notifications\n\n\\`\\`\\`\n\n\n\nThe synchronous API still works exactly the same, so you can migrate gradually. All your existing code continues to work without changes.\n\n\n\n\\*\\*Why I built this:\\*\\*\n\nThe official community SDK is great but hasn't been updated with modern Python features. I wanted something that works seamlessly with async frameworks like FastAPI and provides the type safety that modern Python developers expect.\n\n\n\n\\*\\*Important note:\\*\\* This is an independent project and not officially maintained by Expo. It's a modern reimplementation with async support.\n\n\n\nGitHub: [https://github.com/tmdgusya/async-expo-notification-sdk](https://github.com/tmdgusya/async-expo-notification-sdk)\n\nPyPI: [https://pypi.org/project/async-expo-push-notifications/](https://pypi.org/project/async-expo-push-notifications/)\n\n\n\nWould love to hear your feedback and contributions are welcome! Let me know if you have any questions.\n\n",
    "author": "kr_roach",
    "timestamp": "2025-10-15T00:12:33",
    "url": "https://reddit.com/r/Python/comments/1o74arl/i_built_a_modern_async_python_sdk_for_expo_push/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6muzv",
    "title": "ButtonPad, a simple GUI framework built on tkinter",
    "content": "## What My Project Does\n\n\n\nInstall: pip install buttonpad\n\nTo view the included demo programs: python -m buttonpad\n\nPyPI page: https://pypi.org/project/buttonpad/\n\nGit repo: https://github.com/asweigart/buttonpad\n\nBlog post: https://inventwithpython.com/blog/buttonpad-introduction.html\n\n\n## Target Audience\n\n* Beginners who want to learn GUI programming without wrestling with verbose frameworks.\n\n* Experienced developers who want to crank out prototypes, internal tools, game ideas, or teaching demos fast.\n\n\n## Comparison\n\nI modeled them after the design of programmable stream deck or drum machine hardware. Lots of times when I'm making small programs, I'd like to create a desktop app that is just a resizable window of a bunch of buttons and text boxes, but I don't want to think too hard about how to put it together.",
    "author": "AlSweigart",
    "timestamp": "2025-10-14T11:02:09",
    "url": "https://reddit.com/r/Python/comments/1o6muzv/buttonpad_a_simple_gui_framework_built_on_tkinter/",
    "score": 18,
    "num_comments": 0,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o69cvi",
    "title": "I built JSONxplode a complex json flattener",
    "content": "I built this tool in python and I hope it will help the community. \n\nThis code flattens deep, messy and complex json data into a simple tabular form without the need of providing a schema.\n\nso all you need to do is: from jsonxplode import flatten flattened_json = flatten(messy_json_data)\n\nonce this code is finished with the json file none of the object or arrays will be left un packed.\n\nyou can access it by doing: pip install jsonxplode\n\ncode and proper documentation can be found at:\n\nhttps://github.com/ThanatosDrive/jsonxplode\n\nhttps://pypi.org/project/jsonxplode/\n\n\n\nin the post i shared at the data engineering sub reddit these were some questions and the answers i provided to them:\n\nwhy i built this code? because none of the current json flatteners handle properly deep, messy and complex json files without the need of having to read into the json file and define its schema.\n\nhow does it deal with some edge case scenarios of eg out of scope duplicate keys? there is a column key counter that increments the column name if it notices that in a row there is 2 of the same columns.\n\nhow does it deal with empty values does it do a none or a blank string? data is returned as a list of dictionaries (an array of objects) and if a key appears in one dictionary but not the other one then it will be present in the first one but not the second one.\n\nif this is a real pain point why is there no bigger conversations about the issue this code fixes? people are talking about it but mostly everyone accepted the issue as something that comes with the job.\n\nhttps://www.reddit.com/r/dataengineering/s/FzZa7pfDYG\n\n\nI hope that this tool will be useful and I look forward to hearing how you're using it in your projects!",
    "author": "Thanatos-Drive",
    "timestamp": "2025-10-14T00:53:41",
    "url": "https://reddit.com/r/Python/comments/1o69cvi/i_built_jsonxplode_a_complex_json_flattener/",
    "score": 48,
    "num_comments": 20,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6agbj",
    "title": "How to Design a Searchable PDF Database Archived on Verbatim 128‚ÄØGB Discs?",
    "content": "Good morning everyone, \nI hope you‚Äôre doing well. \n\nHow would you design and index a searchable database of 200,000 PDF books stored on Verbatim 128 GB optical discs?\n\nWhich software tools or programs should be integrated to manage and query the database prior to disc burning?\nWhat data structure and search architecture would you recommend for efficient offline retrieval?\n\nThe objective is to ensure that, within 20 years, the entire archive can be accessed and searched locally using a standard PC with disc reader, without any internet connectivity.\n",
    "author": "Atronem",
    "timestamp": "2025-10-14T02:05:26",
    "url": "https://reddit.com/r/Python/comments/1o6agbj/how_to_design_a_searchable_pdf_database_archived/",
    "score": 36,
    "num_comments": 15,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6mv11",
    "title": "[Project] Plugboard - A framework for complex process modelling",
    "content": "Hi everyone\n\nI've been helping to build [plugboard](https://github.com/plugboard-dev/plugboard) - a framework for modelling complex processes.\n\n## What is it for?\n\nWe originally started out helping data scientists to build models of industrial processes where there are lots of stateful, interconnected components. Think of a digital twin for a mining process, or a simulation of multiple steps in a factory production line.\n\nPlugboard lets you define each component of the model as a Python class and then takes care of the flow of data between the components as you run your model. It really shines when you have many components and lots of connections between them (including loops and branches). \n\nWe've since enhanced it with:\n\n* Support for event-based models;\n* Built-in optimisation, so you can fine-tune your model to achieve/optimise a specific output;\n* Integration with [Ray](https://github.com/ray-project/ray) for running computationally intensive models in a distributed environment.\n\n## Target audience\n\nAnyone who is interested in modelling complex systems, processes, and digital twins. Particularly if you've faced the challenges of running data-intensive models in Python, and wished for a framework to make it easier. Would love to hear from anyone with experience in these areas.\n\n## Links\n\n* Repo: https://github.com/plugboard-dev/plugboard\n* Documentation: https://docs.plugboard.dev/latest/\n* Tutorials: https://docs.plugboard.dev/latest/examples/tutorials/hello-world/\n* Usage examples: https://docs.plugboard.dev/latest/examples/demos/fundamentals/001_simple_model/simple-model/\n\n## Key Features\n\n- **Reusable classes** containing the core framework, which you can extend to define your own model logic;\n- Support for different simulation paradigms: **discrete time** and **event based**.\n- **YAML model specification** format for saving model definitions, allowing you to run the same model locally or in cloud infrastructure;\n- A **command line interface** for executing models;\n- Built to handle the **data intensive simulation** requirements of industrial process applications;\n- Modern implementation with **Python 3.12 and above** based around **asyncio** with complete type annotation coverage;\n- Built-in integrations for **loading/saving data** from cloud storage and SQL databases;\n- **Detailed logging** of component inputs, outputs and state for monitoring and process mining or surrogate modelling use-cases.\n",
    "author": "top-dogs",
    "timestamp": "2025-10-14T11:02:11",
    "url": "https://reddit.com/r/Python/comments/1o6mv11/project_plugboard_a_framework_for_complex_process/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6rtc1",
    "title": "An open source access logs analytics script to block Bot attacks",
    "content": "We built a small Python project for web server access logs analyzing to classify and dynamically block bad bots, such as L7 (application-level) DDoS bots, web scrappers and so on.\n\nWe'll be happy to gather initial feedback on usability and features, especially from people having good or bad experience wit bots.\n\nThe project is available at [Github](https://github.com/tempesta-tech/webshield/) and has a [wiki page](https://tempesta-tech.com/knowledge-base/Bot-Protection/)\n\n\n**Requirements**\n\nThe analyzer relies on 3 Tempesta FW specific features which you still can get with other HTTP servers or accelerators:\n\n1.  [JA5 client fingerprinting](https://tempesta-tech.com/knowledge-base/Traffic-Filtering-by-Fingerprints/). This is a HTTP and TLS layers fingerprinting, similar to [JA4](https://blog.foxio.io/ja4%2B-network-fingerprinting) and JA3 fingerprints. The last is also available in [Envoy](https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/listener/tls\\_inspector/v3/tls\\_inspector.proto.html) or [Nginx module](https://github.com/fooinha/nginx-ssl-ja3), so check the documentation for your web server\n2. Access logs are directly written to Clickhouse analytics database, which can cunsume large data batches and quickly run analytic queries. For other web proxies beside Tempesta FW, you typically need to build a custom pipeline to load access logs into Clickhouse. Such pipelines aren't so rare though.\n3. Abbility to block web clients by IP or JA5 hashes. IP blocking is probably available in any HTTP proxy.\n\n\n\n**How does it work**\n\nThis is a daemon, which\n\n1. Learns normal traffic profiles: means and standard deviations for client requests per second, error responses, bytes per second and so on. Also it remembers client IPs and fingerprints.\n2. If it sees a spike in [z-score](https://en.wikipedia.org/wiki/Standard\\_score) for traffic characteristics or can be triggered manually. Next, it goes in data model search mode\n3. For example, the first model could be top 100 JA5 HTTP hashes, which produce the most error responses per second (typical for password crackers). Or it could be top 1000 IP addresses generating the most requests per second (L7 DDoS). Next, this model is going to be verified\n4. The daemon repeats the query, but for some time, long enough history, in the past to see if in the past we saw a hige fraction of clients in both the query results. If yes, then the model is bad and we got to previous step to try another one. If not, then we (likely) has found the representative query.\n5. Transfer the IP addresses or JA5 hashes from the query results into the web proxy blocking configuration and reload the proxy configuration (on-the-fly).\n",
    "author": "krizhanovsky",
    "timestamp": "2025-10-14T14:06:49",
    "url": "https://reddit.com/r/Python/comments/1o6rtc1/an_open_source_access_logs_analytics_script_to/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o66tho",
    "title": "Pyrefly eats CPU like nobodies business.",
    "content": "So I recently tried out the pyrefly and the ty typecheckers/LSPs in my project for ML. While ty wasn't as useful with it's errors and imports, pyrefly was great in that department. Only problem with the latter was that it sent CPU use to near 100% the whole time it ran.   \n  \nThis was worse than even rust-analyzer, notorious for being a heavy-weight tool, which only uses a ton of CPU on startup but works on low CPU throughout but using a ton of RAM.\n\nIs there some configuration for pyrefly I was missing or is this a bug and if it's the latter should I report it?   \n  \nOr even worse, is this intended behavior? If so, pyrefly will remain unusable to anyone without a really beefy computer making it completely useless for me. Hopefully not thought, cause I can't have an LSP using over 90% CPU while it runs in background running on my laptop.",
    "author": "Different-Ad-8707",
    "timestamp": "2025-10-13T22:13:55",
    "url": "https://reddit.com/r/Python/comments/1o66tho/pyrefly_eats_cpu_like_nobodies_business/",
    "score": 35,
    "num_comments": 25,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o66c21",
    "title": "[Beta] Django + PostgreSQL Anonymizer - DB-level masking for realistic dev/test datasets",
    "content": "&gt;**TL;DR**  \n`django-postgres-anonymizer` lets you mask PII at the **database layer** and create **sanitized dumps** for dev/CI‚Äîno app-code rewrites.\n\n&gt;GitHub: [https://github.com/CuriousLearner/django-postgres-anonymizer](https://github.com/CuriousLearner/django-postgres-anonymizer)\n\n&gt;Docs: [https://django-postgres-anonymizer.readthedocs.io/](https://django-postgres-anonymizer.readthedocs.io/)\n\n&gt;Example: `/example_project` (2-min try)\n\n# What My Project Does\n\nA thin Django integration over the **PostgreSQL anon** extension that lets you declare **DB-level masking policies** and then (a) run queries under a masked role or (b) produce anonymized dumps. Because policies live in Postgres, they apply to *any* client (ORM, psql, ETL).\n\n**Key bits (beta):** management commands like `anon_init`/`anon_dump`, `AnonRoleMiddleware` for automatic role switching, `anonymized_data` context manager, `use_anonymized_data` decorator, admin helpers, and presets for common PII. Requires Postgres with the anonymizer extension enabled.\n\n**Quickstart**\n\n    pip install django-postgres-anonymizer==0.1.0b1\n    # add app + settings, then:\n    python manage.py anon_init\n\n(You‚Äôll need a Postgres where you can install/enable the anonymizer extension before using the Django layer.)\n\n# Target Audience\n\n* **Django teams on Postgres** who need **production-like datasets** for local dev, CI, or ephemeral review apps - without shipping live PII.\n* Orgs that prefer **DB-enforced masking** (central policy, fewer ‚Äúmissed spots‚Äù in app code).\n* Current status: **beta (**`v0.1.0b1`**) -** great for dev/test pipelines; evaluate carefully before critical prod paths.\n\nTypical workflows: share realistic fixtures within the team/CI, seed preview environments with masked data, and reproduce bugs that only surface with prod-like distributions.\n\n# Comparison (how it differs)\n\n* **vs Faker/synthetic fixtures:** Faker creates *plausible but synthetic* data; distributions often drift. DB-level masking preserves **real distributions and relationships** while removing PII.\n* **vs app-layer masking (serializers/views):** easy to miss code paths. DB policies apply across **ORM, psql, ETL**, etc., reducing leakage risk.\n* **vs using the extension directly:** this package adds **Django-friendly commands/middleware/decorators/presets** so teams don‚Äôt hand-roll plumbing each time.\n\n**Status &amp; Asks**  \nThis is **beta**‚ÄîI‚Äôd love feedback on:\n\n* Missing PII recipes\n* Managed-provider quirks (does your provider expose the extension?)\n* DX rough edges in admin/tests/CI\n\nIf it‚Äôs useful, a ‚≠ê on the repo and comments here really help prioritize the roadmap. üôè",
    "author": "curiousyellowjacket",
    "timestamp": "2025-10-13T21:46:13",
    "url": "https://reddit.com/r/Python/comments/1o66c21/beta_django_postgresql_anonymizer_dblevel_masking/",
    "score": 15,
    "num_comments": 4,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6bxx5",
    "title": "[Project] Antback - A Tiny, Transparent Backtesting Library",
    "content": "Hey everyone,\n\nI‚Äôve built a lightweight backtesting library called `Antback`\n\n**What my project does**\n\nAntback is a small, practical tool for backtesting trading ideas. It was primarily designed for rotational strategies, calendar effects, or other situations where a vectorized approach is difficult or impossible. It‚Äôs built to be clear, explicit, and easy to use with any kind of data. The README has some documentation, but the [examples](https://github.com/ts-kontakt/antback/tree/main/examples) are the best place to start: \n\n**Target audience**\n\nAntback is for anyone who wants to experiment with different investment strategies, inspect each transaction in detail, or compare results with other libraries.\n\n**Comparison**\n\nUnlike many backtesting frameworks that rely on an inheritance-based approach like `class SmaCross(Strategy)` or hide logic behind layers of abstraction, Antback takes a more explicit, function-driven design. It uses efficient stateful helper functions and data containers instead of complex class hierarchies. This makes it easier to understand what‚Äôs happening at each step. Antback also produces interactive HTML or XLSX reports, so you can clearly filter and inspect every trade.\n\nRepo: [https://github.com/ts-kontakt/antback](https://github.com/ts-kontakt/antback)\n\n\n\n",
    "author": "No_Pineapple449",
    "timestamp": "2025-10-14T03:35:49",
    "url": "https://reddit.com/r/Python/comments/1o6bxx5/project_antback_a_tiny_transparent_backtesting/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6arym",
    "title": "extend operation of list is threading safe in no-gil version??",
    "content": "I found a code piece about web spider using 3.14 free threading,but `all_stories` is no lock between mutli thread operate, is the extend implement threading safe?\n\nraw link is [https://py-free-threading.github.io/examples/asyncio/](https://py-free-threading.github.io/examples/asyncio/)\n\n    async def worker(queue: Queue, all_stories: list) -&gt; None:\n        async with aiohttp.ClientSession() as session:\n            while True:\n                async with asyncio.TaskGroup() as tg:\n                    try:\n                        page = queue.get(block=False)\n                    except Empty:\n                        break\n                    html = await fetch(session, page)\n                    stories = parse_stories(html)\n                    if not stories:\n                        break\n                    # for story in stories:\n                    #     tg.create_task(fetch_story_with_comments(session, story))\n                all_stories.extend(stories)",
    "author": "LoVeF23",
    "timestamp": "2025-10-14T02:26:18",
    "url": "https://reddit.com/r/Python/comments/1o6arym/extend_operation_of_list_is_threading_safe_in/",
    "score": 3,
    "num_comments": 15,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6agzo",
    "title": "Exercise to Build the Right Mental Model for Python Data",
    "content": "An exercise to build the right mental model for Python data. The ‚ÄúSolution‚Äù link below uses memory_graph to visualize execution and reveals what‚Äôs actually happening.\n\nWhat is the output of this Python program?\n\n     import copy\n     \n     def fun(c1, c2, c3, c4):\n         c1[0].append(1)\n         c2[0].append(2)\n         c3[0].append(3)\n         c4[0].append(4)\n     \n     mylist = [[]]\n     c1 = mylist\n     c2 = mylist.copy()\n     c3 = copy.copy(mylist)\n     c4 = copy.deepcopy(mylist)\n     fun(c1, c2, c3, c4)\n     \n     print(mylist)\n     # --- possible answers ---\n     # A) [[1]]\n     # B) [[1, 2]]\n     # C) [[1, 2, 3]]\n     # D) [[1, 2, 3, 4]]\n\n- [Solution](https://memory-graph.com/#codeurl=https://raw.githubusercontent.com/bterwijn/memory_graph_videos/refs/heads/main/exercises/exercise8.py&amp;breakpoints=11&amp;continues=1&amp;play)\n- [Explanation](https://github.com/bterwijn/memory_graph?tab=readme-ov-file#python-data-model)\n- [More Exercises](https://www.reddit.com/r/Python_memory_graph/)",
    "author": "Sea-Ad7805",
    "timestamp": "2025-10-14T02:06:38",
    "url": "https://reddit.com/r/Python/comments/1o6agzo/exercise_to_build_the_right_mental_model_for/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6tx7x",
    "title": "[Hiring] A experienced python coder that is very experienced with discord bots/ocr extractions.",
    "content": "Looking for revisions on two bots I use right now. Forwarder &gt; OCR bot that extracts specific parts of text. If you‚Äôve got experience with discord bots, python, OCR APIs, and image processing, DM if you need a more descriptive explanation.",
    "author": "Which-Step-8623",
    "timestamp": "2025-10-14T15:30:23",
    "url": "https://reddit.com/r/Python/comments/1o6tx7x/hiring_a_experienced_python_coder_that_is_very/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6q53q",
    "title": "Improved projects",
    "content": "A Spotify premiere handler has already been made available soon on my website. A new version of Influent Package Maker will be created now with bundle support and an OS emulator type test installer, everything looks like Android + WSA Apps with information and software protection to provide security to the code. We will be working in C# for the animations since Python does not support it, now it will have a new look",
    "author": "Murky_Conference_894",
    "timestamp": "2025-10-14T13:04:32",
    "url": "https://reddit.com/r/Python/comments/1o6q53q/improved_projects/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.18,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5ro8i",
    "title": "ChanX: Type-Safe WebSocket Framework for Django and FastAPI",
    "content": "# What My Project Does\n\nChanX is a batteries-included WebSocket framework that works with both Django Channels and FastAPI. It eliminates the boilerplate and repetitive patterns in WebSocket development by providing:\n\n* Automatic message routing using Pydantic discriminated unions - no more if-else chains\n* Type safety with full mypy/pyright support and runtime Pydantic validation\n* Auto-generated AsyncAPI 3.0 documentation - like OpenAPI/Swagger but for WebSockets\n* Channel layer integration for broadcasting messages across servers with Redis\n* Event system to trigger WebSocket messages from anywhere in your application (HTTP views, Celery tasks, management commands)\n* Built-in authentication with Django REST framework permissions support\n* Comprehensive testing utilities for both frameworks\n* Structured logging with automatic request/response tracing\n\nThe same decorator-based API works for both Django Channels and FastAPI:\n\n    from typing import Literal\n    from chanx.messages.base import BaseMessage\n    from chanx.core.decorators import ws_handler, channel\n    from chanx.channels.websocket import AsyncJsonWebsocketConsumer  # Django\n    # from chanx.fast_channels.websocket import AsyncJsonWebsocketConsumer  # FastAPI\n    \n    class ChatMessage(BaseMessage):\n        action: Literal[\"chat\"] = \"chat\"\n        payload: str\n    \n    (name=\"chat\")\n    class ChatConsumer(AsyncJsonWebsocketConsumer):\n        groups = [\"chat_room\"]\n    \n        \n        async def handle_chat(self, msg: ChatMessage) -&gt; None:\n            await self.broadcast_message(\n                ChatNotification(payload=NotificationPayload(\n                    message=msg.payload,\n                    timestamp=datetime.now()\n                ))\n            )\n\n# Target Audience\n\nChanX is designed for production use and is ideal for:\n\n* Teams building real-time features who want consistent patterns and reduced code review overhead\n* Django projects wanting to eliminate WebSocket boilerplate while maintaining REST API-like consistency\n* FastAPI projects needing robust WebSocket capabilities (ChanX brings Django Channels' channel layers, broadcasting, and group management to FastAPI)\n* Type-safety advocates who want comprehensive static type checking for WebSocket development\n* API-first teams who need automatic documentation generation\n\nBuilt from years of real-world WebSocket development experience, ChanX provides battle-tested patterns used in production environments. It has:\n\n* Comprehensive test coverage with pytest\n* Full type checking with mypy and pyright\n* Complete documentation with high interrogate coverage\n* Active maintenance and support\n\n# Comparison\n\nvs. Raw Django Channels:\n\n* ChanX adds automatic routing via decorators (vs. manual if-else chains)\n* Type-safe message validation with Pydantic (vs. manual dict checking)\n* Auto-generated AsyncAPI docs (vs. manual documentation)\n* Enforced patterns for team consistency\n\nvs. Raw FastAPI WebSockets:\n\n* ChanX adds channel layers for broadcasting (FastAPI has none natively)\n* Group management for multi-user features\n* Event system to trigger messages from anywhere\n* Same decorator patterns as Django Channels\n\nvs. Broadcaster:\n\n* ChanX provides full WebSocket consumer abstraction, not just pub/sub\n* Type-safe message handling with automatic routing\n* AsyncAPI documentation generation\n* Testing utilities included\n\nvs. Socket.IO:\n\n* Native Python/ASGI implementation (no Node.js required)\n* Integrates directly with Django/FastAPI ecosystems\n* Type safety with Python type hints\n* Leverages existing Django Channels or FastAPI infrastructure\n\nDetailed comparison: [https://chanx.readthedocs.io/en/latest/comparison.html](https://chanx.readthedocs.io/en/latest/comparison.html)\n\n# Tutorials\n\nI've created comprehensive hands-on tutorials for both frameworks:\n\nDjango Tutorial: [https://chanx.readthedocs.io/en/latest/tutorial-django/prerequisites.html](https://chanx.readthedocs.io/en/latest/tutorial-django/prerequisites.html)\n\n* Real-time chat with broadcasting\n* AI assistant with streaming responses\n* Notification system\n* Background tasks with WebSocket notifications\n* Complete integration tests\n\nFastAPI Tutorial: [https://chanx.readthedocs.io/en/latest/tutorial-fastapi/prerequisites.html](https://chanx.readthedocs.io/en/latest/tutorial-fastapi/prerequisites.html)\n\n* Echo WebSocket with system messages\n* Real-time chat rooms with channel layers\n* ARQ background jobs with WebSocket updates\n* Multi-layer architecture\n* Comprehensive testing\n\nBoth use Git repositories with checkpoints so you can start anywhere or compare implementations.\n\n# Installation\n\n    # For Django\n    pip install \"chanx[channels]\"\n    \n    # For FastAPI\n    pip install \"chanx[fast_channels]\"\n\n# Links\n\n* GitHub: [https://github.com/huynguyengl99/chanx](https://github.com/huynguyengl99/chanx)\n* Documentation: [https://chanx.readthedocs.io/](https://chanx.readthedocs.io/)\n* PyPI: [https://pypi.org/project/chanx/](https://pypi.org/project/chanx/)\n\nI'd love to hear feedback or answer questions about WebSocket development in Python.",
    "author": "huygl99",
    "timestamp": "2025-10-13T11:19:45",
    "url": "https://reddit.com/r/Python/comments/1o5ro8i/chanx_typesafe_websocket_framework_for_django_and/",
    "score": 17,
    "num_comments": 9,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5ymks",
    "title": "Let's Build a Quant Trading Strategy: Part 1 - ML Model in PyTorch",
    "content": "I created a series where we build a quant trading strategy in Python using PyTorch and polars.\n\nhttps://youtu.be/iWSDY8_5N3U?si=NkFjg9B1sjPXNwKc",
    "author": "memlabs",
    "timestamp": "2025-10-13T15:39:55",
    "url": "https://reddit.com/r/Python/comments/1o5ymks/lets_build_a_quant_trading_strategy_part_1_ml/",
    "score": 7,
    "num_comments": 6,
    "upvote_ratio": 0.65,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5vt9d",
    "title": "Jinx: a toy interpreter for the J programming language",
    "content": "[https://github.com/ajcr/jinx](https://github.com/ajcr/jinx)\n\n# What My Project Does\n\nI wrote this toy interpreter for a chunk of the¬†[J programming language](https://www.jsoftware.com/#/README)¬†(an array programming language) using NumPy as the array engine.\n\nMy goal was to understand J a bit better. J was an influence on NumPy, but is markedly different in how the user is able to build and control the application of functions over a multidimensional arrays (you control the¬†*rank*¬†of the method you're applying, you don't specify axes or think about broadcasting).\n\nJ has a¬†[large set of primitives](https://code.jsoftware.com/wiki/NuVoc)¬†that operate on arrays, or else produce new objects that operate on arrays. It can look confusing at first. For example:\n\n    +/ % #\n\nare three distinct verbs (think: function) that, when arranged in this way, create a new verb that find the arithmetic mean of an array. Similarly:\n\n    1&amp;|.&amp;.#:\n\ncreates a verb that solves the¬†[Josephus problem](https://code.jsoftware.com/wiki/Essays/Josephus_Problem).\n\nDespite looking unusual, parsing J code and executing it it is actually relatively straightforward. There is no complicated grammar or precedence rules. In my project:\n\n* Tokenization (breaking the code into words) is done in¬†[word\\_formation.py](https://github.com/ajcr/jinx/blob/main/src/jinx/word_formation.py)¬†(using a transition table and single scan from left-to-right)\n* Spelling (recognising these words as parts of J) is done in¬†[word\\_spelling.py](https://github.com/ajcr/jinx/blob/main/src/jinx/word_spelling.py)¬†(just a few methods to detect what the words are, and parsing of numbers)\n* Evaluation (executing the code) is done in¬†[word\\_evaluation.py](https://github.com/ajcr/jinx/blob/main/src/jinx/word_evaluation.py)¬†(repeated use of`case`/¬†`match`¬†to check for 8 different patterns in a fragment of the code)\n\nMost of the complexity I found was in defining the different language primitives in terms of NumPy and Python and working out how to apply these primitives to multidimensional arrays of different shapes (see for example¬†[application.py](https://github.com/ajcr/jinx/blob/main/src/jinx/execution/numpy/application.py)¬†and¬†[verbs.py](https://github.com/ajcr/jinx/blob/main/src/jinx/execution/numpy/verbs.py)).\n\nThe main reference books I used were:\n\n1. [An Implementation of J](https://www.jsoftware.com/books/pdf/aioj.pdf)\n2. [J for C Programmers](https://www.jsoftware.com/help/jforc/contents.htm)\n\n# Target Audience\n\nAnyone interested in programming with arrays or tensors, or understanding how J and similar array languages can be implemented.\n\nMaybe you've used NumPy or PyTorch before and are interested in seeing a different approach to working with multidimensional arrays.\n\n# Comparison\n\nI'm not aware of any other full or partial implementations of J written in Python. A few other toy implementations exist in other languages, but they do not seem to implement as much of J as my project does.\n\nThe [official J source code](https://github.com/jsoftware/jsource) is here.",
    "author": "aajjccrr",
    "timestamp": "2025-10-13T13:49:46",
    "url": "https://reddit.com/r/Python/comments/1o5vt9d/jinx_a_toy_interpreter_for_the_j_programming/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5kwve",
    "title": "gRPC: Client side vs Server side load balancing, which one to choose?",
    "content": "Hello everyone,  \nMy setup:¬†Two FastAPI apps¬†calling gRPC ML¬†services (layout¬†analysis + table detection). Need to scale both the services.\n\nQuestion:¬†For¬†GPU-based ML inference¬†over gRPC, does NGINX load balancing significantly hurt¬†performance vs client-side load balancing?\n\nMain¬†concerns:\n\n* Losing¬†HTTP/2 multiplexing benefits\n* Extra¬†latency (though probably¬†negligible vs 2-5s processing time)\n* Need priority handling for time-critical clients\n\nCurrent¬†thinking:¬†NGINX seems¬†simpler operationally, but want to make sure I'm not shooting myself in the foot¬†performance-wise.\n\nExperience¬†with gRPC + NGINX? Client-side LB worth the complexity¬†for this use case?",
    "author": "Constant_Fun_5643",
    "timestamp": "2025-10-13T07:16:19",
    "url": "https://reddit.com/r/Python/comments/1o5kwve/grpc_client_side_vs_server_side_load_balancing/",
    "score": 18,
    "num_comments": 6,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5ofkq",
    "title": "Proxy parser / formatter for Python - proxyutils",
    "content": "Hey everyone!\n\nOne of my first struggles when building CLI tools for end-users in Python was that customers always had problems inputting proxies. They often struggled with the `scheme://user:pass@ip:port` format, so a few years ago I made a parser that could turn any user input into Python's proxy format with a one-liner.   \nAfter a long time of thinking about turning it into a library, I finally had time to publish it. Hope you find it helpful ‚Äî feedback and stars are appreciated :)\n\n# What My Project Does\n\nproxyutils parses any format of proxy into Python's niche proxy format with one-liner . It can also generate proxy extension files / folders for libraries Selenium.\n\n# Target Audience\n\nPeople who does scraping and automating with Python and uses proxies. It also concerns people who does such projects for end-users.\n\n# Comparison\n\nSadly, I didn't see any libraries that handles this task before. Generally proxy libraries in Python are focusing on collecting free proxies from various websites.\n\nIt worked excellently, and finally, I didn‚Äôt need to handle complaints about my clients‚Äô proxy providers and their odd proxy formats\n\n\n\n[https://github.com/meliksahbozkurt/proxyutils](https://github.com/meliksahbozkurt/proxyutils)",
    "author": "heyoneminute",
    "timestamp": "2025-10-13T09:25:16",
    "url": "https://reddit.com/r/Python/comments/1o5ofkq/proxy_parser_formatter_for_python_proxyutils/",
    "score": 10,
    "num_comments": 2,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o6bpjq",
    "title": "White pop up in Terminal",
    "content": "I'm getting this really weird white box overlay in my console terminal. It contains all the text i enter into the terminal and also adds a weird text overlay to anything written in the terminal. Would really like help, shame i cannot upload a photo of this issue.\n\nI'm trying my best to describe it put cannot find anything online with people having a similar issue.\n\nedit: I have attached a photo of the image.\n\n[https://imgur.com/a/q1bfASa](https://imgur.com/a/q1bfASa)",
    "author": "Background-Shape9756",
    "timestamp": "2025-10-14T03:22:46",
    "url": "https://reddit.com/r/Python/comments/1o6bpjq/white_pop_up_in_terminal/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.13,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o60ghk",
    "title": "Tuesday Daily Thread: Advanced questions",
    "content": "# Weekly Wednesday Thread: Advanced Questions üêç\n\nDive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.\n\n## How it Works:\n\n1. **Ask Away**: Post your advanced Python questions here.\n2. **Expert Insights**: Get answers from experienced developers.\n3. **Resource Pool**: Share or discover tutorials, articles, and tips.\n\n## Guidelines:\n\n* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.\n* Questions that are not advanced may be removed and redirected to the appropriate thread.\n\n## Recommended Resources:\n\n* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.\n\n## Example Questions:\n\n1. **How can you implement a custom memory allocator in Python?**\n2. **What are the best practices for optimizing Cython code for heavy numerical computations?**\n3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**\n4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**\n5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**\n6. **What are some advanced use-cases for Python's decorators?**\n7. **How can you achieve real-time data streaming in Python with WebSockets?**\n8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**\n9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**\n10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**\n\nLet's deepen our Python knowledge together. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-13T17:00:30",
    "url": "https://reddit.com/r/Python/comments/1o60ghk/tuesday_daily_thread_advanced_questions/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5jkb9",
    "title": "Parsegument! - Argument Parsing and function routing",
    "content": "Project Source code: [https://github.com/RyanStudioo/Parsegument](https://github.com/RyanStudioo/Parsegument)\n\nProject Docs: [https://www.ryanstudio.dev/docs/parsegument/](https://www.ryanstudio.dev/docs/parsegument/)\n\n# What My Project Does\nParsegument allows you to easily define Command structures with Commands and CommandGroups.\nParsegument also automatically parses arguments, converts them to your desired type, then executes functions automatically, all with just one method call and a string.\n\n# Target Audience\nParsegument is targetted for people who would like to simplify making CLIs. I started this project as I was annoyed at having to use lines and lines of switch case statements for another project I was working on\n\n# Comparison\nCompared to python's built in argparse, Parsegument has a more intuitive syntax, and makes it more convenient to route and execute functions. \n\nThis project is still super early in development, I aim to add other features like aliases, annotations, and more suggestions from you guys!",
    "author": "RyanStudioDev",
    "timestamp": "2025-10-13T06:21:51",
    "url": "https://reddit.com/r/Python/comments/1o5jkb9/parsegument_argument_parsing_and_function_routing/",
    "score": 7,
    "num_comments": 6,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4uyrv",
    "title": "Advice on logging libraries: Logfire, Loguru, or just Python's built-in logging?",
    "content": "Hey everyone,\n\nI‚Äôm exploring different logging options for my projects (fastapi backend with langgraph) and I‚Äôd love some input.\n\nSo far I‚Äôve looked at:\n\n* **Python‚Äôs built-in** `logging` module\n* **Loguru**\n* **Logfire**\n\nI‚Äôm mostly interested in:\n\n* Clean and beautiful output (readability really matters)\n* Ease of use / developer experience\n* Flexibility for future scaling (e.g., larger apps, integrations)\n\nHas anyone here done a serious comparison or has strong opinions on which one strikes the best balance?  \nIs there some hidden gem I should check out instead?\n\nThanks in advance!",
    "author": "Ranteck",
    "timestamp": "2025-10-12T10:22:27",
    "url": "https://reddit.com/r/Python/comments/1o4uyrv/advice_on_logging_libraries_logfire_loguru_or/",
    "score": 202,
    "num_comments": 78,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5gurz",
    "title": "Need advice on simulating real time bus movement and eta predictions",
    "content": "Hello Everyone,\n\nI'm currently studying in college and for semester project i have selected project which can simulate real time bus movement and can predict at what bus will arrive that the certain destination.\n\nWhat I have:\n\n1. Bus departure time from station\n2. Distance between each bus stop\n3. Bus stop map coordinates\n\nWhat I'm trying to achive:\n\n1. Simulating bus moving on real map\n2. Variable speeds, dwell times, traffic variation.\n3. Estimate arrival time per stop using distance and speed.\n4. Live dashboard predicting at what time will reach certain stop based upon traffic flow,speed\n\nHelp I need:\n\n1. How to simulate it on real map (showing bus is actually moving along the route)\n2. What are the best tools for this project\n3. How to model traffic flow\n\nThanks\n\n",
    "author": "Weird_Celebration651",
    "timestamp": "2025-10-13T04:14:16",
    "url": "https://reddit.com/r/Python/comments/1o5gurz/need_advice_on_simulating_real_time_bus_movement/",
    "score": 5,
    "num_comments": 12,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o654sz",
    "title": "Bluetooth beacon and raspberry Pi",
    "content": "I have a python coding project, but dont know how to code. We already have the code but cant solve an fsm issue for the bluetooth scanner we are working with is there any freelancer who can work on this and solve the issue. URGENT NEED!!!",
    "author": "LividStep1672",
    "timestamp": "2025-10-13T20:42:27",
    "url": "https://reddit.com/r/Python/comments/1o654sz/bluetooth_beacon_and_raspberry_pi/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5w6re",
    "title": "Web package documentation",
    "content": "Is it me or is web package documentation just terrible? Authlib, itsdangerous, oauthlib2client, google-auth-oauthlib, etc. They're all full of holes on what I'd consider pretty basic functionality. The authlib authors spent so much time formatting their little website to make it look pretty that they forgot to document how to create timed web tokens.",
    "author": "Over_Palpitation_658",
    "timestamp": "2025-10-13T14:03:28",
    "url": "https://reddit.com/r/Python/comments/1o5w6re/web_package_documentation/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.3,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5jej5",
    "title": "Erdos: data science open-source AI IDE",
    "content": "We're launching¬†**Erdos**, an¬†AI IDE for data science! ([https://www.lotas.ai/erdos](https://www.lotas.ai/erdos),¬†[https://github.com/lotas-ai/erdos](https://github.com/lotas-ai/erdos))\n\n# What My Project Does\n\nErdos is built for data science - it has:\n\n* An AI that searches, reads, and writes all common data science file formats including Jupyter notebooks, Python, R, and Quarto\n* Built-in Python and R consoles accessible to the user and AI\n* Single-click sign in to a secure, zero data retention backend; or users can bring their own keys\n* Plots pane with plots history organized by file and time\n* Help pane for Python and R documentation\n* Database pane for connecting to SQL and FTP databases and manipulating data\n* Environment pane for managing python environments and Python and R packages\n* AGPLv3 license\n\n# Target Audience\n\nData scientists at any level\n\n# Comparison\n\nOther AI IDEs are primarily built for software development and don't have the things data scientists need like efficient Jupyter notebook editing, plots, environment management, and database connections. We bring all these together and add an AI that understands them too.\n\nWould love feedback and questions!  \n",
    "author": "SigSeq",
    "timestamp": "2025-10-13T06:15:12",
    "url": "https://reddit.com/r/Python/comments/1o5jej5/erdos_data_science_opensource_ai_ide/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4jul0",
    "title": "Cronboard - A terminal-based dashboard for managing cron jobs",
    "content": "## What My Project Does\n\n**Cronboard** is a terminal-based application built with **Python** that lets you manage and schedule cron jobs both locally and on remote servers. It provides an interactive way to view, create, edit, and delete cron jobs, all from your terminal, without having to manually edit crontab files.\n\nPython powers the entire project: it runs the CLI interface, parses and validates cron expressions, manages SSH connections via `paramiko`, and formats job schedules in a human-readable way.\n\n## Target Audience\n\nCronboard is mainly aimed at **developers, sysadmins, and DevOps engineers** who work with cron jobs regularly and want a cleaner, more visual way to manage them.\n## Comparison\n\nUnlike tools such as `crontab -e` or GUI-based schedulers, Cronboard focuses on **terminal usability** and **clarity**. It gives immediate feedback when creating or editing jobs, translates cron expressions into plain English, and will soon support remote SSH-based management out of the box using ssh keys (for now, it supports remote ssh using hostname, username and password).\n\n## Features\n- Check existing cron jobs\n- Create cron jobs with validation and human-readable feedback\n- Pause and resume cron jobs\n- Edit existing cron jobs\n- Delete cron jobs\n- View formatted last and next run times\n- Connect to servers using SSH\n\nThe project is still in early development, so I‚Äôd really appreciate any feedback or suggestions!\n\n**GitHub Repository:** [github.com/antoniorodr/Cronboard](https://github.com/antoniorodr/Cronboard)",
    "author": "NorskJesus",
    "timestamp": "2025-10-12T01:34:40",
    "url": "https://reddit.com/r/Python/comments/1o4jul0/cronboard_a_terminalbased_dashboard_for_managing/",
    "score": 153,
    "num_comments": 29,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5p1hi",
    "title": "Cool project idea (master's degree final project)",
    "content": "Hi, guys.\n\nI wanted to ask for some project ideas in adition to my list.\n\nCurrently I was thinking about an app that makes text summarization and data analysis based on documents uploaded by the users (with the help of AI agents).\n\nMy second idea was to make an app that lets the users track their eating and workout routine and also suggest changes in their routine, calorie and protein intake recomandations and so on.\n\nWhat do you think? I would like to experiment with cool libraries such as TensorFlow or PyTorch because I've never used them and consider this a good opportunity.",
    "author": "ThisUsernam31sTaken",
    "timestamp": "2025-10-13T09:47:00",
    "url": "https://reddit.com/r/Python/comments/1o5p1hi/cool_project_idea_masters_degree_final_project/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o54sdj",
    "title": "Monday Daily Thread: Project ideas!",
    "content": "# Weekly Thread: Project Ideas üí°\n\nWelcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.\n\n## How it Works:\n\n1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.\n2. **Build &amp; Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.\n3. **Explore**: Looking for ideas? Check out Al Sweigart's [\"The Big Book of Small Python Projects\"](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.\n\n## Guidelines:\n\n* Clearly state the difficulty level.\n* Provide a brief description and, if possible, outline the tech stack.\n* Feel free to link to tutorials or resources that might help.\n\n# Example Submissions:\n\n## Project Idea: Chatbot\n\n**Difficulty**: Intermediate\n\n**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar \n\n**Description**: Create a chatbot that can answer FAQs for a website.\n\n**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)\n\n# Project Idea: Weather Dashboard\n\n**Difficulty**: Beginner\n\n**Tech Stack**: HTML, CSS, JavaScript, API\n\n**Description**: Build a dashboard that displays real-time weather information using a weather API.\n\n**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)\n\n## Project Idea: File Organizer\n\n**Difficulty**: Beginner\n\n**Tech Stack**: Python, File I/O\n\n**Description**: Create a script that organizes files in a directory into sub-folders based on file type.\n\n**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)\n\nLet's help each other grow. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-12T17:00:32",
    "url": "https://reddit.com/r/Python/comments/1o54sdj/monday_daily_thread_project_ideas/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4sswc",
    "title": "üöÄ Blinter The Linter - A Cross Platform Batch Script Linter",
    "content": "**Yes, it's 2025. Yes, people still write batch scripts. No, they shouldn't crash.**\n\n## What It Does\n‚úÖ **158 rules** across Error/Warning/Style/Security/Performance  \n‚úÖ **Catches the nasty stuff**: Command injection, path traversal, unsafe temp files  \n‚úÖ **Handles the weird stuff**: Variable expansion, FOR loops, multilevel escaping  \n‚úÖ **10MB+ files?** No problem. **Unicode?** Got it. **Thread-safe?** Always.\n\n## Get It Now\n```bash\npip install Blinter\n```\nOr grab the standalone `.exe` from [GitHub Releases](https://github.com/tboy1337/Blinter/releases/latest)\n\n## One Command\n```bash\npython -m blinter script.bat\n```\n\nThat's it. No config needed. No ceremony. Just point it at your `.bat` or `.cmd` files.\n\n---\n\n**The first professional-grade linter for Windows batch files.**  \nBecause your automation scripts shouldn't be held together with duct tape.\n\n[üì¶ PyPI](https://pypi.org/project/Blinter/) ‚Ä¢ [‚öôÔ∏è GitHub](https://github.com/tboy1337/Blinter)\n\nWhat My Project Does\nA cross platform linter for batch scripts.\n\nTarget Audience\nDevelopers, primarily Windows based.\n\nComparison\nThere is no comparison, it's the only batch linter so theres nothing to compare it to.",
    "author": "Ok_Bottle8789",
    "timestamp": "2025-10-12T08:59:50",
    "url": "https://reddit.com/r/Python/comments/1o4sswc/blinter_the_linter_a_cross_platform_batch_script/",
    "score": 9,
    "num_comments": 3,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5oeo6",
    "title": "Guess The Output",
    "content": "matrix = \\[\\[1, 2, 3\\], \\[4, 5, 6\\], \\[7, 8, 9\\]\\]\n\nprint(matrix\\[1\\]\\[2\\])\n\n  \nWhat is the answer to this nested list? how do you guys learn faster?",
    "author": "CryBright2629",
    "timestamp": "2025-10-13T09:24:22",
    "url": "https://reddit.com/r/Python/comments/1o5oeo6/guess_the_output/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4q4vt",
    "title": "rovr v0.4.0: an update to the modern terminal file explorer",
    "content": "source code: [https://github.com/nspc911/rovr](https://github.com/nspc911/rovr)\n\nwhat my project does:\n\n* it's a file manager in the terminal, made with the textual framework\n\ncomparison:\n\n* as a python project, it cannot compete in performance with yazi at all, nor can it compete with an ncurses-focused ranger. superfile is also catching up, with its async-based preview that was just released.\n* the main point of rovr was to make it a nice experience in the terminal, and also to have touch support, something that lacked, or just felt weird, when using other file explorers.\n\nhey everyone, this follow-up on [https://www.reddit.com/r/Python/comments/1mx7zzj/rovr\\_a\\_modern\\_customizable\\_and\\_aesthetically/](https://www.reddit.com/r/Python/comments/1mx7zzj/rovr_a_modern_customizable_and_aesthetically/) that I released about a month ago, and during the month, there have been quite a lot of changes! A shortcut list was added in #71 that can be spawned with `?`, so if you are confused about any commands, just press the question mark! You can also search for any keybinds if necessary. rovr also integrates with [fd](https://github.com/sharkdp/fd), so you can simply enable the `finder` plugin and press `f` to start searching! yazi/spf style `--chooser-file` flag has also been added. An extra flag `--cwd-file` Also exists to allow you to grab the file if necessary (I'm planning to remove cd on quit to favour this instead) cases where opening a file results in a ui overwrite have also been resolved, and a lot more bugfixes!\n\nI would like to hear your opinion on how this can be improved. So far, the things that need to be done are a PDF preview, a config specifying flag, non-case-sensitivity of the rename operation and a bunch more. For those interested, the next milestone is also up for [v0.5.0](https://github.com/NSPC911/rovr/milestone/5) !",
    "author": "NotSoProGamerR",
    "timestamp": "2025-10-12T07:13:42",
    "url": "https://reddit.com/r/Python/comments/1o4q4vt/rovr_v040_an_update_to_the_modern_terminal_file/",
    "score": 13,
    "num_comments": 0,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5i5rg",
    "title": "What is the best Python learning course?",
    "content": "\n\nI have been searching for days for the best course that can qualify me to learn back-end and machine learning.I need recommendations based on experience.\nEdit :\nFor your information, I do not have a large background, so I am distracted by the large amount of content on YouTube. ",
    "author": "Soft-Razzmatazz-9385",
    "timestamp": "2025-10-13T05:20:46",
    "url": "https://reddit.com/r/Python/comments/1o5i5rg/what_is_the_best_python_learning_course/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4oozb",
    "title": "My first medium blog on GIL",
    "content": "Hi everyone, today I tried my first attempt at writing a tech blog on GIL basics like what is it, why it is needed as recent 3.14 gil removal created a lot of buzz around it. Please give it a read. Only a 5 min read. Please suggest if anything wrong or any improvements needed.\n\n[**GIL in Python: The Lock That Makes and Breaks It**](https://medium.com/@randomedev/gil-in-python-the-lock-that-makes-and-breaks-it-cbc87c30cb17)\n\nPS: I wrote it by myself based on my understanding. Only used llm as proof readers so it may appear unpolished here and there.",
    "author": "suntzuhere",
    "timestamp": "2025-10-12T06:11:10",
    "url": "https://reddit.com/r/Python/comments/1o4oozb/my_first_medium_blog_on_gil/",
    "score": 12,
    "num_comments": 5,
    "upvote_ratio": 0.61,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4p7rj",
    "title": "Comet 3I/Atlas - Some calculations",
    "content": "Hey everyone,\n\n  \nhave you heard about Comet Atlas? The interstellar visitor? If yes: well maybe you have also heard about weird claims of the comet being an interstellar artificial visitor. Because of its movement and its shape.\n\nHmm... weird claims indeed.\n\nSo I am a astrophysicsts who works on asteroids, comet, cosmic dust. You name it; the small universe stuff.\n\nAnd I just created 2 small Python scripts regarding its hyperbolic movement, and regarding the \"cylindric shape\" (that is indeed an artifact of how certain cameras in space are tracking stars and not comets).\n\nIf you like, take a look at the code here:\n\n[https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/CompressedCosmos/CompressedCosmos\\_Interstellar\\_Comets.ipynb](https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/CompressedCosmos/CompressedCosmos_Interstellar_Comets.ipynb)\n\n[https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/CompressedCosmos/CompressedCosmos\\_CometMovement.ipynb](https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/CompressedCosmos/CompressedCosmos_CometMovement.ipynb)\n\nAnd the corresponding short videos:\n\n[https://youtu.be/zaOoZ7WL9B0](https://youtu.be/zaOoZ7WL9B0)\n\n[https://youtu.be/Z\\_-J8jZQIHE](https://youtu.be/Z_-J8jZQIHE)\n\nIf you have heard of further weird claims, please let me know. It is kinda fun to catch these claims and use Python to \"debunk\" it. Well... people who \"believe\" in certain things won't belive me anyway, but I do it for fun.",
    "author": "MrAstroThomas",
    "timestamp": "2025-10-12T06:33:56",
    "url": "https://reddit.com/r/Python/comments/1o4p7rj/comet_3iatlas_some_calculations/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o5lsbv",
    "title": "The Key Python 3.14 Updates To Make Your Coding Easier, Faster, and Better",
    "content": "Finally, the Python 3.14 was released. \n\nIt catched so much attention,given that Python is the de facto ruling language now.\n\nI tried it for a few days and summarised the top 7 most useful updates [here](https://medium.com/techtofreedom/7-key-python-3-14-updates-to-make-your-coding-easier-faster-and-better-17ace5791d09?sk=956f8f3ce66db7feca6f448f141d2cbc).\n\nWhat do you think?",
    "author": "wyhjsbyb",
    "timestamp": "2025-10-13T07:49:38",
    "url": "https://reddit.com/r/Python/comments/1o5lsbv/the_key_python_314_updates_to_make_your_coding/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3voso",
    "title": "I made a game that is teaching you Python! :) After more than three years, I finally released it!",
    "content": "It's called The Farmer Was Replaced\n\nProgram and optimize a drone to automate a farm and watch it do the work for you. Collect resources to unlock better technology and become the most efficient farmer in the world. Improve your problem solving and coding skills.\n\nUnlike most programming games the game isn't divided into distinct levels that you have to complete but features a continuous progression.\n\nFarming earns you resources which can be spent to unlock new technology.\n\nProgramming is done in a simple language similar to Python. The beginning of the game is designed to teach you all the basic programming concepts you will need by introducing them one at a time.\n\nWhile it introduces everything that is relevant, it won't hold your hand when it comes to solving the various tasks in the game. You will have to figure those out for yourself, and that can be very challenging if you have never programmed before.\n\nIf you are an experienced programmer, you should be able to get through the early game very quickly and move on to the more complex tasks of the later game, which should still provide interesting challenges.\n\nAlthough the programming language isn't exactly Python, it's similar enough that Python IntelliSense works well with it. All code is stored in .py files and can optionally be edited using external code editors like VS Code. When the \"File Watcher\" setting is enabled, the game automatically detects external changes.\n\nYou can find it here:¬†[https://store.steampowered.com/app/2060160/The\\_Farmer\\_Was\\_Replaced/](https://store.steampowered.com/app/2060160?utm_source=RD)",
    "author": "AdSad9018",
    "timestamp": "2025-10-11T06:29:46",
    "url": "https://reddit.com/r/Python/comments/1o3voso/i_made_a_game_that_is_teaching_you_python_after/",
    "score": 454,
    "num_comments": 71,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4gvj5",
    "title": "I built dataspot to find fraud patterns automatically [Open Source]",
    "content": "\nAfter years detecting fraud, I noticed every fraud has a data concentration somewhere.\n\nBuilt a tool to find them:\n\n```python\npip install dataspot\n\nfrom dataspot import Dataspot\n\nds = Dataspot()\nhotspots = ds.find(your_data)\n```\n\n**What My Project Does**\nAutomatically finds data concentrations that indicate fraud, bot networks, or coordinated attacks. No manual thresholds needed.\n\n**Target Audience**\nFraud analysts, data scientists, security teams working with transactional or behavioral data.\n\n**Comparison**\nUnlike scikit-learn's anomaly detection (needs feature engineering) or PyOD (requires ML expertise), dataspot works directly on raw data structures and finds patterns automatically.\n\nFull story: \nhttps://3l1070r.dev/en/2025/01/24/building-dataspot.html\n\nUsed it in production to detect attacks and anomalies.\n\n- GitHub: https://github.com/frauddi/dataspot\n- PyPI: https://pypi.org/project/dataspot/\n- Docs: https://frauddi.github.io/dataspot/\n\nQuestions welcome.\n",
    "author": "Competitive_Side4457",
    "timestamp": "2025-10-11T22:27:49",
    "url": "https://reddit.com/r/Python/comments/1o4gvj5/i_built_dataspot_to_find_fraud_patterns/",
    "score": 13,
    "num_comments": 4,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4hjl6",
    "title": "Those who have managed to get into IT in the last couple of years, please share your experiences!",
    "content": "I'm finishing my fourth year of university as a software engineer. Looking at companies' requirements, I realize it's easier to get into IT with your product than to go through a three- or even five-stage interview process for a meager salary.",
    "author": "South_Machine_5075",
    "timestamp": "2025-10-11T23:08:21",
    "url": "https://reddit.com/r/Python/comments/1o4hjl6/those_who_have_managed_to_get_into_it_in_the_last/",
    "score": 8,
    "num_comments": 5,
    "upvote_ratio": 0.59,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3p4bf",
    "title": "Best practices for using Python &amp; uv inside Docker",
    "content": "Getting¬†`uv`¬†right inside¬†Docker¬†is a bit tricky and even their¬†[official recommendations](https://docs.astral.sh/uv/guides/integration/docker/)¬†are not optimal.\n\nIt is better to use a [two-step build process](https://ashishb.net/programming/using-python-uv-inside-docker/) to eliminate¬†`uv`¬†from the final image size.\n\nA two-step build process not only saves disk space but also reduces attack surface against [security vulerabilities](https://astral.sh/blog/uv-security-advisory-cve-2025-54368)",
    "author": "ashishb_net",
    "timestamp": "2025-10-11T00:12:35",
    "url": "https://reddit.com/r/Python/comments/1o3p4bf/best_practices_for_using_python_uv_inside_docker/",
    "score": 182,
    "num_comments": 111,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4ap8i",
    "title": "Sunday Daily Thread: What's everyone working on this week?",
    "content": "# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è\n\nHello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!\n\n## How it Works:\n\n1. **Show &amp; Tell**: Share your current projects, completed works, or future ideas.\n2. **Discuss**: Get feedback, find collaborators, or just chat about your project.\n3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.\n\n## Guidelines:\n\n* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.\n* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.\n\n## Example Shares:\n\n1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!\n2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.\n3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!\n\nLet's build and grow together! Share your journey and learn from others. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-11T17:00:31",
    "url": "https://reddit.com/r/Python/comments/1o4ap8i/sunday_daily_thread_whats_everyone_working_on/",
    "score": 7,
    "num_comments": 7,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4lh7w",
    "title": "I wrote some optimizers for TensorFlow",
    "content": "**What My Project Does**\n\nThe optimizers is a lightweight library that implements a collection of advanced optimization algorithms specifically for TensorFlow and Keras. These optimizers are designed to drop right into your existing training pipelines‚Äîjust like the built-in Keras optimizers. The goal is to give you more tools to experiment with for faster convergence, better handling of complex loss landscapes, and improved performance on deep learning models.\n\n**Target Audience**\n\n\\* TensorFlow / Keras researchers and engineers looking to experiment with different optimizers.\n\n\\* Deep learning / reinforcement-learning practitioners who want quick, API-compatible optimizer swaps.\n\n\\* Students and small teams who prefer lightweight, source-first libraries.\n\n**Comparison**\n\n\\* vs. built-in Keras optimizers: offers additional/experimental variants for quick comparisons.\n\n\\* vs. larger 3rd-party ecosystems (e.g. tensorflow-addons or JAX/Optax): this repo is a lightweight, code-first collection focused on TensorFlow/Keras.\n\n[https://github.com/NoteDance/optimizers](https://github.com/NoteDance/optimizers)",
    "author": "NoteDancing",
    "timestamp": "2025-10-12T03:19:00",
    "url": "https://reddit.com/r/Python/comments/1o4lh7w/i_wrote_some_optimizers_for_tensorflow/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o53ave",
    "title": "I built an ultra-strict typing setup in Python (FastAPI + LangGraph + Pydantic + Pyright + Ruff) üöÄ",
    "content": "Hey everyone,\n\nI recently worked on a project using FastAPI + LangGraph, and I kept running into typing headaches. So I went down the rabbit hole and decided to build the strictest setup I could, making sure no Any could sneak in.\n\nHere‚Äôs the stack I ended up with:\n\n* Pydantic / Pydantic-AI ‚Üí strong data validation.\n* types-requests ‚Üí type stubs for requests.\n* Pyright ‚Üí static checker in \"strict\": true mode.\n* Ruff ‚Üí linter + enforces typing/style rules.\n\nWhat I gained:\n\n* Catching typing issues before running anything.\n* Much less uncertainty when passing data between FastAPI and LangGraph.\n* VSCode now feels almost like I‚Äôm writing TypeScript‚Ä¶ but in Python üòÖ.\n\nHere‚Äôs my pyproject.toml if anyone wants to copy, tweak, or criticize it:\n\n\n```toml\n# ============================================================\n# ULTRA-STRICT PYTHON PROJECT TEMPLATE\n# Maximum strictness - TypeScript strict mode equivalent\n# Tools: uv + ruff + pyright/pylance + pydantic v2\n# Python 3.12+\n# ============================================================\n\n[build-system]\nrequires = [\"setuptools&gt;=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"your-project-name\"\nversion = \"0.1.0\"\ndescription = \"Your project description\"\nauthors = [{ name = \"Your Name\", email = \"your.email@example.com\" }]\nlicense = { text = \"MIT\" }\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"pydantic\",\n    \"pydantic-ai-slim[openai]\",\n    \"types-requests\",\n    \"python-dotenv\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pyright\",\n    \"ruff\",\n    \"gitingest\",\n\t\t\"poethepoet\"\n]\n\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"*\"]\nexclude = [\"tests*\", \"scripts*\", \"docs*\", \"examples*\"]\n\n# ============================================================\n# POE THE POET - Task Runner\n# ============================================================\n[tool.poe.tasks]\n# Run with: poe format or uv run poe format\n# Formats code, fixes issues, and type checks\nformat = [\n    {cmd = \"ruff format .\"},\n    {cmd = \"ruff check . --fix\"},\n    {cmd = \"pyright\"}\n]\n\n# Run with: poe check\n# Lint and type check without fixing\ncheck = [\n    {cmd = \"ruff check .\"},\n    {cmd = \"pyright\"}\n]\n\n# Run with: poe lint or uv run poe lint\n# Only linting, no type checking\nlint = {cmd = \"ruff check . --fix\"}\n\n# Run with: poe lint-unsafe or uv run poe lint-unsafe\n# Lint with unsafe fixes enabled (more aggressive)\nlint-unsafe = {cmd = \"ruff check . --fix --unsafe-fixes\"}\n\n# ============================================================\n# RUFF CONFIGURATION - MAXIMUM STRICTNESS\n# ============================================================\n[tool.ruff]\ntarget-version = \"py312\"\nline-length = 88\nindent-width = 4\nfix = true\nshow-fixes = true\n\n[tool.ruff.lint]\n# Comprehensive rule set for strict checking\nselect = [\n    \"E\",      # pycodestyle errors\n    \"F\",      # pyflakes\n    \"I\",      # isort\n    \"UP\",     # pyupgrade\n    \"B\",      # flake8-bugbear\n    \"C4\",     # flake8-comprehensions\n    \"T20\",    # flake8-print (no print statements)\n    \"SIM\",    # flake8-simplify\n    \"N\",      # pep8-naming\n    \"Q\",      # flake8-quotes\n    \"RUF\",    # Ruff-specific rules\n    \"ASYNC\",  # flake8-async\n    \"S\",      # flake8-bandit (security)\n    \"PTH\",    # flake8-use-pathlib\n    \"ERA\",    # eradicate (commented-out code)\n    \"PL\",     # pylint\n    \"PERF\",   # perflint (performance)\n    \"ANN\",    # flake8-annotations\n    \"ARG\",    # flake8-unused-arguments\n    \"RET\",    # flake8-return\n    \"TCH\",    # flake8-type-checking\n]\n\nignore = [\n    \"E501\",    # Line too long (formatter handles this)\n    \"S603\",    # subprocess without shell=True (too strict)\n    \"S607\",    # Starting a process with a partial path (too strict)\n]\n\n# Per-file ignores\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\n    \"F401\",    # Allow unused imports in __init__.py\n]\n\"tests/**/*.py\" = [\n    \"S101\",    # Allow assert in tests\n    \"PLR2004\", # Allow magic values in tests\n    \"ANN\",     # Don't require annotations in tests\n]\n\n[tool.ruff.lint.isort]\nknown-first-party = [\"your_package_name\"]  # CHANGE THIS\ncombine-as-imports = true\nforce-sort-within-sections = true\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n[tool.ruff.lint.flake8-type-checking]\nstrict = true\n\n[tool.ruff.format]\nquote-style = \"double\"\nindent-style = \"space\"\nskip-magic-trailing-comma = false\nline-ending = \"auto\"\n\n# ============================================================\n# PYRIGHT CONFIGURATION - MAXIMUM STRICTNESS\n# TypeScript strict mode equivalent\n# ============================================================\n[tool.pyright]\npythonVersion = \"3.12\"\ntypeCheckingMode = \"strict\"\n\n# ============================================================\n# IMPORT AND MODULE CHECKS\n# ============================================================\nreportMissingImports = true\nreportMissingTypeStubs = true  # Stricter: require type stubs\nreportUndefinedVariable = true\nreportAssertAlwaysTrue = true\nreportInvalidStringEscapeSequence = true\n\n# ============================================================\n# STRICT NULL SAFETY (like TS strictNullChecks)\n# ============================================================\nreportOptionalSubscript = true\nreportOptionalMemberAccess = true\nreportOptionalCall = true\nreportOptionalIterable = true\nreportOptionalContextManager = true\nreportOptionalOperand = true\n\n# ============================================================\n# TYPE COMPLETENESS (like TS noImplicitAny + strictFunctionTypes)\n# ============================================================\nreportMissingParameterType = true\nreportMissingTypeArgument = true\nreportUnknownParameterType = true\nreportUnknownLambdaType = true\nreportUnknownArgumentType = true   # STRICT: Enable (can be noisy)\nreportUnknownVariableType = true   # STRICT: Enable (can be noisy)\nreportUnknownMemberType = true     # STRICT: Enable (can be noisy)\nreportUntypedFunctionDecorator = true\nreportUntypedClassDecorator = true\nreportUntypedBaseClass = true\nreportUntypedNamedTuple = true\n\n# ============================================================\n# CLASS AND INHERITANCE CHECKS\n# ============================================================\nreportIncompatibleMethodOverride = true\nreportIncompatibleVariableOverride = true\nreportInconsistentConstructor = true\nreportUninitializedInstanceVariable = true\nreportOverlappingOverload = true\nreportMissingSuperCall = true  # STRICT: Enable\n\n# ============================================================\n# CODE QUALITY (like TS noUnusedLocals + noUnusedParameters)\n# ============================================================\nreportPrivateUsage = true\nreportConstantRedefinition = true\nreportInvalidStubStatement = true\nreportIncompleteStub = true\nreportUnsupportedDunderAll = true\nreportUnusedClass = \"error\"        # STRICT: Error instead of warning\nreportUnusedFunction = \"error\"     # STRICT: Error instead of warning\nreportUnusedVariable = \"error\"     # STRICT: Error instead of warning\nreportUnusedImport = \"error\"       # STRICT: Error instead of warning\nreportDuplicateImport = \"error\"    # STRICT: Error instead of warning\n\n# ============================================================\n# UNNECESSARY CODE DETECTION\n# ============================================================\nreportUnnecessaryIsInstance = \"error\"         # STRICT: Error\nreportUnnecessaryCast = \"error\"               # STRICT: Error\nreportUnnecessaryComparison = \"error\"         # STRICT: Error\nreportUnnecessaryContains = \"error\"           # STRICT: Error\nreportUnnecessaryTypeIgnoreComment = \"error\"  # STRICT: Error\n\n# ============================================================\n# FUNCTION/METHOD SIGNATURE STRICTNESS\n# ============================================================\nreportGeneralTypeIssues = true\nreportPropertyTypeMismatch = true\nreportFunctionMemberAccess = true\nreportCallInDefaultInitializer = true\nreportImplicitStringConcatenation = true  # STRICT: Enable\n\n# ============================================================\n# ADDITIONAL STRICT CHECKS (Progressive Enhancement)\n# ============================================================\nreportImplicitOverride = true    # STRICT: Require @override decorator (Python 3.12+)\nreportShadowedImports = true     # STRICT: Detect shadowed imports\nreportDeprecated = \"warning\"     # Warn on deprecated usage\n\n# ============================================================\n# ADDITIONAL TYPE CHECKS\n# ============================================================\nreportImportCycles = \"warning\"\n\n# ============================================================\n# EXCLUSIONS\n# ============================================================\nexclude = [\n    \"**/__pycache__\",\n    \"**/node_modules\",\n    \".git\",\n    \".mypy_cache\",\n    \".pyright_cache\",\n    \".ruff_cache\",\n    \".pytest_cache\",\n    \".venv\",\n    \"venv\",\n    \"env\",\n    \"logs\",\n    \"output\",\n    \"data\",\n    \"build\",\n    \"dist\",\n    \"*.egg-info\",\n]\n\nvenvPath = \".\"\nvenv = \".venv\"\n\n# ============================================================\n# PYTEST CONFIGURATION\n# ============================================================\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\", \"*_test.py\"]\npython_classes = [\"Test*\"]\npython_functions = [\"test_*\"]\naddopts = [\n    \"--strict-markers\",\n    \"--strict-config\",\n    \"--tb=short\",\n    \"--cov=.\",\n    \"--cov-report=term-missing:skip-covered\",\n    \"--cov-report=html\",\n    \"--cov-report=xml\",\n    \"--cov-fail-under=80\",  # STRICT: Require 80% coverage\n]\nmarkers = [\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n    \"integration: marks tests as integration tests\",\n    \"unit: marks tests as unit tests\",\n]\n\n# ============================================================\n# COVERAGE CONFIGURATION\n# ============================================================\n[tool.coverage.run]\nsource = [\".\"]\nbranch = true  # STRICT: Enable branch coverage\nomit = [\n    \"*/tests/*\",\n    \"*/test_*.py\",\n    \"*/__pycache__/*\",\n    \"*/.venv/*\",\n    \"*/venv/*\",\n    \"*/scripts/*\",\n]\n\n[tool.coverage.report]\nprecision = 2\nshow_missing = true\nskip_covered = false\nfail_under = 80  # STRICT: Require 80% coverage\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n    \"if __name__ == .__main__.:\",\n    \"if TYPE_CHECKING:\",\n    \"@abstractmethod\",\n    \"@overload\",\n]\n\n# ============================================================\n# QUICK START GUIDE\n# ============================================================\n#\n# 1. CREATE NEW PROJECT:\n#    mkdir my-project &amp;&amp; cd my-project\n#    cp STRICT_PYPROJECT_TEMPLATE.toml pyproject.toml\n#\n# 2. CUSTOMIZE (REQUIRED):\n#    - Change project.name to \"my-project\"\n#    - Change project.description\n#    - Change project.authors\n#    - Change tool.ruff.lint.isort.known-first-party to [\"my_project\"]\n#\n# 3. SETUP ENVIRONMENT:\n#    uv venv\n#    source .venv/bin/activate  # Linux/Mac\n#    .venv\\Scripts\\activate     # Windows\n#    uv pip install -e \".[dev]\"\n#\n# 4. CREATE PROJECT STRUCTURE:\n#    mkdir -p src/my_project tests\n#    touch src/my_project/__init__.py\n#    touch tests/__init__.py\n#\n# 5. CREATE .gitignore:\n#    echo \".venv/\n#    __pycache__/\n#    *.py[cod]\n#    .pytest_cache/\n#    .ruff_cache/\n#    .pyright_cache/\n#    .coverage\n#    htmlcov/\n#    dist/\n#    build/\n#    *.egg-info/\n#    .env\n#    .DS_Store\" &gt; .gitignore\n#\n# 6. DAILY WORKFLOW:\n#    # Format code\n#    uv run ruff format .\n#\n#    # Lint and auto-fix\n#    uv run ruff check . --fix\n#\n#    # Type check (strict!)\n#    uv run pyright\n#\n#    # Run tests with coverage\n#    uv run pytest\n#\n#    # Full check (run before commit)\n#    uv run ruff format . &amp;&amp; uv run ruff check . &amp;&amp; uv run pyright &amp;&amp; uv run pytest\n#\n# 7. VS CODE SETUP (recommended):\n#    Create .vscode/settings.json:\n#    {\n#      \"python.defaultInterpreterPath\": \".venv/bin/python\",\n#      \"python.analysis.typeCheckingMode\": \"strict\",\n#      \"python.analysis.autoImportCompletions\": true,\n#      \"editor.formatOnSave\": true,\n#      \"editor.codeActionsOnSave\": {\n#        \"source.organizeImports\": true,\n#        \"source.fixAll\": true\n#      },\n#      \"[python]\": {\n#        \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n#      },\n#      \"ruff.enable\": true,\n#      \"ruff.lint.enable\": true,\n#      \"ruff.format.args\": [\"--config\", \"pyproject.toml\"]\n#    }\n#\n# 8. GITHUB ACTIONS CI (optional):\n#    Create .github/workflows/ci.yml:\n#    name: CI\n#    on: [push, pull_request]\n#    jobs:\n#      test:\n#        runs-on: ubuntu-latest\n#        steps:\n#          - uses: actions/checkout@v4\n#          - uses: astral-sh/setup-uv@v1\n#          - run: uv pip install -e \".[dev]\"\n#          - run: uv run ruff format --check .\n#          - run: uv run ruff check .\n#          - run: uv run pyright\n#          - run: uv run pytest\n#\n# ============================================================\n# PYDANTIC V2 PATTERNS (IMPORTANT)\n# ============================================================\n#\n# ‚úÖ CORRECT (Pydantic v2):\n# from pydantic import BaseModel, field_validator, model_validator, ConfigDict\n#\n# class User(BaseModel):\n#     model_config = ConfigDict(strict=True)\n#     name: str\n#     age: int\n#\n#     @field_validator('age')\n#     @classmethod\n#     def validate_age(cls, v: int) -&gt; int:\n#         if v &lt; 0:\n#             raise ValueError('age must be positive')\n#         return v\n#\n#     @model_validator(mode='after')\n#     def validate_model(self) -&gt; 'User':\n#         return self\n#\n# ‚ùå WRONG (Pydantic v1 - deprecated):\n# class User(BaseModel):\n#     class Config:\n#         strict = True\n#\n#     @validator('age')\n#     def validate_age(cls, v):\n#         return v\n#\n# ============================================================\n# STRICTNESS LEVELS\n# ============================================================\n#\n# This template is at MAXIMUM strictness. To reduce:\n#\n# LEVEL 1 - Production Ready (Recommended):\n#   - Keep all current settings\n#   - This is the gold standard\n#\n# LEVEL 2 - Slightly Relaxed:\n#   - reportUnknownArgumentType = false\n#   - reportUnknownVariableType = false\n#   - reportUnknownMemberType = false\n#   - reportUnused* = \"warning\" (instead of \"error\")\n#\n# LEVEL 3 - Gradual Adoption:\n#   - typeCheckingMode = \"standard\"\n#   - reportMissingSuperCall = false\n#   - reportImplicitOverride = false\n#\n# ============================================================\n# TROUBLESHOOTING\n# ============================================================\n#\n# Q: Too many type errors from third-party libraries?\n# A: Add to exclude list or set reportMissingTypeStubs = false\n#\n# Q: Pyright too slow?\n# A: Add large directories to exclude list\n#\n# Q: Ruff \"ALL\" too strict?\n# A: Replace \"ALL\" with specific rule codes (see template above)\n#\n# Q: Coverage failing?\n# A: Reduce fail_under from 80 to 70 or 60\n#\n# Q: How to ignore specific errors temporarily?\n# A: Use # type: ignore[error-code] or # noqa: RULE_CODE\n#    But fix them eventually - strict mode means no ignores!\n#\n\n```\n\n",
    "author": "Ranteck",
    "timestamp": "2025-10-12T15:51:51",
    "url": "https://reddit.com/r/Python/comments/1o53ave/i_built_an_ultrastrict_typing_setup_in_python/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.34,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4wjlr",
    "title": "Pyautogui n√£o manipula o gerenciador de dom√≠nios do Windows por que?",
    "content": "Estou tentando fazer um c√≥digo que abra aquela tela de onde se gerencia o dom√≠nio do Windows.  \nL√° dentro o script dever√° colocar o hostname da m√°quina , mandar buscar a m√°quina , clicar em cima dela e coloc√°-la no GRUPO PC\\_ESTADOS\\_UNIDOS e depois mover a m√°quina para o UO Michigan depois o UO Detroit.\n\nOk, fiz o c√≥digo mas ao tentar mandar o texto do hostname usando uma imagem como referencia, o Python + Pyautogui at√© acha o campo, mas ao inv√©s de mandar o texto para o campo, ele manda para o console como se fosse um comando a ser executado. Ok, se voc√™ tenta executar o script com um click isso n√£o ocorre, porem n√£o manda texto nenhum e o c√≥digo para clicar no bot√£o buscar faz o bot√£o ser real√ßado porem ele n√£o clica, seja com o click direito ou esquerdo ou com ambos v√°rias vezes, simplesmente n√£o ocorre nada.  \n\n\nEssa tela do windows √© aprova de automatiza√ß√£o?",
    "author": "Gabriel_Cinzao",
    "timestamp": "2025-10-12T11:22:43",
    "url": "https://reddit.com/r/Python/comments/1o4wjlr/pyautogui_n√£o_manipula_o_gerenciador_de_dom√≠nios/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3uy4y",
    "title": "I made a Better Notepad alternative using PySide6",
    "content": "# What My Project Does\n\nZenNotes is a minimalistic Notepad app with a sleek design inspired by the Fluent Design. It offers the familiar look of the Windows Notepad while having much more powerful features like Translate, TTS, etc.\n\n# Target Audience\n\nAnyone who uses Windows Notepad, or noepads in general\n\n# Comparison¬†\n\nThe target competition is Windows Notepad. ZenNotes is like an \"extension\" of Windows Notepad, with similar looks but much more features, like TTS, Translate, etc.\n\n# GitHub\n\n[https://github.com/rohankishore/ZenNotes](https://github.com/rohankishore/ZenNotes)",
    "author": "Specialist-Arachnid6",
    "timestamp": "2025-10-11T05:56:17",
    "url": "https://reddit.com/r/Python/comments/1o3uy4y/i_made_a_better_notepad_alternative_using_pyside6/",
    "score": 45,
    "num_comments": 21,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3sqqz",
    "title": "Announcing html-to-markdown v2: Rust rewrite, full CommonMark 1.2 compliance, and hOCR support",
    "content": "Hi Pythonistas,\n\nI'm glad to announce the v2 release of [html-to-markdown](https://github.com/Goldziher/html-to-markdown).\n\nThis library started life as a fork of `markdownify`, a Python library for converting HTML to Markdown. I forked it originally because I needed modern type hints, but then found myself rewriting the entire thing. Over time it became essential for [kreuzberg](https://github.com/Goldziher/kreuzberg), where it serves as a backbone for both html -&gt; markdown and hOCR -&gt; markdown.\n\nI am working on Kreuzberg v4, which migrates much of it to Rust. This necessitated updating this component as well, which led to a full rewrite in Rust, offering improved performance, memory stability, and a more robust feature set.\n\nv2  delivers Rust-backed HTML ‚Üí Markdown conversion with Python bindings, a CLI and a Rust crate. The rewrite makes this by far the most performance and complete solution for HTML to Markdown conversion in python. Here are some benchmarks:\n\nApple M4 ‚Ä¢ Real Wikipedia documents ‚Ä¢ `convert()` (Python)\n\n| Document            | Size  | Latency | Throughput | Docs/sec |\n| ------------------- | ----- | ------- | ---------- | -------- |\n| Lists (Timeline)    | 129KB | 0.62ms  | 208‚ÄØMB/s   | 1,613    |\n| Tables (Countries)  | 360KB | 2.02ms  | 178‚ÄØMB/s   | 495      |\n| Mixed (Python wiki) | 656KB | 4.56ms  | 144‚ÄØMB/s   | 219      |\n\n&gt; V1 averaged ~2.5‚ÄØMB/s (Python/BeautifulSoup). V2‚Äôs Rust engine delivers 60‚Äì80x higher throughput.\n\nThe Python package still exposes `markdownify`-style calls via `html_to_markdown.v1_compat`, so migrations are relatively straightforward, although the v2 did introduce some breaking changes (see CHANGELOG.md for full details).\n\n## Highlights\n\nHere are the key highlights of the v2 release aside from the massive performance improvements:\n\n- CommonMark-compliant defaults with explicit toggles when you need legacy behaviour.\n- Inline image extraction (`convert_with_inline_images`) that captures data URI assets and inline SVGs with sizing and quota controls.\n- Full hOCR 1.2 spec compliance, including hOCR table reconstruction and YAML frontmatter for metadata to keep OCR output structured.\n- Memory is kept kept in check by dedicated harnesses: repeated conversions stay under 200‚ÄØMB RSS on multi-megabyte corpora.\n\n## Target Audience\n- Engineers replacing BeautifulSoup-based converters that fall apart on large documents or OCR outputs.\n- Python, Rust, and CLI users who need identical Markdown from libraries, pipelines, and batch tools.\n- Teams building document understanding stacks (including the kreuzberg ecosystem) that rely on tight memory behaviour and parallel throughput.\n- OCR specialists who need to process hOCR efficiently.\n\n## Comparison to Alternatives\n- [`markdownify`](https://github.com/matthewwithanm/python-markdownify): the spiritual ancestor, but still Python + BeautifulSoup. html-to-markdown v2 keeps the API shims while delivering 60‚Äì80√ó more throughput, table-aware hOCR support, and deterministic memory usage across repeated conversions.\n- [`html2text`](https://github.com/Alir3z4/html2text): solid for quick scripts, yet it lacks CommonMark compliance and tends to drift on complex tables and OCR layouts; it also allocates heavily under pressure because it was never built with long-running processes in mind.\n- [`pandoc`](https://github.com/jgm/pandoc): extremely flexible (and amazing!), but large, much slower for pure HTML ‚Üí Markdown pipelines, and not embeddable in Python without subprocess juggling. html-to-markdown v2 offers a slim Rust core with direct bindings, so you keep the performance while staying in-process.\n\nIf you end up using the rewrite, a ‚≠êÔ∏è on the [repo](https://github.com/Goldziher/html-to-markdown) always makes yours truly happy!",
    "author": "Goldziher",
    "timestamp": "2025-10-11T04:01:26",
    "url": "https://reddit.com/r/Python/comments/1o3sqqz/announcing_htmltomarkdown_v2_rust_rewrite_full/",
    "score": 51,
    "num_comments": 5,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o41tgv",
    "title": "[FOSS] Flint: A 100% Config-Driven ETL Framework",
    "content": "I'd like to share **Flint**, a configuration-driven ETL framework that lets you define complete data pipelines through JSON/YAML instead of code.\n\n## What My Project Does\n\nFlint transforms straightforward ETL workflows from programming tasks into declarative configuration. Define your sources, transformations (select, filter, join, cast, etc.), and destinations in JSON or YAML - the framework handles execution. The processing engine is abstracted away, currently supporting Apache Spark with Polars in development.\n\n**It's not intended to replace all ETL development** - complex data engineering still needs custom code. Instead, it handles routine ETL tasks so engineers can focus on more interesting problems.\n\n## Target Audience\n- Data engineers tired of writing boilerplate for basic pipelines, so they ahve more time for more interesting programming tasks than straightforward ETL pipelines.\n- Teams wanting standardized ETL patterns\n- Organizations needing pipeline logic accessible to non-developers\n- Projects requiring multi-engine flexibility\n\n100% test coverage (unit + e2e), strong typing, extensive documentation with class and activity diagrams, and configurable alerts/hooks.\n\n## Comparison\n\nUnlike other transformation tools like DBT this one is configuration focused to reduce complexity and programming knowledge to make the boring ETL task simple, to keep more time for engineers for more intersting issues. This focuses on pure configuration without vendor lock-in as the backend key can be changed anytime with another implementation.\n\n## Future expansion\n\nThe foundation is solid - now looking to expand with new engines, add tracing/metrics, migrate CLI to Click, move from azure devops CICD to github actions, extend Polars transformations, and more.\n\n**[GitHub: config-driven-ETL-framework](https://github.com/krijnvanderburg/config-driven-ETL-framework)**. If you like the project idea then consider giving it a star, it means the world to get a project started from the ground.\n\n```jsonc\n{\n    \"runtime\": {\n        \"id\": \"customer-orders-pipeline\",\n        \"description\": \"ETL pipeline for processing customer orders data\",\n        \"enabled\": true,\n        \"jobs\": [\n            {\n                \"id\": \"silver\",\n                \"description\": \"Combine customer and order source data into a single dataset\",\n                \"enabled\": true,\n                \"engine_type\": \"spark\", // Specifies the processing engine to use\n                \"extracts\": [\n                    {\n                        \"id\": \"extract-customers\",\n                        \"extract_type\": \"file\", // Read from file system\n                        \"data_format\": \"csv\", // CSV input format\n                        \"location\": \"examples/join_select/customers/\", // Source directory\n                        \"method\": \"batch\", // Process all files at once\n                        \"options\": {\n                            \"delimiter\": \",\", // CSV delimiter character\n                            \"header\": true, // First row contains column names\n                            \"inferSchema\": false // Use provided schema instead of inferring\n                        },\n                        \"schema\": \"examples/join_select/customers_schema.json\" // Path to schema definition\n                    }\n                ],\n                \"transforms\": [\n                    {\n                        \"id\": \"transform-join-orders\",\n                        \"upstream_id\": \"extract-customers\", // First input dataset from extract stage\n                        \"options\": {},\n                        \"functions\": [\n                            {\"function_type\": \"join\", \"arguments\": {\"other_upstream_id\": \"extract-orders\", \"on\": [\"customer_id\"], \"how\": \"inner\"}},\n                            {\"function_type\": \"select\", \"arguments\": {\"columns\": [\"name\", \"email\", \"signup_date\", \"order_id\", \"order_date\", \"amount\"]}}\n                        ]\n                    }\n                ],\n                \"loads\": [\n                    {\n                        \"id\": \"load-customer-orders\",\n                        \"upstream_id\": \"transform-join-orders\", // Input dataset for this load\n                        \"load_type\": \"file\", // Write to file system\n                        \"data_format\": \"csv\", // Output as CSV\n                        \"location\": \"examples/join_select/output\", // Output directory\n                        \"method\": \"batch\", // Write all data at once\n                        \"mode\": \"overwrite\", // Replace existing files if any\n                        \"options\": {\n                            \"header\": true // Include header row with column names\n                        },\n                        \"schema_export\": \"\" // No schema export\n                    }\n                ],\n                \"hooks\": {\n                    \"onStart\": [], // Actions to execute before pipeline starts\n                    \"onFailure\": [], // Actions to execute if pipeline fails\n                    \"onSuccess\": [],  // Actions to execute if pipeline succeeds\n                    \"onFinally\": [] // Actions to execute after pipeline completes (success or failure)\n                }\n            }\n        ]\n    }\n}\n```",
    "author": "TeamFlint",
    "timestamp": "2025-10-11T10:42:21",
    "url": "https://reddit.com/r/Python/comments/1o41tgv/foss_flint_a_100_configdriven_etl_framework/",
    "score": 8,
    "num_comments": 0,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4yx19",
    "title": "HIRING: Scrape 300,000 PDFs and Archive to 128‚ÄØGB VERBATIM Discs",
    "content": "We are seeking an operator to extract approximately 300,000 book titles from AbeBooks.com, applying specific filtering parameters that will be provided.\n\nOnce the dataset is obtained, the corresponding PDF files should be retrieved from the Wayback Machine or Anna‚Äôs Archive, when available.\nThe estimated total storage requirement is around 4 TB. Data will be temporarily stored on a dedicated server during collection and subsequently transferred to 128 GB Verbatim or Panasonic optical discs for long-term preservation.\n\n\nThe objective is to ensure the archive‚Äôs readability and transferability for at least 100 years, relying solely on commercially available hardware and systems.",
    "author": "Atronem",
    "timestamp": "2025-10-12T12:54:09",
    "url": "https://reddit.com/r/Python/comments/1o4yx19/hiring_scrape_300000_pdfs_and_archive_to_128_gb/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.19,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3r5tp",
    "title": "I shared 300+ Python Data Science Videos on YouTube (Tutorials, Projects and Full Courses)",
    "content": "Hello, I am sharing free Python Data Science Tutorials for over 2 years on YouTube and I wanted to share my playlists. I believe they are great for learning the field, I am sharing them below. Thanks for reading!\n\nPython Tutorials -&gt;¬†[https://youtube.com/playlist?list=PLTsu3dft3CWgJrlcs\\_IO1eif7myukPPKJ&amp;si=fYIz2RLJV1dC6nT5](https://youtube.com/playlist?list=PLTsu3dft3CWgJrlcs_IO1eif7myukPPKJ&amp;si=fYIz2RLJV1dC6nT5)\n\nData Science Full Courses &amp; Projects:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra\\_5PGH](https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH)\n\nAI Tutorials (LangChain, LLMs &amp; OpenAI API):¬†[https://youtube.com/playlist?list=PLTsu3dft3CWhAAPowINZa5cMZ5elpfrxW](https://youtube.com/playlist?list=PLTsu3dft3CWhAAPowINZa5cMZ5elpfrxW)\n\nMachine Learning Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWhSJh3x5T6jqPWTTg2i6jp1](https://youtube.com/playlist?list=PLTsu3dft3CWhSJh3x5T6jqPWTTg2i6jp1)\n\nDeep Learning Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWghrjn4PmFZlxVBileBpMjj](https://youtube.com/playlist?list=PLTsu3dft3CWghrjn4PmFZlxVBileBpMjj)\n\nNatural Language Processing Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWjYPJi5RCCVAF6DxE28LoKD](https://youtube.com/playlist?list=PLTsu3dft3CWjYPJi5RCCVAF6DxE28LoKD)\n\nTime Series Analysis Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWibrBga4nKVEl5NELXnZ402](https://youtube.com/playlist?list=PLTsu3dft3CWibrBga4nKVEl5NELXnZ402)\n\nStreamlit Based Python Web App Development Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWhBViLMhL0Aqb75rkSz\\_CL-](https://youtube.com/playlist?list=PLTsu3dft3CWhBViLMhL0Aqb75rkSz_CL-)\n\nData Cleaning Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWhOUPyXdLw8DGy\\_1l2oK1yy](https://youtube.com/playlist?list=PLTsu3dft3CWhOUPyXdLw8DGy_1l2oK1yy)\n\nData Analysis Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWhwPJcaAc-k6a8vAqBx2\\_0t](https://youtube.com/playlist?list=PLTsu3dft3CWhwPJcaAc-k6a8vAqBx2_0t)\n\nEnd-to-End Data Science Projects:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWg69zbIVUQtFSRx\\_UV80OOg](https://youtube.com/playlist?list=PLTsu3dft3CWg69zbIVUQtFSRx_UV80OOg)",
    "author": "onurbaltaci",
    "timestamp": "2025-10-11T02:24:12",
    "url": "https://reddit.com/r/Python/comments/1o3r5tp/i_shared_300_python_data_science_videos_on/",
    "score": 25,
    "num_comments": 2,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3s3yj",
    "title": "Feedback Request for API Key Management Library for FastAPI",
    "content": "Hello,\n\nIn my work, I build many FastAPI applications, both internal and external, that expose endpoints to other product, business, and data teams, accessible via API keys. Each project eventually ended up with its own slightly different API key system, so I finally took the time to extract the common parts and combine them into a reusable library.\n\n[https://github.com/Athroniaeth/fastapi-api-key](https://github.com/Athroniaeth/fastapi-api-key)\n\nBefore publishing it publicly (not yet on PyPI, and the mkdocs documentation is still local), I‚Äôd like to get feedback from people who have solved similar problems (or just see what they think).\n\nThe goal is to see if I can improve this project or if there are any major security flaws (which would be problematic for an API key system).\n\nI built the library as follows:\n\n* **Security-first**: secrets are hashed with a salt and a pepper, and never logged or returned after creation\n* **Easy-to-use**: just inherited from the repository and use service\n* **Prod-ready**: services and repositories are async, and battle-tested\n* **Agnostic hasher**: you can use any async-compatible hashing strategy (default: Argon2)\n* **Agnostic backend**: you can use any async-compatible database (default: SQLAlchemy)\n* **Factory**: create a Typer, FastAPI router wired to api key systems (only SQLAlchemy for now)\n\n**I‚Äôd love feedback on (but not limited to) the following:**\n\n* Are there features you would expect that don‚Äôt exist?\n* Does the SQLAlchemy Mixin approach seem good for handling custom field extensions?\n* Do you see any potential flaws with the current hashing/peppering strategy?\n* What do you think about the extras/packaging approach (‚Äúcore‚Äù, ‚Äúfastapi‚Äù, ‚Äúall‚Äù)?\n\nIs there anything else I should add to make it more usable? If you want to browse the code, start with the preliminary README (which includes usage examples). There‚Äôs also mkdocs documentation with quickstarts and usage guides.",
    "author": "__secondary__",
    "timestamp": "2025-10-11T03:24:01",
    "url": "https://reddit.com/r/Python/comments/1o3s3yj/feedback_request_for_api_key_management_library/",
    "score": 14,
    "num_comments": 3,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3vzbm",
    "title": "sdax - an API for asyncio for handling parallel tasks declaratively",
    "content": "Parallel async¬†is fast, but managing failures and cleanup across¬†multiple dependent operations is hard.\n\nsdax - (Structured Declarative Async eXecution) does all the heavy lifting. You just need to write the async functions and wire them into \"levels\".\n\nI'm working on an extension to sdax for doing all the initialization using decorators - coming next.\n\nRequires Python 3.11 or higher since it uses asyncio.TaskGroup and ExceptionGroup which were introduced in 3.11.\n\nSee: [https://pypi.org/project/sdax](https://pypi.org/project/sdax/), [https://github.com/owebeeone/sdax](https://github.com/owebeeone/sdax)",
    "author": "GianniMariani",
    "timestamp": "2025-10-11T06:43:02",
    "url": "https://reddit.com/r/Python/comments/1o3vzbm/sdax_an_api_for_asyncio_for_handling_parallel/",
    "score": 8,
    "num_comments": 5,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3ijyt",
    "title": "How much Python do I really need to know to land my first dev job?",
    "content": "Hey everyone,\nI‚Äôve been working as a Data Analyst at an energy distribution company for about a year and a half. My long-term goal has always been to build the skills needed to transition into a developer role.\nI feel like it‚Äôs finally time to sharpen my knowledge and make that pivot ‚Äî but honestly, I still feel like I know nothing, even though I‚Äôm a bit of a Swiss Army knife in my current job.\nHere‚Äôs a quick overview of what I already know and where I‚Äôm at:\nSeveral Python certificates (Coursera and Cisco).\nCertified and experienced in SQL databases (DDL and DML).\nComfortable working with Linux systems.\nProcess automation experience using PDI Spoon and batch scripts.\nCurrently studying Data Analytics and Machine Learning with Python.\nI haven‚Äôt worked with APIs or HTTP requests yet, and my English level is low, but I‚Äôm improving.\nWhere should I focus next? Do I need to go deeper in Python itself, or start learning web frameworks, APIs, or something else to move toward a dev job?",
    "author": "Secure-Hornet7304",
    "timestamp": "2025-10-10T18:09:54",
    "url": "https://reddit.com/r/Python/comments/1o3ijyt/how_much_python_do_i_really_need_to_know_to_land/",
    "score": 48,
    "num_comments": 45,
    "upvote_ratio": 0.7,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o2viq3",
    "title": "uv cheatsheet with most common/useful commands",
    "content": "I've been having lots of fun using Astral's uv and also teaching it to friends and students, so I decided to create a cheatsheet with the most common/useful commands.\n\n[uv cheatsheet with most common/useful commands](https://mathspp.com/blog/uv-cheatsheet)\n\nI included sections about\n\n - project creation;\n - dependency management;\n - project lifecycle &amp; versioning;\n - installing/working with tools;\n - working with scripts;\n - uv's interface for `pip` and `venv`; and\n - some meta &amp; miscellaneous commands.\n\nThe link above takes you to a page with all these sections as regular tables and to high-resolution/print-quality downloadable files you can  get for yourself from the link above.\n\nI hope this is helpful for you and if you have any feedback, I'm all ears!",
    "author": "RojerGS",
    "timestamp": "2025-10-10T01:42:23",
    "url": "https://reddit.com/r/Python/comments/1o2viq3/uv_cheatsheet_with_most_commonuseful_commands/",
    "score": 388,
    "num_comments": 73,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3v4cs",
    "title": "Built an automated GitHub-RAG pipeline system with incremental sync",
    "content": "**What My Project Does**\n\nRAGIT is a fully automated RAG pipeline for GitHub repositories. Upload a repo and it handles collection, preprocessing, embedding, vector indexing, and incremental synchronization automatically. Context is locked to specific commits to avoid version confusion. When you ask questions, hybrid search finds relevant code with citations and answers consistently across multiple files.\n\n**Target Audience**\n\nProduction-ready system for development teams working with large codebases. Built with microservices architecture (Gateway-Backend-Worker pattern) using PostgreSQL, Redis, and Milvus. Fully dockerized for easy deployment. Useful for legacy code analysis, project onboarding, and ongoing codebase understanding.\n\n**Comparison**\n\nUnlike manually copying code into ChatGPT/Claude which loses context and version tracking, RAGIT automates the entire pipeline and maintains commit-level consistency. Compared to other RAG frameworks that require manual chunking and indexing, RAGIT handles GitHub repos end-to-end with automatic sync when code changes. More reproducible and consistent than direct LLM usage.\n\nApache 2.0 licensed.\n\nGitHub: https://github.com/Gyu-Chul/RAGIT\nDemo: https://www.youtube.com/watch?v=VSBDDvj5_w4\n\nOpen to feedback.",
    "author": "RecentHearing8646",
    "timestamp": "2025-10-11T06:04:00",
    "url": "https://reddit.com/r/Python/comments/1o3v4cs/built_an_automated_githubrag_pipeline_system_with/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3323m",
    "title": "PipeFunc: Build Lightning-Fast Pipelines with Python: DAGs Made Easy",
    "content": "Hey r/Python!\n\nI'm excited to share `pipefunc` ([github.com/pipefunc/pipefunc](https://github.com/pipefunc/pipefunc)), a Python library designed to make building and running complex computational workflows incredibly fast and easy. If you've ever dealt with intricate dependencies between functions, struggled with parallelization, or wished for a simpler way to create and manage DAG pipelines, `pipefunc` is here to help.\n\n**What My Project Does:**\n\n`pipefunc` empowers you to easily construct **Directed Acyclic Graph (DAG)** pipelines in Python. It handles:\n\n1. **Automatic Dependency Resolution:**  `pipefunc` automatically determines the correct execution order of your functions, eliminating manual dependency management.\n2. **Lightning-Fast Execution:** With minimal overhead (around **10 ¬µs per function call**), `pipefunc` ensures your pipelines run super fast.\n3. **Effortless Parallelization:** `pipefunc` automatically parallelizes independent tasks, whether on your local machine or a SLURM cluster. It supports any `concurrent.futures.Executor`!\n4. **Intuitive Visualization:** Generate interactive graphs to visualize your pipeline's structure and understand data flow.\n5. **Simplified Parameter Sweeps:**  `pipefunc`'s `mapspec` feature lets you easily define and run N-dimensional parameter sweeps, which is perfect for scientific computing, simulations, and hyperparameter tuning.\n6. **Resource Profiling:** Gain insights into your pipeline's performance with detailed CPU, memory, and timing reports.\n7. **Caching:** Avoid redundant computations with multiple caching backends.\n8. **Type Annotation Validation:** Ensures type consistency across your pipeline to catch errors early.\n9. **Error Handling:** Includes an `ErrorSnapshot` feature to capture detailed information about errors, making debugging easier.\n\n**Target Audience:**\n\n`pipefunc` is ideal for:\n\n*   **Scientific Computing:** Streamline simulations, data analysis, and complex computational workflows.\n*   **Machine Learning:** Build robust and reproducible ML pipelines, including data preprocessing, model training, and evaluation.\n*   **Data Engineering:** Create efficient ETL processes with automatic dependency management and parallel execution.\n*   **HPC:** Run `pipefunc` on a SLURM cluster with minimal changes to your code.\n*   **Anyone** working with interconnected functions who wants to improve code organization, performance, and maintainability.\n\n`pipefunc` is designed to be flexible (great tool for prototyping and experimentation) and easy to adopt!\n\n**Comparison:**\n\n*  **vs. Hamilton:** Hamilton also compiles Python functions into\nDAGs, but it centers on column-level DataFrame engineering, ships modifiers like @with_columns/@extract_columns, and offers built-in data/schema validation plus an optional UI for lineage and observability; pipefunc leans toward low-overhead scientific/HPC pipelines, executor-agnostic parallelism, and N-D sweeps via mapspecs.\n*   **vs. Dask:** `pipefunc` offers a higher-level, more declarative way to define pipelines. It automatically manages task scheduling and execution based on your function definitions and `mapspec`s, without requiring you to write explicit parallel code.\n*   **vs. Luigi/Airflow/Prefect/Kedro:** While those tools excel at ETL and event-driven workflows, `pipefunc` focuses on scientific computing, simulations, and computational workflows where fine-grained control over execution and resource allocation is crucial. Also, it's way easier to setup and develop with, with minimal dependencies!\n*   **vs. Pandas:** You can easily combine `pipefunc` with `Pandas`! Use `pipefunc` to manage the execution of `Pandas` operations and parallelize your data processing pipelines. But it also works well with `Polars`, `Xarray`, and other libraries!\n*   **vs. Joblib:** `pipefunc` offers several advantages over `Joblib`. `pipefunc` automatically determines the execution order of your functions, generates interactive visualizations of your pipeline, profiles resource usage, and supports multiple caching backends. Also, `pipefunc` allows you to specify the mapping between inputs and outputs using `mapspec`s, which enables complex map-reduce operations.\n\n**Examples:**\n\n**Simple Example:**\n\n```python\nfrom pipefunc import pipefunc, Pipeline\n\n@pipefunc(output_name=\"c\")\ndef add(a, b):\n    return a + b\n\n@pipefunc(output_name=\"d\")\ndef multiply(b, c):\n    return b * c\n\npipeline = Pipeline([add, multiply])\nresult = pipeline(\"d\", a=2, b=3)  # Automatically executes 'add' first\nprint(result)  # Output: 15\n\npipeline.visualize() # Visualize the pipeline\n```\n\n**Parallel Example with `mapspec`:**\n\nParallelizes for all combinations of inputs `a` and `b` automatically!\n\n```python\nimport numpy as np\nfrom pipefunc import pipefunc, Pipeline\nfrom pipefunc.map import load_outputs\n\n@pipefunc(output_name=\"c\", mapspec=\"a[i], b[j] -&gt; c[i, j]\")\ndef f(a: int, b: int):\n    return a + b\n\n@pipefunc(output_name=\"mean\") # no mapspec, so receives 2D `c[:, :]`\ndef g(c: np.ndarray):\n    return np.mean(c)\n\npipeline = Pipeline([f, g])\ninputs = {\"a\": [1, 2, 3], \"b\": [4, 5, 6]}\nresult_dict = pipeline.map(inputs, run_folder=\"my_run_folder\", parallel=True)\nresult = load_outputs(\"mean\", run_folder=\"my_run_folder\") # can load now too\nprint(result)  # Output: 7.0\n```\n\n**Getting Started:**\n\n*   **Docs:** [https://pipefunc.readthedocs.io/](https://pipefunc.readthedocs.io/)\n*   **Source:** [https://github.com/pipefunc/pipefunc](https://github.com/pipefunc/pipefunc)\n\nI'm exctited to hear your feedback and answer any questions you have. Give `pipefunc` a try and let me know how it can improve your workflows!\n",
    "author": "basnijholt",
    "timestamp": "2025-10-10T07:52:08",
    "url": "https://reddit.com/r/Python/comments/1o3323m/pipefunc_build_lightningfast_pipelines_with/",
    "score": 56,
    "num_comments": 14,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3qk1p",
    "title": "UV on termux Debian (android)",
    "content": "Anybody managed to build it? And if so, pretty please with chocolate chips, how? I've made the obvious attempts (pip install, cargo...) but no joy so far.",
    "author": "DharmaBird",
    "timestamp": "2025-10-11T01:45:27",
    "url": "https://reddit.com/r/Python/comments/1o3qk1p/uv_on_termux_debian_android/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o396tf",
    "title": "SPDL - Scalable and Performant Data Loading",
    "content": "Hi Python community,\n\nInspired by recent showcases on pipeline libraries ([Pipevine](https://www.reddit.com/r/Python/comments/1o2n119/ergonomic_concurrency/), [pipefunc](https://www.reddit.com/r/Python/comments/1o3323m/pipefunc_build_lightningfast_pipelines_with/)), I‚Äôd like to share my project:¬†**SPDL (Scalable and Performant Data Loading)**. \n\n# What My Project Does\n\nSPDL is designed to address the data loading bottleneck in machine learning (ML) and AI training pipelines. You break down data loading into discrete tasks with different constraints (network, CPU, GPU transfer etc) and construct a pipeline, and SPDL executes them efficiently. It features a task execution engine (pipeline abstraction) built on asyncio, alongside an independent I/O module for media processing. \n\n# Resources:\n\n* **Repo:**¬†[https://github.com/facebookresearch/spdl](https://github.com/facebookresearch/spdl)\n* **Documentation:**¬†[SPDL Docs](https://facebookresearch.github.io/spdl/main/)\n* **PyPI:**\n   * Install with `pip install spdl`\n      * [spdl-core](https://pypi.org/project/spdl-core/) (no dependency)\n      * [spdl-io](https://pypi.org/project/spdl-io/) (requires NumPy, and optionally PyTorch / Numba / Jax) \n* **arXiv:** [2504.20067](https://arxiv.org/abs/2504.20067)\n\n# Target Audience\n\nML practitioners whose focus is model training rather than software engineering. It is production-ready.\n\n# Core Principles\n\n* **High Throughput &amp; Efficiency:**¬†SPDL maximizes data loading speed and minimizes CPU/memory overhead to keep GPUs busy.\n* **Flexibility:**¬†The pipeline abstraction is highly customizable, allowing users to tailor the structure to their environment, data, and requirements.\n* **Observability:**¬†SPDL provides runtime statistics for each pipeline component, helping users identify bottlenecks and optimize performance.\n* **Intuitive Construction:**¬†Pipelines are easy to build and reason about, with clear separation of stages and bounding factors.\n\n# Architecture Overview\n\n* **Pipeline Abstraction:**¬†With SPDL, you break down data loading into discrete tasks with different constraints (network, CPU, GPU transfer etc) and construct a pipeline that executes each task concurrently.\n* **Multi-threading &amp; Multi-processing:**¬†SPDL uses multi-threading by default for parallelism, with optional multi-processing for workloads that benefit from process isolation. In production, we‚Äôve successfully used multi-threading with Python 3.10 by composing functions that release the GIL. Support for InterpreterPoolExecutor in Python 3.14 is planned.\n* **Async Event Loop:**¬†The task execution engine is built on an async event loop, supporting both async and regular functions.\n* **Media I/O Module:**¬†Includes a high-performance I/O module for audio, video, and image processing, designed from scratch for maximum throughput. It also supports loading NumPy array fast from memory.\n* **Non-invasive:** SPDL orchestrates the execution of given functions, and the only requirement for the function is that it is univariate function. No requirements to change your algorithms/business logic to pipelining it with SPDL.\n\n# Monitoring &amp; Optimization\n\nSPDL exports detailed runtime statistics for each pipeline stage, making it easy to monitor throughput, resource usage, and identify bottlenecks. For more on production bottleneck analysis, see the¬†[Optimization Guide](https://l.facebook.com/l.php?u=https%3A%2F%2Ffacebookresearch.github.io%2Fspdl%2Fmain%2Foptimization_guide%2Fanalysis.html&amp;h=AT3Hi60CVKm_FzR6YfXpfbCm4kDWAVz5144UjUlM7PbqiIaM-7yzNqESVRJqwnHieAIYd_BoPn5GkG8-t66ZfmbEbyS3hyMsNXu-CqwngZ5FVw1ZdbXwhRVG1xYKrE4d81n7m4rYPBbi2XQBwy898DT9nWE).\n\n# Comparison\n\n* Unlike previously shared projects, the feature set is more specific to ML efficiency. (though the pipeline abstraction is generic, and library is agnostic to ML framework)\n* Supports single chain pipelining with different concurrency. Merging pipeline is also supported but not branching or general graph structure.",
    "author": "moto900",
    "timestamp": "2025-10-10T11:38:53",
    "url": "https://reddit.com/r/Python/comments/1o396tf/spdl_scalable_and_performant_data_loading/",
    "score": 15,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o35xlh",
    "title": "EPUBLib - New python library for creating and editing EPUB3 files",
    "content": "I wrote a python library to edit and create EPUB3 files.\n\n* Repository:¬†[gitlab.com/joaoseckler/epublib](http://gitlab.com/joaoseckler/epublib)\n* PyPi:¬†[pypi.org/project/epublib/](http://pypi.org/project/epublib/)\n* Documentation:¬†[epub-lib.readthedocs.io/en/latest/](http://epub-lib.readthedocs.io/en/latest/)\n\nAny suggestions and criticisms are welcome! And if you know any other places where people might be interested in this tool, please let me know.\n\n**What My Project Does:**\n\nIt is a library for creating and editing EPUB documents according to the EPUB3 specification. Example from the [documentation](https://epub-lib.readthedocs.io/en/latest/index.html#basic-usage):\n\n    from epublib import EPUB\n    \n    with EPUB(\"book.epub\") as book:\n        book.metadata.title = \"New title\"\n    \n        for doc in book.documents:\n            new_script = doc.soup.new_tag(\"script\", attrs={\"src\": \"../Misc/myscript.js\"})\n            doc.soup.head.append(new_script)\n    \n            new_heading = doc.soup.new_tag(\"h1\", string=\"New heading\")\n            doc.soup.body.insert(0, new_heading)\n    \n        book.update_manifest_properties()\n        book.write(\"book-modified.epub\")\n\nSee the¬†[usage section¬†](https://epub-lib.readthedocs.io/en/latest/#basic-usage)of the documentation for a more usage examples.\n\n**Target Audience:**\n\nPeople working with publishing digital books using the EPUB format.\n\n**Comparison:**\n\nThere is already an active python library called¬†[EbookLib](https://pypi.org/project/EbookLib/)¬†for handling EPUBs. A few things EPUBLib does differently:\n\n1. Handles the EPUB non-intrusively, e.g. won't regenerate the package document/metadata before writing, can edit toc without recreating the entire navigation document;\n2. Built-in XML parsing with BeautifulSoup;\n3. Extra features: rename files, remove files, spine reordering etc;\n4. Use nomenclature from the specification when possible (e.g. \"resource\" instead of \"item\").\n\n",
    "author": "joaoseckler",
    "timestamp": "2025-10-10T09:37:48",
    "url": "https://reddit.com/r/Python/comments/1o35xlh/epublib_new_python_library_for_creating_and/",
    "score": 18,
    "num_comments": 0,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o4167n",
    "title": "Sell me (and my team) on UV",
    "content": "I think UV is great so far, I only recently started using it. I would like to move myself and my team to using it as our official package manager, but I don‚Äôt really know the extent of why ‚Äúthis tool is better than venv/pip‚Äù. It was hard enough to convince them we should be using venv in the first place, but now I feel like I‚Äôm trying to introduce a tool that adds seemingly quite a bit more complexity.\n\nJust curious on all the benefits and what I can say to encourage the movement.\n\nThanks!",
    "author": "lukanixon",
    "timestamp": "2025-10-11T10:16:08",
    "url": "https://reddit.com/r/Python/comments/1o4167n/sell_me_and_my_team_on_uv/",
    "score": 0,
    "num_comments": 23,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o2yh3k",
    "title": "Vision Agents 0.1",
    "content": "First steps here, we've just released 0.1 of Vision Agents. [https://github.com/GetStream/Vision-Agents](https://github.com/GetStream/Vision-Agents)\n\n**What My Project Does**\n\nThe idea is that it makes it super simple to build vision agents, combining fast models like Yolo with Gemini/Openai realtime. We're going for low latency &amp; a completely open sdk. So you can use any vision model or video edge network. \n\nHere's an example of running live video through Yolo and then passing it to Gemini\n\n    agent = Agent(\n        edge=getstream.Edge(),\n        agent_user=agent_user,\n        instructions=\"Read @golf_coach.md\",\n        llm=openai.Realtime(fps=10),\n        #llm=gemini.Realtime(fps=1), # Careful with FPS can get expensive\n        processors=[ultralytics.YOLOPoseProcessor(model_path=\"yolo11n-pose.pt\")],\n    )\n\n**Target Audience**¬†\n\nVision AI is like chatgpt in 2022. It's really fun to see how it works and what's possible. Anything from live coaching, to sports, to physical therapy, robotics, drones etc. But it's not production quality yet. Gemini and OpenAI both hallucinate a ton for vision AI. It seems close to being viable though, especially fun to have it describe your surroundings etc.\n\n**Comparison**\n\nSimilar to Livekit Agents (livekit specific) and Pipecat (daily). We're going for open to all edge networks, low latency and with a focus on vision AI (voice works, but we're focused on live video)\n\nThis has been fun to work on with the team, finally at 0.1 :)\n\n\n\n",
    "author": "tschellenbach",
    "timestamp": "2025-10-10T04:37:10",
    "url": "https://reddit.com/r/Python/comments/1o2yh3k/vision_agents_01/",
    "score": 19,
    "num_comments": 2,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3ru8r",
    "title": "Intermediate-level project suggestions",
    "content": "I need intermediate-level project ideas that I can do with Python. Other languages can be added to the project as well, that‚Äôs not a problem. They need to look good on GitHub and on my CV.",
    "author": "learning_linuxsystem",
    "timestamp": "2025-10-11T03:07:39",
    "url": "https://reddit.com/r/Python/comments/1o3ru8r/intermediatelevel_project_suggestions/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o2u6gy",
    "title": "Fiatlight: Instantly turn Python functions into interactive GUI apps or workflows",
    "content": "**What Fiatlight Does**\n\n[Fiatlight](https://pthom.github.io/fiatlight_doc) is a Python toolkit that lets you build interactive graphical applications by providing an *automatic user interface* for functions and dataclasses. It is published under the MIT license.\n\nYou may think of Fiatlight as *\"ComfyUI for any type of data and functions\"*: easy visual and interactive pipelines for any domain of interest. You do not have to write any UI code, and you can connect multiple functions to build workflows instantly: the outputs flow from one function to the next\n\nUsers can then adjust every parameter of the functions and save/reload their work. \n\nFiatlight is built on top of Dear ImGui Bundle. It is very fast, and can provide feedback in real-time (at 120 FPS!). Since Dear ImGui Bundle is available via Pyodide, Fiatlight applications can be used locally or deployed as static web pages, without any server-side component.\n\nAs a prototyping tool, fiatlight does *not* provide full design control over the UI. It does however provide advanced viewer for many data types (standard python types, images, files, dataframes, matplotlib figures,  etc.), and is easily extensible.\n \n*Links:*\n- [Video tutorials](https://pthom.github.io/fiatlight_doc/flgt/video_tutorials.html)\n- [Documentation](https://pthom.github.io/fiatlight_doc)\n- [GitHub repo](https://github.com/pthom/fiatlight)\n\n\n**Target Audience**\n\n* Hobbyists wanting to create interactive applications quickly\n* Educators and instructors needing interactive tools for teaching programming or algorithms\n* Researchers who need shareable demos or visualizations of their work\n* Developers who want to fine tune their algorithms, with visual feedback\n* Library authors who want to showcase or demonstrate how to use and compose their functions\n* Data scientists and analysts wanting instant GUI dashboards for exploring data\n\n\n**Comparison**\n\n* Broader scope than ComfyUI\n* Often faster than streamlit or gradio (runs locally or serverless on a static web page)\n* LabVIEW is famous for data acquisition, hardware integration, and quick GUI building, but is expensive and highly niche/proprietary\n* Unreal Blueprints are widely used for visual scripting in games and rapid prototyping, but tightly coupled to Unreal Engine and less suitable for general Python/data workflows\n\n**Example**\n\nThe example below showcases a simple pipeline where the user edits the input float value, and automatically sees the output of each function. Widgets for each parameter are generated according to type and customized with attributes.\n\n```python\n# Our functions\ndef float_source(x: float = 1.0) -&gt; float:\n    return x\ndef double(x: float = 1.0) -&gt; float:\n    return 2 * x\n\n# Below, our GUI, where the user can edit the input for float_source, and see the output of both functions\n\nimport fiatlight as fl\n# Set range for slider\nfl.add_fiat_attributes(float_source, x__edit_type=\"slider\", x__range=(0.0, 10.0))  \n\n# Display a GUI for the composition of these two functions\nfl.run([float_source, double])  \n```\n",
    "author": "pstomi",
    "timestamp": "2025-10-10T00:12:39",
    "url": "https://reddit.com/r/Python/comments/1o2u6gy/fiatlight_instantly_turn_python_functions_into/",
    "score": 22,
    "num_comments": 0,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3h3uy",
    "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread",
    "content": "# Weekly Thread: Resource Request and Sharing üìö\n\nStumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!\n\n## How it Works:\n\n1. **Request**: Can't find a resource on a particular topic? Ask here!\n2. **Share**: Found something useful? Share it with the community.\n3. **Review**: Give or get opinions on Python resources you've used.\n\n## Guidelines:\n\n* Please include the type of resource (e.g., book, video, article) and the topic.\n* Always be respectful when reviewing someone else's shared resource.\n\n## Example Shares:\n\n1. **Book**: [\"Fluent Python\"](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \\- Great for understanding Pythonic idioms.\n2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \\- Excellent overview of Python's built-in data structures.\n3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \\- A deep dive into decorators.\n\n## Example Requests:\n\n1. **Looking for**: Video tutorials on web scraping with Python.\n2. **Need**: Book recommendations for Python machine learning.\n\nShare the knowledge, enrich the community. Happy learning! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-10T17:00:34",
    "url": "https://reddit.com/r/Python/comments/1o3h3uy/saturday_daily_thread_resource_request_and/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3otup",
    "title": "Neend some career advice",
    "content": "I am bpharm 4 yr student and I wanted to work in the field of programming and development I basically have no knowledge about programming skills I am currently 22 yr should I pursue with programming or I should just stick to the pharmacy ",
    "author": "Agreeable-Ad1713",
    "timestamp": "2025-10-10T23:54:33",
    "url": "https://reddit.com/r/Python/comments/1o3otup/neend_some_career_advice/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o2n119",
    "title": "Ergonomic Concurrency",
    "content": "**Project name:** Pipevine  \n**Project link:** [https://github.com/arrno/pipevine](https://github.com/arrno/pipevine)\n\n**What My Project Does**  \nPipevine is a lightweight async pipeline and worker-pool library for Python.  \nIt helps you compose concurrent dataflows with backpressure, retries, and cancellation.. without all the asyncio boilerplate.\n\n**Target Audience**  \nDevelopers who work with data pipelines, streaming, or CPU/IO-bound workloads in Python.  \nIt‚Äôs designed to be production-ready but lightweight enough for side projects and experimentation.\n\n**How to Get Started**\n\n    pip install pipevine\n\n    import asyncio\n    from pipevine import Pipeline, work_pool\n    \n    @work_pool(buffer=10, retries=3, num_workers=4)\n    async def process_data(item, state):\n        # Your processing logic here\n        return item * 2\n    \n    @work_pool(buffer=5, retries=1)\n    async def validate_data(item, state):\n        if item &lt; 0:\n            raise ValueError(\"Negative values not allowed\")\n        return item\n    \n    # Create and run pipeline\n    pipe = Pipeline(range(100)) &gt;&gt; process_data &gt;&gt; validate_data\n    result = await pipe.run()\n    \n\n**Feedback Requested**  \nI‚Äôd love thoughts on:\n\n* API ergonomics (does it feel Pythonic?)\n* Use cases where this could simplify your concurrency setup\n* Naming and documentation clarity",
    "author": "kwargs_",
    "timestamp": "2025-10-09T17:50:54",
    "url": "https://reddit.com/r/Python/comments/1o2n119/ergonomic_concurrency/",
    "score": 25,
    "num_comments": 15,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o3luxp",
    "title": "Automating the Upgrade to Python 3.14",
    "content": "I detailed the process I followed to get OpenAI‚Äôs codex cli  to upgrade a complex project with lots of dependencies to python 3.14 with uv:\n\nhttps://x.com/doodlestein/status/1976478297744699771?s=46\n\nCharlie Marsh retweeted it, so you can trust that it‚Äôs not a bunch of nonsense! Hope you guys find it useful.",
    "author": "dicklesworth",
    "timestamp": "2025-10-10T21:01:59",
    "url": "https://reddit.com/r/Python/comments/1o3luxp/automating_the_upgrade_to_python_314/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.32,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o24iwv",
    "title": "T Strings - Why there is no built in string rendering?",
    "content": "I like the idea of T Strings and here is a toy example:\n\n    name: str = 'Bob'\n    age: int = 30\n    template = t'Hello, {name}! You are {age} years old.'\n    print (template.strings)\n    print(template. interpolations)\n    print(template. values)\n    \n    ('Hello, ', '! You are ', ' years old.')\n    (Interpolation('Bob', 'name', None, ''), Interpolation(30, 'age', None, ''))\n    ('Bob', 30)\n\nBut why isn't there a\n\n`print(template.render)`\n\n`# ‚Üí '`Hello, Bob! You are 30 years old.'",
    "author": "UsernamesArentClever",
    "timestamp": "2025-10-09T05:25:45",
    "url": "https://reddit.com/r/Python/comments/1o24iwv/t_strings_why_there_is_no_built_in_string/",
    "score": 129,
    "num_comments": 65,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o29byq",
    "title": "Single Source of Truth - Generating ORM, REST, GQL, MCP, SDK and Tests from Pydantic",
    "content": "# What My Project Does\n\nI built an extensible AGPL-3.0 Python server framework on FastAPI and SQLAlchemy after getting sick of writing the same thing 4+ times in different ways. It takes your Pydantic models and automatically generates:\n\n* The ORM models with relationships\n* The migrations\n* FastAPI REST endpoints (CRUD - including batch, with relationship navigation and field specifiers)\n* GraphQL schema via Strawberry (including nested relationships)\n* MCP (Model Context Protocol) integration\n* SDK for other projects\n* Pytest tests for all of the above\n* Coming Soon: **External API federation** from third-party APIs directly into your models (including **into the GQL schema**) - [early preview screenshot](https://imgur.com/a/Cem1FOT)\n\n# Target Audience\n\nAnyone who's also tired of writing the same thing 4 different ways and wants to ship ASAP.\n\n# Comparison\n\nMost tools solve one piece of this problem:\n\n* `SQLModel` generates SQLAlchemy models from Pydantic but doesn't handle REST/GraphQL/tests\n* `Strawberry/Graphene Extensions` generate GraphQL schemas but require separate REST endpoints and ORM definitions\n* `FastAPI-utils/FastAPI-CRUD` generate REST endpoints but require manual GraphQL and testing setup\n* `Hasura/PostGraphile` auto-generate GraphQL from databases but aren't Python-native and don't integrate with your existing Pydantic models\n\nThis framework generates all of it - ORM, REST, GraphQL, SDK, and tests - from a single Pydantic definition. The API federation feature also lets you integrate external APIs (Stripe, etc.) directly into your generated GraphQL schema, which most alternatives can't do.\n\n# Links\n\nDocumentation available on GitHub and well-organized through Obsidian after cloning: [https://github.com/JamesonRGrieve/ServerFramework](https://github.com/JamesonRGrieve/ServerFramework)\n\nI also built a NextJS companion front end that's designed to be similarly extensible.\n\n[https://github.com/JamesonRGrieve/ClientFramework](https://github.com/JamesonRGrieve/ClientFramework)\n\nFeedback and contributions welcome!",
    "author": "ZephyrWarrior",
    "timestamp": "2025-10-09T08:41:09",
    "url": "https://reddit.com/r/Python/comments/1o29byq/single_source_of_truth_generating_orm_rest_gql/",
    "score": 64,
    "num_comments": 15,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o2uzxf",
    "title": "Loadouts for Genshin Impact v0.1.11 is OUT NOW with support for Genshin Impact v6.0 Phase 2",
    "content": "# About\n\nThis is a desktop application that allows travelers to manage their custom equipment of artifacts and weapons for playable characters and makes it convenient for travelers to calculate the associated statistics based on their equipment using the semantic understanding of how the gameplay works. Travelers can create their bespoke loadouts consisting of characters, artifacts and weapons and share them with their fellow travelers. Supported file formats include a human-readable¬†**Yet Another Markup Language (YAML)**¬†serialization format and a JSON-based¬†**Genshin Open Object Definition (GOOD)**¬†serialization format.\n\nThis project is currently in its beta phase and we are committed to delivering a quality experience with every release we make. If you are excited about the direction of this project and want to contribute to the efforts, we would greatly appreciate it if you help us boost the project visibility by¬†**starring the project repository**, address the releases by¬†**reporting the experienced errors**, choose the direction by¬†**proposing the intended features**, enhance the usability by¬†**documenting the project repository**, improve the codebase by¬†**opening the pull requests**¬†and finally, persist our efforts by¬†**sponsoring the development members**.\n\n# Technologies\n\n* Pydantic\n* Pytesseract\n* PySide6\n* Pillow\n\n# Updates\n\n[Loadouts for Genshin Impact v0.1.11](https://gridhead.net/loadouts-for-genshin-impact-v0-1-11-released/) is OUT NOW with the addition of support for recently released artifacts like¬†*Night of the Sky's Unveiling*¬†and¬†*Silken Moon's Serenade*, recently released characters like¬†*Aino*,¬†*Lauma*¬†and¬†*Flins*¬†and for recently released weapons like¬†*Blackmarrow Lantern*,¬†*Bloodsoaked Ruins*,¬†*Etherlight Spindlelute*,¬†*Master Key*,¬†*Moonweaver's Dawn*,¬†*Nightweaver's Looking Glass*,¬†*Propsector's Shovel*,¬†*Serenity's Call*¬†and¬†*Snare Hook*¬†from¬†**Genshin Impact Luna I or v6.0 Phase 2**. Take this FREE and OPEN SOURCE application for a spin using the links below to manage the custom equipment of artifacts and weapons for the playable characters.\n\n# Resources\n\n* [Loadouts for Genshin Impact - GitHub](https://github.com/gridhead/gi-loadouts?ref=gridhead.net)\n* [Loadouts for Genshin Impact - PyPI](https://pypi.org/project/gi-loadouts/?ref=gridhead.net)\n* [Loadouts for Genshin Impact v0.1.11](https://github.com/gridhead/gi-loadouts/releases/tag/0.1.11?ref=gridhead.net)\n   * [Executable for GNU/Linux distributions](https://github.com/gridhead/gi-loadouts/releases/download/0.1.11/gi-loadouts-0.1.11?ref=gridhead.net)\n   * [Executable for Microsoft Windows](https://github.com/gridhead/gi-loadouts/releases/download/0.1.11/gi-loadouts-0.1.11.exe?ref=gridhead.net)\n\n# Installation\n\nBesides its availability as a¬†[*repository package on PyPI*](https://pypi.org/project/gi-loadouts/?ref=gridhead.net)¬†and as an¬†[*archived binary on PyInstaller*](https://github.com/gridhead/gi-loadouts/releases/tag/0.1.10?ref=gridhead.net), Loadouts for Genshin Impact is now available as an¬†[*installable package on Fedora Linux*](https://src.fedoraproject.org/rpms/gi-loadouts?ref=gridhead.net). Travelers using¬†[*Fedora Linux 42 and above*](https://bodhi.fedoraproject.org/updates/?search=gi-loadouts&amp;user=t0xic0der&amp;ref=gridhead.net)¬†can install the package on their operating system by executing the following command.\n\n    $ sudo dnf install gi-loadouts --assumeyes --setopt=install_weak_deps=False\n\n# Appeal\n\nWhile allowing you to experiment with various builds and share them for later, Loadouts for Genshin Impact lets you take calculated risks by showing you the potential of your characters with certain artifacts and weapons equipped that you might not even own. Loadouts for Genshin Impact has been and always will be a free and open source software project, and we are committed to delivering a quality experience with every release we make.\n\n# Disclaimer\n\nWith an extensive suite of over 1503 diverse functionality tests and impeccable 100% source code coverage, we proudly invite auditors and analysts from MiHoYo and other organizations to review our free and open source codebase. This thorough transparency underscores our unwavering commitment to maintaining the fairness and integrity of the game.\n\nThe users of this ecosystem application can have complete confidence that their accounts are safe from warnings, suspensions or terminations when using this project. The ecosystem application ensures complete compliance with the terms of services and the regulations regarding third-party software established by MiHoYo for Genshin Impact.\n\nAll rights to Genshin Impact assets used in this project are reserved by miHoYo Ltd. and Cognosphere Pte., Ltd. Other properties belong to their respective owners.",
    "author": "t0xic0der",
    "timestamp": "2025-10-10T01:07:25",
    "url": "https://reddit.com/r/Python/comments/1o2uzxf/loadouts_for_genshin_impact_v0111_is_out_now_with/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o2j714",
    "title": "New Stockdex release",
    "content": "Hi reddit, \n\ni have released a new version of my open-source python package, Stockdex with new detailed documentation that you can find [here](https://ahnazary.github.io/stockdex/). I would love to hear your feedback and suggestions for future improvements.\n\n# What my project does?\n\nStockdex is a Python package that provides a simple interface to get financial data from various sources in pandas DataFrames and Plotly figures. It supports multiple data sources including Yahoo Finance, Digrin, Finviz, Macrotrends, and JustETF (for EU ETFs). \n\n# Main differences with other packages\n\n- Various data sources: Provides data from multiple sources (e.g. Yahoo Finance, Digrin, Finviz, Macrotrends, JustETF).\n- Historical data: Provides a wide time range of data, e.g. Digrin and Macrotrends sources provide historical data in a span of years, unlike other packages like `yfinance` which only 4 - 5 years of historical data at max. \n- Numerous data categories: Stockdex provides financials criteria including financial statements, earnings, dividends, stock splits, list of key executives, major shareholders and more.\n- Plotting capabilities (new feature): Plotting financial data using bar, line, and sankey plots. Detailed documentation with examples is available [here](https://ahnazary.github.io/stockdex/plots_and_figures.html).\n\n# Installation\n\nSimple pip install:\n\n```bash\npip install stockdex -U\n```\n\n# Target audience\n\nAnyone interested in financial data analysis.\n\n[Github repo](https://github.com/ahnazary/stockdex)\n[PyPI](https://pypi.org/project/stockdex/)",
    "author": "nginx26",
    "timestamp": "2025-10-09T14:57:47",
    "url": "https://reddit.com/r/Python/comments/1o2j714/new_stockdex_release/",
    "score": 7,
    "num_comments": 6,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o2lz3l",
    "title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays",
    "content": "# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è\n\nWelcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!\n\n## How it Works:\n\n1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.\n2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.\n3. **News &amp; Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.\n\n## Guidelines:\n\n* All topics should be related to Python or the /r/python community.\n* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).\n\n## Example Topics:\n\n1. **New Python Release**: What do you think about the new features in Python 3.11?\n2. **Community Events**: Any Python meetups or webinars coming up?\n3. **Learning Resources**: Found a great Python tutorial? Share it here!\n4. **Job Market**: How has Python impacted your career?\n5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!\n6. **Community Ideas**: Something you'd like to see us do? tell us.\n\nLet's keep the conversation going. Happy discussing! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-09T17:01:00",
    "url": "https://reddit.com/r/Python/comments/1o2lz3l/friday_daily_thread_rpython_meta_and_freetalk/",
    "score": 8,
    "num_comments": 2,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o34d3a",
    "title": "Reflex Build Free Tier Is Back!",
    "content": "A few days ago, **Reflex** re-introduced the **free tier** for their AI builder: **Reflex Build**.\n\n[Reflex Build](https://build.reflex.dev/) is a powerful, Python-first AI app builder built on top of the Reflex framework. It generates **production-ready, enterprise-grade web apps** ‚Äî all in Python.\n\nWhether you're building dashboards, internal tools, data viz apps, or just simple static pages, Reflex Build handles both **frontend and backend** in Python.\n\n# Main Features\n\n* **Plug-and-Play Integrations** Built-in support for popular tools like Databricks, Azure, Google Auth, and more ‚Äî no setup headaches.\n* **Polished UI with Tailwind 4** Clean, responsive components out of the box, styled with the latest Tailwind CSS.\n* **Private or Public Apps** Choose whether your apps are accessible to the world or kept private by default.\n* **Fast, Tuned Agent Runtime** A finely optimized agent gets your app logic up and running instantly.\n* **Built-In Testing** Ship with confidence using integrated testing tools for your app‚Äôs logic and behavior.\n* **Customizable Themes** Use predefined themes or build your own to match your brand or aesthetic.\n* **Markdown Support** Easily render rich content and documentation directly inside your apps.\n* **Mobile-Ready by Default** Fully responsive layouts ensure your app looks great on all devices.\n\nIf you build something neat, share a screenshot or a link, I‚Äôd love to see what you're making.\n\n",
    "author": "Wonderful-Today-497",
    "timestamp": "2025-10-10T08:39:41",
    "url": "https://reddit.com/r/Python/comments/1o34d3a/reflex_build_free_tier_is_back/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o2t7zq",
    "title": "aiar: A pypi CLI tool for managing self-extracting archives suited to LLMs",
    "content": "\n\nAnnouncing the release of aiar, a command-line utility for packaging/extracting file collections via a single archive.\n\nThe primary use case is to simplify sending and receiving multi-file projects in text-only environments, particularly when interacting with LLMs. LLMs will find it particularly easy to create these files since there is no need to escape any characters. In particular, you don‚Äôt even need the aiar tool if you trust your LLM to generate the self-extracting script for you.\n\n### **Key Features**\n\n* **Self-Contained:** Archives contain both the extraction logic and data. No external tools like zip or tar are required to unpack.  \n* **Multi-Format Output:** Generate self-extracting archives as Bash, Python, Node.js, or PowerShell scripts.  \n* **LLM-Centric Design:** Includes a data-only \"bare\" (.aiar) format, which is a simple text specification for LLMs to generate without writing any code.  (Not that LLMs are able to easily create bash aiar files.)  \n* **Extracting languages supported**: python, bash/zsh, nodejs and powershell and of course, no language ‚Äú.aiar‚Äù bare format which does not include the extraction code. Bare format files (as well as all the language specific archive formats) can be extracted using the aiar tool.\n\n### **Usage**\n\n**Installation:**\n\npip install aiar\n\n**Creating an Archive:**\n\n\\# Create a self-extracting scripts  \naiar create \\-o archive.py my\\_stuff/  \\# python\n\naiar create \\-o archive.bash my\\_stuff/ \\# bash or zsh\n\naiar create \\-o archive.ps1 my\\_stuff/ \\# powershell\n\n**Extracting an Archive using the built in script:**\n\npython archive.py \\# python\n\nbash archive.bash\n\npowershell archive.ps1\n\n\\# Or, extract any format (including bare) with the tool  \naiar extract archive.py\n\nFeedback and contributions are welcome.\n\n**Links:**\n\n* **PyPI:** [https://pypi.org/project/aiar/](https://pypi.org/project/aiar/)  \n* **GitHub:** [https://github.com/owebeeone/aiar](https://github.com/owebeeone/aiar)",
    "author": "GianniMariani",
    "timestamp": "2025-10-09T23:13:13",
    "url": "https://reddit.com/r/Python/comments/1o2t7zq/aiar_a_pypi_cli_tool_for_managing_selfextracting/",
    "score": 0,
    "num_comments": 11,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o2frc2",
    "title": "[Release] PyCopyX ‚Äî a Windows GUI around robocopy with precise selection, smart excludes",
    "content": "# What my project does\n\n* Dual-pane GUI (Source/Destination) built with **PySide6**\n* **Precise selection:** Ctrl-click and Shift-select in the Source pane\n   * **Files only** ‚Üí `robocopy SRC DST file1 file2 ‚Ä¶ /LEV:1` (**no recursion**), so subfolders don‚Äôt sneak in\n   * **Folders** ‚Üí `/E` (or **/MIR** in Mirror mode) per folder\n* **Preview-first:** shows the exact `robocopy` command (with `/L`) plus the resolved **/XD** (dir excludes) and **/XF** (file masks)\n* **Rock-solid excludes:** dir-name wildcards like `*env*` go to **/XD** *as-is* **and** are pre-expanded to absolute paths (defensive fallback if an environment is picky with wildcards). If `*Env` accidentally lands under file masks, PyCopyX also treats it as a dir-name glob and feeds it into **/XD**\n* **Thread control:** sensible default **/MT:16**, clamped 1‚Ä¶128\n* **Mirror safety:** Mirror is **folders-only**; if files are selected, it warns and aborts\n* **Safe Delete:** optional Recycle Bin delete via **Send2Trash**\n\n# Source Code\n\n* **Repo:** [https://github.com/rozsit/116\\_PyCopyX](https://github.com/rozsit/116_PyCopyX)\n* **Signed .exe:** [https://github.com/rozsit/116\\_PyCopyX/releases/tag/v1.0.0](https://github.com/rozsit/116_PyCopyX/releases/tag/v1.0.0) *Windows only (because* `robocopy`\\*). Packaging via PyInstaller. License: Apache-2.0.\\*\n\n# Target Audience\n\n* **Python developers** who need to copy/move/mirror **only parts** of a project tree while skipping virtualenvs, caches, and build artifacts\n* **Windows users** wanting a predictable, GUI-driven front end for `robocopy`\n* Teams handling **lots of small files** and wanting **multi-threaded** throughput with clear previews and safe defaults\n\n# Why?\n\nI often needed to copy/move/mirror only parts of a project tree‚Äîwithout dragging virtualenvs, caches, or build artifacts‚Äîand I wanted to see exactly what would happen before pressing ‚ÄúRun.‚Äù PyCopyX gives me that control while staying simple\n\n# Typical excludes (just works)\n\n* Virtual envs / caches / builds: `.venv`, `venv`, `__pycache__`, `.mypy_cache`, `.pytest_cache`, `.ruff_cache`, `build`, `dist`\n* Catch-all for env-like names (any depth): `*env*`\n* Git/IDE/Windows cruft: `.git`, `.idea`, `.vscode`, `Thumbs.db`, `desktop.ini`\n\n&gt;\n\n# Roadmap / feedback\n\n* Quick presets for common excludes, a TC-style **toggle selection** hotkey (Space), and QoL polish.\n* Feedback welcome on edge cases (very long paths, locked files, Defender interaction) and real-world exclude patterns.\n\nIssues/PRs welcome. Thanks! üôå",
    "author": "rozsit",
    "timestamp": "2025-10-09T12:44:15",
    "url": "https://reddit.com/r/Python/comments/1o2frc2/release_pycopyx_a_windows_gui_around_robocopy/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.58,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o1ipfo",
    "title": "Pydantic v2.12 release (Python 3.14)",
    "content": "[https://pydantic.dev/articles/pydantic-v2-12-release](https://pydantic.dev/articles/pydantic-v2-12-release)\n\n* Support for Python 3.14\n* New experimental `MISSING` sentinel\n* Support for PEP 728 (`TypedDict` with `extra_items`)\n* Preserve empty URL paths (`url_preserve_empty_path`)\n* Control timestamp validation unit (`val_temporal_unit`)\n* New `exclude_if` field option\n* New `ensure_ascii` JSON serialization option\n* Per-validation `extra` configuration\n* Strict version check for `pydantic-core`\n* JSON Schema improvements (regex for Decimal, custom titles, etc.)\n* Only latest mypy version officially supported\n* Slight validation performance improvement\n\n",
    "author": "__secondary__",
    "timestamp": "2025-10-08T11:26:53",
    "url": "https://reddit.com/r/Python/comments/1o1ipfo/pydantic_v212_release_python_314/",
    "score": 174,
    "num_comments": 14,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o2pcg8",
    "title": "pytrends not working, anyone same?",
    "content": "I tried to retrieve data from parents but found it not working. Is it still working? Has anyone used it recently?\nDon‚Äôt know whether I should continue debugging the script.\n",
    "author": "No_Winner_3807",
    "timestamp": "2025-10-09T19:42:40",
    "url": "https://reddit.com/r/Python/comments/1o2pcg8/pytrends_not_working_anyone_same/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o1ixgu",
    "title": "Good SQLBuilder for Python?",
    "content": "Hello!  \nI need to develop a small-medium forum with basic functionalities but I also need to make sure it supports DB swaps easily. I don't like to use ORMs because of their poor performance and I know SQL good enough not to care about it's conveinences. \n\nMany suggest SQLAlchemy Core but for 2 days I've been trying to read the official documentation. At first I thought \"woah, so much writing, must be very solid and straightforward\" only to realize I don't understand much of it. Or perhaps I don't have the patience.\n\nAnother alternative is PyPika which has a very small and clear documentation, easy to memorize the API after using it a few times and helps with translating an SQL query to multiple SQL dialects. \n\nJust curious, are there any other alternatives?  \nThanks!",
    "author": "yughiro_destroyer",
    "timestamp": "2025-10-08T11:34:58",
    "url": "https://reddit.com/r/Python/comments/1o1ixgu/good_sqlbuilder_for_python/",
    "score": 31,
    "num_comments": 37,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o1qqhu",
    "title": "Just launched a data dashboard showing when and how I take photos",
    "content": "What My Project Does:\n\nThis dashboard connects to my personal photo gallery database and turns my photo uploads into interactive analytics. It visualizes:\n\n- Daily photo activity\n- Most used camera models\n- Tag frequency and distribution\n- Thumbnail previews of recent uploads\n\nIt updates automatically with cached data and can be manually refreshed. Built with Python, Streamlit, Plotly, and SQLAlchemy, it allows me to explore my photography data in a visually engaging way.\n\n\nTarget Audience:\n\nThis is mainly a personal project, but it‚Äôs designed to be production-ready ‚Äî anyone with a photo collection stored in Postgres could adapt it. It‚Äôs suitable for hobbyists, photographers, or developers exploring data storytelling with Streamlit dashboards.\n\n\nComparison:\n\nUnlike basic photo galleries that only show images, this dashboard focuses on analytics and visualization. While platforms like Google Photos provide statistics, this project is:\n\nFully customizable\n\nOpen source (you can run or modify it yourself)\n\nDesigned for integrating custom metrics and tags\n\nBuilt using Python/Streamlit, making it easy to expand with new charts or interactive components\n\n\nüîó Live dashboard: https://a-k-holod-photo-stats.streamlit.app/\n\nüì∑ Gallery: https://a-k-holod-gallery.vercel.app/\n\nüíª Code: https://github.com/a-k-holod/photo-stats-dashboard\n\n\nIf you can't call 20 pictures gallery, then it's an album! ",
    "author": "ANDZELEK",
    "timestamp": "2025-10-08T16:29:09",
    "url": "https://reddit.com/r/Python/comments/1o1qqhu/just_launched_a_data_dashboard_showing_when_and/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o2gptj",
    "title": "Looking for *free* library or API to track market index",
    "content": "I‚Äôm looking for a library or api, preferably an api that will let me look at the DWCF market index. I tried the yfinance library but the firewall at work is blocking it and not letting connect it to properly. I also tried the alpha vantage api but they do not have any data on DWCF. I also need historical data, like 20+ years worth :).\n\nIs there anything available that someone can recommend?",
    "author": "boyIDK",
    "timestamp": "2025-10-09T13:20:41",
    "url": "https://reddit.com/r/Python/comments/1o2gptj/looking_for_free_library_or_api_to_track_market/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o1j2b3",
    "title": "My project to learn descriptors, rich comparison functions, asyncio, and type hinting",
    "content": "[https://github.com/gdchinacat/reactions](https://github.com/gdchinacat/reactions)\n\nI began this project a couple weeks ago based on an idea from another post (link below). I realized it would be a great way to learn some aspects of python I was not yet familiar with. \n\nThe idea is that you can implement classes with fields and then specify conditions for when methods should be called in reaction to those field changing. For example:\n\n\n    @dataclass\n    class Counter:\n        count: Field[int] = Field(-1)\n\n        @ count &gt;= 0\n        async def loop(self, field, old, new):\n                self.count += 1\n\n\n\nWhen count is changed to non negative number it will start counting. Type annotations and some execution management code has been removed. For working examples see src/test/examples directory.\n\nThe code has liberal todos in it to expand the functionality, but the core of it is stable, so I thought it was time to release it.\n\nPlease let me know your thoughts, or feel free to ask questions about how it works or why I did things a certain way. Thanks!\n\nThe post that got me thinking about this: [https://www.reddit.com/r/Python/comments/1nmta0f/i\\_built\\_a\\_full\\_programming\\_language\\_interpreter/](https://www.reddit.com/r/Python/comments/1nmta0f/i_built_a_full_programming_language_interpreter/)",
    "author": "gdchinacat",
    "timestamp": "2025-10-08T11:39:50",
    "url": "https://reddit.com/r/Python/comments/1o1j2b3/my_project_to_learn_descriptors_rich_comparison/",
    "score": 12,
    "num_comments": 2,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o17ogm",
    "title": "TOML marries Argparse",
    "content": "I wanted to share a small Python library I havee been working on that might help with managing ML experiment configurations.\n\nJump here directly to the repository: [https://github.com/florianmahner/tomlparse](https://github.com/florianmahner/tomlparse)\n\n**What is it?**\n\ntomlparse is a lightweight wrapper around Python's argparse that lets you use TOML files for configuration management while keeping all the benefits of argparse. It is designed to make hyperparameter management less painful for larger projects.\n\n**Why TOML?**\n\nIf you've been using YAML or JSON for configs, TOML offers some nice advantages:\n\n* Native support for dates, floats, integers, booleans, and arrays\n* Clear, readable syntax without significant whitespace issues\n* Official Python standard library support (tomllib in Python 3.11+)\n* Comments that actually stay comments\n\n**Key Features**\n\nThe library adds minimal overhead to your existing argparse workflow:\n\n    import tomlparse\n    \n    parser = tomlparse.ArgumentParser()\n    parser.add_argument(\"--foo\", type=int, default=0)\n    parser.add_argument(\"--bar\", type=str, default=\"\")\n    args = parser.parse_args()\n\nThen run with:\n\n    python experiment.py --config \"example.toml\"\n\n**What I find useful:**\n\n1. **Table support** \\- Organize configs into sections and switch between them easily\n2. **Clear override hierarchy** \\- CLI args &gt; TOML table values &gt; TOML root values &gt; defaults\n3. **Easy experiment tracking** \\- Keep different TOML files for different experiment runs\n\n**Example use case with tables:**\n\n    # This is a TOML File\n    # Parameters without a preceding [] are not part of a table (called root-table)\n    foo = 10\n    bar = \"hello\"\n    \n    # These arguments are part of the table [general]\n    [general]\n    foo = 20\n    \n    # These arguments are part of the table [root]\n    [root]\n    bar = \"hey\"\n\nYou can then specify which table to use:\n\n    python experiment.py --config \"example.toml\" --table \"general\"\n    # Returns: {\"foo\": 20, \"bar\": \"hello\"}\n    \n    python experiment.py --config \"example.toml\" --table \"general\" --root-table \"root\"\n    # Returns: {\"foo\": 20, \"bar\": \"hey\"}\n\nAnd you can always override from the command line:\n\n    python experiment.py --config \"example.toml\" --table \"general\" --foo 100\n\n**Install:**\n\n    pip install tomlparse\n\n**GitHub:** [https://github.com/florianmahner/tomlparse](https://github.com/florianmahner/tomlparse)\n\nWould love to hear thoughts or feedback if anyone tries it out! It has been useful for my own work, but I am sure there are edge cases I haven't considered.\n\n*Disclaimer: This is a personal project, not affiliated with any organization.*",
    "author": "Prestigious-Nerve851",
    "timestamp": "2025-10-08T04:20:18",
    "url": "https://reddit.com/r/Python/comments/1o17ogm/toml_marries_argparse/",
    "score": 39,
    "num_comments": 14,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o150hi",
    "title": "Use uv with Python 3.14 and IIS sites",
    "content": "After the upgrade to Python 3.14, there's no longer the concept of a \"system-wide\" Python. Therefore, when you create a virtual environment, the hardlinks (if they are really hardlinks) point to `%LOCALAPPDATA%\\Python\\pythoncore-3.14-64\\python.exe`. The problem is that if you have a virtual environment for an IIS website, e.g. spanandeggs.example.com, this will by default run with the virtual user IISAPPPOOL\\spamandeggs.example.com. And that user most certainly doesn't have access to your personal `%LOCALAPPDATA%` directory. So, if you try to run the site, you'll get this error:\n\n`did not find executable at '¬´%LOCALAPPDATA%¬ª\\Python\\pythoncore-3.14-64\\python.exe': Access is denied.`\n\nTo make this work I've had to:\n\n1. Download python to a separate directory (`uv python install 3.14 --install-dir C:\\python\\`)\n2. Sync the virtual environment with the new Python version: `uv sync --upgrade --python C:\\Python\\cpython-3.14.0-windows-x86_64-none\\`)\n\nFor completeness, where's an example web.config to make a site run natively under IIS (this assumes there's an app.py). I'm not 100% sure that all environment variables are required:\n\n    &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n    &lt;configuration&gt;\n        &lt;system.webServer&gt;\n            &lt;modules runAllManagedModulesForAllRequests=\"true\" /&gt;\n            &lt;handlers&gt;\n                &lt;clear/&gt;\n                &lt;add name=\"httpPlatformHandler\" path=\"*\" verb=\"*\" modules=\"httpPlatformHandler\" resourceType=\"Unspecified\" requireAccess=\"Script\" /&gt;\n            &lt;/handlers&gt;\n            &lt;httpPlatform processPath=\".\\.venv\\Scripts\\python.exe\" arguments=\"-m flask run --port %HTTP_PLATFORM_PORT%\"&gt;\n                &lt;environmentVariables&gt;\n                    &lt;environmentVariable name=\"SERVER_PORT\" value=\"%HTTP_PLATFORM_PORT%\" /&gt;\n                    &lt;environmentVariable name=\"PYTHONPATH\" value=\".\" /&gt;\n                    &lt;environmentVariable name=\"PYTHONHOME\" value=\"\" /&gt;\n                    &lt;environmentVariable name=\"VIRTUAL_ENV\" value=\".venv\" /&gt;\n                    &lt;environmentVariable name=\"PATH\" value=\".venv\\Scripts\" /&gt;\n                &lt;/environmentVariables&gt;\n            &lt;/httpPlatform&gt;\n        &lt;/system.webServer&gt;\n    &lt;/configuration&gt;",
    "author": "gschizas",
    "timestamp": "2025-10-08T01:36:50",
    "url": "https://reddit.com/r/Python/comments/1o150hi/use_uv_with_python_314_and_iis_sites/",
    "score": 52,
    "num_comments": 36,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o17iwq",
    "title": "Interesting discussion to shift Apache's Arrow release cycle forward to align with Python's release",
    "content": "There's an interesting discussion in the PyArrow community about shifting their release cycle to better align with Python's annual release schedule. Currently, PyArrow often becomes the last major dependency to support new Python versions, with support arriving about a month after Python's stable release, which creates a bottleneck for the broader data engineering ecosystem.\n\nThe proposal suggests moving Arrow's feature freeze from early October to early August, shortly after Python's ABI-stable release candidate drops in late July, which would flip the timeline so PyArrow wheels are available around a month before Python's stable release rather than after.\n\n[https://github.com/apache/arrow/issues/47700](https://github.com/apache/arrow/issues/47700)",
    "author": "Balance-",
    "timestamp": "2025-10-08T04:11:59",
    "url": "https://reddit.com/r/Python/comments/1o17iwq/interesting_discussion_to_shift_apaches_arrow/",
    "score": 33,
    "num_comments": 2,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0gfp1",
    "title": "Python 3.14 Released",
    "content": "https://docs.python.org/3.14/whatsnew/3.14.html\n\nInterpreter improvements:\n\n* PEP 649 and PEP 749: Deferred evaluation of annotations\n* PEP 734: Multiple interpreters in the standard library\n* PEP 750: Template strings\n* PEP 758: Allow except and except* expressions without brackets\n* PEP 765: Control flow in finally blocks\n* PEP 768: Safe external debugger interface for CPython\n* A new type of interpreter\n* Free-threaded mode improvements\n* Improved error messages\n* Incremental garbage collection\n\nSignificant improvements in the standard library:\n\n* PEP 784: Zstandard support in the standard library\n* Asyncio introspection capabilities\n* Concurrent safe warnings control\n* Syntax highlighting in the default interactive shell, and color output in several standard library CLIs\n\nC API improvements:\n\n* PEP 741: Python configuration C API\n\nPlatform support:\n\n* PEP 776: Emscripten is now an officially supported platform, at tier 3.\n\nRelease changes:\n\n* PEP 779: Free-threaded Python is officially supported\n* PEP 761: PGP signatures have been discontinued for official releases\n* Windows and macOS binary releases now support the experimental just-in-time compiler\n* Binary releases for Android are now provided",
    "author": "chinawcswing",
    "timestamp": "2025-10-07T07:32:22",
    "url": "https://reddit.com/r/Python/comments/1o0gfp1/python_314_released/",
    "score": 1042,
    "num_comments": 107,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o1rea5",
    "title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",
    "content": "# Weekly Thread: Professional Use, Jobs, and Education üè¢\n\nWelcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.\n\n---\n\n## How it Works:\n\n1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.\n2. **Education Q&amp;A**: Ask or answer questions about Python courses, certifications, and educational resources.\n3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.\n\n---\n\n## Guidelines:\n\n- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.\n- Keep discussions relevant to Python in the professional and educational context.\n  \n---\n\n## Example Topics:\n\n1. **Career Paths**: What kinds of roles are out there for Python developers?\n2. **Certifications**: Are Python certifications worth it?\n3. **Course Recommendations**: Any good advanced Python courses to recommend?\n4. **Workplace Tools**: What Python libraries are indispensable in your professional work?\n5. **Interview Tips**: What types of Python questions are commonly asked in interviews?\n\n---\n\nLet's help each other grow in our careers and education. Happy discussing! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-08T17:00:33",
    "url": "https://reddit.com/r/Python/comments/1o1rea5/thursday_daily_thread_python_careers_courses_and/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o18p3a",
    "title": "Feature Store Summit - 2025 - Free and Online.",
    "content": "**Hello Pytonistas !**  \n  \nWe are organising the Feature Store Summit. An annual online event where we invite some of the most technical speakers from some of the world‚Äôs most advanced engineering teams to talk about their infrastructure for AI, ML and oftentime how this fits in the pythonic ecosystem.   \n  \n**Some of this year‚Äôs speakers are coming from:**  \nUber, Pinterest, Zalando, Lyft, Coinbase, Hopsworks and More!\n\n**What to Expect:**  \nüî• Real-Time Feature Engineering at scale  \nüî•¬†Vector Databases &amp; Generative AI in production  \nüî•¬†The balance of Batch &amp; Real-Time workflows  \nüî•¬†Emerging trends driving the evolution of Feature Stores in 2025\n\n**When:**  \nüóìÔ∏è¬†October 14th  \n‚è∞¬†Starting 8:30AM PT  \n‚è∞ Starting 5:30PM CET  \n  \nLink;¬†[https://www.featurestoresummit.com/register](https://www.featurestoresummit.com/register?utm_source=reddit)\n\nPS; it is free, online, and if you register you will be receiving the recorded talks afterward! \n\n  \n",
    "author": "logicalclocks",
    "timestamp": "2025-10-08T05:11:06",
    "url": "https://reddit.com/r/Python/comments/1o18p3a/feature_store_summit_2025_free_and_online/",
    "score": 11,
    "num_comments": 2,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0jr55",
    "title": "My favorite new features in Python 3.14",
    "content": "I have been using Python 3.14 as my primary version while teaching and writing one-off scripts for over 6 months. My favorite features are the ones that immediately impact newer Python users.\n\nMy favorite new features in Python 3.14:\n\n* All the color (REPL &amp; PDB syntax highlighting, argparse help, unittest, etc.)\n* pathlib's copy &amp; move methods: no more need for shutil\n* date.strptime: no more need for datetime.strptime().date()\n* uuid7: random but also orderable/sortable\n* argparse choice typo suggestions\n* t-strings: see [awesome-t-strings](https://github.com/t-strings/awesome-t-strings) for libraries using them\n* concurrent subinterpreters: the best of both threading &amp; multiprocessing\n* import tab completion\n\nI recorded [a 6 minute demo](https://youtu.be/bcMXCxefUPk) of these features and [wrote an article on them](https://pym.dev/python314/).",
    "author": "treyhunner",
    "timestamp": "2025-10-07T09:34:51",
    "url": "https://reddit.com/r/Python/comments/1o0jr55/my_favorite_new_features_in_python_314/",
    "score": 390,
    "num_comments": 50,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0wajw",
    "title": "T-Strings: Worth using for SQL in Python 3.14?",
    "content": "This video breaks down one of the proposed use-cases for the new t-string feature from PEP 750: SQL sanitization. Handling SQL statements is not new for Python, so t-strings are compared to the standard method of manually inserting placeholder characters for safe SQL queries:\n\n[https://youtu.be/R5ov9SbLaYc](https://youtu.be/R5ov9SbLaYc)\n\nThe tl;dw: in some contexts, switching to t-string notation makes queries significantly easier to read, debug, and manage. But for simple SQL statements with only one or two parameters, hand-placing parameters in the query will still be the simplest standard.\n\nWhat do you think about using t-strings for handling complex SQL statements in Python programs?",
    "author": "simplysalamander",
    "timestamp": "2025-10-07T17:35:57",
    "url": "https://reddit.com/r/Python/comments/1o0wajw/tstrings_worth_using_for_sql_in_python_314/",
    "score": 74,
    "num_comments": 20,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o211ed",
    "title": "Blogpost: Python‚Äôs Funniest Features: A Developer‚Äôs Field Guide",
    "content": "I hope this is okay. I thought I'd share this [latest take](https://medium.com/@mskadu/pythons-funniest-features-a-developer-s-field-guide-34fc7f7ee9b5) on the funnies that exist in our fav language - a bit of a departure from the usual tech-tech chat that happens here.\n\n*PS*: Fwiw, it's behind a paywall and a loginwall. If you don't have a paid account on Medium (edit: or _don't_ want to create one), the visible part of the post should have a link to view it for free and without needing an account. Most (if not all) of my posts should be so. Let me know if aren't able to spot it.",
    "author": "Mskadu",
    "timestamp": "2025-10-09T02:05:29",
    "url": "https://reddit.com/r/Python/comments/1o211ed/blogpost_pythons_funniest_features_a_developers/",
    "score": 0,
    "num_comments": 23,
    "upvote_ratio": 0.3,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0zsa2",
    "title": "Pyloid: Electron for Python Developer ‚Ä¢ Modern Web-based desktop app framework",
    "content": "**I updated so many features!**  \n**I'm excited to introduce this project! üéâ**\n\n[Pyloid](https://pyloid.com/): Electron for Python Developer ‚Ä¢ Modern Web-based desktop app framework\n\nthis project based on `Pyside6` and `QtWebengine`\n\nthis project is an alternative to `Electron` for python dev\n\n**What My Project Does:** With this project, you can build any desktop app.\n\n**Target Audience:** All desktop app developer.\n\n# Key Features\n\n* **All Frontend Frameworks**¬†are supported\n* **All Backend Frameworks**¬†are supported\n* **All features necessary**¬†for a desktop application are implemented\n* **Cross-Platform Support**¬†(Windows, macOS, Linux)\n* **Many Built-in Tools**¬†(Builder, Server, Tray, Store, Timer, Monitor, Optimizer, etc.)\n\n# simple example 1\n\n`pip install pyloid`\n\n    from pyloid import Pyloid\n    \n    app = Pyloid(app_name=\"Pyloid-App\")\n    \n    win = app.create_window(title=\"hello\")\n    win.load_html(\"&lt;h1&gt;Hello, Pyloid!&lt;/h1&gt;\")\n    \n    win.show_and_focus()\n\n# simple example 2 (with React)\n\n    from pyloid.serve import pyloid_serve\n    from pyloid import Pyloid\n    \n    app = Pyloid(app_name=\"Pyloid-App\")\n    \n    app.set_icon(get_production_path(\"src-pyloid/icons/icon.png\"))\n    \n    \n    if is_production():\n    ¬† ¬† url = pyloid_serve(directory=get_production_path(\"dist-front\"))\n    ¬† ¬† win = app.create_window(title=\"hello\")\n    ¬† ¬† win.load_url(url)\n    else:\n    ¬† ¬† win = app.create_window(\n    ¬†       title=\"hello-dev\",\n    ¬† ¬† ¬† ¬† dev_tools=True¬† ¬† \n        )\n    ¬† ¬† win.load_url(\"http://localhost:5173\")\n    \n    win.show_and_focus()\n    \n    app.run()\n\n# Get started\n\n[You need 3 tools (python, node.js, uv)](https://pyloid.com/docs/prequisites)\n\n    npm create pyloid-app@latest\n\nif you want more info,  [https://pyloid.com/](https://pyloid.com/)\n\n# Links\n\n* site &amp; docs : [https://pyloid.com/](https://pyloid.com/)\n* github: [https://github.com/pyloid/pyloid](https://github.com/pyloid/pyloid)",
    "author": "Ok-Method-9403",
    "timestamp": "2025-10-07T20:24:18",
    "url": "https://reddit.com/r/Python/comments/1o0zsa2/pyloid_electron_for_python_developer_modern/",
    "score": 21,
    "num_comments": 0,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o1kd5j",
    "title": "Building SimpleGrad: A Deep Learning Framework Between Tinygrad and PyTorch",
    "content": "I just built¬†**SimpleGrad**, a Python deep learning framework that sits¬†**between Tinygrad and PyTorch**. It‚Äôs simple and educational like Tinygrad, but¬†**fully functional**¬†with tensors, autograd, linear layers, activations, and optimizers like PyTorch.\n\nIt‚Äôs¬†**open-source**, and I‚Äôd love for the community to¬†**test it, experiment, or contribute**.\n\nCheck it out here:¬†[https://github.com/mohamedrxo/simplegrad](https://github.com/mohamedrxo/simplegrad)\n\nWould love to hear your feedback and see what cool projects people build with it!",
    "author": "PerspectiveJolly952",
    "timestamp": "2025-10-08T12:27:31",
    "url": "https://reddit.com/r/Python/comments/1o1kd5j/building_simplegrad_a_deep_learning_framework/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0u0ts",
    "title": "Is there conventional terminology for \"non-callable attribute\"",
    "content": "I am writing what I suppose could be considered a tutorial, and I would like to use a term for non-callable attributes that will be either be familiar to the those who have some familiarity with classes or at least understandable to those learners without additional explanation. The terminology does not need to be precise.\n\nSo far I am just using the term \"attribute\" ambiguously. Sometimes I am using to to refer attributes of an object that aren't methods and sometimes I am using it in the more technical sense that includes methods. I suspect that this is just what I will have to keep doing and rely on the context to to disambiguate.\n\n**Update**: ‚Äúmember variable‚Äù is the term I was looking for. Thank you, u/PurepointDog/\n\n",
    "author": "jpgoldberg",
    "timestamp": "2025-10-07T15:55:23",
    "url": "https://reddit.com/r/Python/comments/1o0u0ts/is_there_conventional_terminology_for_noncallable/",
    "score": 37,
    "num_comments": 19,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0echb",
    "title": "Bringing NumPy's type-completeness score to nearly 90%",
    "content": "&gt; Because NumPy is one of the most downloaded packages in the Python ecosystem, any incremental improvement can have a large impact on the data science ecosystem. In particular, improvements related to static typing can improve developer experience and help downstream libraries write safer code. We'll tell the story about how we (Quansight Labs, with support from Meta's Pyrefly team) helped bring its type-completeness score to nearly 90% from an initial 33%.\n\nFull blog post: https://pyrefly.org/blog/numpy-type-completeness/",
    "author": "BeamMeUpBiscotti",
    "timestamp": "2025-10-07T06:10:07",
    "url": "https://reddit.com/r/Python/comments/1o0echb/bringing_numpys_typecompleteness_score_to_nearly/",
    "score": 185,
    "num_comments": 35,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o1pqgf",
    "title": "Craziest python projects you know?",
    "content": "Trying to find ideas for some cool python projects. I can‚Äôt think of anything. If you have any really cool not too hard projects, tell me!",
    "author": "CartographerOld3769",
    "timestamp": "2025-10-08T15:45:00",
    "url": "https://reddit.com/r/Python/comments/1o1pqgf/craziest_python_projects_you_know/",
    "score": 0,
    "num_comments": 23,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o085tj",
    "title": "I pushed Python to 20,000 requests sent/second. Here's the code and kernel tuning I used.",
    "content": "  \n**What My Project Does:** Push Python to 20k req/sec.\n\n**Target Audience**: People who need to make a ton of requests.\n\n**Comparison**: Previous articles I found ranged from 50-500 requests/sec with python, figured i'd give an update to where things are at now.\n\nI wanted to share a personal project exploring the limits of Python for high-throughput network I/O. My clients would always say \"lol no python, only go\", so I wanted to see what was actually possible.\n\nAfter a lot of tuning, I managed to get a stable **\\~20,000 requests/second** from a single client machine.\n\nThe code itself is based on `asyncio` and a library called `rnet`, which is a Python wrapper for the high-performance Rust library `wreq`. This lets me get the developer-friendly syntax of Python with the raw speed of Rust for the actual networking.\n\nThe most interesting part wasn't the code, but the OS tuning. The default kernel settings on Linux are nowhere near ready for this kind of load. The application would fail instantly without these changes.\n\nHere are the most critical settings I had to change on both the client and server:\n\n* Increased Max File Descriptors: Every socket is a file. The default limit of 1024 is the first thing you'll hit.ulimit -n 65536\n* Expanded Ephemeral Port Range: The client needs a large pool of ports to make outgoing connections from.net.ipv4.ip\\_local\\_port\\_range = 1024 65535\n* Increased Connection Backlog: The server needs a bigger queue to hold incoming connections before they are accepted. The default is tiny.net.core.somaxconn = 65535\n* Enabled TIME\\_WAIT Reuse: This is huge. It allows the kernel to quickly reuse sockets that are in a TIME\\_WAIT state, which is essential when you're opening/closing thousands of connections per second.net.ipv4.tcp\\_tw\\_reuse = 1\n\nI've open-sourced the entire test setup, including the client code, a simple server, and the full tuning scripts for both machines. You can find it all here if you want to replicate it or just look at the code:\n\n**GitHub Repo:** [**https://github.com/lafftar/requestSpeedTest**](https://github.com/lafftar/requestSpeedTest)\n\nOn an 8-core machine, this setup hit \\~15k req/s, and it scaled to \\~20k req/s on a 32-core machine. Interestingly, the CPU was never fully maxed out, so the bottleneck likely lies somewhere else in the stack.\n\nI'll be hanging out in the comments to answer any questions. Let me know what you think!\n\n**Blog Post (I go in a little more detail)**: [https://tjaycodes.com/pushing-python-to-20000-requests-second/](https://tjaycodes.com/pushing-python-to-20000-requests-second/)",
    "author": "Lafftar",
    "timestamp": "2025-10-07T00:27:57",
    "url": "https://reddit.com/r/Python/comments/1o085tj/i_pushed_python_to_20000_requests_sentsecond/",
    "score": 171,
    "num_comments": 57,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0m9yt",
    "title": "Tired of Messy WebSockets? I Built Chanx to End the If/Else Hell in Real-Time Python App",
    "content": "After 3 years of building AI agents and real-time applications across Django and FastAPI, I kept hitting the same wall: WebSocket development was a mess of if/else chains, manual validation, and zero documentation. When working with FastAPI, I'd wish for a powerful WebSocket framework that could match the elegance of its REST API development. To solve this once and for all, I built Chanx ‚Äì the WebSocket toolkit I wish existed from day one.\n\n## What My Project Does\n\n## The Pain Point Every Python Developer Knows\n\nBuilding WebSocket apps in Python is a nightmare we all share:\n\n```python\n# The usual FastAPI WebSocket mess\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    while True:\n        data = await websocket.receive_json()\n        action = data.get(\"action\")\n        if action == \"echo\":\n            await websocket.send_json({\"action\": \"echo_response\", \"payload\": data.get(\"payload\")})\n        elif action == \"ping\":\n            await websocket.send_json({\"action\": \"pong\", \"payload\": None})\n        elif action == \"join_room\":\n            # Manual room handling...\n        # ... 20 more elif statements\n```\n\nPlus manual validation, zero documentation, and trying to send events from Django views or FastAPI endpoints to WebSocket clients? Pure pain.\n\n**Chanx eliminates all of this** with decorator automation that works consistently across frameworks.\n\n## How Chanx Transforms Your Code\n\n```python\nfrom typing import Literal\nfrom pydantic import BaseModel\nfrom chanx.core.decorators import ws_handler, event_handler, channel\nfrom chanx.core.websocket import AsyncJsonWebsocketConsumer\nfrom chanx.messages.base import BaseMessage\n\n# Define your message types (action-based routing)\nclass EchoPayload(BaseModel):\n    message: str\n\nclass NotificationPayload(BaseModel):\n    alert: str\n    level: str = \"info\"\n\n# Client Messages\nclass EchoMessage(BaseMessage):\n    action: Literal[\"echo\"] = \"echo\"\n    payload: EchoPayload\n\n# Server Messages\nclass EchoResponseMessage(BaseMessage):\n    action: Literal[\"echo_response\"] = \"echo_response\"\n    payload: EchoPayload\n\nclass NotificationMessage(BaseMessage):\n    action: Literal[\"notification\"] = \"notification\"\n    payload: NotificationPayload\n\n# Events (for server-side broadcasting)\nclass SystemNotifyEvent(BaseMessage):\n    action: Literal[\"system_notify\"] = \"system_notify\"\n    payload: NotificationPayload\n\n@channel(name=\"chat\", description=\"Real-time chat API\")\nclass ChatConsumer(AsyncJsonWebsocketConsumer):\n    @ws_handler(summary=\"Handle echo messages\", output_type=EchoResponseMessage)\n    async def handle_echo(self, message: EchoMessage) -&gt; None:\n        await self.send_message(EchoResponseMessage(payload=message.payload))\n\n    @event_handler(output_type=NotificationMessage)\n    async def handle_system_notify(self, event: SystemNotifyEvent) -&gt; NotificationMessage:\n        return NotificationMessage(payload=event.payload)\n```\n\n**Key features:**\n- üéØ **Decorator-based routing** - No more if/else chains\n- üìö **Auto AsyncAPI docs** - Generate comprehensive WebSocket API documentation\n- üîí **Type safety** - Full mypy/pyright support with Pydantic validation\n- üåê **Multi-framework** - Django Channels, FastAPI, any ASGI framework\n- üì° **Event broadcasting** - Send events from HTTP views, background tasks, anywhere\n- üß™ **Enhanced testing** - Framework-specific testing utilities\n\n## Target Audience\n\n**Chanx is production-ready** and designed for:\n- Python developers building real-time features (chat, notifications, live updates)\n- Django teams wanting to eliminate WebSocket boilerplate\n- FastAPI projects needing robust WebSocket capabilities\n- Full-stack applications requiring seamless HTTP ‚Üî WebSocket event broadcasting\n- Type-safety advocates who want comprehensive IDE support for WebSocket development\n- API-first teams needing automatic AsyncAPI documentation\n\nBuilt from 3+ years of experience developing AI chat applications, real-time voice recording systems, and live notification platforms - solving every pain point I encountered along the way.\n\n## Comparison\n\n**vs Raw Django Channels/FastAPI WebSockets:**\n- ‚ùå Manual if/else routing ‚Üí ‚úÖ Automatic decorator-based routing\n- ‚ùå Manual validation ‚Üí ‚úÖ Automatic Pydantic validation\n- ‚ùå No documentation ‚Üí ‚úÖ Auto-generated AsyncAPI 3.0 specs\n- ‚ùå Complex event sending ‚Üí ‚úÖ Simple broadcasting from anywhere\n\n**vs Broadcaster:**\n- Broadcaster is just pub/sub messaging\n- Chanx provides complete WebSocket consumer framework with routing, validation, docs\n\n**vs FastStream:**\n- FastStream focuses on message brokers (Kafka, RabbitMQ, etc.) for async messaging\n- Chanx focuses on real-time WebSocket applications with decorator-based routing, auto-validation, and seamless HTTP integration\n- Different use cases: FastStream for distributed systems, Chanx for interactive real-time features\n\n## Installation\n\n```bash\n# Django Channels\npip install \"chanx[channels]\"  # Includes Django, DRF, Channels Redis\n\n# FastAPI\npip install \"chanx[fast_channels]\"  # Includes FastAPI, fast-channels\n\n# Any ASGI framework\npip install chanx  # Core only\n```\n\n## Real-World Usage\n\nSend events from anywhere in your application:\n\n```python\n# From FastAPI endpoint\n@app.post(\"/api/posts\")\nasync def create_post(post_data: PostCreate):\n    post = await create_post_logic(post_data)\n\n    # Instantly notify WebSocket clients\n    await ChatConsumer.broadcast_event(\n        NewPostEvent(payload={\"title\": post.title}),\n        groups=[\"feed_updates\"]\n    )\n    return {\"status\": \"created\"}\n\n# From Django views, Celery tasks, management scripts\nChatConsumer.broadcast_event_sync(\n    NotificationEvent(payload={\"alert\": \"System maintenance\"}),\n    groups=[\"admin_users\"]\n)\n```\n\n**Links:**\n- üîó **GitHub:** https://github.com/huynguyengl99/chanx\n- üì¶ **PyPI:** https://pypi.org/project/chanx/\n- üìñ **Documentation:** https://chanx.readthedocs.io/\n- üöÄ **Django Examples:** https://chanx.readthedocs.io/en/latest/examples/django.html\n- ‚ö° **FastAPI Examples:** https://chanx.readthedocs.io/en/latest/examples/fastapi.html\n\nGive it a try in your next project and let me know what you think! If it saves you development time, a ‚≠ê on GitHub would mean the world to me. Would love to hear your feedback and experiences!\n",
    "author": "huygl99",
    "timestamp": "2025-10-07T11:04:07",
    "url": "https://reddit.com/r/Python/comments/1o0m9yt/tired_of_messy_websockets_i_built_chanx_to_end/",
    "score": 19,
    "num_comments": 9,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o04g6v",
    "title": "I benchmarked 5 different FastAPI file upload methods (1KB to 1GB)",
    "content": "### What my project does\n\nI've created a [benchmark](https://github.com/fedirz/fastapi-file-upload-benchmark) to test 5 different ways to handle file uploads in FastAPI across 21 file sizes from 1KB to 1GB:\n- `File()` - sync and async variants\n- `UploadFile` - sync and async variants\n- `request.stream()` - async streaming\n\nKey findings for large files (128MB+):\n- `request.stream()` hits ~1500 MB/s throughput vs ~750 MB/s for the others\n- Additional memory used: `File()` consumes memory equal to the file size (1GB file = 1GB RAM), while `request.stream()` and `UploadFile` don't use extra memory\n- For a 1GB upload: streaming takes 0.6s, others take 1.2-1.4s\n\nFull benchmark code, plots, results, and methodology: https://github.com/fedirz/fastapi-file-upload-benchmark\nTest hardware: MacBook Pro M3 Pro (12 cores, 18GB RAM)\n\n### Target Audience\nThose who write Web API in Python\n\n### Comparison\nN/A\n\nHappy to answer questions about the setup or findings.\n",
    "author": "fedirz",
    "timestamp": "2025-10-06T20:49:51",
    "url": "https://reddit.com/r/Python/comments/1o04g6v/i_benchmarked_5_different_fastapi_file_upload/",
    "score": 116,
    "num_comments": 12,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0iu40",
    "title": "I made a multiplayer Tic Tac Toe game in Python using sockets",
    "content": "Hey everyone, I just finished a multiplayer Tic Tac Toe game in Python. It runs using only Python's built-in modules, and players can connect and play live from their own terminals using sockets.\n\nWhat my project does:\n\n* Lets multiple players play Tic Tac Toe over a network.\n* Uses Python's socket module to send and receive moves in real time.\n* Automatically handles turns, move validation, and win/draw checks.\n* Completely terminal-based, so no extra software is needed.\n\nTarget Audience:\n\n* Python beginners wanting to learn about network programming.\n* People curious about how real-time multiplayer games work.\n* Developers looking for a simple multiplayer game example without extra dependencies.\n\nComparison:\nMost Tic Tac Toe projects are limited to two players on the same machine. This one allows multiple players to connect over a network using raw sockets. It's lightweight, easy to run, and simple to understand.\n\nCheck it out on GitHub: [https://github.com/itzpremsingh/tictactoe](https://github.com/itzpremsingh/tictactoe)\n\nI‚Äôd love to hear your feedback and ideas!",
    "author": "itzpremsingh",
    "timestamp": "2025-10-07T09:00:48",
    "url": "https://reddit.com/r/Python/comments/1o0iu40/i_made_a_multiplayer_tic_tac_toe_game_in_python/",
    "score": 12,
    "num_comments": 0,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0jona",
    "title": "This Thursday: Astral CEO (ruff, uv creator) and Fal VP Eng discussing python in production",
    "content": "Sharing an event I came across about building scalable python. I think it's this Thursday.\n\n**Name:**¬†Python in Production with [Astral](http://astral.sh/) and [fal AI](http://fal.ai/)\n\n**Description:**¬†  \nPython dominates the AI/ML ecosystem‚Äîfrom research notebooks to model training to inference backends‚Äîbut operating Python reliably at production scale remains one of the most critical challenges teams face. Astral is revolutionizing the Python developer experience with lightning-fast tools like Ruff (the Rust-powered linter) and uv (the game-changing package manager), while fal AI has built one of the industry's most performant inference platforms, serving billions of AI predictions with sub-second latency. Join Charlie Marsh, CEO of Astral, and Batuhan Taskaya, VP Engineering at fal AI, diving into the technical decisions and operational patterns that enable Python to power mission-critical AI services at scale.\n\nRSVP Link: [https://bvp.zoom.us/webinar/register/WN\\_GkFIHtpdS2CojdqoKaXuLA#/registration](https://bvp.zoom.us/webinar/register/WN_GkFIHtpdS2CojdqoKaXuLA#/registration)\n\nWhat will be covered:  \n\n\n* Modern dependency management with uv: solving the reproducibility crisis, managing virtual environments at scale, and accelerating CI/CD pipelines \n* Production Python architecture patterns: working around the GIL, async vs threading considerations, and when to reach for Rust extensions \n* Performance engineering for AI backends: profiling bottlenecks, optimizing hot paths, and balancing latency vs throughput \n* Observability and debugging in production: structured logging, distributed tracing, and catching issues before customers do \n* Deployment strategies: containerization best practices, zero-downtime deployments, and managing Python version migrations \n* Live demonstrations of uv workflows and fal's production Python stack \n\nWho Should Attend: Engineering leaders, AI/ML engineers, platform teams, and founders building Python-based products‚Äîparticularly those scaling AI backends and looking to improve developer velocity without sacrificing production reliability.  \n",
    "author": "One-Construction7805",
    "timestamp": "2025-10-07T09:32:18",
    "url": "https://reddit.com/r/Python/comments/1o0jona/this_thursday_astral_ceo_ruff_uv_creator_and_fal/",
    "score": 10,
    "num_comments": 6,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0gdex",
    "title": "Otary now includes 17 image binarization methods",
    "content": "**What does my project does**: Otary is an open-source Python library dedicated to image manipulation and 2D geometry processing. It gets even smarter with the addition of 17 binarization methods now available! [Jump to the documentation straight away](https://alexandrepoupeau.com/otary/api/image/transformers/thresholding/).\n\n**Target Audience**: Python developers or researchers focused on image processing and computer vision tasks.\n\n**Comparison**: you could actually use Numpy, OpenCV directly. They are used behind the scene by Otary.\n\nOtary now includes 17 binarization methods, designed to make experimentation both simple for beginners and powerful for advanced users.\n\nüîπ **5 basic methods**: easily accessible for quick and efficient use: simple, otsu, adaptive, bradley, and sauvola.\n\nThese methods are the most classic and effective, perfect for new users and for 90% of practical cases.\n\nüîπ **12 advanced methods**: for users who want to explore, compare, and understand more sophisticated approaches.\n\nThey are intended for image processing specialists and researchers who want to experiment with new ideas.\n\nüìñ The documentation presents a summary table of the 17 methods, classified by year of publication and accompanied by links to the original scientific articles.\n\n‚ú® My revelation: FAIR binarization.\n\nFAIR stands for ‚ÄúFast Algorithm for document Image Restoration‚Äù and it has completely changed the way I approach binarization. Rather than binarizing the entire image, it:\n\n1. First detects edge pixels with a custom Canny edge detector\n2. Applies a clustering algorithm to small windows centered around the edge pixels.\n3. Performs post-processing to complete the total binarization of the image\n\nThis is the approach I found most innovative among all those I have explored and implemented. It uses the Expectation-Maximization algorithm to identify text pixels versus background pixels by assuming a Gaussian mixture distribution: it's simply brilliant!\n\nüí¨ I sincerely hope that this update will make the work of developers, engineers, and researchers who manipulate images easier and inspire new explorations.\n\nüôè I would also like to encourage everyone to contribute, add new binarization methods, improve existing ones, or even invent new approaches.\n\nIf you spot an error or have ideas for improving Otary, your contributions are welcome, that's the spirit of open source.\n\n**Github link**: [https://github.com/poupeaua/otary](https://github.com/poupeaua/otary)",
    "author": "Narrow-Treacle-6460",
    "timestamp": "2025-10-07T07:29:56",
    "url": "https://reddit.com/r/Python/comments/1o0gdex/otary_now_includes_17_image_binarization_methods/",
    "score": 14,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0tdmf",
    "title": "Crawlee for Python team AMA",
    "content": "Hi everyone! [We posted last week](https://www.reddit.com/r/Python/comments/1nu8tt6/crawlee_for_python_v10_is_live/) to say that we had moved [Crawlee for Python](https://crawlee.dev/python) out of beta and promised we would be back to answer your questions about webscraping, Python tooling, community-driven development, testing, versioning, and anything else.\n\nWe're pretty enthusiastic about the work we put into this library and the tools we've built it with, so would love to dive into these topics with you today. Ask us anything!\n\n&gt;Thanks for the questions folks! If you didn't make it in time to ask your questions, don't worry and ask away, we'll respond anyway.",
    "author": "ellatronique",
    "timestamp": "2025-10-07T15:28:10",
    "url": "https://reddit.com/r/Python/comments/1o0tdmf/crawlee_for_python_team_ama/",
    "score": 2,
    "num_comments": 12,
    "upvote_ratio": 0.54,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0no3m",
    "title": "This Thursday: Astral CEO (ruff, uv creator) and Fal VP Eng discussing python in production",
    "content": "RSVP Link:¬†[https://bvp.zoom.us/webinar/register/WN\\_GkFIHtpdS2CojdqoKaXuLA#/registration](https://bvp.zoom.us/webinar/register/WN_GkFIHtpdS2CojdqoKaXuLA#/registration)",
    "author": "One-Construction7805",
    "timestamp": "2025-10-07T11:53:14",
    "url": "https://reddit.com/r/Python/comments/1o0no3m/this_thursday_astral_ceo_ruff_uv_creator_and_fal/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nzl1nj",
    "title": "Why is Python type hinting so maddening compared to other implementations?",
    "content": "I work professionally with a bunch of languages, as an integration engineer. Python is a fairly common one and sometimes I need to add just the right API I need for my integration work to a project. I don't compromise on anything that helps me catch bugs before runtime, so I always have the strictest type checking enabled, which however... when it comes to Python, drives me insane. Once one starts building complex applications, `# type: ignore` becomes your best friend, because handling e.g. a GeoPandas type error would require one hour, even though runtime-wise it has no repercussions whatsoever.\n\nI have worked with other type hinting systems, namely Erlang's, Elixir's and PHP's and I must say, none of them has given me the headaches that Python's regularly gives me. So, I was wondering if there is something inherent to Python that makes type hints a nightmare? Is the tooling \"bad\"? What is the issue exactly?",
    "author": "ataltosutcaja",
    "timestamp": "2025-10-06T07:45:17",
    "url": "https://reddit.com/r/Python/comments/1nzl1nj/why_is_python_type_hinting_so_maddening_compared/",
    "score": 309,
    "num_comments": 160,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nzhs1s",
    "title": "NiceGUI 3.0: Write web interfaces in Python. The nice way.",
    "content": "We're happy to announce the **third major release of NiceGUI**.\n\n[NiceGUI](https://nicegui.io) is a powerful yet simple-to-use UI framework to build applications, dashboards, and tools that run in the browser. You write Python; NiceGUI builds the frontend and handles the browser plumbing. It's great for modern web apps, internal tools, data science apps, robotics interfaces, and embedded/edge UIs ‚Äî anywhere you want a polished web interface without frontend framework complexity.\n\nWe recently discussed NiceGUI on the Talk Python To Me podcast ‚Äî [watch on YouTube](https://www.youtube.com/watch?v=74UXonJfl6o).\n\n# Highlights\n\n* **Single-Page Apps** with `ui.run(root=...)` \\+ [`ui.sub_pages`](https://nicegui.io/documentation/sub_pages/)\n* New **script mode** for small and tight Python scripts (see below).\n* Lightweight [**Event system**](https://nicegui.io/documentation/event) to connect short‚Äëlived UIs with long‚Äëlived Python services.\n* **Observables**: modify props/classes/style and the UI updates automatically.\n* **Tables / AG Grid**: update live via `table.rows/columns` or `aggrid.options`.\n* **Simplified pytest** [setup](https://nicegui.io/documentation/section_testing) and improved [`user` fixture](https://nicegui.io/documentation/user) for fast UI tests.\n* **Tailwind 4** support.\n\nFull notes &amp; migration: [3.0.0 release](https://github.com/zauberzeug/nicegui/releases/tag/v3.0.0)\n\n# Minimal examples\n\n**Script mode**\n\n    from nicegui import ui\n    \n    ui.label('Hello, !')\n    ui.button('Click me', on_click=lambda: ui.notify('NiceGUI 3.0'))\n    \n    ui.run()\n\nRun the file; your browser will show the app at `http://localhost:8080`.\n\n**Single‚ÄëPage App (SPA)**\n\n    from nicegui import ui\n    \n    ui.link.default_classes('no-underline')\n    \n    def root():\n        with ui.header().classes('bg-gray-100'):\n            ui.link('Home', '/')\n            ui.link('About', '/about')\n        ui.sub_pages({\n            '/': main,\n            '/about': about,\n        })\n    \n    def main():\n        ui.label('Main page')\n    \n    def about():\n        ui.label('About page')\n    \n    ui.run(root)\n\nWhen started, every visit to [`http://localhost:8080`](http://localhost:8080) executes `root` and shows a header with links to the `main` and `about` pages.\n\n# Why it matters\n\n* **Build UI in the backend**: one codebase/language with direct access to domain state and services. Fewer moving parts and tighter security boundaries.\n* **Async by default**: efficient I/O, WebSockets, and streaming keep UIs responsive under load.\n* **FastAPI under the hood**: REST + UI in one codebase, fully typed, and proven middleware/auth.\n* **Tailwind utilities + Quasar components**: consistent, responsive styling, and polished widgets without frontend setup.\n* **General‚Äëpurpose apps**: explicit routing, Pythonic APIs, and intuitive server‚Äëside state handling.\n\n# Get started\n\n* Install: `pip install nicegui`\n* Documentation &amp; Quickstart: [nicegui.io](https://nicegui.io) (built with NiceGUI itself)\n* 3.0 release notes &amp; migration: [3.0.0 release](https://github.com/zauberzeug/nicegui/releases/tag/v3.0.0)\n* License: MIT. Python 3.9+.\n\nIf you build something neat, share a screenshot or repo. We‚Äôd love to see it!",
    "author": "r-trappe",
    "timestamp": "2025-10-06T05:33:36",
    "url": "https://reddit.com/r/Python/comments/1nzhs1s/nicegui_30_write_web_interfaces_in_python_the/",
    "score": 270,
    "num_comments": 56,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0m5e8",
    "title": "Event on Thursday: Astral CEO (ruff, uv creator) and Fal VP Eng discussing python in production",
    "content": "Sharing an event I came across about building scalable python. I think it's this Thursday.\n\n**Name:**¬†Python in Production with¬†Astral and Fal AI\n\n**When:** Thursday Oct 9 at 10am PT // 1pm ET\n\n**Description:**¬†  \nCharlie Marsh, CEO of Astral, and Batuhan Taskaya, VP Engineering at fal AI, are diving into the technical decisions and operational patterns that enable Python to power mission-critical AI services at scale.\n\nRSVP Link:¬†[https://bvp.zoom.us/webinar/register/WN\\_GkFIHtpdS2CojdqoKaXuLA#/registration](https://bvp.zoom.us/webinar/register/WN_GkFIHtpdS2CojdqoKaXuLA#/registration)\n\nWhat will be covered:\n\n* Modern dependency management with uv: solving the reproducibility crisis, managing virtual environments at scale, and accelerating CI/CD pipelines\n* Production Python architecture patterns: working around the GIL, async vs threading considerations, and when to reach for Rust extensions\n* Performance engineering for AI backends: profiling bottlenecks, optimizing hot paths, and balancing latency vs throughput\n* Observability and debugging in production: structured logging, distributed tracing, and catching issues before customers do\n* Deployment strategies: containerization best practices, zero-downtime deployments, and managing Python version migrations\n* Live demonstrations of uv workflows and fal's production Python stack\n\nWho Should Attend: Engineering leaders, AI/ML engineers, platform teams, and founders building Python-based products‚Äîparticularly those scaling AI backends and looking to improve developer velocity without sacrificing production reliability.\n\n",
    "author": "One-Construction7805",
    "timestamp": "2025-10-07T10:59:48",
    "url": "https://reddit.com/r/Python/comments/1o0m5e8/event_on_thursday_astral_ceo_ruff_uv_creator_and/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0jw4w",
    "title": "Instrument AI PDF Splitter ‚Äì Split full orchestral PDFs into per-instrument parts",
    "content": "Hey everyone,\n\nI‚Äôve been building a small open-source Python project called Instrument AI PDF Splitter. It takes massive orchestra PDFs (with all instruments in one file) and automatically splits them into clean PDFs for each part.\n\n\n---\n\nWhat My Project Does\n\nDetects instrument names, voice numbers (like ‚ÄúTrumpet 2‚Äù or ‚ÄúViolin I‚Äù), and their start/end pages automatically using OpenAI.\n\nWorks with both scanned and digital sheet music PDFs.\n\nSaves per-instrument PDFs in a neat folder and outputs structured JSON metadata.\n\nAvoids re-uploading the same file by hashing it.\n\nAllows custom instrument lists if needed.\n\nCan be integrated into orchestral score management software ‚Äî I‚Äôm currently developing a project for managing full digital orchestral scores, which this tool will complement.\n\n\n\n---\n\nTarget Audience\n\nOrchestras, ensembles, and developers building tools for digital music management.\n\nAnyone who needs to extract individual parts from combined sheet music PDFs.\n\nNot a full score management solution on its own, but a practical building block for such workflows.\n\n\n\n---\n\nComparison\nUnlike existing PDF splitters or music OCR tools, this project:\n\nAutomatically detects instruments and voice numbers instead of requiring manual input.\n\nHandles both scanned and digital PDFs.\n\nProduces ready-to-use per-instrument PDFs plus structured JSON metadata.\n\nIs lightweight, open-source, and easy to integrate into larger orchestral score management systems.\n\n\n\n---\n\nInstall\n\n`pip install instrumentaipdfsplitter`\n\nRequires Python 3.10+ and an OpenAI API key.\n\n\n---\n\nQuick example\n\n```python\nfrom instrumentaipdfsplitter import InstrumentAiPdfSplitter\n\nsplitter = InstrumentAiPdfSplitter(api_key=\"YOUR_OPENAI_API_KEY\")\n\n# Analyze the score\ndata = splitter.analyse(\"path/to/score.pdf\")\n\n# Split it into instrument parts\nresults = splitter.split_pdf(\"path/to/score.pdf\")\n```\n\n---\n\nüîó [PyPI](https://pypi.org/project/instrumentaipdfsplitter/)\nüîó [GitHub](https://github.com/DiscoveryFox/InstrumentAiPdfSplitter)\n\nI‚Äôd love to hear your feedback! Hopefully this makes splitting full scores easier and can help feed into orchestral score management systems ‚Äî stay tuned, I‚Äôll be posting about that project in a few days.",
    "author": "Discovery_Fox",
    "timestamp": "2025-10-07T09:39:57",
    "url": "https://reddit.com/r/Python/comments/1o0jw4w/instrument_ai_pdf_splitter_split_full_orchestral/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nzib8m",
    "title": "uv overtakes pip in CI (for Wagtail &amp; FastAPI)",
    "content": "**for Wagtail: 66% of CI downloads with uv; for Django: 43%; for FastAPI: 60%**. For all downloads CI or no, it‚Äôs at 28% for Wagtail users; 21% for Django users; 31% for FastAPI users. If the current adoption trends continue, it‚Äôll be the most used installer on those projects in about 12-14 months.\n\nArticle: [uv overtakes pip in CI (for Wagtail users)](https://wagtail.org/blog/uv-overtakes-pip-in-ci/).",
    "author": "thibaudcolas",
    "timestamp": "2025-10-06T05:56:55",
    "url": "https://reddit.com/r/Python/comments/1nzib8m/uv_overtakes_pip_in_ci_for_wagtail_fastapi/",
    "score": 161,
    "num_comments": 35,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0j06z",
    "title": "Built a BLE Proximity Alert System in Python",
    "content": "# I‚Äôve been experimenting with Bluetooth Low Energy and wrote a simple Python script that detects nearby BLE devices based on signal strength (RSSI).\n\nThe script triggers a sound when a specific device comes within range ‚Äî a fun way to explore how proximity detection works in Python using the **BleuIO** USB dongle (it handles the BLE scanning).\n\nIt‚Äôs great for learning or building small application like access control, IoT automation, or security demos.  \nCode and full walkthrough here:\n\n[https://www.bleuio.com/blog/ble-device-proximity-alert-system-using-bleuio/](https://www.bleuio.com/blog/ble-device-proximity-alert-system-using-bleuio/)",
    "author": "bleuio",
    "timestamp": "2025-10-07T09:06:59",
    "url": "https://reddit.com/r/Python/comments/1o0j06z/built_a_ble_proximity_alert_system_in_python/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nzkswp",
    "title": "fastquadtree: a Rust-powered quadtree for Python that is ~14x faster than PyQtree",
    "content": "Quadtrees are great for organizing spatial data and checking for 2D collisions, but all the existing Python quadtree packages are slow and outdated.\n\nMy package, fastquadtree, leverages a Rust core to outperform the most popular Python package, pyqtree, by being **14x faster**. It also offers a more convenient Python API for tracking objects and KNN queries.\n\n**PyPI page:** [https://pypi.org/project/fastquadtree/](https://pypi.org/project/fastquadtree/)  \n**GitHub Repo:** [https://github.com/Elan456/fastquadtree](https://github.com/Elan456/fastquadtree)  \n**Wheels Shipped**: Linux, Mac, and Windows\n\n    pip install fastquadtree\n\nThe GitHub Repo contains utilities for visualizing how the quadtree works using Pygame and running the benchmarks yourself.\n\nBenchmark Comparison\n\n* Points: **250,000**, Queries: **500**\n* Fastest total: **fastquadtree** at **0.120 s**\n\n|Library|Build (s)|Query (s)|Total (s)|Speed vs PyQtree|\n|:-|:-|:-|:-|:-|\n|fastquadtree|0.031|0.089|0.120|14.64√ó|\n|Shapely STRtree|0.179|0.100|0.279|6.29√ó|\n|nontree-QuadTree|0.595|0.605|1.200|1.46√ó|\n|Rtree|0.961|0.300|1.261|1.39√ó|\n|e-pyquadtree|1.005|0.660|1.665|1.05√ó|\n|PyQtree|1.492|0.263|1.755|1.00√ó|\n|quads|1.407|0.484|1.890|0.93√ó|",
    "author": "Awkward-Target4899",
    "timestamp": "2025-10-06T07:36:13",
    "url": "https://reddit.com/r/Python/comments/1nzkswp/fastquadtree_a_rustpowered_quadtree_for_python/",
    "score": 76,
    "num_comments": 14,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0is69",
    "title": "dirstree: an another library for iterating through the contents of a directory",
    "content": "Hello¬†[r/Python](https://www.reddit.com/r/Python/)! üëã\n\nI have released a new micro library that allows recursively iterating over files in a given directory: [dirstree](https://github.com/pomponchik/dirstree). Now I will briefly describe why it is needed.\n\n# What My Project Does\n\nThere are a lot of libraries that allow recursively traversing files in a directory. It's also easy to do without third-party libraries, all the necessary batteries are included. Why do we need `dirstree`?\n\nThis library provides several advantages:\n\n1. The most compact and pythonic interface for iterating through files.\n2. The ability to filter files by extensions, text templates in `.gitignore` format, as well as using custom functions.\n3. Support for [cancellation tokens](https://github.com/pomponchik/cantok). This is useful if your program can run for a long time with a large number of files.\n4. The ability to easily combine several different directory crawl conditions into a single object.\n5. 100% test coverage, of course!\n\nThe simplest example of syntax:\n\n```python\nfrom dirstree import Crawler\n\ncrawler = Crawler('.')\n\nfor file in crawler:\n    print(file)\n```\n\nAs you can see, it's beautiful and there's nothing superfluous.\n\n# Target Audience\n\nAnyone who has to work with the file system throw Python.\n\n# Comparison\n\nThere are many similar libraries, but the same combination of beautiful python syntax, support for cancellation tokens, and a large number of types of filtering no longer exists.",
    "author": "pomponchik",
    "timestamp": "2025-10-07T08:58:48",
    "url": "https://reddit.com/r/Python/comments/1o0is69/dirstree_an_another_library_for_iterating_through/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0hk15",
    "title": "I built an Instagram checker with smart anti-ban logic &amp; multi-threading. Open for feedback!",
    "content": "**mYCheckerForInstagram**  \nAn advanced Instagram checker with smart anti-ban logic.\n\n* Fast (multi-threaded)\n* Auto-switching proxies\n* Smart throttling\n* Built for educational purposes.\n\nRepo:  \n[github.com/0xkhalz/mYCheckerForInstagram](http://github.com/hd0r/mYCheckerForInstagram)  \n\\#pentesting #bugbounty #python #automation #github",
    "author": "hd0rr",
    "timestamp": "2025-10-07T08:13:50",
    "url": "https://reddit.com/r/Python/comments/1o0hk15/i_built_an_instagram_checker_with_smart_antiban/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.3,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0h25y",
    "title": "A Telegram Bot for Finding Perfume &amp; Clones",
    "content": "**What My Project Does**\n\nPerfume Twins is a Telegram bot that helps you find expensive designer perfumes and instantly pairs them with affordable dupes.\n\nThe bot contains a database of 2000+ perfumes (originals + clones, gathered from fragrance communities).  \nBuilt entirely in Python.  \nInitial data is included in CSV tables.  \nEnglish &amp; Russian interface versions.\n\nI would appreciate any feedback: on the code, the data, or the user experience.  \nThank you.\n\n**Target Audience**\n\nAnyone looking for reliable, affordable alternatives to luxury scents. Suitable for production use via Telegram, not just a toy project.\n\n**Comparison**\n\nUnlike browsing forums or subreddits manually, Perfume Twins offers the biggest, cleanest, and instantly searchable database of originals and clones. The search is typo-tolerant and structured for fast results, saving users hours of searching. Free, open, and easy to use.\n\nLinks\n\nTry the Bot: @ parfumanalogbot\n\nSource Code: [github.com/rustam-k0/perfume-bot-public](http://github.com/rustam-k0/perfume-bot-public)\n\n*Note: I previously posted a link to this project, but I changed the structure of the post and the bot didn‚Äôt like it.*",
    "author": "Superb-East9538",
    "timestamp": "2025-10-07T07:55:55",
    "url": "https://reddit.com/r/Python/comments/1o0h25y/a_telegram_bot_for_finding_perfume_clones/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.18,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nzznk3",
    "title": "Tuesday Daily Thread: Advanced questions",
    "content": "# Weekly Wednesday Thread: Advanced Questions üêç\n\nDive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.\n\n## How it Works:\n\n1. **Ask Away**: Post your advanced Python questions here.\n2. **Expert Insights**: Get answers from experienced developers.\n3. **Resource Pool**: Share or discover tutorials, articles, and tips.\n\n## Guidelines:\n\n* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.\n* Questions that are not advanced may be removed and redirected to the appropriate thread.\n\n## Recommended Resources:\n\n* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.\n\n## Example Questions:\n\n1. **How can you implement a custom memory allocator in Python?**\n2. **What are the best practices for optimizing Cython code for heavy numerical computations?**\n3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**\n4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**\n5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**\n6. **What are some advanced use-cases for Python's decorators?**\n7. **How can you achieve real-time data streaming in Python with WebSockets?**\n8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**\n9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**\n10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**\n\nLet's deepen our Python knowledge together. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-06T17:00:41",
    "url": "https://reddit.com/r/Python/comments/1nzznk3/tuesday_daily_thread_advanced_questions/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nzr2bv",
    "title": "Crank.py - Build web UIs with async/generator functions, powered by Crank.js/PyScript.",
    "content": "I just released the first public version of [Crank.py](http://github.com/bikeshaving/crankpy), Crank bindings for Crank.js.\n\n## Links:\n\n* GitHub repo: [https://github.com/bikeshaving/crankpy](https://github.com/bikeshaving/crankpy)\n* Blog post: [http://crank.js.org/blog/introducing-crank-py ](http://crank.js.org/blog/introducing-crank-py)\n* TodoMVC example: [https://pyscript.com/@brainkim/crank-todomvc/latest](https://pyscript.com/@brainkim/crank-todomvc/latest)\n* PyPI: [https://pypi.org/project/crankpy/](https://pypi.org/project/crankpy/)\n\n## What My Project Does\n\nCrank.py provides PyScript bindings to Crank.js, allowing users to write frontend UI components with Python generator and async functions. Here‚Äôs a quick example:\n\n```python\nfrom js import document\nfrom pyodide.http import pyfetch\nfrom crank import component, h\nfrom crank.dom import renderer\nimport asyncio\n\n@component\nasync def Definition(ctx, props):\n    word = props['word']\n    # API courtesy https://dictionaryapi.dev\n    res = await pyfetch(f\"https://api.dictionaryapi.dev/api/v2/entries/en/{word}\")\n    data = await res.json()\n\n    # Check if API returned an error (not an array)\n    if not isinstance(data, list):\n        return h.div[f\"No definition found for {word}\"]\n\n    # Extract data exactly like the JavaScript version\n    # const {phonetic, meanings} = data[0];\n    # const {partOfSpeech, definitions} = meanings[0];\n    # const {definition} = definitions[0];\n    phonetic = data[0].get('phonetic', '')\n    meanings = data[0]['meanings']\n    part_of_speech = meanings[0]['partOfSpeech']\n    definitions = meanings[0]['definitions']\n    definition = definitions[0]['definition']\n\n    return h.div[\n        h.p[word, \" \", h.code[phonetic]],\n        h.p[h.b[f\"{part_of_speech}.\"], \" \", definition]\n    ]\n\n@component\ndef Dictionary(ctx):\n    word = \"\"\n\n    @ctx.refresh\n    def onsubmit(ev):\n        nonlocal word\n        ev.preventDefault()\n        # Get the input value directly from the DOM\n        input_el = document.getElementById(\"word\")\n        word1 = input_el.value\n        if word1 and word1.strip():\n            word = word1.strip()\n\n    for _ in ctx:\n        yield h.div[\n            h.form(\n                action=\"\",\n                method=\"get\",\n                onsubmit=onsubmit,\n                style={\"margin-bottom\": \"15px\"}\n            )[\n                h.div(style={\"margin-bottom\": \"15px\"})[\n                    h.label(htmlFor=\"word\")[\"Define: \"],\n                    h.input(type=\"text\", name=\"word\", id=\"word\", required=True)\n                ],\n                h.div[\n                    h.input(type=\"submit\", value=\"Search\")\n                ]\n            ],\n            h(Definition, word=word) if word else None\n        ]\n\nrenderer.render(h(Dictionary), document.body)\n```\n## Target Audience\n\nCrank.py is for Python developers who want to write web UIs with Python instead of JavaScript. It‚Äôs perfect for rich client-side Python apps, teaching web development with Python, and building interactive Python data apps which leverage the entire Python ecosystem.\n\n## Comparison\n\nCompared to [Pue.py](https://puepy.dev), Crank.py uses Python functions exclusively for component definitions, and provides an innovative template syntax as a replacement for JSX/templates.",
    "author": "bikeshaving",
    "timestamp": "2025-10-06T11:26:23",
    "url": "https://reddit.com/r/Python/comments/1nzr2bv/crankpy_build_web_uis_with_asyncgenerator/",
    "score": 8,
    "num_comments": 7,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nz2qwk",
    "title": "Turns Python functions into web UIs",
    "content": "A year ago I posted [FuncToGUI](https://github.com/offerrall/FuncToGUI) here (220 upvotes, thanks!) - a tool that turned Python functions into desktop GUIs. Based on feedback, I rebuilt it from scratch as **FuncToWeb** for web interfaces instead.\n\n# What My Project Does\n\nFuncToWeb automatically generates web interfaces from Python functions using type hints. Write a function, call `run()`, and get an instant form with validation.\n\n    from func_to_web import run\n    \n    def divide(a: int, b: int):\n        return a / b\n    \n    run(divide)\n\nOpen `localhost:8000` \\- you have a working web form.\n\nIt supports all Python types (`int`, `float`, `str`, `bool`, `date`, `time`), special inputs (color picker, email validation), file uploads with type checking (`ImageFile`, `DataFile`), Pydantic validation constraints, and dropdown selections via `Literal`.\n\n**Key feature:** Returns PIL images and matplotlib plots automatically - no need to save/load files.\n\n    from func_to_web import run, ImageFile\n    from PIL import Image, ImageFilter\n    \n    def blur_image(image: ImageFile, radius: int = 5):\n        img = Image.open(image)\n        return img.filter(ImageFilter.GaussianBlur(radius))\n    \n    run(blur_image)\n\nUpload image and see processed result in browser.\n\n# Target Audience\n\nThis is for **internal tools and rapid prototyping**, not production apps. Specifically:\n\n* Teams needing quick utilities (image resizers, data converters, batch processors)\n* Data scientists prototyping experiments before building proper UIs\n* DevOps creating one-off automation tools\n* Anyone who needs a UI \"right now\" for a Python function\n\n**Not suitable for:**\n\n* Production web applications (no authentication, basic security)\n* Public-facing tools\n* Complex multi-page applications\n\nThink of it as duct tape for internal tooling - fast, functional, disposable.\n\n# Comparison\n\n**vs Gradio/Streamlit:**\n\n* **Scope:** They're frameworks for building complete apps. FuncToWeb wraps individual functions.\n* **Use case:** Gradio/Streamlit for dashboards and demos. FuncToWeb for one-off utilities.\n* **Complexity:** They have thousands of lines. This is 350 lines of Python + 700 lines HTML/CSS/JS.\n* **Philosophy:** They're opinionated frameworks. This is a minimal library.\n\n**vs FastAPI Forms:**\n\n* FastAPI requires writing HTML templates and routes manually\n* FuncToWeb generates everything from type hints automatically\n* FastAPI is for building APIs. This is for quick UIs.\n\n**vs FuncToGUI (my previous project):**\n\n* Web-based instead of desktop (Kivy)\n* Works remotely, easier to share\n* Better image/plot support\n* Cleaner API using `Annotated`\n\n# Technical Details\n\n**Built with:** FastAPI, Pydantic, Jinja2\n\n**Features:**\n\n* Real-time validation (client + server)\n* File uploads with type checking\n* Smart output detection (text/JSON/images/plots)\n* Mobile-responsive UI\n* **Multi-function support**¬†\\- Serve multiple tools from one server\n\nThe repo has 14 runnable examples covering basic forms, image processing, and data visualization.\n\n# Installation\n\n    pip install func-to-web\n\n**GitHub:** [https://github.com/offerrall/FuncToWeb](https://github.com/offerrall/FuncToWeb)\n\nFeedback is welcome!",
    "author": "drboom9",
    "timestamp": "2025-10-05T15:56:01",
    "url": "https://reddit.com/r/Python/comments/1nz2qwk/turns_python_functions_into_web_uis/",
    "score": 153,
    "num_comments": 56,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nz9vl6",
    "title": "Edazer ‚Äî Fast EDA Toolkit (pandas + polars compatible",
    "content": "Hey everyone üëã\nI built a small Python library called Edazer to make quick Exploratory Data Analysis (EDA) less painful and more fun.\nIt‚Äôs designed to give you a full dataset summary in just a few lines ‚Äî no need to keep rewriting the same EDA boilerplate every project.\n\nüîç What It Does\n\nEdazer can:\n\nSummarize missing values, descriptive stats &amp; data types\n\nFind duplicated rows\n\nShow unique values by column\n\nIntegrate YData Profiling for full reports\n\nEven make your DataFrame interactive with one function\n\n\nAll that ‚Äî literally in 4 lines of code üòÖ\n\nüéØ Who It‚Äôs For\n\nIf you‚Äôre a data scientist, analyst, or ML student who starts every project with the same 10 lines of EDA setup‚Ä¶ this is for you.\nIt‚Äôs super handy for quick dataset exploration, Kaggle projects, or teaching demos.\n\n\n‚öñÔ∏è How It‚Äôs Different\n\nCompared to tools like pandas-profiling or Sweetviz:\n\nLightweight ‚Äî only the essentials\n\nWorks with both pandas and polars\n\nRuns faster and uses less memory on medium datasets\n\nSuper simple API, ideal for notebooks and quick checks\n\n\nüíª GitHub: https://github.com/adarsh-79/edazer\nüìä Kaggle: https://www.kaggle.com/code/adarsh79x/edazer-for-quick-eda-pandas-polars-profiling\n\n",
    "author": "Adarsh3690704",
    "timestamp": "2025-10-05T21:42:53",
    "url": "https://reddit.com/r/Python/comments/1nz9vl6/edazer_fast_eda_toolkit_pandas_polars_compatible/",
    "score": 40,
    "num_comments": 2,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1o0cyz4",
    "title": "Is hello world that complicated?",
    "content": "So I just came across this tweet, and here he talks about what goes on when we write hello world. Is it really that complicated?\n\nLike so many things going on just 1 simple syntax \n\nhttps://x.com/aBlackPigeon/status/1975294226163507455?t=jktU6ixa_tV0gJONrx6J9g&amp;s=19",
    "author": "EffectiveMaterial781",
    "timestamp": "2025-10-07T05:10:17",
    "url": "https://reddit.com/r/Python/comments/1o0cyz4/is_hello_world_that_complicated/",
    "score": 0,
    "num_comments": 37,
    "upvote_ratio": 0.34,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nz4695",
    "title": "Monday Daily Thread: Project ideas!",
    "content": "# Weekly Thread: Project Ideas üí°\n\nWelcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.\n\n## How it Works:\n\n1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.\n2. **Build &amp; Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.\n3. **Explore**: Looking for ideas? Check out Al Sweigart's [\"The Big Book of Small Python Projects\"](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.\n\n## Guidelines:\n\n* Clearly state the difficulty level.\n* Provide a brief description and, if possible, outline the tech stack.\n* Feel free to link to tutorials or resources that might help.\n\n# Example Submissions:\n\n## Project Idea: Chatbot\n\n**Difficulty**: Intermediate\n\n**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar \n\n**Description**: Create a chatbot that can answer FAQs for a website.\n\n**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)\n\n# Project Idea: Weather Dashboard\n\n**Difficulty**: Beginner\n\n**Tech Stack**: HTML, CSS, JavaScript, API\n\n**Description**: Build a dashboard that displays real-time weather information using a weather API.\n\n**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)\n\n## Project Idea: File Organizer\n\n**Difficulty**: Beginner\n\n**Tech Stack**: Python, File I/O\n\n**Description**: Create a script that organizes files in a directory into sub-folders based on file type.\n\n**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)\n\nLet's help each other grow. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-05T17:00:32",
    "url": "https://reddit.com/r/Python/comments/1nz4695/monday_daily_thread_project_ideas/",
    "score": 37,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nzkeo0",
    "title": "FineTuned IBM Granite-4 with Python and UnslothüöÄ",
    "content": "Hey all, thanks for reading this!\n\nI have finetuned the latest IBM's Granite-4.0 model using Python and the Unsloth library, since the model is quite small, I felt that it might not be able to give good results, but the results were far from what I expected.\n\nThis small model was able to generate output with **low latency** and with **great accuracy**. I even tried to lower the temperature to allow it to be more creative, but still the model managed to produce quality and to the point output.\n\nI have pushed the LoRA model on Hugging Face and have also written an article dealing with all the nuances and intricacies of¬†**finetuning**¬†the¬†**latest IBM's Granite-4.0**¬†model.\n\nCurrently working on adding the model card to the model.\n\nPlease share your thoughts and feedback!  \nThank you!\n\nHere's the [model](https://huggingface.co/krishanwalia30/granite-4.0-h-micro_lora_model).\n\nHere's the [article](https://medium.com/towards-artificial-intelligence/ibms-granite-4-0-fine-tuning-made-simple-create-custom-ai-models-with-python-and-unsloth-4fc11b529c1f).",
    "author": "krishanndev",
    "timestamp": "2025-10-06T07:21:08",
    "url": "https://reddit.com/r/Python/comments/1nzkeo0/finetuned_ibm_granite4_with_python_and_unsloth/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ny6svl",
    "title": "I made PyPIPlus.com ‚Äî a faster way to see all dependencies of any Python package",
    "content": "Hey folks\n\nI built a small tool called PyPIPlus.com that helps you quickly see all dependencies for any Python package on PyPI.\n\nIt started because I got tired of manually checking dependencies when installing packages on servers with limited or no internet access. We all know that pain trying to figure out what else you need to download by digging through package metadata or pip responses.\n\nWith PyPIPlus, you just type the package name and instantly get a clean list of all its dependencies (and their dependencies). No installation, no login, no ads ‚Äî just fast info.\n\nWhy it‚Äôs useful: ‚Ä¢ Makes offline installs a lot easier (especially for isolated servers) ‚Ä¢ Saves time ‚Ä¢ Great for auditing or just understanding what a package actually pulls in\n\nWould love to hear your thoughts ‚Äî bugs, ideas, or anything you think would make it better. It‚Äôs still early and I‚Äôm open to improving it.\n\n[https://pypiplus.com](https://pypiplus.com)\n\nUPDATE: thank you everyone for the positive comments and feedback, please feel free share any additional ideas we can make this a better tool. I‚Äôll be making sure of taking each comment and feature requests mentioned and try to make it available in the next push update üôè\n\nUPDATE #2: Added extra detailed packages information, dependents view, and an offline bundle generator that includes all dependency wheels, pinned requirements, universal installer, SBOM, and license summaries for one-step installations. Improved UI and performance. More updates coming soon based on feedback and comments [new updates post](https://www.reddit.com/r/Python/comments/1o9dey5/i_just_released_pypipluscom_20_offlineready/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button)",
    "author": "RoyalW1zard",
    "timestamp": "2025-10-04T14:58:32",
    "url": "https://reddit.com/r/Python/comments/1ny6svl/i_made_pypipluscom_a_faster_way_to_see_all/",
    "score": 170,
    "num_comments": 112,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nz6lku",
    "title": "Sometimes regressing your Python version is the way. Use pyenv to manage multiple versions of Python",
    "content": "TL;DR: get pyenv to manage multiple versions of python on your system.\n\nThis is a beginner tech tip.\n\nTurns out the newest version of Python / pip on my Mac doesn't let me install PyTorch - some version related error.\n\nLuckily, it is very easy to manage multiple versions of python on a single system using pyenv (https://github.com/pyenv/pyenv).\n\nI was able to install an older version, which let me install Pytorch.",
    "author": "vishalontheline",
    "timestamp": "2025-10-05T18:56:04",
    "url": "https://reddit.com/r/Python/comments/1nz6lku/sometimes_regressing_your_python_version_is_the/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.37,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nxtuvm",
    "title": "Do you let linters modify code in your CI/CD pipeline?",
    "content": "For example, with black you can have it check but not modify. Do you think it‚Äôs safe enough to let it modify? I‚Äôve never heard of a horror story‚Ä¶ but maybe that‚Äôs because people don‚Äôt do it?",
    "author": "mbsp5",
    "timestamp": "2025-10-04T06:21:44",
    "url": "https://reddit.com/r/Python/comments/1nxtuvm/do_you_let_linters_modify_code_in_your_cicd/",
    "score": 61,
    "num_comments": 130,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nxy1nj",
    "title": "AnvPy ‚Äî Run &amp; Build Python Apps Natively on Android",
    "content": "Check out our intro video: https://youtu.be/A04UM53TRZw?si=-90Mkja0ojRS8x5p\n\nAnvPy is a next-generation framework designed for Python developers to build, deploy, and run Python applications directly on Android devices offline. With AnvPy, you can:\n\nWrite your project in pure Python\n\nInstantly generate a native Android APK\n\nEnjoy seamless execution on mobile without external dependencies\n\nLeverage familiar Python libraries and toolchains\n\n\nWhether you're prototyping mobile apps, teaching Python, or shipping real-world tools ‚Äî AnvPy makes mobile development accessible and fast. Dive into the video to see a live demo and get started today!",
    "author": "Ajay7750",
    "timestamp": "2025-10-04T09:10:17",
    "url": "https://reddit.com/r/Python/comments/1nxy1nj/anvpy_run_build_python_apps_natively_on_android/",
    "score": 22,
    "num_comments": 16,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nyrenf",
    "title": "For VScode users: What's your opinion on Github Copilot's autocompletion feature?",
    "content": "I use GitHub Copilot pretty much daily in my coding projects. My usual process is to start typing a line and see what Copilot suggests, then decide if it's what I'm looking for or not. If it makes sense, I'll accept it; if not, I'll either modify it or write it myself.\n\n\n\nHonestly, it's made my coding way faster and more efficient. But I've got friends who think this isn't \"real coding\" and that I'm just letting the AI do all the work. Some call it \"vibe coding,\" which I guess is a thing now?\n\n\n\nI don't really agree though. You still need to understand the code and syntax to know whether Copilot's suggestion is actually good or complete garbage. It's more like having a really smart coding buddy who sometimes gives great suggestions and sometimes suggests weird stuff you have to ignore.\n\n\n\nWhat's everyone's take on this? Are you team Copilot or do you think it's not worthy of being called coding?",
    "author": "MaleficentBed1249",
    "timestamp": "2025-10-05T08:36:23",
    "url": "https://reddit.com/r/Python/comments/1nyrenf/for_vscode_users_whats_your_opinion_on_github/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.24,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ny9jqm",
    "title": "Sunday Daily Thread: What's everyone working on this week?",
    "content": "# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è\n\nHello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!\n\n## How it Works:\n\n1. **Show &amp; Tell**: Share your current projects, completed works, or future ideas.\n2. **Discuss**: Get feedback, find collaborators, or just chat about your project.\n3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.\n\n## Guidelines:\n\n* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.\n* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.\n\n## Example Shares:\n\n1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!\n2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.\n3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!\n\nLet's build and grow together! Share your journey and learn from others. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-04T17:00:33",
    "url": "https://reddit.com/r/Python/comments/1ny9jqm/sunday_daily_thread_whats_everyone_working_on/",
    "score": 1,
    "num_comments": 7,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nx0oxk",
    "title": "PEP 810 ‚Äì Explicit lazy imports",
    "content": "PEP: https://pep-previews--4622.org.readthedocs.build/pep-0810/\n\nDiscussion: https://discuss.python.org/t/pep-810-explicit-lazy-imports/104131\n\nThis PEP introduces lazy imports as an explicit language feature. Currently, a module is eagerly loaded at the point of the import statement. Lazy imports defer the loading and execution of a module until the first time the imported name is used.\n\nBy allowing developers to mark individual imports as lazy with explicit syntax, Python programs can reduce startup time, memory usage, and unnecessary work. This is particularly beneficial for command-line tools, test suites, and applications with large dependency graphs.\n\nThe proposal preserves full backwards compatibility: normal import statements remain unchanged, and lazy imports are enabled only where explicitly requested.",
    "author": "JanEric1",
    "timestamp": "2025-10-03T07:28:35",
    "url": "https://reddit.com/r/Python/comments/1nx0oxk/pep_810_explicit_lazy_imports/",
    "score": 476,
    "num_comments": 151,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nxtj9r",
    "title": "I created a framework for turning PyTorch training scripts into event driven systems.",
    "content": "**What My Project Does**\n\nHi! I've been training a lot of neural networks recently and want to share with you a tool I created.\n\nWhile training pytorch models, I noticed that it is very hard to write reusable code for training models. There are packages that help track metrics, logs, and checkpoints, but they often create more problems than they solve. As a result, training pipelines become bloated with infrastructure code that obscures the actual business logic.\n\nThat‚Äôs why I created TorchSystem a package designed to help you build extensible training systems using domain-driven design principles, to replace ugly training scripts with clean, modular, and fully featured training services, with type annotations and modern python syntax.\n\n**Repository**:[ https://github.com/entropy-flux/TorchSystem](https://github.com/entropy-flux/TorchSystem)\n\n**Documentation**:[ https://entropy-flux.github.io/TorchSystem/](https://entropy-flux.github.io/TorchSystem/)\n\n**Full working example**:[ https://github.com/entropy-flux/TorchSystem/tree/main/examples/mnist-mlp](https://github.com/entropy-flux/TorchSystem/tree/main/examples/mnist-mlp)\n\n**Target Audience**\n\n* ML engineers building **complex training pipelines** who need modularity.\n* Researchers experimenting with **custom training loops** without reinventing boilerplate.\n* Developers who want **DDD-inspired architecture** in their AI projects.\n* Anyone frustrated with hard-to-maintain \"script soup\" training code.\n\n**Comparison**\n\n* [pytorch-lightning](https://github.com/Lightning-AI/pytorch-lightning): There aren't any framework doing this,[ pytorch-lightning](https://github.com/Lightning-AI/pytorch-lightning) come close by encapsulating all kind of infrastructure and the training loop inside a custom class, but it doesn't provide a way to actually decouple the logic from the implementation details. You can use a LightningModule¬† instead of my Aggregate class, and use the whole the message system of the library to bind it with other tools you want.\n* [mlflow](https://github.com/mlflow/mlflow): Helps with model tracking and checkpoints, but again, you will end up with a lot of infrastructure logic inside your training loop, you can actually plug tracking libraries like this inside Consumer or a Subscriber and pass metrics as events or to topics as serializable messages.\n* [neptune.ai](https://neptune.ai/): Web infra for metric tracking, like[ mlflow](https://github.com/mlflow/mlflow) you can plug it like a consumer or a subscriber, the good thing is that thanks to dependency inversion you can plug many of these tracking libraries at the same time to the same publisher and send the metrics to all of them.\n\nHope you find it useful!",
    "author": "EricHermosis",
    "timestamp": "2025-10-04T06:07:46",
    "url": "https://reddit.com/r/Python/comments/1nxtj9r/i_created_a_framework_for_turning_pytorch/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.61,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nxt42j",
    "title": "pyro-mysql: a fast MySQL client library",
    "content": "* **Repo**\n   * [https://github.com/elbaro/pyro-mysql/](https://github.com/elbaro/pyro-mysql/)\n* **Bench**\n   * [https://github.com/elbaro/pyro-mysql/blob/main/report/chart.png?raw=true](https://github.com/elbaro/pyro-mysql/blob/main/report/chart.png?raw=true)\n* **What My Project Does**\n   * **pyro-mysql** is a fast MySQL client library.\n* **Target Audience** (e.g., Is it meant for production, just a toy project, etc)\n   * **pyro-mysql** benefits the reliability and speed of Rust.\n   * **pyro-mysql** delegates the protocol implementation to the existing Rust libraries, and the Python layer focuses on managing the lifetime of wrapped objects. This reduces the maintenance work of the Python package.\n   * It is meant for production, but needs more battle-tests.\n* **Comparison** (A brief comparison explaining how it differs from existing alternatives.)\n   * `pyro-mysql` does not implement PEP 249.\n      * There is no cursor.\n   * `mysqlclient`, `pymysql` \\- they are synchronous.\n      * `pyro_mysql.sync` is faster.\n   * `aiomysql`, `asyncmy` \\- they are asynchoronous.\n      * In my last workplace, our prod experience with them was not good.\n      * `FastAPI + aiomysql/asyncmy` setup had protocol errors (Packet Sequence Number wrong) in highly congested environment. We also often ran into critical bugs mixing the query result - the result of query1 was returned to query2.",
    "author": "Sad_Tap_9191",
    "timestamp": "2025-10-04T05:49:07",
    "url": "https://reddit.com/r/Python/comments/1nxt42j/pyromysql_a_fast_mysql_client_library/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nx3hkg",
    "title": "[Show &amp; Tell] PyClue/Cluedo-style deduction game in Python (pygame)",
    "content": "**What My Project Does**  \nI built a small Clue/Cluedo-style deduction game in **Python** using **pygame**. It‚Äôs a scene-based desktop game with clean, portable asset handling. You can run it from source or as a single **Windows .exe** (PyInstaller one-file). The repo is meant to be a practical reference for packaging pygame apps reliably.\n\n**Source code (GitHub):**  \n[https://github.com/rozsit/112\\_PyClue\\_Game](https://github.com/rozsit/112_PyClue_Game)\n\n*(Windows build is in the GitHub Release ‚Äî see ‚ÄúDownloads‚Äù below.)*\n\n**Target Audience**\n\n* Python devs interested in **pygame** architecture and **packaging to .exe**.\n* Learners who want a small, readable codebase (scenes, UI, audio, animations).\n* Casual players who just want to double-click an `.exe` and try a Clue-like game.\n\n**Comparison**  \nCompared with other ‚Äúpygame Clue clones‚Äù or small hobby games, this repo focuses on **robust distribution** and **developer ergonomics**:\n\n* Works the same in **dev** and **frozen** modes (PyInstaller).\n* Global hooks route string paths for `pygame.image.load`, `pygame.mixer.Sound`, and `pygame.mixer.music.load` ‚Üí fewer path bugs after packaging.\n* **Audio init** on Windows is hardened (`ensure_audio()` tries multiple drivers/buffer sizes).\n* **Animated GIF** support via **Pillow** (e.g., winner screen fireworks ‚Üí frames + per-frame duration).\n* Comes with a one-command **build script** (PowerShell) and a **SHA-256** file for integrity checks.\n\n**How Python Is Used**\n\n* **pygame** for windowing, scenes, input, and rendering.\n* **Pillow** to decode animated GIFs into (surface, duration) frames.\n* **PyInstaller** (one-file) to ship a single `.exe`.\n\n**Minimal snippets (the core ideas):**\n\n    # resource_path: dev + PyInstaller (_MEIPASS) friendly\n    from pathlib import Path\n    import sys\n    def resource_path(*parts):\n        if hasattr(sys, \"_MEIPASS\"):\n            base = Path(sys._MEIPASS)\n        else:\n            here = Path(__file__).resolve()\n            base = next((p for p in [here] + list(here.parents) if (p / \"assets\").exists()), here)\n        return str((base / Path(*parts)).resolve())\n    \n\n    # global hooks so string paths work after packaging, too\n    import pygame\n    _orig_img = pygame.image.load\n    def _img_wrapped(path, *a, **kw):\n        from utils import resource_path\n        if isinstance(path, str): path = resource_path(path)\n        return _orig_img(path, *a, **kw)\n    pygame.image.load = _img_wrapped\n    \n    # similar tiny wrappers exist for pygame.mixer.Sound and pygame.mixer.music.load\n    \n\n**Run from Source**\n\n    git clone https://github.com/rozsit/112_PyClue_Game\n    cd 112_PyClue_Game\n    python -m venv .venv\n    .\\.venv\\Scripts\\activate           # Windows\n    pip install -r requirements.txt\n    python main.py\n    \n\n**Downloads (Windows .exe)**  \nGrab the one-file build from the Release page:  \n[https://github.com/rozsit/112\\_PyClue\\_Game/releases/tag/v1.0.0](https://github.com/rozsit/112_PyClue_Game/releases/tag/v1.0.0)\n\n**(Optional) Verify SHA-256 on Windows**\n\n    Get-FileHash .\\PyClue.exe -Algorithm SHA256\n    # or\n    certutil -hashfile .\\PyClue.exe SHA256\n    \n\nThe output should match the `PyClue.exe.sha256` provided in the release.\n\n**Roadmap / PRs Welcome**\n\n* New boards, items, rule variants\n* Simple AI opponents\n* Local/online multiplayer\n* Localization (EN/HU)\n* Save/load &amp; stats\n\nI‚Äôd love feedback on packaging tricks (PyInstaller + pygame), audio reliability on different Windows setups, and ergonomics of the scene/asset layout.",
    "author": "rozsit",
    "timestamp": "2025-10-03T09:13:36",
    "url": "https://reddit.com/r/Python/comments/1nx3hkg/show_tell_pycluecluedostyle_deduction_game_in/",
    "score": 32,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nxf9sn",
    "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread",
    "content": "# Weekly Thread: Resource Request and Sharing üìö\n\nStumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!\n\n## How it Works:\n\n1. **Request**: Can't find a resource on a particular topic? Ask here!\n2. **Share**: Found something useful? Share it with the community.\n3. **Review**: Give or get opinions on Python resources you've used.\n\n## Guidelines:\n\n* Please include the type of resource (e.g., book, video, article) and the topic.\n* Always be respectful when reviewing someone else's shared resource.\n\n## Example Shares:\n\n1. **Book**: [\"Fluent Python\"](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \\- Great for understanding Pythonic idioms.\n2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \\- Excellent overview of Python's built-in data structures.\n3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \\- A deep dive into decorators.\n\n## Example Requests:\n\n1. **Looking for**: Video tutorials on web scraping with Python.\n2. **Need**: Book recommendations for Python machine learning.\n\nShare the knowledge, enrich the community. Happy learning! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-03T17:00:33",
    "url": "https://reddit.com/r/Python/comments/1nxf9sn/saturday_daily_thread_resource_request_and/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nx12sv",
    "title": "How to Level Up Your Python Logs with Structlog",
    "content": "For modern applications, structured and context-aware logging is essential for observability. [Structlog](https://www.structlog.org/) is one of the better tools in the Python ecosystem for achieving this with a more intuitive model than the standard logging's system of handlers, formatters, and filters.\n\n[I wrote a guide](https://www.dash0.com/guides/python-logging-with-structlog) that provides a step-by-step walkthrough for implementing clean, production-ready logging with Structlog.\n\nKeen to hear your thoughts, and if you think it's worth switching to from the `logging` module.",
    "author": "finallyanonymous",
    "timestamp": "2025-10-03T07:43:30",
    "url": "https://reddit.com/r/Python/comments/1nx12sv/how_to_level_up_your_python_logs_with_structlog/",
    "score": 27,
    "num_comments": 13,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ny2zk8",
    "title": "Is zfill() useless in Python?",
    "content": "I‚Äôm trying to learn all of Python‚Äôs built-in functions before starting OOP, so I‚Äôm curious how this function could be used in real projects.",
    "author": "Timely-Cat-6587",
    "timestamp": "2025-10-04T12:23:55",
    "url": "https://reddit.com/r/Python/comments/1ny2zk8/is_zfill_useless_in_python/",
    "score": 0,
    "num_comments": 14,
    "upvote_ratio": 0.24,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwv8re",
    "title": "pyya - Simple tool that converts YAML/TOML configuration files to Python objects",
    "content": "New version 0.1.11 is ready, now *pyya* can convert and validate configuaration from TOML files. In the previous version, I also added a CLI tool to generate stub files from your YAML/TOML configuaration fil, so that tools like mypy can validate type hints and varoius LSPs can autocomplete dynamic attribute-style dictionary. Check README for more info. Contributions/suggestions are welcome as always.\n\nCheck GitHub Page: [https://github.com/shadowy-pycoder/pyya](https://github.com/shadowy-pycoder/pyya)  \nCheck PyPi Page: [https://pypi.org/project/pyya/](https://pypi.org/project/pyya/)",
    "author": "wit4er",
    "timestamp": "2025-10-03T03:19:40",
    "url": "https://reddit.com/r/Python/comments/1nwv8re/pyya_simple_tool_that_converts_yamltoml/",
    "score": 19,
    "num_comments": 6,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwnr98",
    "title": "PyThermite - Rust backed object indexer",
    "content": "Attention ‚ö†Ô∏è : NOT another AI wrapper\n\nBeta released today - open to feedback - especially bugs\n\nhttps://github.com/tylerrobbins5678/PyThermite\n\nhttps://pypi.org/project/pythermite/\n\n-**what My Project Does**\n\nPyThermite is a rust backed python object indexer that supports nested objects and queries with real-time data. In plain terms, this means that complex data relations can be conveyed in objects, maintained state, and queried easily. For example, if I have a list of 100k cars in a city and want to get a list of cars moving between 20 and 40 mph and the owner of the car is named \"Jim\" that was born after 2005, that can be a single built query with sub 1 ms response. Keep in mind that the cars speed is constantly changing, updating the data structures as it goes.\n\nIn testing, its significantly (20- 50x) faster than pandas dataframe filtering on a data size of 100k. Query time complexity is roughly O(q + r) where q is the amount of query operations (and, or, in, eq, gt, nesting, etc) and r is the result size. \n\nThe cost to index is defined paid and building the structure takes around 6-7x longer than a dataframe consuming a list, but definitely worth it if the data is queried more than 3-4 times\n\nPerformance has been and is still a constant battle with the hashmap and b-tree inserts consuming most of the process time. \n\n-**Target Audience**\n\nCurrently this is not production ready as it is not tested thoroughly. Once proven, it will be supported and continue driving towards ETL and simulation within OOP driven code. At this current state it should only be used for analytics and analysis \n\n-**Conparison**\n\nThis competes with traditional dataframes like arrow, pandas, and polars, except it is the only one that handles native objects internally as well as indexes attributes for highly performant lookup. There's a few small alternatives out there, but nothing written with this much focus on performance.",
    "author": "Interesting-Frame190",
    "timestamp": "2025-10-02T19:51:04",
    "url": "https://reddit.com/r/Python/comments/1nwnr98/pythermite_rust_backed_object_indexer/",
    "score": 44,
    "num_comments": 17,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwy2zb",
    "title": "OCR-StringDist - Learn and Fix OCR Errors",
    "content": "# What My Project Does\n\nI built this library to fix errors in product codes read from images.\n\nFor example, \"O\" and \"0\" look very similar and are therefore often mixed up by OCR models. However, most string distance implementations do not consider character similarity.\n\nTherefore, I implemented a weighted Levenshtein string distance with configurable costs on a character- or token-level.\n\nThese weights can either be configured manually or they can be learned from a dataset of (read, true) labels using a probabilistic learning algorithm.\n\n# Basic Usage\n\n    from ocr_stringdist import WeightedLevenshtein\n    \n    training_data = [\n        (\"128\", \"123\"), # 3 misread as 8\n        (\"567\", \"567\"),\n    ]\n    # Holds learned substitution, insertion and deletion weights\n    wl = WeightedLevenshtein.learn_from(training_data)\n    \n    ocr_output = \"Product Code 148\"\n    candidates = [\n        \"Product Code 143\",\n        \"Product Code 848\",\n    ]\n    distances: list[float] = wl.batch_distance(ocr_output, candidates)\n\n# Target Audience\n\nProfessionals who work on data extraction from images.\n\n# Comparison\n\nThere are multiple string distance libraries, such as [rapidfuzz](https://github.com/rapidfuzz/RapidFuzz), [jellyfish](https://github.com/jamesturk/jellyfish), [textdistance](https://github.com/life4/textdistance) and [weighted-levenshtein](https://github.com/infoscout/weighted-levenshtein), with most of them being a bit faster and having more diverse string distances.\n\nHowever, there are very few good implementations that support character- or token-level weights and I am not aware of any that support learning weights from training data.\n\n# Links\n\n[Repository](https://github.com/NiklasvonM/ocr-stringdist) [pypi](https://pypi.org/project/ocr-stringdist/) [Documentation](https://niklasvonm.github.io/ocr-stringdist/)\n\nI'm grateful for any feedback and hope that my project might be useful to someone.",
    "author": "NiklasvonM",
    "timestamp": "2025-10-03T05:43:16",
    "url": "https://reddit.com/r/Python/comments/1nwy2zb/ocrstringdist_learn_and_fix_ocr_errors/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwxqad",
    "title": "Simulate Apache Spark Workloads Without a Cluster using FauxSpark",
    "content": "**What My Project Does**\n\n[FauxSpark](https://github.com/fhalde/fauxspark) is a discrete event simulation of Apache Spark using SimPy. It lets you experiment with Spark workloads and cluster configurations without spinning up a real cluster ‚Äì perfect for testing failures, scheduling, or capacity planning to observe the impact it has on your workload.\n\nThe first version includes:\n\n* DAG scheduling with stages, tasks, and dependencies\n* Automatic retries on executor or shuffle-fetch failures\n* Single-job execution with configurable cluster parameters\n* Simple CLI to tweak cluster size, simulate failures, and scaling up executors\n\n**Target Audience**\n\n* **Data &amp; Infrastructure engineers** running Apache Spark who want to experiment with cluster configurations\n* Anyone curious about **Spark internals**\n\nI'd love feedback from anyone with experience in discrete event simulation, especially on the planned features, as well as from anyone who found this useful. I have created some example DAGs for you to try it out!\n\nGH repo [https://github.com/fhalde/fauxspark](https://github.com/fhalde/fauxspark)",
    "author": "No_Direction_5276",
    "timestamp": "2025-10-03T05:27:12",
    "url": "https://reddit.com/r/Python/comments/1nwxqad/simulate_apache_spark_workloads_without_a_cluster/",
    "score": 7,
    "num_comments": 1,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwi0jd",
    "title": "PyCharm Pro Gift Code | 1-Year FREE",
    "content": "**Hail**, fellow Python lovers!\n\nI randomly found a great deal today. I was going to subscribe to PyCharm Pro monthly for personal use (they have a few features that integrate with GCloud I would like to leverage). On the checkout page, I saw a \"Have a gift code?\" prompt. I googled \"PyCharm Pro coupon code\" or something like that.\n\nOne of the first few websites in the results had a handful of coupons listed to use. First try, boom 25% off, not bad. Second try, boom 25% off again, not bad. Third try, boom... wait... 100 percent off, what in the hell?!?! I selected PayPal as my payment option. Since the total was $0.00, it did not ask me for my PayPal email. It showed the purchase success page with a receipt for $0.00. Paying nothing for a product that normally costs $209.99/year felt pretty good!\n\nThe coupon code you enter on the checkout page is:\n\n**Chand\\_Sheikh**\n\nYou can only redeem the Gift Code once per account! You can choose one of the eleven IDEs offered by IntelliJ (PyCharm, PHPStorm, RustRover, RubyMine, ReSharper, etc, etc.). So choose wisely!\n\nThe only thing I ask in return for this information is that you take a moment to try to make someone else's day a bit better üíñ It can be anyone. Spread love!\n\n**TLDR**: You can get a free year of one of the eleven premium IDEs IntelliJ sells by using the gift code \"*Chand\\_Sheikh*\". Do something to make another person's day a bit better.\n\n*Parts of this post were* ***NOT*** *written with ChatGPT or Ai. I prefer to add my own touch.*",
    "author": "DrDeems",
    "timestamp": "2025-10-02T15:23:40",
    "url": "https://reddit.com/r/Python/comments/1nwi0jd/pycharm_pro_gift_code_1year_free/",
    "score": 82,
    "num_comments": 32,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwhdmt",
    "title": "Snakebar ‚Äî a tqdm-style progress bar that snakes across your terminal",
    "content": "## What My Project Does  \nSnakebar is a `tqdm`-like progress bar for Python. Instead of a plain horizontal bar, it draws a one-character snake that fills your terminal via a random space-filling curve.    \nIt still reports percentage, iterations done, ETA, and rate (it/s), but makes waiting more fun.  \n  \n## Target Audience  \nAnyone who runs long scripts, pipelines, or training loops ‚Äî data scientists, ML engineers, researchers, developers with heavy ETL or simulations.    \nIt‚Äôs meant as a lightweight library you can drop in as a direct replacement for `tqdm`. It‚Äôs production-ready but also works fine as a fun toy project in personal scripts.  \n  \n## Comparison  \nCompared to `tqdm`:  \n- Same semantics (`snake_bar` works like `tqdm`).  \n- Still shows % complete, ETA, and rate.  \n- Instead of a static bar, progress is visualized as a snake filling the screen.  \n- Fits automatically to your terminal size.  \n  \n## Installation  \n```bash  \npip install snakebar  \n```\n\n## Links  \n- PyPI: [https://pypi.org/project/snakebar/](https://pypi.org/project/snakebar/)  \n- GitHub: [https://github.com/majoburo/snakebar](https://github.com/majoburo/snakebar)  \n- License: MIT",
    "author": "Library-Extra",
    "timestamp": "2025-10-02T14:58:11",
    "url": "https://reddit.com/r/Python/comments/1nwhdmt/snakebar_a_tqdmstyle_progress_bar_that_snakes/",
    "score": 80,
    "num_comments": 21,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nx8bzo",
    "title": "My journey from PyCon Accra 2024 to preparing for PyCon Africa 2025 in South Africa",
    "content": "I‚Äôm [DJAKPA Koffi](https://www.linkedin.com/in/djakpa-koffi), a tech enthusiast from Togo.\n\nIn October 2024, I had the incredible opportunity to attend **PyCon Africa in Accra, Ghana**. I learned a lot, met inspiring developers from across Africa, and returned home motivated to share my knowledge.\n\nAt my return, with friends, we organized an **Extender in Lom√©**, which brought together nearly 200 registrants and over 150 attendees. It was amazing to see the engagement and interest from participants, confirming that our efforts were having a real impact.\n\nNow, I am preparing to attend **PyCon Africa 2025 in South Africa** to continue learning and bring back even more knowledge to share with young learners.  \nA few days before the event, reality catches up with me, and to change that reality, I need your support ( [https://gofund.me/2df7717be](https://gofund.me/2df7717be) ), whatever form it may take. Thank you for your time and attention.",
    "author": "Kof7029",
    "timestamp": "2025-10-03T12:14:15",
    "url": "https://reddit.com/r/Python/comments/1nx8bzo/my_journey_from_pycon_accra_2024_to_preparing_for/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwxo9z",
    "title": "Real-time Air Quality Monitoring with Python, BLE, and Ubidots",
    "content": "Built a real-time air quality monitoring system in Python using a BleuIO dongle and visualize in Ubidots. It listens to BLE packets from a HibouAir sensor, decodes CO2/temperature/humidity, and streams the data to a live dashboard.  \n[https://www.bleuio.com/blog/connecting-bleuio-to-ubidots-a-practical-industrial-iot-air-quality-solution/](https://www.bleuio.com/blog/connecting-bleuio-to-ubidots-a-practical-industrial-iot-air-quality-solution/)",
    "author": "bleuio",
    "timestamp": "2025-10-03T05:24:40",
    "url": "https://reddit.com/r/Python/comments/1nwxo9z/realtime_air_quality_monitoring_with_python_ble/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nws4l2",
    "title": "Introducing Aird ‚Äì A Lightweight, Cross-Device File Sharing Tool",
    "content": "Hi everyone,\n\nI'm excited to share my open-source project called Aird.\n\n# What My Project Does\n\nAird is a simple and efficient file-sharing web server built with Python Tornado. It's designed to help you quickly share files across devices on the same network or remotely. It provides a clean web interface for file management and utilizes WebSockets for real-time transfer updates, ensuring a smooth user experience.\n\n# Target Audience\n\nThis tool is for developers, sysadmins, or anyone looking for a straightforward alternative to cumbersome file-sharing applications. It is well-suited for both technical and non-technical users who need a quick way to transfer files in a local network, for remote sharing, or within collaborative environments. It can be used as a personal project or deployed in a production setting for teams.\n\n# Comparison\n\nUnlike many popular file-sharing services that rely on third-party cloud servers, Aird is self-hosted, giving you complete control over your data. Compared to other local-first tools, Aird offers a modern web UI and real-time updates via WebSockets, which many simpler scripts or command-line tools lack. Its lightweight nature and minimal setup also make it a more efficient alternative to heavier, resource-intensive solutions.\n\n# Key Features:\n\n* Cross-device file sharing with instant web-based access\n* WebSocket-based real-time file transfers and updates\n* Minimal setup, lightweight, and great performance\n* Web UI for easy file management and uploads\n* Perfect for local networks, remote sharing, or collaborative environments\n\nThe code is fully open-source, and contributions are welcome. Give Aird a try!\n\n**GitHub link:**¬†[`https://github.com/blinkerbit/aird`](https://github.com/blinkerbit/aird)\n\nI'd love to hear your feedback, ideas, or feature requests! Thanks for checking it out",
    "author": "Good-Definition-7148",
    "timestamp": "2025-10-03T00:00:22",
    "url": "https://reddit.com/r/Python/comments/1nws4l2/introducing_aird_a_lightweight_crossdevice_file/",
    "score": 6,
    "num_comments": 3,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwk7ps",
    "title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays",
    "content": "# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è\n\nWelcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!\n\n## How it Works:\n\n1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.\n2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.\n3. **News &amp; Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.\n\n## Guidelines:\n\n* All topics should be related to Python or the /r/python community.\n* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).\n\n## Example Topics:\n\n1. **New Python Release**: What do you think about the new features in Python 3.11?\n2. **Community Events**: Any Python meetups or webinars coming up?\n3. **Learning Resources**: Found a great Python tutorial? Share it here!\n4. **Job Market**: How has Python impacted your career?\n5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!\n6. **Community Ideas**: Something you'd like to see us do? tell us.\n\nLet's keep the conversation going. Happy discussing! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-02T17:00:51",
    "url": "https://reddit.com/r/Python/comments/1nwk7ps/friday_daily_thread_rpython_meta_and_freetalk/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nvvsub",
    "title": "OneCode ‚Äî Python library to turn scripts into deployable apps",
    "content": "# What My Project Does\n\n**OneCode** is an open-source Python library that lets you convert your scripts to apps with minimal boilerplate. Using simple decorators/parameters, you define inputs/outputs, and OneCode automatically generates a UI for you.\n\nGithub link is here: [https://github.com/deeplime-io/onecode](https://github.com/deeplime-io/onecode)\n\nOn **OneCode Cloud**, those same apps can be deployed instantly, with authentication, scaling, and access controls handled for you.\n\nThe cloud platform is here: [https://www.onecode.rocks/](https://www.onecode.rocks/) (free tier includes 3 apps, 1Gb of storage and up to 5 hours of compute).\n\nOneCode allows you to run the same code locally or on the cloud platform (one code ;)). You can connect your github account and automatically sync code to generate the app.\n\n# Target Audience\n\n* **Python developers** who want to share tools without building a web frontend\n* **Data scientists / researchers** who need to wrap analysis scripts with a simple interface\n* **Teams** that want internal utilities, but don‚Äôt want to manage deployment infrastructure\n* Suitable for **production apps** (access-controlled, secure), but lightweight enough for **prototyping and demos**.\n\n# Comparison\n\n* Unlike **Streamlit/Gradio**, OneCode doesn‚Äôt focus on dashboards, instead it auto-generates minimal UIs from your function signatures. OneCode cloud is also usable with long running compute, big machines are available, and compute is scalable with the number of users.\n* Unlike **Flask/FastAPI**, you don‚Äôt need to wire up endpoints, HTML, or auth, it‚Äôs all handled automatically.\n* The **cloud offering** provides secure runtime, scaling, and sharing out of the box, whereas most libraries stop at local execution.\n\nCode examples:\n\n`INPUTS`\n\n    `# instead of: df = pd.read_csv('test.csv')`\n    \n    `df = csv_reader('your df', 'test.csv')`\n    \n    \n    \n    `# instead of: for i in range(5):`\n    \n    `for i in range(slider('N', 5, min=0, max=10)):  # inlined`\n        # do stuff\n\n    `# instead of: choice = 'cat'`\n    \n    `choice = dropdown('your choice', 'cat', options=['dog', 'cat', 'fish'])` \n    \n    `#not inlined`\n    \n    `Logger.info(f'Your choice is {choice}')`\n\n`OUTPUTS`\n\n    `# instead of: plt.savefig('stuff.png')`\n    \n    `plt.savefig(file_output('stuff', 'stuff.png'))  # inlined`\n    \n    \n    \n    `# instead of: filepath = 'test.txt'`\n    \n    `filepath = file_output('test', 'test.txt')  # not inlined`\n    \n    `with open(filepath, 'w') as f:`\n          # do stuff\n\n\n\nHappy to answer questions or provide more examples! We have a few example apps on the cloud already which are available to everyone. You can find a webinar on the library and cloud here:\n\n[https://www.youtube.com/watch?v=BPj\\_cbRUwLk](https://www.youtube.com/watch?v=BPj_cbRUwLk)\n\nWe are looking for any feedback at this point! cheers",
    "author": "goochop",
    "timestamp": "2025-10-01T22:58:48",
    "url": "https://reddit.com/r/Python/comments/1nvvsub/onecode_python_library_to_turn_scripts_into/",
    "score": 53,
    "num_comments": 5,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nweuv0",
    "title": "BuildLog: a simple tool to track and version your Python builds",
    "content": "Hey r/Python! üëã\n\nI‚Äôd like to share BuildLog, a Python CLI tool for tracking and versioning build outputs. It‚Äôs designed for standalone executables built with PyInstaller, Nuitka, or any other build command.\n\n# What my project does\n\nBasically, when you run a build, BuildLog captures all the new files/folders built at the current state of your repository, recording SHA256 hashes of executables, and logging Git metadata (commit, branch, tags, commit message). Everything goes into a .buildlog folder so you can always trace which build came from which commit.\n\nOne cool thing: it doesn‚Äôt care which build tool you use. It basically just wraps whatever command you pass and tracks what it produces. So even if you use something other than PyInstaller or Nuitka, it should still work.\n\n# Target Audience\n\n- Python developers building standalone executables.\n\n- Teams that need reproducible builds and clear history.\n\n- Anyone needing traceable builds.\n\n# Comparison\n\nI did not find similar tools to match my use cases, so I thought to build my own and I‚Äôm now happy to share it with you. Any feedback is welcome. \n\nCheck it out here to find more:  [BuildLog](https://github.com/adghin/buildlog) ‚Äì if you like it, feel free to give it a ‚≠ê!",
    "author": "Frosty-Jackfruit-977",
    "timestamp": "2025-10-02T13:22:30",
    "url": "https://reddit.com/r/Python/comments/1nweuv0/buildlog_a_simple_tool_to_track_and_version_your/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nvnyjr",
    "title": "Open Source Google Maps Street View Panorama Scraper.",
    "content": "  \n**What My Project Does**  \n  \n\\- With [gsvp-dl](https://github.com/yousephzidan/gsvp-dl), an open source solution written in Python, you are able to download millions of panorama images off Google Maps Street View.\n\n**Comparison**\n\n\\- Unlike other existing solutions (which fail to address major edge cases), [gsvp-dl](https://github.com/yousephzidan/gsvp-dl) downloads panoramas in their correct form and size with unmatched accuracy. Using Python Asyncio and Aiohttp, it can handle bulk downloads, scaling to millions of panoramas per day.\n\n\\- Other solutions don‚Äôt match up because they ignore edge cases, especially pre-2016 images with different resolutions. They used fixed width and height that only worked for post-2016 panoramas, which caused black spaces in older ones.\n\n**Target Audience**¬†\n\n\"For educational purposes only\" ***- just in case Google is watching.*** \n\nIt was a fun project to work on, as there was no documentation whatsoever, whether by Google or other existing solutions. So, I documented the key points that explain why a panorama image looks the way it does based on the given inputs (mainly zoom levels).\n\nThe way I was able to reverse engineer Google Maps Street View API was by sitting all day for a week, doing nothing but observing the results of the endpoint, testing inputs, assembling panoramas, observing outputs, and repeating. With no documentation, no lead, and no reference, it was all trial and error.\n\nI believe I have covered most edge cases, though I still doubt I may have missed some. Despite testing hundreds of panoramas at different inputs, I‚Äôm sure there could be a case I didn‚Äôt encounter. So feel free to fork the repo and make a pull request if you come across one, or find a bug/unexpected behavior.\n\nThanks for checking it out!",
    "author": "yousephx",
    "timestamp": "2025-10-01T16:25:10",
    "url": "https://reddit.com/r/Python/comments/1nvnyjr/open_source_google_maps_street_view_panorama/",
    "score": 28,
    "num_comments": 4,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nvy1iv",
    "title": "Local image and video classification tool using Google's sigLIP 2 So400m (naflex)",
    "content": "Hey everyone! I built a tool to search for images and videos locally using natural language with [Google's sigLIP 2 model](https://huggingface.co/google/siglip2-so400m-patch16-naflex).\n\nI'm looking for people to test it and share feedback, especially about how it runs on different hardware.\n\nDon't mind the ugly GUI, I just wanted to make it as simple and accessible as possible, but you can still use it as a command line tool anyway if you want to. You can find the repository here: [https://github.com/Gabrjiele/siglip2-naflex-search](https://github.com/Gabrjiele/siglip2-naflex-search)\n\n# What My Project Does\n\nMy project, `siglip2-naflex-search`, is a desktop tool that lets you search your local image and video files using natural language. You can find media by typing a description (of varying degrees of complexity) or by using an existing image to find similar ones. It features both a user-friendly graphical interface and a command-line interface for automation. The tool uses Google's powerful SigLIP 2 model to understand the content of your files and stores the data locally in an SQLite database for fast, private searching.\n\n# Target Audience\n\nThis tool is designed for anyone with a large local collection of photos and videos who wants a better way to navigate them. It is particularly useful for:\n\n* **Photographers and videographers** needing to quickly find specific shots within their archives.\n* **AI enthusiasts and developers** looking for a hands-on project that uses a SOTA vision-language model.\n* **Privacy-conscious users** who prefer an offline solution for managing their personal media without uploading it to the cloud.\n\n**IT IS NOT INTENDED FOR LARGE SCALE ENTERPRISE PRODUCTION**.\n\n# Comparison\n\nThis project stands apart from alternatives like `rclip` and other search tools built on the original CLIP model in a few significant ways:\n\n* **Superior model**: It is built on **Google's SigLIP 2**, a more recent and powerful model that provides better performance and efficiency in image-text retrieval compared to the original CLIP used by `rclip`. SigLIP 2's training method leads to improved semantic understanding.\n* **Flexible resolution (NaFlex)**: The tool utilizes the `naflex` variant of SigLIP 2, which can process images at various resolutions while preserving their original aspect ratio. This is a major advantage over standard CLIP models that often resize images to a fixed square, which can distort content and reduce accuracy (especially in OCR applications).\n* **GUI and CLI**: Unlike `rclip` which is primarily a command-line tool, this project offers both a **very simple graphical interface (will update in the future) and a command line interface**. This makes it accessible to a broader audience, from casual users to developers who need scripting capabilities.\n* **Integrated video search**: It's one of the very few tools that provides video searching as a built-in feature: it extracts and indexes frames to make video content searchable out of the box.",
    "author": "AnywhereTypical5677",
    "timestamp": "2025-10-02T01:19:52",
    "url": "https://reddit.com/r/Python/comments/1nvy1iv/local_image_and_video_classification_tool_using/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nv3tgp",
    "title": "Logly üöÄ ‚Äî a Rust-powered, super fast, and simple logging library for Python",
    "content": "**What My Project Does**\n\ni am building an Logly a  **logging library for Python** that combines simplicity with **high performance** using a Rust backend. It supports:\n\n* Console and file logging\n* JSON / structured logging\n* Async background writing to reduce latency\n* Pretty formatting with minimal boilerplate\n\nIt‚Äôs designed to be **lightweight, fast, and easy to use**, giving Python developers a modern logging solution without the complexity of the built-in `logging` module.\n\n**Latency Microbenchmark (30,000 messages):**\n\n|Percentile|`logging`Python|Logly|Speedup|\n|:-|:-|:-|:-|\n|p50|0.014 ms|0.002 ms|7√ó|\n|p95|0.029 ms|0.002 ms|14.5√ó|\n|p99|0.043 ms|0.015 ms|2.9√ó|\n\n&gt;\n\n\\&gt; **Note:** Performance may vary depending on your OS, CPU, Python version, and system load. Benchmarks show **up to 10√ó faster performance** under high-volume or multi-threaded workloads, but actual results will differ based on your environment.\n\n**Target Audience**\n\n* Python developers needing high-performance logging\n* Scripts, web apps, or production systems\n* Developers who want structured logging or async log handling without overhead\n\n**Logging Library Comparison**\n\n|Feature / Library|`logging`Python|Loguru|Structlog|**Logly (v0.1.1)**|\n|:-|:-|:-|:-|:-|\n|**Backend**|Python|Python|Python|Rust|\n|**Async Logging**|‚ùå|‚úÖ (basic)|‚úÖ|‚úÖ (high-performance, async background writer)|\n|**File &amp; Console Logging**|‚úÖ|‚úÖ|‚úÖ|‚úÖ|\n|**JSON / Structured Logging**|‚úÖ (manual)|‚úÖ|‚úÖ|‚úÖ (built-in, easy)|\n|**Ease of Use**|Medium|High|Medium|High (simple API, minimal boilerplate)|\n|**Performance (single-threaded)**|Baseline|\\~1.5‚Äì2√ó faster|\\~1√ó|\\~3.5√ó faster|\n|**Performance (multi-threaded / concurrent)**|Baseline|\\~2‚Äì3√ó|\\~1√ó|**up to 10√ó faster** üöÄ|\n|**Pretty Formatting / Color**|‚ùå / limited|‚úÖ|‚ùå|‚úÖ|\n|**Rotation / Retention**|‚úÖ (config-heavy)|‚úÖ|Limited|‚úÖ|\n|**Additional Notes**|Standard library, reliable, but verbose and slower|Easy setup, friendly API|Structured logging focus|Rust backend, optimized for high-volume, async, low-latency logging|\n\n**Example Usage**\n\n    from logly import logger\n    \n    logger.info(\"Hello from Logly!\")\n    logger.debug(\"Logging asynchronously to a file\")\n    logger.error(\"Structured logging works too!\", extra={\"user\": \"alice\"})\n\n**Links**\n\n* GitHub (open source): [https://github.com/muhammad-fiaz/logly/](https://github.com/muhammad-fiaz/logly/)\n* PyPI: [https://pypi.org/project/logly/](https://pypi.org/project/logly/)\n\n**To Get Started:**\n\n    pip install logly\n\nPlease feel free to **check it out, give feedback, and report any issues** on GitHub or PyPI. I‚Äôd really appreciate your thoughts and contributions! üôÇ\n\n**Note: This Project is Not Vibe-Coded or AI Used i am Only Have Used AI for Documentation Purposes to Speed up Initial Development Only, the Code itself is Mine and Implemented by Mine (No AI Usage from the start itself) and also the performance of logly is not tested fully yet because this project is still in active development!**\n\n**UPDATE!!! üöÄ** (03-10-2025) Thanks for all the feedback, everyone! Based on user requests, I‚Äôve improved **Logly v0.1.4** (Released now) and added some new features. I‚Äôve also updated the documentation for better clarity.\n\n‚úÖ Currently, Logly supports **Linux, Windows, and macOS** for **Python 3.10 to 3.13**. üìñ Please report any issues or errors directly on **GitHub,** that‚Äôs the best place for bug reports and feature requests (not Reddit). For broader conversations, please use [**GitHub Discussions**](https://github.com/muhammad-fiaz/logly/discussions).\n\nFor those [asking for proof of my work](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nhdisrd/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button): I‚Äôve been actively coding and tracking my projects via [WakaTime](https://wakatime.com/@muhammadfiaz), and I have a good community supporting my work.\n\nI understand some people may not like the project, and that‚Äôs fine, you‚Äôre free to have your opinion. But if you want to give constructive feedback, please do it openly on GitHub under your real account instead of throwaway or anonymous ones. That way, the feedback is more helpful and transparent.\n\nBTW! I take **docstrings and documentation very seriously :) ,** I personally review every single one each time to ensure quality and clarity. If anything is missing or not updated for the latest release, you can always create an issue or a PR. I always welcome contributions.\n\nAlso, judging whether I used AI just based on [my *constant*¬†bullet points, bold text or docstrings ](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nh9h8ua/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button)in the Rust code? That‚Äôs really childish,  comments and docstrings alone aren‚Äôt proof of anything. I always make sure to add both to keep everything well-documented for contributors, and also saying \"[Rust Devs don't use comments and docstrings](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nhe8f9v/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button)\" but I‚Äôve seen plenty of experienced Rust developers use them as well, in Rust and across all programming languages.\n\nFinally üôÇ I am not promoting or making statements about whether using AI is right or wrong, good practice or bad practice, it depends entirely on your use case and personal preference and up to you only.\n\nIf you still insist this is ‚Äúvibe coding,‚Äù then fine, that‚Äôs your opinion. If not, then it‚Äôs whatever, I don‚Äôt care. I am using my **real name** and being transparent. Just because I work on this project personally doesn‚Äôt mean it‚Äôs for a [job or resume](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nh7zjyc/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button); I‚Äôve clearly stated that in my profile. If you want to collaborate, feel free to do so for **improvements**, but commenting about [useless things](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nhdyz6a/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) or **misleading claims by** [puppeteer accounts](https://www.reddit.com/user/bay400/) doesn‚Äôt help anyone.\n\nI wrote this message for people who are genuinely interested in creating new methods or contributing. [I am not promoting the project simply because it‚Äôs in Rust](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nhdyp23/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button),I wanted **feedback**, which is why I‚Äôm asking for input here for improvement, not for childish debates about whether I used AI or not.\n\nAt the end of the day, we‚Äôre all here to learn, whether you have 20+ years of experience in IT or you‚Äôre just a newbie. **Constructive discussion and improvements help everyone grow.**\n\nAnd just to be clear I‚Äôm doing this to build awesome things in public and grow in public, so people can see the progress, learn, and contribute along the way :)\n\n**Thanks again for all your support! üôèüôÇ**",
    "author": "muhammad-fiaz",
    "timestamp": "2025-10-01T02:43:28",
    "url": "https://reddit.com/r/Python/comments/1nv3tgp/logly_a_rustpowered_super_fast_and_simple_logging/",
    "score": 252,
    "num_comments": 138,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwhu1f",
    "title": "An interesting open-source tool for turning LLM prompts into testable, version-controlled artifacts.",
    "content": "Hey everyone,\n\nIf you've been working with LLMs in Python, you've probably found yourself juggling complex f-strings or Jinja templates to manage your prompts. It can get messy fast, and there's no good way to test or version them.\n\nI wanted a more robust, \"Pythonic\" way to handle this, so I built ProML (Prompt Markup Language).\n\nIt's an open-source toolchain, written in Python and installable via pip, that lets you define, test, and manage prompts as first-class citizens in your project.\n\nInstead of just strings, you define prompts in .proml files, which are validated against a formal spec. You can then load and run them easily within your Python code:\n\nimport proml\n\nLoad a structured prompt from a file\n\nprompt = proml.load(\"prompts/sentiment_analysis.proml\")\n\nExecute it with type-safe inputs\n\nresult = prompt.run(comment=\"This is a great product!\")\n\nprint(result.content)\n\n=&gt; \"positive\"\n\nSome of the key features:\n\nPure Python &amp; Pip Installable: The parser, runtime, and CLI are all built in Python.\n\nFull CLI Toolchain: Includes commands to lint, fmt, test, run, and publish your prompts.\n\nTesting Framework: You can define test cases directly in the prompt files to validate LLM outputs against regex, JSON Schema, etc.\n\nLibrary Interface: Designed to be easily integrated into any Python application\n\nVersioning &amp; Registry: A local registry system to manage and reuse prompts across projects with semver.\n\nI'm the author and would love to get feedback from the Python community. What do you think of this approach?\n\nYou can check out the source and more examples on GitHub, or install it and give it a try.\n\nGitHub: https://github.com/Caripson/ProML\n\nDocs : https://github.com/Caripson/ProML/blob/main/docs/index.md\n\nTarget audience: LLM developers, prompt-engineers \n\nComparison: haven‚Äôt found any similar ",
    "author": "FarCardiologist7256",
    "timestamp": "2025-10-02T15:16:17",
    "url": "https://reddit.com/r/Python/comments/1nwhu1f/an_interesting_opensource_tool_for_turning_llm/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nv696e",
    "title": "Just built a tool that turns any Python app into a native windows service",
    "content": "What My Project Does\n\nI built a tool called Servy that lets you run any Python app (or other executables) as a native Windows service. You just set the Python executable path, add your script and arguments (for example -u for unbuffered mode if you want stdout and stderr logging), choose the startup type, working directory, and environment variables, configure any optional parameters, click install ‚Äî and you‚Äôre done. Servy comes with a GUI, CLI, PowerShell integration, and a manager app for monitoring services in real time.\n\nTarget Audience\n\nServy is meant for developers or sysadmins who need to keep Python scripts running reliably in the background without having to rewrite them as Windows services. It works equally well for Node.js, .NET, or any executable, but I built it with Python apps in mind. It‚Äôs designed for production use on Windows 7 through Windows 11 as well as Windows Server.\n\nComparison\n\nCompared to tools like sc or nssm, Servy adds important features that make managing services easier. It lets you set a custom working directory (avoiding the common C:\\Windows\\System32 issue that breaks relative paths), redirect stdout and stderr to rotating log files, and configure health checks with automatic recovery and restart policies. It also provides a clean, modern UI and real-time service management, making it more user-friendly and capable than existing options.\n\nRepo: https://github.com/aelassas/servy\n\nDemo video: https://www.youtube.com/watch?v=biHq17j4RbI\n\nAny feedback is welcome.",
    "author": "AdUnhappy5308",
    "timestamp": "2025-10-01T05:01:07",
    "url": "https://reddit.com/r/Python/comments/1nv696e/just_built_a_tool_that_turns_any_python_app_into/",
    "score": 78,
    "num_comments": 8,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nw0gy6",
    "title": "Exercises to Build the Right Mental Model for Python Data",
    "content": "An exercise to build the right mental model for Python data. The ‚ÄúSolution‚Äù link below uses memory\\_graph to visualize execution and reveal what‚Äôs actually happening.\n\nWhat is the output of this Python program?\n\n    a = [1]\n    b = a\n    b += [2]\n    b.append(3)\n    b = b + [4]\n    b.append(5)\n    \n    print(a)\n    # --- possible answers ---\n    # A) [1]\n    # B) [1, 2]\n    # C) [1, 2, 3]\n    # D) [1, 2, 3, 4]\n    # E) [1, 2, 3, 4, 5]\n\n* [Solution](https://memory-graph.com/#codeurl=https://raw.githubusercontent.com/bterwijn/memory_graph_videos/refs/heads/main/exercises/exercise9.py&amp;play)\n* [Explanation](https://github.com/bterwijn/memory_graph?tab=readme-ov-file#python-data-model)\n* [More Exercises](https://www.reddit.com/r/Python_memory_graph/)",
    "author": "Sea-Ad7805",
    "timestamp": "2025-10-02T03:50:23",
    "url": "https://reddit.com/r/Python/comments/1nw0gy6/exercises_to_build_the_right_mental_model_for/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwa9zx",
    "title": "Hello! I‚Äôm very new in tech industry and right now I went to learn. Which  language should I learn?",
    "content": "Is there any private classes to take? I really want to learn and develop app, website and so‚Ä¶. But I don‚Äôt new where to start, can someone support my? ",
    "author": "Ok-Ambassador-9114",
    "timestamp": "2025-10-02T10:32:44",
    "url": "https://reddit.com/r/Python/comments/1nwa9zx/hello_im_very_new_in_tech_industry_and_right_now/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwd03c",
    "title": "My new package in pypi",
    "content": "https://github.com/keikurono7/keywordx\nhttps://pypi.org/project/keywordx/\n\nWhat my project does:\nThis package helps you extract keywords from sentences not only by similarity but even context related. It needs improvement but this is the initial stage.\n\nTarget audience:\nIt can be used in any field from digital assistant to web search. This package integration helps in getting important information in more better way. \n\nComparison:\nUnlike other keyword extractor tools it is not limited to date and time or not a similar word marker. It finds the best match based on the meanings the whole sentence gives \n\nComment for any suggestions or anything ",
    "author": "madpool04",
    "timestamp": "2025-10-02T12:13:14",
    "url": "https://reddit.com/r/Python/comments/1nwd03c/my_new_package_in_pypi/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.15,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nvoqx0",
    "title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",
    "content": "# Weekly Thread: Professional Use, Jobs, and Education üè¢\n\nWelcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.\n\n---\n\n## How it Works:\n\n1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.\n2. **Education Q&amp;A**: Ask or answer questions about Python courses, certifications, and educational resources.\n3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.\n\n---\n\n## Guidelines:\n\n- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.\n- Keep discussions relevant to Python in the professional and educational context.\n  \n---\n\n## Example Topics:\n\n1. **Career Paths**: What kinds of roles are out there for Python developers?\n2. **Certifications**: Are Python certifications worth it?\n3. **Course Recommendations**: Any good advanced Python courses to recommend?\n4. **Workplace Tools**: What Python libraries are indispensable in your professional work?\n5. **Interview Tips**: What types of Python questions are commonly asked in interviews?\n\n---\n\nLet's help each other grow in our careers and education. Happy discussing! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-10-01T17:00:38",
    "url": "https://reddit.com/r/Python/comments/1nvoqx0/thursday_daily_thread_python_careers_courses_and/",
    "score": 3,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nwb0fy",
    "title": "Real-time crypto pattern recognition dashboard built with Python + Dash",
    "content": "Hi all,\n\nI'm trying to build a real-time crypto pattern recognition dashboard using Python, Dash, and CCXT. It allows you to:\n\n\\- Predict the future by comparing real-time cryptocurrency charts with past chart patterns.\n\n\\- Limit pattern selection to avoid duplicates.\n\n\\- Analyze multiple coins (BTC, ETH, XRP) with an optional heatmap.\n\nI'm new to programming and currently using ChatGPT to bring my idea to real life. But I realized that ChatGPT and I alone wouldn't achieve what I wanted. \n\nRepo: [https://github.com/JuNov03/crypto-pattern-dashboard](https://github.com/JuNov03/crypto-pattern-dashboard)\n\nLooking for suggestions to improve pattern detection accuracy and UI/UX.\n\nThanks!",
    "author": "Noveim",
    "timestamp": "2025-10-02T10:59:16",
    "url": "https://reddit.com/r/Python/comments/1nwb0fy/realtime_crypto_pattern_recognition_dashboard/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nw7gah",
    "title": "Released Agent Builder project. Looking for feedback!",
    "content": "Hi everyone!\n\nI‚Äôve been working on a project called **PipesHub**, an open-source developer platform for building AI agent pipelines that integrate with real-world business data.\n\nThe main idea: teams often need to connect multiple apps (like Google Drive, Gmail, Confluence, Jira, etc.) and provide that context to agents. PipesHub makes it easier to set up those connections, manage embeddings, and build production-ready retrieval pipelines.\n\n**What the project does**\n\n* Provides connectors for major business apps\n* Supports embedding and chat models through standard endpoints\n* Includes tools like CSV/Excel/Docx/PPTX handling, web search, coding sandbox, etc.\n* Offers APIs and SDKs so developers can extend and integrate quickly\n* Designed to be modular: you can add connectors, filters, or agent tools as needed\n\n**Target audience**  \nThis project is mainly for developers who want to experiment with building agent-based applications that need enterprise-style context. It‚Äôs still evolving, but I‚Äôd love feedback on design, structure, and developer experience.\n\n**Repo**: [https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)\n\nAny suggestions, critiques, or contributions are super welcome üôè",
    "author": "Effective-Ad2060",
    "timestamp": "2025-10-02T08:48:03",
    "url": "https://reddit.com/r/Python/comments/1nw7gah/released_agent_builder_project_looking_for/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nweioo",
    "title": "Python code for battleship game",
    "content": "Hi everyone, does anyone have a code made in python to make a battleship game? Or probably from any other game that is ‚Äúeasy‚Äù. ",
    "author": "Outrageous_Willow603",
    "timestamp": "2025-10-02T13:09:41",
    "url": "https://reddit.com/r/Python/comments/1nweioo/python_code_for_battleship_game/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.15,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nw694f",
    "title": "Alimentar un asistente de GPT",
    "content": "Hola gente de reddit, estoy desarrollando una aplicaci√≥n conversacional en la uso python de la mano de Streamlit, invoco a un asistente que hice en ChatGPT para que mantenga la conversaci√≥n, el almacenamiento de las conversaciones lo hago por sesi√≥n, pero me gustar√≠a mantener un registro y as√≠ el los usuarios puedan recuperar conversaciones pasadas y el asistente pueda estar alimentado. ¬øComo lo podr√≠a ver donde almacenar las conversaciones? Todav√≠a soy algo poco experimentado en asistentes de GPT, pero ¬øEstos se pueden alimentar? Acepto recomendaciones y preguntas! ",
    "author": "Unable_Campaign6018",
    "timestamp": "2025-10-02T08:03:33",
    "url": "https://reddit.com/r/Python/comments/1nw694f/alimentar_un_asistente_de_gpt/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.21,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nv72oz",
    "title": "Typing the test suite",
    "content": "What is everyone's experience with adding type hints to the test suite? Do you do it (or are required to do it at work)? Do you think it is worth it?\n\nI tried it with a couple of my own projects recently, and it did uncover some bugs, API inconsistencies, and obsolete tests that just happened to still work despite types not being right. But there were also a number of annoyances (which perhaps would not be as noticeable if I added typing as I wrote the tests and not all at once). Most notably, due to the unfortunate convention of `mypy`, I had to add `-&gt; None` to all the test functions. There were also a number of cases where I used duck typing to make the tests visually simpler, which had to be amended to be more strict. Overall I'm leaning towards doing it in the future for new projects.",
    "author": "fjarri",
    "timestamp": "2025-10-01T05:39:24",
    "url": "https://reddit.com/r/Python/comments/1nv72oz/typing_the_test_suite/",
    "score": 16,
    "num_comments": 42,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nw3xsy",
    "title": "The list `awesome polars` close to 1,000 stars ü§©",
    "content": "\\`awesome polars\\` is close to reaching 1,000 stars on GitHub.\n\nIf you are interested in the Polars project, go take a look.\n\n[https://github.com/ddotta/awesome-polars](https://github.com/ddotta/awesome-polars)",
    "author": "damiendotta",
    "timestamp": "2025-10-02T06:34:21",
    "url": "https://reddit.com/r/Python/comments/1nw3xsy/the_list_awesome_polars_close_to_1000_stars/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.32,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nv5lcw",
    "title": "py-capnweb - A Python implementation of Cap'n Web's RPC protocol",
    "content": "I've just released v0.3.0 of a project I've been working on called **py-capnweb**.\n\nIt's a Python implementation of the [Cap'n Web protocol](https://github.com/cloudflare/capnweb), a fascinating new RPC protocol announced a couple of weeks ago. My implementation is **fully interoperable** with the official TypeScript version, so you can have a Python backend talking to a TypeScript/JS frontend (and vice-versa) seamlessly.\n\n# What The Project Does\n\n`py-capnweb` is designed to eliminate the friction of client-server communication. It makes remote function calls feel just like local function calls. Instead of manually writing HTTP endpoints, serializing data, and dealing with network waterfalls, you can design your APIs like you would a normal JavaScript or Python library.\n\nTwo main features stand out: **capability-based security** and **promise pipelining**. This means you pass around secure object references instead of raw data, and you can chain multiple dependent calls into a single network round trip, which can be a huge performance win.\n\n# Target Audience &amp; Production Readiness\n\nThis project is for developers building interactive, cross-language applications (e.g., Python backend, JS/TS frontend) who are tired of the boilerplate and latency issues that come with traditional REST or even GraphQL APIs.\n\n**Is it production-ready?** The protocol itself is new but built on the mature foundations of Cap'n Proto. My implementation is at `v0.3.0` and passes a comprehensive cross-implementation test suite. It's stable and ready for real-world use cases, especially for teams that want to be on the cutting edge of RPC technology.\n\n# How is it Different from REST, gRPC, or GraphQL?\n\nThis is the most important question! Here‚Äôs a quick comparison:\n\n* **vs. REST:** REST is resource-oriented, using a fixed set of verbs (GET, POST, etc.). Cap'n Web is object-oriented, allowing you to call methods on remote objects directly. This avoids the \"N+1\" problem and complex state management on the client, thanks to promise pipelining.\n* **vs. gRPC:** gRPC is a high-performance RPC framework, but it's schema-based (using Protocol Buffers). Cap'n Web is schema-less, making it more flexible and feel more native to dynamic languages like Python and JavaScript, which means less boilerplate. While gRPC has streaming, Cap'n Web's promise pipelining and bidirectional nature provide a more expressive way to handle complex, stateful interactions.\n* **vs. GraphQL:** GraphQL is excellent for querying complex data graphs in one go. However, it's a specialized query language and can be awkward for mutations or chained operations. Cap'n Web solves the same \"over-fetching\" problem as GraphQL but feels like writing regular code, not a query. You can intuitively chain calls (`user.getProfile()`, `profile.getFriends()`, etc.) in a single, efficient batch.\n\n# Key Features of py-capnweb\n\n* **100% TypeScript Interoperability**: Fully tested against the official `capnweb` library.\n* **Promise Pipelining**: Batch dependent calls into a single network request to slash latency.\n* **Capability-Based Security**: Pass around secure object references, not exposed data.\n* **Bidirectional RPC**: It's peer-to-peer; the \"server\" can call the \"client\" just as easily.\n* **Pluggable Transports**: Supports HTTP batch and WebSocket out-of-the-box. (More planned!)\n* **Fully Async**: Built on Python's `asyncio`.\n* **Type-Safe**: Complete type hints (tested with `pyrefly`/`mypy`).\n\n# See it in Action\n\nHere‚Äôs how simple it is to get started.\n\n**(Server,** `server.py`\\*\\*)\\*\\*\n\n    import asyncio\n    from typing import Any\n    from capnweb.server import Server, ServerConfig\n    from capnweb.types import RpcTarget\n    from capnweb.error import RpcError\n    \n    class Calculator(RpcTarget):\n        async def call(self, method: str, args: list[Any]) -&gt; Any:\n            match method:\n                case \"add\":\n                    return args[0] + args[1]\n                case \"subtract\":\n                    return args[0] - args[1]\n                case _:\n                    raise RpcError.not_found(f\"Method {method} not found\")\n    \n    async def main() -&gt; None:\n        config = ServerConfig(host=\"127.0.0.1\", port=8080)\n        server = Server(config)\n        server.register_capability(0, Calculator()) # Register main capability\n        await server.start()\n        print(\"Calculator server listening on http://127.0.0.1:8080/rpc/batch\")\n        await asyncio.Event().wait()\n    \n    if __name__ == \"__main__\":\n        asyncio.run(main())\n\n**(Client,** `client.py`\\*\\*)\\*\\*\n\n    import asyncio\n    from capnweb.client import Client, ClientConfig\n    \n    async def main() -&gt; None:\n        config = ClientConfig(url=\"http://localhost:8080/rpc/batch\")\n        async with Client(config) as client:\n            result = await client.call(0, \"add\", [5, 3])\n            print(f\"5 + 3 = {result}\")  # Output: 5 + 3 = 8\n    \n            result = await client.call(0, \"subtract\", [10, 4])\n            print(f\"10 - 4 = {result}\")  # Output: 10 - 4 = 6\n    \n    if __name__ == \"__main__\":\n        asyncio.run(main())\n\n# Check it out!\n\nI'd love for you to take a look, try it out, and let me know what you think. I believe this paradigm can genuinely improve how we build robust, cross-language distributed systems.\n\n* **GitHub Repo**: [https://github.com/abilian/py-capnweb](https://github.com/abilian/py-capnweb)\n* **Installation**: `pip install capnweb` or `uv add capnweb`\n\nThe project is dual-licensed under MIT or Apache-2.0. All feedback, issues, and contributions are welcome!\n\n**TL;DR:** I built a Python version of the new Cap'n Web RPC protocol that's 100% compatible with the official TypeScript version. It's built on `asyncio`, is schema-less, and uses promise pipelining to make distributed programming feel more like local development.",
    "author": "sfermigier",
    "timestamp": "2025-10-01T04:27:20",
    "url": "https://reddit.com/r/Python/comments/1nv5lcw/pycapnweb_a_python_implementation_of_capn_webs/",
    "score": 11,
    "num_comments": 9,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nuaqe8",
    "title": "Stories from running a workflow engine, e.g., Hatchet, in Production",
    "content": "Hi everybody! I find myself in need of a workflow engine (I'm DevOps, so I'll be using it and administering it), and it seems the Python space is exploding with options right now. I'm passingly familiar with Celery+Canvas and DAG-based tools such as Airflow, but the hot new thing seems to be Durable Execution frameworks like Temporal.io, DBOS, Hatchet, etc. I'd love to hear stories from people actually using and managing such things in the wild, as part of evaluating which option is best for me.\n\nJust from reading over these projects docs, I can give my initial impressions:\n\n* Temporal.io - enterprise-ready, lots of operational bits and bobs to manage, seems to want to take over your entire project\n* DBOS - way less operational impact, but also no obvious way to horizontally scale workers independent of app servers (which is sort of a key feature for me)\n* Hatchet - evolving fast, Durable Execution/Workflow bits seem fairly recent, no obvious way to logically segment queues, etc. by tenant (Temporal has Namespaces, Celery+Canvas has Virtual Hosts in RabbitMQ, DBOS‚Ä¶ might be leveraging your app database, so it inherits whatever you are doing there?)\n\nAm I missing any of the big (Python) players? What has your experience been like?",
    "author": "gthank",
    "timestamp": "2025-09-30T04:34:07",
    "url": "https://reddit.com/r/Python/comments/1nuaqe8/stories_from_running_a_workflow_engine_eg_hatchet/",
    "score": 106,
    "num_comments": 16,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nviqak",
    "title": "Free Release - Vanity-S.E.T.",
    "content": "https://github.com/SolSpliff/Vanity-SET\n\nI‚Äôve released my Python script, fully open source on GitHub which generates Vanity wallets for: Sol, Eth &amp; Ton.\n\nEnjoy. Any issues, open a ticket or push an update.",
    "author": "SassyKassy21",
    "timestamp": "2025-10-01T12:58:35",
    "url": "https://reddit.com/r/Python/comments/1nviqak/free_release_vanityset/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nvjw17",
    "title": "14-year-old here teaching Python basics on YouTube ‚Äì made this course for students like me",
    "content": "Hey everyone! I'm 14 and I've been learning computer science for a while now. I realized there aren't many beginner-friendly Python tutorials made BY teens FOR teens (and college students too), so I decided to create my own course on YouTube.\n\nI'm covering all the fundamentals ‚Äì variables, loops, functions, and working up to more interesting projects. My goal is to explain things the way I wish someone had explained them to me when I was starting out.\n\nI'd really appreciate a view or subscribe! Every bit of support helps me keep making content and improving the course.\n\n\nChannel Name: Bytesize Code\n\nhttps://youtube.com/@hussein-bytesizecode?si=dlmY53Z2pbeS81vu",
    "author": "Separate_Airline_427",
    "timestamp": "2025-10-01T13:41:03",
    "url": "https://reddit.com/r/Python/comments/1nvjw17/14yearold_here_teaching_python_basics_on_youtube/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nu8tt6",
    "title": "Crawlee for Python v1.0 is LIVE!",
    "content": "Hi everyone, our team just launched¬†[**Crawlee for Python üêç**](https://github.com/apify/crawlee-python/)¬†**v1.0**, an open source web scraping and automation library. We launched the beta version in Aug 2024¬†[here](https://www.reddit.com/r/Python/comments/1dyyaky/crawlee_for_python_is_live/), and got a lot of feedback. With new features like Adaptive crawler, unified storage client system, Impit HTTP client, and a lot of new things, the library is ready for its public launch.\n\n**What My Project Does**\n\nIt's an open-source web scraping and automation library, which provides a unified interface for HTTP and browser-based scraping, using popular libraries like¬†[beautifulsoup4](https://pypi.org/project/beautifulsoup4/)¬†and¬†[Playwright](https://playwright.dev/python/)¬†under the hood.\n\n**Target Audience**\n\nThe target audience is developers who wants to try a scalable crawling and automation library which offers a suite of features that makes life easier than others. We launched the beta version a year ago, got a lot of feedback, worked on it with help of early adopters and launched Crawlee for Python v1.0.\n\n**New features**\n\n* **Unified storage client system**: less duplication, better extensibility, and a cleaner developer experience. It also opens the door for the community to build and share their own storage client implementations.\n* **Adaptive Playwright crawler**: makes your crawls faster and cheaper, while still allowing you to reliably handle complex, dynamic websites. In practice, you get the best of both worlds: speed on simple pages and robustness on modern, JavaScript-heavy sites.\n* **New default HTTP client** (ImpitHttpClient, powered by the¬†[Impit](https://github.com/apify/impit)¬†library): fewer false positives, more resilient crawls, and less need for complicated workarounds. Impit is also developed as an open-source project by Apify, so you can dive into the internals or contribute improvements yourself: you can also create your own instance, configure it to your needs (e.g. enable HTTP/3 or choose a specific browser profile), and pass it into your crawler.\n* **Sitemap request loader**: easier to start large-scale crawls where sitemaps already provide full coverage of the site\n* **Robots exclusion standard**: not only helps you build ethical crawlers, but can also save time and bandwidth by skipping disallowed or irrelevant pages\n* **Fingerprinting**: each crawler run looks like a real browser on a real device. Using fingerprinting in Crawlee is straightforward: create a fingerprint generator with your desired options and pass it to the crawler.\n* **Open telemetry**: monitor real-time dashboards or analyze traces to understand crawler performance. easier to integrate Crawlee into existing monitoring pipelines\n\n**Find out more**\n\nOur team will be here in r/Python for an **AMA** on **Wednesday 8th October 2025, at 9am EST/2pm GMT/3pm CET/6:30pm IST**. We will be answering questions about webscraping, Python tooling, moving products out of beta, testing, versioning, and much more!\n\nCheck out our GitHub repo and blog for more info!\n\n**Links**\n\nGitHub:¬†[https://github.com/apify/crawlee-python/](https://github.com/apify/crawlee-python/)  \nDiscord:¬†[https://apify.com/discord](https://apify.com/discord)  \nCrawlee website:¬†[https://crawlee.dev/python/](https://crawlee.dev/python/)  \nBlogpost: [https://crawlee.dev/blog/crawlee-for-python-v1](https://crawlee.dev/blog/crawlee-for-python-v1)",
    "author": "B4nan",
    "timestamp": "2025-09-30T02:42:48",
    "url": "https://reddit.com/r/Python/comments/1nu8tt6/crawlee_for_python_v10_is_live/",
    "score": 75,
    "num_comments": 25,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nuc9fy",
    "title": "I made: Dungeon Brawl ‚öîÔ∏è ‚Äì Text-based Python battle game with attacks, specials, and healing",
    "content": "**What My Project Does:**  \nDungeon Brawl is a **text-based, turn-based battle game** in Python. Players fight monsters using normal attacks, special moves, and healing potions. The game uses **classes, methods, and the random module** to handle combat mechanics and damage variability.\n\n**Target Audience:**  \nIt‚Äôs a **toy/learning project** for Python beginners or hobbyists who want to see OOP, game logic, and input/output in action. Perfect for someone who wants a small but playable Python project.\n\n**Comparison:**  \nUnlike most beginner Python games that are static or single-turn, Dungeon Brawl is **turn-based with limited special attacks, healing, and randomized combat**, making it more interactive and replayable than simple text games.\n\nCheck it out here: [https://github.com/itsleenzy/dungeon-brawl/](https://github.com/itsleenzy/dungeon-brawl/)",
    "author": "leenzy-leen",
    "timestamp": "2025-09-30T05:47:37",
    "url": "https://reddit.com/r/Python/comments/1nuc9fy/i_made_dungeon_brawl_textbased_python_battle_game/",
    "score": 25,
    "num_comments": 7,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nv928s",
    "title": "üîçRAISearcher v.1.0 - A Super-Fast Files &amp; Folder searching tool for Windows",
    "content": "# üîçRAISearcher\n\n**What My Project Does**\n\nRAISearcher is a **Super-Fast and Reliable File &amp; Folder Searcher Tool** for Windows, built in Python with CustomTkinter.\n\nIt‚Äôs designed to be **faster** than Windows Explorer search, **lightweight**, and **user-friendly**.\n\n\\---\n\n# ‚ú® Features\n\n* \\- ‚ö° Super-fast **multithreaded scanning**\n* \\- üìÇ Search **inside drives** or **folders**\n* \\- üîé Search by **keywords** or **exact match**\n* \\- üéØ **Filter** by file extension (\\`.png\\`, \\`.exe\\`, \\`.pdf\\`, etc.)\n* \\- üìë **Copy** file/folder path to clipboard\n* \\- üñ•Ô∏è Modern **dark-themed GUI** (CustomTkinter)\n* \\- ‚õî **Stop** search anytime\n\n\\---\n\n**Target Audience**\n\n* Regular Windows users\n* People who want fast search results\n* People who constantly search for files\n\n\\---\n\n**Comparison**\n\n|Features|Windows Explorer Search|RAISearcher|\n|:-|:-|:-|\n|Extension Filtering|Difficult and Not user friendly|Very easy, User friendly and can be selected from a dropdown menu or can be typed manually|\n|Exact match|Also, difficult and Not user friendly|Can be achieved by just checking a checkbox|\n|Speed|\\~10-20sec|\\~1-2sec|\n|Indexing|‚úÖ - Adds up and consumes memory. Not good for PCs with small memory|‚ùå- Built for slow &amp; low memory PCs|\n|Multi-threaded Searching|‚ùå|‚úÖ|\n\n\\---\n\n# üì• Download\n\nVisit the GitHub page to see more and download the latest release!\n\nüëâ [Visit GitHub Page](https://github.com/RAI-Official/RAISearcher)\n\n**Note:**\n\nCouldn't upload images because it is not allowing me to upload. (Images &amp; Videos tab is greyed out)  \nAlso tell me if the .exe doesn't work",
    "author": "Necessary_Mind7337",
    "timestamp": "2025-10-01T07:03:02",
    "url": "https://reddit.com/r/Python/comments/1nv928s/raisearcher_v10_a_superfast_files_folder/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.3,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nu9n4l",
    "title": "Telelog: A high-performance diagnostic &amp; visualization tool for Python, powered by Rust",
    "content": "**GitHub Link:** [https://github.com/vedant-asati03/telelog](https://github.com/vedant-asati03/telelog)\n\n# What My Project Does\n\nTelelog is a diagnostic framework for Python with a Rust core. It helps you understand *how* your code runs, not just *what* it outputs.\n\n* **Visualizes Code Flow:** Automatically generates flowcharts and timelines from your code's execution.\n* **High-Performance:** 5-8x faster than the built-in `logging` module.\n* **Built-in Profiling:** Find bottlenecks easily with `with logger.profile():`.\n* **Smart Context:** Adds persistent context (`user_id`, `request_id`) to all events.\n\n# Target Audience\n\n* Developers debugging complex systems (e.g., data pipelines, state machines).\n* Engineers building performance-sensitive applications.\n* Anyone who wants to visually understand and document their code's logic.\n\n# Comparison (vs. built-in logging)\n\n* **Scope:** `logging` is for text records. Telelog is an **instrumentation framework** with profiling &amp; visualization.\n* **Visualization:** Telelog's automatic diagram generation is a unique feature.\n* **Performance:** Telelog's Rust core offers a significant speed advantage.",
    "author": "Vedant-03",
    "timestamp": "2025-09-30T03:33:15",
    "url": "https://reddit.com/r/Python/comments/1nu9n4l/telelog_a_highperformance_diagnostic/",
    "score": 24,
    "num_comments": 21,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nv68ds",
    "title": "Seeking Free Python Certification Courses - Anyone Know Reputable Ones?",
    "content": "Hey guys,Looking to skill up on Python and wondering if anyone's aware of free certification courses out there? üëÄ #python #Programming #coding ",
    "author": "Great_Estimate_1564",
    "timestamp": "2025-10-01T05:00:14",
    "url": "https://reddit.com/r/Python/comments/1nv68ds/seeking_free_python_certification_courses_anyone/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nu0gdd",
    "title": "Pandas 2.3.3 released with Python 3.14 support",
    "content": "Pandas was the last major package in the Python data analysis ecosystem that needed to be updated for Python 3.14.\n\nhttps://github.com/pandas-dev/pandas/releases/tag/v2.3.3",
    "author": "Balance-",
    "timestamp": "2025-09-29T18:43:01",
    "url": "https://reddit.com/r/Python/comments/1nu0gdd/pandas_233_released_with_python_314_support/",
    "score": 91,
    "num_comments": 18,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ntmao7",
    "title": "Why would I not use Visual Studio code",
    "content": "I‚Äôm doing a college project that wants me to use Mobaxterm for my terminal and WinSCP to transfer files and I‚Äôm using a college provided Linux server. In mobaxterm I use a code editor called nedit. \n\nI‚Äôve used VSC on a project before and it was so much easier , and everything was built in one. I told the professor and he said well you could but I think this is better. \n\nI‚Äôm confused how this slow multi step process can be better than VSC?\n\n(This is a bioinformatics project using biopython)",
    "author": "saddickstic",
    "timestamp": "2025-09-29T09:14:14",
    "url": "https://reddit.com/r/Python/comments/1ntmao7/why_would_i_not_use_visual_studio_code/",
    "score": 283,
    "num_comments": 174,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nv01qm",
    "title": "I built Poottu ‚Äî an offline, privacy-first password manager in Python",
    "content": "Hey everyone ‚Äî I wanted to share a project I‚Äôve been working on recently: **Poottu**, a desktop password manager written in Python.\n\n# What it does\n\nAt its core, Poottu is meant to be a **secure, offline, local vault** for credentials (usernames, passwords, URLs, notes).\n\n* Fully **offline by default** ‚Äî no telemetry or automatic cloud sync built in\n* Clean, minimal GUI (using **PySide6**)\n* Groups/categories to organize entries\n* Live search across title, username, URL, notes\n* Entry preview pane with ‚Äúshow password‚Äù option\n* Context menu actions: copy username, password, URL, old password, notes\n* Timed clipboard clearing (after N seconds) to reduce exposure\n* Encrypted backup / restore of vault\n* Password generator built in\n* Keyboard shortcuts support\n\n# Target audience\n\nWho is Poottu for?\n\n* **Privacy-focused users** who do not want their credentials stored in cloud services by default\n* People who prefer **local, device-only control** over their vault\n* Those who want a **lightweight password manager** with no vendor lock-in\n\n# Comparison\n\nMost existing password managers fall into two camps: **command-line tools** like `pass` or `gopass`, and **cloud-based managers** like Bitwarden, 1Password, or LastPass.\n\nCLI tools are lightweight and fully offline, but they often feel unintuitive for non-technical users. Cloud-based solutions, on the other hand, are polished and offer seamless cross-device sync, but they usually come with privacy trade-offs, vendor lock-in, or a subscription cost.\n\n**Poottu tries to strike a balance between the two** ‚Äî it‚Äôs completely offline and open-source like CLI tools, but it also provides a clean, beginner-friendly desktop GUI that makes managing entries much easier.\n\nThe trade-off is that Poottu doesn‚Äôt ship with built-in sync. In short: Poottu aims to sit between a low-level CLI vault like `pass` and full-featured cloud managers ‚Äî offering **local safety plus a friendly UI**.\n\n# Availability\n\n* GitHub: [github.com/manikandancode/poottu](https://github.com/manikandancode/poottu)\n* PyPI: [pypi.org/project/poottu](https://pypi.org/project/poottu/)\n\n# License\n\nMIT License\n\n# Installation\n\nYou can install from PyPI:\n\n    pip install poottu\n\nThen run:\n\n    poottu\n\nI beautified and commented the code using AI to improve readability and inline documentation. If you try it out ‚Äî I‚Äôd love feedback, issues, or ideas for improvements and security. Thanks for checking it out. Hope it‚Äôs useful to someone here! üôè",
    "author": "MrShortCircuitMan",
    "timestamp": "2025-09-30T22:37:38",
    "url": "https://reddit.com/r/Python/comments/1nv01qm/i_built_poottu_an_offline_privacyfirst_password/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.39,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nuw799",
    "title": "Watch out for your commas!!!",
    "content": "You might already know the memes behind forgetting to end a line with a semi-colon `(;)`.\n\nAnd it's kind of funny how Python doesn't fall into this problem.\n\nYou should, however, watch out for not ending a line with a comma in this particular scenario.\n\nThis scenario being having a list that extends multiple lines vertically.\n\n```python\nEXAMPLE_CONST_LIST = [\n    \"main.py\",     #  &lt;------ Make sure there is a trailing comma\n    \"__pycache__\"\n]\n```\n\n&gt; Python does not throw an error and errors silently\n\n## What Happened to me?\nI recently had this issue where I forgot to end an element with a comma. My program wasn't following the logical rules that I wanted it to follow. And this was a simple script, a little script. Nothing fancy, not a **HUGE** project. Just one file with a few lines:\n\n```python\nimport os\n\nEXCEPTIONS = [\n    \"main.py\"  # missing a comma here\n    \"__pycache__\"\n]\n\nfor file in os.listdir():\n    if file in EXCEPTIONS:\n        continue\n    \n    # Rest of logic that I wanted\n```\n\nNotice the missing comma, I couldn't deduce the problem for 3 minutes straight. No errors thrown, just errored silently and executed some unwanted logic.\n\nIf you might not know, without adding a comma, the constant variable `EXCEPTIONS` from the previous snippet turns into:\n\n```text\n[\"main.py__pycache__\"]\n```\n\nEssentially, concatenates the underlying elements. Not sure why Python decided this was a good idea but if you find yourself defining an array vertically, let this serve as a reminder to make sure to end each element with comma.",
    "author": "Flaky_Arugula9146",
    "timestamp": "2025-09-30T19:14:28",
    "url": "https://reddit.com/r/Python/comments/1nuw799/watch_out_for_your_commas/",
    "score": 0,
    "num_comments": 15,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ntnbgn",
    "title": "uv-ship: a CLI tool for shipping with uv",
    "content": "Hello¬†[r/Python](https://www.reddit.com/r/Python/).  \nI know, I know, there are several release-bumping tools out there, but none integrate with uv the way I would like them to. They also feel kind of bloated for what I need them to do. I simply wanted to use¬†`uv version`¬†to update my project metadata, paired with a small pipeline that safeguards the process and ships the changes + version tag to the repo.\n\nIf you're curious, please check out¬†[**uv-ship**](https://floraths.github.io/uv-ship/)\n\n**What My Project Does**\n\n&gt;\n\n&gt;**preflight checks**: guard your release workflow by verifying branch, tags, and a clean working tree before shipping.\n\n&gt;**changelog generation**: auto-builds changelog sections from commits since the latest tag.\n\n&gt;**one-shot release**: stage, commit, tag, and push in a single step.\n\n&gt;**dry-run mode**: preview every action before making changes.\n\n**Target Audience**¬†\n\nmaintainers of uv-managed projects with strict release workflows.\n\n**Comparison**  \nuv-ship is similar in scope to¬†[bump-my-version](https://callowayproject.github.io/bump-my-version/)¬†but it integrates with uv out-of-the-box. For example, if you use bump-my-version you need to set up the following workflow:\n\n1. execute version bump with¬†`bump-my-version bump minor`\n2. include a pre-commit hook that runs¬†`uv sync`\n3. tell bump-my-version that pyproject.toml and uv.lock need to be committed\n4. create the tag and push it manually\n\nbump-my-version offers automation with pre- and post-commit hooks, but it does not evaluate if the tag is safe to be pushed (all requirements met for release?)\n\nall those steps are completed and validated during the uv-ship pipeline:\n\nthe command syntax for the same operation (and some more) is:¬†`$ uv-ship next minor`\n\nyou can play around in¬†`--dry-run`¬†mode to see the CLI in action. I would love your feedback  \n[https://github.com/floRaths/uv-ship](https://github.com/floRaths/uv-ship)",
    "author": "Ruths138",
    "timestamp": "2025-09-29T09:52:51",
    "url": "https://reddit.com/r/Python/comments/1ntnbgn/uvship_a_cli_tool_for_shipping_with_uv/",
    "score": 47,
    "num_comments": 6,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nu118l",
    "title": "sparkenforce: Type Annotations &amp; Runtime Schema Validation for PySpark DataFrames",
    "content": "sparkenforce is a PySpark type annotation package that lets you specify and enforce DataFrame schemas using Python type hints.\n\n## What My Project Does\n\nWorking with PySpark DataFrames can be frustrating when schemas don‚Äôt match what you expect, especially when they lead to runtime errors downstream.\n\nsparkenforce solves this by:\n\n* Adding type annotations for DataFrames (columns + types) using Python type hints.\n* Providing a `@validate` decorator to enforce schemas at runtime for function arguments and return values.\n* Offering clear error messages when mismatches occur (missing/extra columns, wrong types, etc.).\n* Supporting flexible schemas with ..., optional columns, and even custom Python ‚Üî Spark type mappings.\n\nExample:\n\n```\nfrom sparkenforce import validate\nfrom pyspark.sql import DataFrame, functions as fn\n\n@validate\ndef add_length(df: DataFrame[\"firstname\": str]) -&gt; DataFrame[\"name\": str, \"length\": int]:\n    return df.select(\n        df.firstname.alias(\"name\"),\n        fn.length(\"firstname\").alias(\"length\")\n    )\n```\n\nIf the input DataFrame doesn‚Äôt contain \"firstname\", you‚Äôll get a `DataFrameValidationError` immediately.\n\n## Target Audience\n\n* PySpark developers who want stronger contracts between DataFrame transformations.\n* Data engineers maintaining ETL pipelines, where schema changes often breaks stuff.\n* Teams that want to make their PySpark code more self-documenting and easier to understand.\n\n## Comparison\n\n* Inspired by [dataenforce](https://github.com/CedricFR/dataenforce) (Pandas-oriented), but extended for PySpark DataFrames.\n* Unlike static type checkers (e.g. mypy), sparkenforce enforces schemas at runtime, catching real mismatches in Spark pipelines.\n* [spark-expectations](https://github.com/Nike-Inc/spark-expectations) has a wider aproach, tackling various data quality rules (validating the data itself, adding observability, etc.). sparkenforce focuses only on schema or structure data contracts.\n\n## Links\n\n* PyPI: [sparkenforce](https://pypi.org/project/sparkenforce/)\n* Source code: [GitHub repo](https://github.com/agustin-recoba/sparkenforce)\n* Demo notebook: [Examples](https://github.com/agustin-recoba/sparkenforce/blob/main/src/demo/demo_notebook.ipynb)",
    "author": "nopasanaranja20",
    "timestamp": "2025-09-29T19:09:47",
    "url": "https://reddit.com/r/Python/comments/1nu118l/sparkenforce_type_annotations_runtime_schema/",
    "score": 7,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nuo0vb",
    "title": "New Online IDE - no logjn",
    "content": "I found this thepythonconsole.com and since my vs code and all ides didn‚Äôt want to work I looked for an online one and this was so simple and free to use it‚Äôs insane. Just recommending it for yall if you ever need even to use python on your phone to showcase something quick. It even supports plots!!! \n\nthepythonconsole.com\n",
    "author": "Interesting_Agent_35",
    "timestamp": "2025-09-30T13:19:07",
    "url": "https://reddit.com/r/Python/comments/1nuo0vb/new_online_ide_no_logjn/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nuk8hi",
    "title": "Running rust code in python",
    "content": "If I compile rust as .whl files using tools Like maturin, and import it in python will this piece of code/method run at rust equivalent speed or python speed ?\n\nAlso other things will be impacted like garbage collection and memory management?\n\nI have an api causing db cpu spike in django which is intensive and I'm trying to write a small rust service which can just run this part and make use of rust advantages.\n\nMy motivation is outlined in this blog post \n\nhttps://wxiaoyun.com/blog/rust-rewrite-case-study/",
    "author": "Cold-Supermarket-715",
    "timestamp": "2025-09-30T10:57:02",
    "url": "https://reddit.com/r/Python/comments/1nuk8hi/running_rust_code_in_python/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nty93i",
    "title": "Tuesday Daily Thread: Advanced questions",
    "content": "# Weekly Wednesday Thread: Advanced Questions üêç\n\nDive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.\n\n## How it Works:\n\n1. **Ask Away**: Post your advanced Python questions here.\n2. **Expert Insights**: Get answers from experienced developers.\n3. **Resource Pool**: Share or discover tutorials, articles, and tips.\n\n## Guidelines:\n\n* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.\n* Questions that are not advanced may be removed and redirected to the appropriate thread.\n\n## Recommended Resources:\n\n* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.\n\n## Example Questions:\n\n1. **How can you implement a custom memory allocator in Python?**\n2. **What are the best practices for optimizing Cython code for heavy numerical computations?**\n3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**\n4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**\n5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**\n6. **What are some advanced use-cases for Python's decorators?**\n7. **How can you achieve real-time data streaming in Python with WebSockets?**\n8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**\n9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**\n10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**\n\nLet's deepen our Python knowledge together. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-29T17:00:31",
    "url": "https://reddit.com/r/Python/comments/1nty93i/tuesday_daily_thread_advanced_questions/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ntwkn1",
    "title": "Best editor/IDE for starting Python in a portfolio management class?",
    "content": "Hello everyone,\n\nI have a question regarding which editor or IDE I should use. For some context: I am just starting with Python for a university class (Backtesting for Portfolio Management). The course provides an introduction to programming for portfolio management applications, in particular the backtesting of quantitative investment strategies using Python.\n\nI have some experience coding, mainly with R and RStudio over the past three years at university and work, but I am completely new to Python. While researching online, I saw that VS Code is often recommended as an editor, while PyCharm is considered a full IDE. Which one should I use, and why? Are there better options I should consider?\n\nThank you!",
    "author": "swissmarketguy",
    "timestamp": "2025-09-29T15:47:21",
    "url": "https://reddit.com/r/Python/comments/1ntwkn1/best_editoride_for_starting_python_in_a_portfolio/",
    "score": 6,
    "num_comments": 36,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nukocw",
    "title": "I got tired of manually searching for dev jobs, so I started building OrionJobs AI!",
    "content": "Hey everyone,\n\nI'm developing an open-source tool to automate the job search and would love to get your feedback on it. As required by the 'Showcase' flair rules, here's the breakdown:\n\nWhat My Project Does\nOrionJobs AI is a platform that automates the collection of job opportunities for developers. It integrates with multiple job board APIs (like RemoteOK and Adzuna) to aggregate, process, and store job listings in a central database. The long-term vision is to use AI to provide personalized job recommendations based on a user's skills and profile.\n\nTarget Audience\nPrimarily, it's for junior to mid-level developers who are actively job hunting and tired of the repetitive, manual process of checking multiple websites every day. It's built for developers who appreciate automation and want a more intelligent way to find their next opportunity.\n\nComparison\nCompared to manually browsing LinkedIn or other job boards, OrionJobs AI saves significant time by centralizing opportunities. Unlike a simple RSS feed, it structures and normalizes the data. The key differentiator in the future will be its AI-powered recommendation engine, which aims to provide more relevant matches than the generic algorithms used by larger platforms.\n\nCurrent Status:\nThe backend is 100% \"cloud-ready,\" built with Python, FastAPI, PostgreSQL, and fully containerized with Docker. The data collection system is operational. The full roadmap is in the README.\n\nI'd love to get your feedback on the code, the architecture, or the general idea. The project is completely open to contributions (I've already tagged some good first issues for anyone looking to get started).\n\nGitHub Repo: https://github.com/GuiDev-01/orion-jobs-ai\n\nThanks for the support!",
    "author": "Ancient-Courage-6130",
    "timestamp": "2025-09-30T11:13:20",
    "url": "https://reddit.com/r/Python/comments/1nukocw/i_got_tired_of_manually_searching_for_dev_jobs_so/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ntvcta",
    "title": "Scroll Art (animated ASCII art, for beginner programmers and hobbyists)",
    "content": "## What My Project Does\n\nScroll art is moving ASCII art produced by stdout text printed from a loop, animated as the text scrolls up the terminal. It's especially useful as creating computing projects for beginner programmers. Because it only uses text, no environment setup or additional libraries are needed and scroll art can be made in every programming language. I've published examples of scroll art on https://scrollart.org, but now I've also included Python implementations in the `scrollart` package.\n\nInstall: pip install scrollart\n\nTo view the list of scroll art: python -m scrollart --help\n\nExample: python -m scrollart starfield\n\nOr copy/paste the code into a single file from here: https://raw.githubusercontent.com/asweigart/scrollart/refs/heads/main/python-package/scrollart/__init__.py\n\nPyPI page: https://pypi.org/project/scrollart/\n\nGit repo: https://github.com/asweigart/scrollart\n\nBlog post: https://inventwithpython.com/blog/scroll-art-python-package.html\n\nView the scroll art online (through JavaScript viewer): https://scrollart.org/\n\n## Target Audience\n\nBeginners, but also instructors looking for project ideas for their students.\n\n## Comparison\n\nThis isn't quite ASCII art (since it's animated) and it's not quite curses (since you can't arbitrarily move the text cursor around the screen). I was surprised there wasn't already a name for this.",
    "author": "AlSweigart",
    "timestamp": "2025-09-29T14:56:50",
    "url": "https://reddit.com/r/Python/comments/1ntvcta/scroll_art_animated_ascii_art_for_beginner/",
    "score": 5,
    "num_comments": 6,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ntc5rg",
    "title": "holm: Next.js developer experience in Python, without JS, built on FastAPI",
    "content": "Hi all!\n\nI've just released `holm` and wanted to show it you. It is the last piece of the FastAPI web development stack I started creating with [FastHX](https://volfpeter.github.io/fasthx/) and [htmy](https://volfpeter.github.io/htmy/).\n\nYou can learn all about it in the docs: https://volfpeter.github.io/holm/. If you've used Next.js before, you will find `holm` very familiar.\n\nThe documentation has a couple of short documents and guides covering all the basics: creating your first app, adding HTMX, error rendering, customization, setting up AI assistance. The rest is standard FastAPI, `htmy`, and FastHX.\n\n### What the project does?\n\nIt's a web development framework that brings the Next.js developer experience to Python (without JavaScript dependencies).\n\n### Key features\n\n- Next.js-like developer experience with **file-system based routing** and page composition.\n- Standard **FastAPI** everywhere, so you can leverage the entire FastAPI ecosystem.\n- JSX-like syntax with async support for components, thanks to `htmy`.\n- First class **HTMX support** with FastHX.\n- Async support everywhere, from APIs and dependencies all the way to UI components.\n- Support for both JSON and HTML (server side rendering) APIs.\n- No build steps, just server side rendering with **fully typed** Python.\n- Stability by building only on the core feature set of dependent libraries.\n- Unopinionated: use any CSS framework for styling and any JavaScript framework for UI interactivity (HTMX, AlpineJS, Datastar, React islands).\n\n### Target audience\n\nEveryone who wants to conveniently create dynamic websites and application in Python.\n\nI hope you'll give `holm` a go for your next web project.",
    "author": "volfpeter",
    "timestamp": "2025-09-29T00:58:06",
    "url": "https://reddit.com/r/Python/comments/1ntc5rg/holm_nextjs_developer_experience_in_python/",
    "score": 48,
    "num_comments": 16,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nstje8",
    "title": "Stop uploading your code to sketchy ‚Äúonline obfuscators‚Äù like freecodingtools.org",
    "content": "So I googled one of those ‚Äúfree online Python obfuscor things‚Äù (say, freecodingtools.org) and oh boy‚Ä¶ I have to rant for a minute.\n\nYou sell pitch is just ‚Äújust paste your code in this box and we‚Äôll keep it for you.‚Äù Right. Because clearly the best way to keep your intellectual property is to deposit it on a who-knows-what site you‚Äôve never ever known, owned and operated people you‚Äôll never ever meet, with no idea anywhere your source goes. Completely secure.\n\nEven if you think the site will not retain a copy of your code, the real ‚Äúobfuscation‚Äù is going to be farcical. We discuss base64, XOR, hex encoding, perhaps zlib compression, in a few spaghetti exec function calls. This isn‚Äôt security, painting and crafts. It can be unwritten anybody who possesses a ten-minute-half-decent Google. But geez, at least it does look menacing from a first glance, doesn‚Äôt it?\n\nYou actually experience a false sense of security and the true probability of having just opened your complete codebase to a dodgy server somewhere. And if you‚Äôre particularly unlucky, they‚Äôll mail back to you a ‚Äúprotected‚Äù file that not only includes a delicious little backdoor but also one you‚Äôll eagerly send off to your unsuspecting users. Well done, you just gave away supply-chain malware for free.\n\nIf you truly do want to protect code, there are actual tools for it. Cython runs to C extensions. Nuitka runs projects to native executables. Encrypts bytecode and does machine binding. Not tricks, but at least make it hard and come from people who don‚Äôt want your source comed to be pushed to their private webserver. And the actual solution? Don‚Äôt push secrets to begin with. Put keys and sensitive logic on a server people can‚Äôt touch.\n\nSo yeh‚Ä¶ do not the next time your eyes glaze over at ‚Äújust plug your Python code into our free web obfuscator.‚Äù Unless your security mindset is ‚Äúkeep my younger brother from cheating and reading my homework,‚Äù congratulations, your secret‚Äôs safe.",
    "author": "GuiltyAd2976",
    "timestamp": "2025-09-28T10:03:25",
    "url": "https://reddit.com/r/Python/comments/1nstje8/stop_uploading_your_code_to_sketchy_online/",
    "score": 394,
    "num_comments": 60,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ntb4a5",
    "title": "Playing Penney's Game Using Python",
    "content": "[Penney's game](https://en.wikipedia.org/wiki/Penney%27s_game), is a head/tail sequence generating game between two or more players. Player A selects a sequence of heads and tails (of length 3 or larger), and shows this sequence to player B. Player B then selects another sequence of heads and tails of the same length. A coin is tossed until either player A's or player B's sequence appears as a consecutive sub-sequence of the coin toss outcomes. The player whose sequence appears first wins. The cool thing about the game is that second person can wisely chose their sequence based on the first ones which highers their winning probability.\n\n**What My Project Does**\n\nHere we have implemented the game in command-line interface (CLI) using pure Python so to play around with the game and find out ways to chose that sequence wisely to win the game; Check it out and comment if you find a general winning strategies for first/second player for longer sequences.\n\n**Target Audience**  \nThis project is mainly for:\n\n* Python learners who want a fun CLI project to play with\n* Math/game enthusiasts curious about probability games\n* Anyone who enjoys experimenting with games\n\nIt‚Äôs an interactive fun/educational project.\n\n**Comparison**  \nOther implementations (e.g., [https://penneys.github.io/](https://penneys.github.io/)) are restricted to two players and fixed sequences of length 3. Our project extends this by supporting multiple players, variable sequence lengths, and a command-line interface for interactive play.\n\nGitHub repo: [https://github.com/sepandhaghighi/penney](https://github.com/sepandhaghighi/penney)",
    "author": "sadrasabouri",
    "timestamp": "2025-09-28T23:48:09",
    "url": "https://reddit.com/r/Python/comments/1ntb4a5/playing_penneys_game_using_python/",
    "score": 9,
    "num_comments": 2,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nu22gk",
    "title": "Anthropic: Claude Sonnet 4.5 is the best coding model in the world.",
    "content": "Claude Sonnet 4.5 is now being packaged as the new default model for general use on Anthropic's platforms, replacing Sonnet 4 in most product experiences. It's broadly available for all users‚Äîincluding through the [Claude.ai](http://Claude.ai) website, mobile apps, and API‚Äîwithout the access restrictions and premium pricing of the Opus models.\n\nAdditionally, Sonnet 4.5 is said to be better than Opus at coding. \n\n&gt;Claude Sonnet 4.5 is the best coding model in the world. [https://www.anthropic.com/news/claude-sonnet-4-5](https://www.anthropic.com/news/claude-sonnet-4-5)",
    "author": "AssociationNo6504",
    "timestamp": "2025-09-29T19:59:30",
    "url": "https://reddit.com/r/Python/comments/1nu22gk/anthropic_claude_sonnet_45_is_the_best_coding/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.22,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nt3lqa",
    "title": "Monday Daily Thread: Project ideas!",
    "content": "# Weekly Thread: Project Ideas üí°\n\nWelcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.\n\n## How it Works:\n\n1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.\n2. **Build &amp; Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.\n3. **Explore**: Looking for ideas? Check out Al Sweigart's [\"The Big Book of Small Python Projects\"](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.\n\n## Guidelines:\n\n* Clearly state the difficulty level.\n* Provide a brief description and, if possible, outline the tech stack.\n* Feel free to link to tutorials or resources that might help.\n\n# Example Submissions:\n\n## Project Idea: Chatbot\n\n**Difficulty**: Intermediate\n\n**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar \n\n**Description**: Create a chatbot that can answer FAQs for a website.\n\n**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)\n\n# Project Idea: Weather Dashboard\n\n**Difficulty**: Beginner\n\n**Tech Stack**: HTML, CSS, JavaScript, API\n\n**Description**: Build a dashboard that displays real-time weather information using a weather API.\n\n**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)\n\n## Project Idea: File Organizer\n\n**Difficulty**: Beginner\n\n**Tech Stack**: Python, File I/O\n\n**Description**: Create a script that organizes files in a directory into sub-folders based on file type.\n\n**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)\n\nLet's help each other grow. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-28T17:00:32",
    "url": "https://reddit.com/r/Python/comments/1nt3lqa/monday_daily_thread_project_ideas/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ntgidy",
    "title": "False positives or malicious trojans in python script?",
    "content": "Hi, my friend sent me a script he made in python which I jokingly scanned with virustotal which showed 28 threats, most of which were labeled as ‚ÄúTrojan‚Äù. I think it‚Äôs important to note he encrypted this with nuitka + upx so it could be false positives. What do you guys thinks? And yes, I have run it and i scanned it with malwarebytes and nothing showed up.",
    "author": "Big_Bicycle_5003",
    "timestamp": "2025-09-29T05:21:09",
    "url": "https://reddit.com/r/Python/comments/1ntgidy/false_positives_or_malicious_trojans_in_python/",
    "score": 0,
    "num_comments": 25,
    "upvote_ratio": 0.43,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nti5ar",
    "title": "Why Today's Python Developers are Embracing Type Hints",
    "content": "Python is incredibly popular in fields where speedy experimentation and iteration are critical, and where developers are coming from a broad range of STEM backgrounds, not necessarily computer science. But as projects grow from experiments to production systems, that same flexibility can become a liability.\n\nSaying \"if you wanted a production system you should have used a different language\" or \"just rewrite it in _\" is missing the point - Python's optional typing features allow projects to gradually adopt type annotations &amp; type checking as they mature, improving reliability without requiring an expensive/disruptive rewrite.\n\nBlog post: https://pyrefly.org/blog/why-typed-python/",
    "author": "BeamMeUpBiscotti",
    "timestamp": "2025-09-29T06:33:07",
    "url": "https://reddit.com/r/Python/comments/1nti5ar/why_todays_python_developers_are_embracing_type/",
    "score": 0,
    "num_comments": 18,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nsi5sh",
    "title": "NiceGUI Component-Based Boilerplate: A scalable architecture for complex Python web UIs",
    "content": "Hello r/Python,\n\nI'm sharing my project, the **NiceGUI Component-Based Boilerplate**. I'm actively looking for feedback from experienced Python developers on its design and architecture.\n\nThis is a complete application boilerplate built using the **NiceGUI** framework, which allows the creation of web-based UIs using **pure Python &amp; HTML/CSS/JS**. My project provides a structure for a larger, multi-page NiceGUI application.\n\n**What My Project Does:**\n\nThis template introduces a modern structure to simplify building and maintaining complex NiceGUI apps, moving beyond single-file examples.\n\nKey structural features:\n\n* **Modular Component System:** UI pages and major elements are reusable Python classes/functions, managing their own layout and logic.\n* **Service Layer:** Business logic (e.g., data handling, API calls) is strictly separated into a dedicated `services/` directory, keeping UI code clean.\n* **Clean Starter Layout:** Provides a production-ready, responsive layout with a collapsible sidebar and consistent routing.\n\n**Target Audience:**\n\nThis is aimed at **experienced Python developers** and **teams** who need a structured foundation for building production-grade or highly-functional internal tools with NiceGUI. It's ideal for those focused on **scalability and maintainability**.\n\n**Comparison:**\n\nStandard NiceGUI documentation often focuses on simple, single-file scripts. This boilerplate differs by offering a full-scale, opinionated structure similar to what is used in modern web development frameworks.\n\n**Source Code:**\n\n**GitHub Repository:** [`https://github.com/frycodelab/nicegui-component-based`](https://github.com/frycodelab/nicegui-component-based)\n\nI welcome all feedback on the architectural patterns and how this template can be improved for real-world use cases.",
    "author": "Defiant-Comedian3967",
    "timestamp": "2025-09-28T00:27:23",
    "url": "https://reddit.com/r/Python/comments/1nsi5sh/nicegui_componentbased_boilerplate_a_scalable/",
    "score": 26,
    "num_comments": 4,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nsj4cy",
    "title": "Ducky: A free, open-source, all-in-one networking &amp; security toolkit for Windows.",
    "content": "Hey everyone, A while ago, I started working on a project called¬†**Ducky**, and I'm blown away by the support and feedback I've received. We recently passed a new star milestone on GitHub, and I just wanted to say a huge¬†**thank you**¬†to everyone who has shown interest, offered feedback, or contributed. It means the world to me.\n\n# What My Project Does\n\nDucky is a free, open-source desktop application that consolidates the essential tools of a network engineer or security enthusiast into a single, easy-to-use interface. Instead of juggling separate applications for terminal connections, network scanning, and diagnostics (like PuTTY, Angry IP Scanner, etc.), Ducky provides a unified workspace to streamline your workflow. Its core features include a tabbed terminal (SSH, Telnet, Serial), an SNMP-powered network topology mapper, a port scanner, and a suite of security utilities like a CVE lookup and hash calculator.\n\n# Target Audience\n\nDucky is built for anyone who works with network hardware and infrastructure. It's intended to be a serious, daily-use tool for professionals, but it's also simple enough for learners.\n\n* **For Production:**¬†Network Engineers &amp; Administrators for daily tasks like configuring switches and routers, troubleshooting connectivity, and documenting network layouts. Cybersecurity professionals can also use it for reconnaissance and analysis.\n* **For Learning:**¬†Students and hobbyists (e.g., studying for CompTIA Network+ or CCNA) can use Ducky as a free, hands-on tool to explore and interact with real or virtual network devices.\n\n# Comparison\n\nDucky aims to fill a gap between powerful but expensive commercial tools and single-function free utilities.\n\n* **Compared to tools like SecureCRT or MobaXterm Pro:**¬†Ducky provides many of the most-used features (tabbed multi-protocol terminal, session management) but is completely free and open-source. While it doesn't have every advanced feature of those paid tools yet, it covers the daily essentials in a clean, modern interface.\n* **Compared to using separate free tools (like PuTTY + Nmap + a separate subnet calculator):**¬†Ducky's main advantage is integration. The tools are designed to work together. For example, you can discover a device on the Topology Map, click it to see its details, and then launch an SSH session to it without ever leaving the application. This creates a much smoother and faster workflow.\n\n**How You Can Help:**\n\nThe best way you can support the project right now is by giving it a star on GitHub! It provides a huge motivation boost and helps more people discover the tool.\n\n**Easy Download (No Python Needed!)**\n\nI've also launched a small website for the project. You can now download a¬†**pre-packaged¬†.exe¬†file**¬†directly from the site‚Äîno need to install Python or any dependencies.\n\n* **GitHub Repository (Please leave a star! ‚≠ê):**¬†[https://github.com/thecmdguy/Ducky](https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fgithub.com%2Fthecmdguy%2FDucky)\n* **Website with Direct Download:**¬†[ducky.ge](https://ducky.ge/#download)\n\nThank you all again for the support",
    "author": "initCMD",
    "timestamp": "2025-09-28T01:30:13",
    "url": "https://reddit.com/r/Python/comments/1nsj4cy/ducky_a_free_opensource_allinone_networking/",
    "score": 20,
    "num_comments": 5,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ntexpa",
    "title": "Please review my projects and tell me if they are strong enough to get a job.",
    "content": "‚Ä¢ ***QR Code Generator (Flask, JavaScript, AWS S3, Docker****)*\n\n*-* Built a full-stack web app that generates QR codes from URLs.\n\n\\- Integrated AWS S3 for secure storage of generated codes.\n\n\\- Containerized the application with Docker for easy deployment.\n\n‚Ä¢ ***Weather Forecast Website*** **(HTML, CSS, JavaScript)**\n\n\\- Developed a responsive website to display real-time weather forecasts.\n\n\\- Integrated third-party weather APIs for accurate data retrieval.\n\n‚Ä¢ ***Email Spam Detection*** **(Python, Decision Trees, Logistic Regression)**\n\n\\- Implemented supervised learning models to classify emails as spam or\n\nnot spam.\n\n\\- Achieved reliable accuracy by comparing performance of multiple\n\nalgorithms.\n\n‚Ä¢ ***Netflix Clone*** **(HTML and CSS)**\n\n\\- Replicated the front-end design of the original Netflix site.",
    "author": "Extension_Sector_320",
    "timestamp": "2025-09-29T03:59:38",
    "url": "https://reddit.com/r/Python/comments/1ntexpa/please_review_my_projects_and_tell_me_if_they_are/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.13,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nsceni",
    "title": "Choosing a C++ to Python wrapper: Boost.Python vs pybind11?",
    "content": "I've built a code search tool as a terminal application in C++, and I'm now working on packaging it as a Python library. I need to create a Python wrapper for the C++ core.\n\nMy project already uses Boost, which has its own Python wrapper (Boost.Python). However, from what I've read, most people seem to be using pybind11.\n\nFor those who have experience with this, what are the pros and cons of the different options?\n\nThe search tool: https://github.com/perghosh/Data-oriented-design/releases/tag/cleaner.1.0.6",
    "author": "gosh",
    "timestamp": "2025-09-27T18:57:29",
    "url": "https://reddit.com/r/Python/comments/1nsceni/choosing_a_c_to_python_wrapper_boostpython_vs/",
    "score": 24,
    "num_comments": 12,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nstx1o",
    "title": "Showcase: Adaptive - open-source intelligent LLM router",
    "content": "**What My Project Does**\n\nAdaptive is an intelligent router for LLM inference.\n\nInstead of sending every request to a fixed model, it:\n\n* Analyzes prompts in real time\n* Estimates **task type, domain, and complexity**\n* Routes to the most suitable model based on benchmarked performance\n\nA common issue we saw with existing approaches is that they either:\n\n* Base routing **solely on complexity scores**, which collapse everything into one dimension, or\n* Use **very broad categories** like ‚Äúcode generation,‚Äù which ignore the nuance between planning, debugging, or writing simple snippets.\n\nAdaptive takes a more granular approach. We use **NVIDIA‚Äôs Prompt Task and Complexity Classifier** for initial signals, but extend it with **model criteria** derived from benchmarks across task types, domains, and multiple complexity levels.\n\nThis lets us distinguish when a prompt needs high reasoning (e.g., planning or debugging) versus when a lightweight model is sufficient (e.g., writing boilerplate).\n\nWe are now integrating Google‚Äôs [UniRoute](https://arxiv.org/abs/2502.08773) and extending it by adding task complexity and domain-awareness to the error-vector method, so routing generalizes to unseen models while staying context-aware.\n\n**Target Audience**\n\nAdaptive is for developers and teams building AI products that need to balance **cost, quality, and reliability** in production.\n\n**Comparison**\n\nMost LLM routing today is naive:\n\n* Route everything to a premium model ‚Üí high quality, but expensive\n* Route everything to a smaller model ‚Üí cheap, but quality suffers\n* Route based only on a single complexity score ‚Üí too coarse, misses nuance\n* Use broad categories like ‚Äúcoding‚Äù ‚Üí ignores the difference between planning, debugging, and writing snippets\n\nAdaptive differs by combining **granular task classification + domain signals + benchmark-driven model criteria** instead of static rules.\n\nThe result: **60‚Äì90% lower inference costs** while keeping quality high for workloads that actually demand complex reasoning.\n\n**Repo (open source):** [github.com/Egham-7/adaptive](https://github.com/Egham-7/adaptive)  \n**Website:** [llmadaptive.uk](https://llmadaptive.uk)\n\nWould love feedback from others working on inference routing or multi-model orchestration.",
    "author": "botirkhaltaev",
    "timestamp": "2025-09-28T10:18:26",
    "url": "https://reddit.com/r/Python/comments/1nstx1o/showcase_adaptive_opensource_intelligent_llm/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nt6u36",
    "title": "üéµ TikTock Video Downloader",
    "content": "# üöÄ Introducing¬†TikTock\n\n\n\nHey everyone! üëã\n\nI‚Äôve been working on a Python project called TikTock ‚Äî a command-line tool that makes downloading TikTok videos simple, fast, and customizable.\n\n# ‚úÖ What My Project Does\n\nTikTock is a Python CLI downloader for TikTok videos. It supports:\n\n* Single and multiple URLs\n* Bulk downloads from .txt / .json files (including TikTok‚Äôs official data export)\n* Watermark-free downloads\n* Custom filename templates\n* Logging and progress bars for smooth tracking\n\n\n\n# üßë‚Äçüíª Target Audience\n\nThis project is mainly for:\n\n* Data hoarders &amp; archivists who want to bulk-save or preserve TikTok content\n* Creators looking to back up their own TikToks without hassle\n* Developers who want an open, flexible tool they can extend or integrate into workflows\n\n# üîç Comparison\n\nThere are plenty of TikTok downloaders out there, but most fall short:\n\n‚ùå Many are websites stuffed with ads or shady practices\n\n‚ùå Others are closed-source with limited flexibility\n\nTikTock is different:\n\n‚úÖ 100% open-source and Python-based\n\n‚úÖ Developer-friendly with rich customization (templates, chunk sizes, logging, etc.)\n\n‚úÖ Transparent and hackable, so you can extend it however you like\n\n\n\n‚ö° Pro tip: Download your videos now before Oracle buys it \n\n\n\nüëâ GitHub Repo: [TikTock on GitHub](https://github.com/Izaan17/TikTock)\n\nIf you find it useful, I‚Äôd love a ‚≠ê on GitHub! Feedback and feature requests are super welcome.",
    "author": "HyperrNuk3z",
    "timestamp": "2025-09-28T19:40:12",
    "url": "https://reddit.com/r/Python/comments/1nt6u36/tiktock_video_downloader/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nsa5ae",
    "title": "Sunday Daily Thread: What's everyone working on this week?",
    "content": "# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è\n\nHello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!\n\n## How it Works:\n\n1. **Show &amp; Tell**: Share your current projects, completed works, or future ideas.\n2. **Discuss**: Get feedback, find collaborators, or just chat about your project.\n3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.\n\n## Guidelines:\n\n* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.\n* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.\n\n## Example Shares:\n\n1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!\n2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.\n3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!\n\nLet's build and grow together! Share your journey and learn from others. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-27T17:00:35",
    "url": "https://reddit.com/r/Python/comments/1nsa5ae/sunday_daily_thread_whats_everyone_working_on/",
    "score": 9,
    "num_comments": 2,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nruwa7",
    "title": "PySide vs. Avalonia: Which for a Solo Dev Building an Electrical Panel Designer ?",
    "content": "Hey,\n\nI'm a solo dev dipping into desktop app territory for the first time, and I'm torn between PySide (Python + Qt) and Avalonia (.NET/C#). The app? A tool for designing electrical panels: users drag-drop hierarchical elements (panels ‚Üí racks ‚Üí components) on a canvas, then auto-generate invoices (PDFs with BOMs). I'd like a modern UI‚Äîdark mode, smooth animations, rounded edges, the works.\n\nPriorities: Cross Platform(MacOS and Windows), high stability/perf (esp. canvas), and minimal new learning juggling other projects.\n\nI know Python and C# basics, but MVVM/XAML trips me up hard (can grind through it, but ugh). Want to stick to \\*one\\* language I can reuse for scripting/automation. No commercial license fees‚Äîproprietary means closed-source binaries I can sell without drama.\n\nQuick Project Fit\n\n\\- Core Needs: Interactive 2D canvas for diagramming (drag-drop hierarchies, snapping/zooming), invoice gen (e.g., ReportLab in Python or PdfSharp in C#), SQLite for component catalogs.\n\n\\- Modern UI Goal: aim for Fluent/Material-inspired polish.\n\n\\- Deployment: Standalone .app/.exe bundles, no web bloat.\n\nCurrent Tilt: PySide\n\nIt checks every box‚Äîcanvas strength, macOS native, Python scripting, easy modernity, and LGPL for sales‚Äîwithout the MVVM wall. Avalonia tempts with .NET ecosystem and MIT simplicity, but the learning hump + diagramming tweaks feel riskier for solo.\n\n\n\nWhat do you guys think? Built something similar? Switched mid-project?",
    "author": "Responsible-Word-137",
    "timestamp": "2025-09-27T06:12:03",
    "url": "https://reddit.com/r/Python/comments/1nruwa7/pyside_vs_avalonia_which_for_a_solo_dev_building/",
    "score": 62,
    "num_comments": 18,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ns6jea",
    "title": "Catch Code Changes as Git Diffs, Not Test Failures",
    "content": "    from difflogtest import register_unittest, get_logger\n    logger = get_logger()\n    \n    @register_unittest(logger=logger)\n    def test_my_function():\n        \"\"\"Test function that produces complex output.\"\"\"\n        logger.info(\"Starting complex computation...\")\n        logger.rule(\"Processing Data\")\n    \n        result = my_complex_function() # This can have all the logs you want\n    \n        logger.success(\"Computation completed successfully\")\n        logger.info(f\"Result shape: {result.shape}\")\n    \n        return result\n\nTL;DR: [difflogtest](https://github.com/affromero/DiffLogTest) monitors how your functions behave by tracking their logs and results in git files. When code changes unexpectedly, you see the differences right in your git status; no test failures, just behavioral drift detection. Just add the decorator to any function, and execute `run-unittests`. It will redirect all the logs to a text file.\n\n**What My Project Does**\n\nIt is a behavioral consistency framework that automatically captures both function logs and return values, storing them as organized text files that serve as behavioral baselines. Instead of traditional test assertions, it uses git diffs to show when function behavior changes unexpectedly during development. This lets you distinguish between intentional improvements and regressions, with built-in normalization filtering out noise like timestamps and memory addresses. \n\n**Target Audience**\n\nWe built for fast-moving startups and ML teams where constant experimentation happens but core functionality needs stability. It's perfect for environments where multiple developers iterate rapidly on shared codebases, and you want to encourage prototyping while catching behavioral drift. If you're in a startup where \"move fast and break things\" is the mantra but some things really shouldn't break, this provides the guardrails you need. We quickly catch bugs because we know exactly where to look when some log deviates.\n\n**Comparison**\n\nWhile pytest frameworks validate final results through explicit checks, difflogtest monitors the entire execution process: capturing logging, intermediate steps, and outputs for a complete behavioral picture. If you care more about how functions behave throughout execution rather than just final results, this gives you comprehensive monitoring without the test writing overhead. \n\nI'm not sure if this already exists, but for our use case we needed something like this and didn't find a good alternative. Happy to hear if someone knows of similar tools.",
    "author": "nifunif4",
    "timestamp": "2025-09-27T14:14:11",
    "url": "https://reddit.com/r/Python/comments/1ns6jea/catch_code_changes_as_git_diffs_not_test_failures/",
    "score": 11,
    "num_comments": 5,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ns3v4y",
    "title": "Saruph: A pixel-based terminal snake game",
    "content": "# [Saruph | GitHub](https://github.com/areebnaqash/Saruph)\n\n  \nHey! I recently created a pixel-based terminal snake game in Python, as my final project for CS50x. I thought I'd share this little game here.\n\n\n\n# What My Project Does\n\nNothing original, I suppose. It's just the basic snake game with the twist that it's inside a terminal and has some pixel art for sprites.\n\n\n\n# Target Audience\n\nAnyone. Yes, it's a toy project. I wanted to see how graphics could be manipulated inside a terminal.\n\n\n\n# Comparison\n\nAs stated above. Unlike most terminal-based snake games, it has some sprites, that's all.",
    "author": "areebnaqash",
    "timestamp": "2025-09-27T12:22:38",
    "url": "https://reddit.com/r/Python/comments/1ns3v4y/saruph_a_pixelbased_terminal_snake_game/",
    "score": 16,
    "num_comments": 1,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nrtgzf",
    "title": "pytest-results ‚Äî Regression testing plugin for pytest",
    "content": "# What My Project Does\n\n[`pytest-results`](https://github.com/100nm/pytest-results) is a pytest plugin that makes writing regression tests easier, especially when working with complex data structures.\n\nInstead of asserting against large nested structures, a test can simply return the object. The plugin serializes it and compares it against a previously stored result. If a difference is detected, the test fails.\n\nSupported return types:\n\n* pydantic.BaseModel\n* msgspec.Struct\n* JSON-serializable Python objects\n* bytes (saved as JSON files)\n\nIt is also possible to directly compare the differences following a regression in your IDE with the `--ide` parameter (e.g., `pytest --ide vscode`).\n\nAll regression files are stored in a `__pytest_results__` directory at the project root.\n\nExample:\n\n    from pydantic import BaseModel\n    \n    class ComplexModel(BaseModel):\n        foo: str\n        bar: str\n        baz: str\n    \n    def test_something() -&gt; ComplexModel:\n        # ...\n        model = ComplexModel(foo=\"foo\", bar=\"bar\", baz=\"baz\")\n        return model\n\n# Target Audience\n\nDevelopers who need regression testing for complex Python objects.\n\nTeams working with API responses, data models, or serialized structures that change over time.\n\nAnyone who wants to reduce the boilerplate of manually asserting large nested objects.\n\n# Comparison\n\nExisting plugins like `pytest-regressions` or `pytest-snapshot`, `pytest-results` differs by:\n\n* Using a return-based API (no extra assertion code required).\n* Providing IDE integration (`pytest --ide vscode` to review diffs directly in VSCode).\n* Supporting an explicit acceptance workflow (`pytest --accept-diff` to update expected results).\n\nSource code: [https://github.com/100nm/pytest-results](https://github.com/100nm/pytest-results)",
    "author": "Skearways",
    "timestamp": "2025-09-27T05:02:42",
    "url": "https://reddit.com/r/Python/comments/1nrtgzf/pytestresults_regression_testing_plugin_for_pytest/",
    "score": 51,
    "num_comments": 5,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nssyap",
    "title": "AI-powered CCTV using YOLOv8 for detection and ChatGPT for classification",
    "content": "I'm sharing my project, the Home Security CCTV Monitor. \n\nThis is a real-time home surveillance tool that uses YOLOv8 for object detection and ChatGPT (or another AI API) as the ‚Äúbrains‚Äù to interpret what the camera sees.\n\nWhat My Project Does:\n\nThis system watches live video and classifies events into NORMAL, CAUTION, or THREAT. It‚Äôs designed to go beyond motion detection by interpreting behavior, not just presence.\n\nKey features:\n\n* YOLOv8 Object Detection: Detects people, cars, trucks, and more in real time.\n* Behavior Rules + AI Reasoning:\n   * Walking past on sidewalks ‚Üí CAUTION\n   * Approaching the house/camera ‚Üí THREAT\n   * Walking away ‚Üí CAUTION\n   * Loitering near or interacting with cars ‚Üí THREAT\n* AI Event Summaries: YOLO handles detection, ChatGPT interprets context and generates concise security-style logs.\n* Timeline Logging: Keeps memory of the last 10 alerts with status and short stories.\n* Snapshots: Automatic evidence images saved into a detections folder.\n* Tkinter GUI: Live video feed, status panel, and event log window.\n\nTarget Audience:\n\nThis is aimed at hobbyists, Python developers, and DIY security enthusiasts who want to explore computer vision + AI for real-world applications. It‚Äôs also useful for anyone curious about extending YOLO beyond raw detection into behavior-aware security.\n\nComparison:\n\nTraditional CCTV or motion detection cameras only capture footage. This project adds a reasoning layer: YOLOv8 detects, and ChatGPT classifies behaviors as normal, caution, or threat. It essentially gives the camera a way to ‚Äúthink‚Äù about what it sees.\n\nSource Code:  \nGitHub Repository: [https://github.com/xogie/Security-Camera-w-AI](https://github.com/xogie/Security-Camera-w-AI)",
    "author": "ChardEmbarrassed7304",
    "timestamp": "2025-09-28T09:39:21",
    "url": "https://reddit.com/r/Python/comments/1nssyap/aipowered_cctv_using_yolov8_for_detection_and/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.37,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ns6wst",
    "title": "Python in ChemE",
    "content": "Hi everyone, I‚Äôm doing my Master‚Äôs in Chemical and Energy Engineering and recently started (learning) Python, with a background in MATLAB. As a ChemE student I‚Äôd like to ask which libraries I should focus on and what path I should take. For example, in MATLAB I mostly worked with plotting and saving data. Any tips from engineers would be appreciated :)",
    "author": "fatimalizade",
    "timestamp": "2025-09-27T14:30:19",
    "url": "https://reddit.com/r/Python/comments/1ns6wst/python_in_cheme/",
    "score": 7,
    "num_comments": 26,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nsj68m",
    "title": "Are the Xcode command line tools required for the precompiled Python from python.org?",
    "content": "The title probably says it all. A lot of internet sources claim that Xcode CLTs are required to install Python. However, this is probably true, if you want to install it from Homebrew or other sources that install it from source. But the precompiled version from Python.org should not be in need of these tools, am I right?",
    "author": "gernophil",
    "timestamp": "2025-09-28T01:33:28",
    "url": "https://reddit.com/r/Python/comments/1nsj68m/are_the_xcode_command_line_tools_required_for_the/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.18,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nrsa7l",
    "title": "Would open-sourcing my OCR-to-HTML document reconstruction tool be useful?",
    "content": "Hey everyone\nI‚Äôm working on a project where we translate scanned documents and we‚Äôre using Azure OCR. As you may know, Azure gives back a very abstract JSON like structure (in my case not really usable as is).\nI‚Äôve been building a tool that takes this raw OCR output (currently designed for Azure OCR‚Äôs format) and reconstructs it into a real document (HTML) that closely matches the original layout. That way, the result can be sent directly into a translation pipeline without tons of manual fixing.\nSo far, it‚Äôs been working really well for my use case.\nMy question is: would it be useful if I turned this into a Python package that others could use?Even if it starts Azure-specific, do you think people would find value in it?\nWould love to hear your thoughts and feedback\n\n",
    "author": "FunBlackberry6173",
    "timestamp": "2025-09-27T03:55:51",
    "url": "https://reddit.com/r/Python/comments/1nrsa7l/would_opensourcing_my_ocrtohtml_document/",
    "score": 8,
    "num_comments": 3,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ns3bts",
    "title": "I built a UI fiddle for Python Devs...",
    "content": "I mainly do front end oriented projects, mainly with a specific Python web framework, and I got the idea of putting something together to make prototyping UI components faster with minimal setup. \n\n\\# What My Project Does\n\nBuridan Play is basically the same as other fiddle apps like tailwind play, code pen, etc etc but build specifically for Reflex framework. It lets users to quickly prototype or visualize no-to-complicated components directly in the browser. No need to create new projects, or setup any environment. \n\n\\# Target Audience\n\nAny Python dev since the the app is built with only python and the UI components are built/designed with python as well.\n\n\\# Comparison\n\nMuch like other JS/HTML fiddles like tailwind play or code pen\n\nSource Video: [https://youtu.be/CANZVUGl0Cw](https://youtu.be/CANZVUGl0Cw)\n\nFramework Used to build: [https://github.com/reflex-dev/reflex](https://github.com/reflex-dev/reflex)",
    "author": "Wonderful-Today-497",
    "timestamp": "2025-09-27T12:00:48",
    "url": "https://reddit.com/r/Python/comments/1ns3bts/i_built_a_ui_fiddle_for_python_devs/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nsiw9i",
    "title": "Python library without external imports only built in",
    "content": "Hey everyone üëã\n\nI just created a new open-source repo called **Advanced Text Processor**.  \nThe idea is simple but with a twist:\n\nüîπ We build a Python text processing library (cleaning, tokenization, n-grams, vectorization, dataset handling, etc.)  \nüîπ **Rule:** No external libraries allowed. Everything must be done with Python‚Äôs built-in standard library.  \nüîπ Purpose: This is not about user acquisition or making money ‚Äî it‚Äôs about practice, collaboration, and seeing how far we can push the limits of \"pure Python\".  \n\nIt‚Äôs open for contributions and discussions.  \nCheck it out here:  https://github.com/SinanDede/advanced_text_processor\n\nWould love your feedback and ideas üôå",
    "author": "Sinan_Dede",
    "timestamp": "2025-09-28T01:15:36",
    "url": "https://reddit.com/r/Python/comments/1nsiw9i/python_library_without_external_imports_only/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.13,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nr2kmn",
    "title": "How pytest fixtures screwed me over",
    "content": "I need to write this of my chest, so to however wants to read this, here is my \"fuck my life\" moment as a python programmer for this week:\n\nI am happily refactoring a bunch of pytest-testcases for a work project. With this, my team decided to switch to explicitly import fixtures into each test-file instead of relying on them \"magically\" existing everywhere. Sounds like a good plan, makes things more explicit and easier to understand for newcomers. Initial testing looks good, everything works.\n\nI commit, the full testsuit runs over night. Next day I come back to most of the tests erroring out. Each one with a connection error. \"But that's impossible?\" We use a scope of session for your connection, there's only one connection for the whole testsuite run. There can be a couple of test running fine and than a bunch who get a connection error. How is the fixture re-connecting? I involve my team, nobody knows what the hecks going on here.\nSo I start digging into it, pytests docs usually suggest to import once in the `contest.py` but there is nothing suggesting other imports should't work.\n\nThan I get my Heureka: unter some obscure stack overflow post is a comment: _pytest resolves fixtures by their *full* import path, not just the symbol used in the file_. What? \n\nBut that's actually why non of the session-fixtures worked as expected. Each import statement creates a new fixture, each with a different import-path, even if they all look the same when used inside tests. Each one gets initialised seperatly and as they are scoped to the session, only destroyed at the end of the testsuite. Great... So back to global imports we went.\n\nI hope this helps some other tormented should and shortens the search for why pytest fixtures sometimes don't work as expected. Keep Coding!",
    "author": "JauriXD",
    "timestamp": "2025-09-26T07:18:15",
    "url": "https://reddit.com/r/Python/comments/1nr2kmn/how_pytest_fixtures_screwed_me_over/",
    "score": 166,
    "num_comments": 64,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ns79ep",
    "title": "Python Data Model Exercise",
    "content": "An exercise to help build the right mental model for Python data, the ‚ÄúSolution‚Äù link uses memory_graph to visualize execution and reveal what‚Äôs actually happening.\n\nWhat is the output of this program?\n```\nimport copy\n\nmydict = {1: [], 2: [], 3: []}\nc1 = mydict\nc2 = mydict.copy()\nc3 = copy.deepcopy(mydict)\nc1[1].append(100)\nc2[2].append(200)\nc3[3].append(300)\n\nprint(mydict)\n# --- possible answers ---\n# A) {1: [], 2: [], 3: []}\n# B) {1: [100], 2: [], 3: []}\n# C) {1: [100], 2: [200], 3: []}\n# D) {1: [100], 2: [200], 3: [300]}\n```\n\n- [Solution](https://memory-graph.com/#codeurl=https://raw.githubusercontent.com/bterwijn/memory_graph_videos/refs/heads/main/exercises/exercise1.py&amp;play)\n- [Explanation](https://github.com/bterwijn/memory_graph?tab=readme-ov-file#python-data-model)\n- [More exercises](https://www.reddit.com/r/Python_memory_graph/)",
    "author": "Sea-Ad7805",
    "timestamp": "2025-09-27T14:45:50",
    "url": "https://reddit.com/r/Python/comments/1ns79ep/python_data_model_exercise/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nr7ozl",
    "title": "Rock Paper Scissors Arena simulator with tkinter",
    "content": "[GitHub link](https://github.com/asweigart/rpsarena) | [PyPI link](https://pypi.org/project/rpsarena) | [Explanatory blog post with video](https://inventwithpython.com/blog/rps-arena.html)\n\n# What My Project Does\n\nRock Paper Scissors \"arena simulator\" where different emojis play a game of tag. Emoji converts the \"prey\" emoji that they catch. [You can see an example video in the blog post.](https://inventwithpython.com/blog/rps-arena.html)\n\n# Target Audience\n\nGeneral Python developers or those interested in simulations\n\n# Comparison\n\nThis is not an original project; many such rock-paper-scissors simulators exist. However, I wanted a pure Python package that didn't have external dependencies and was suitable for a \"screensaver\" or a \"simulation experiments\" style of execution.",
    "author": "AlSweigart",
    "timestamp": "2025-09-26T10:36:23",
    "url": "https://reddit.com/r/Python/comments/1nr7ozl/rock_paper_scissors_arena_simulator_with_tkinter/",
    "score": 26,
    "num_comments": 1,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nrb0eg",
    "title": "Realtime support added to Inngest (durable workflows) Python SDK",
    "content": "# What my project does\n\nInngest provides a durable workflow engine that enables devs to ship reliable backend processes, from multi-stage pipelines to AI workflows.\n\n# What's new with this release\n\nToday's release (`0.5.9`) adds built-in realtime support powered by WebSockets. This now allows the async, durable workflows to push messages or stream updates to the client side without additional libraries or infrastructure.\n\n# Use cases\n\nThe main purpose of this is to combine the typically long-running, multi-step durable workflows with realtime channels which can send progress updates, LLM chunks or other data to the browser to make applications more interactive.\n\n# Github, docs, guides\n\n* More info: [https://www.inngest.com/blog/python-realtime](https://www.inngest.com/blog/python-realtime)\n* Python quick start: [https://www.inngest.com/docs/getting-started/python-quick-start](https://www.inngest.com/docs/getting-started/python-quick-start)\n* Realtime guide: [https://www.inngest.com/docs/features/realtime?guide=python](https://www.inngest.com/docs/features/realtime?guide=python)\n* Example project on Github: [https://github.com/inngest/inngest-py/tree/main/examples/fast\\_api\\_realtime](https://github.com/inngest/inngest-py/tree/main/examples/fast_api_realtime)\n* SDK repo: [https://github.com/inngest/inngest-py](https://github.com/inngest/inngest-py)\n* Discord: [https://inngest.com/discord](https://inngest.com/discord)\n\n# Target Audience\n\nPython developers who want a solution to run reliable background work that also \n\nDevs that are building AI workflows often see this problem. LLMs are slow or you might chain multiple calls together so you reach for a queue, but then the user doesn't get feedback while they wait. Folks cobble things together with streaming APIs, but then loose the reliability of queues. \n\n# Comparison\n\nExisting solutions like Celery and RabbitMQ are good for queuing tasks, but is missing durable execution. [Durable execution](https://www.inngest.com/blog/principles-of-durable-execution) adds incremental execution of steps, fault tolerance, state persistence. Inngest's event-driven durable execution adds more reliability to these workflows without having to manage infrastructure.",
    "author": "self-taught16",
    "timestamp": "2025-09-26T12:46:51",
    "url": "https://reddit.com/r/Python/comments/1nrb0eg/realtime_support_added_to_inngest_durable/",
    "score": 16,
    "num_comments": 0,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nrgunj",
    "title": "Sphinx extension to fix broken GitHub links in your docs",
    "content": "# The problem\n\nOne thing that has always annoyed me when writing docs with Sphinx is that links in the README render fine on GitHub, but they always break in the built documentation.\n\nFor example:\n\n    `Installation Guide &lt;/docs/installation.rst&gt;`_\n\nlooks fine on GitHub, but Sphinx doesn‚Äôt understand it. If you switch to Sphinx-style references, for example\n\n    `Installation Guide &lt;installation&gt;`_\n\nworks in the docs but not on GitHub.\n\nI always had to keep 2 files which had almost the same information and that I always forgot to keep synchronized.\n\n# What my project does\n\nI ended up writing a small extension, **sphinx-linkfix**, that rewrites GitHub-style links into proper Sphinx references at build time. This way the same README and docs links work in *both* places \n\n* GitHub: [https://github.com/j-moralejo-pinas/sphinx-linkfix](https://github.com/j-moralejo-pinas/sphinx-linkfix)\n\nIt‚Äôs a tiny thing, but it has saved me a lot of frustration. I just built it just for myself, but there‚Äôs no point in keeping it private.\n\n# Target Audience\n\nIt is not a production grade extension, but it will be useful for anyone that likes to write their documentation with Sphinx, while keeping it renderable in Github. For now, it only serves my purposes, but if you want something added, you can ask for it.\n\n# Comparison\n\nAs far as i looked for something like this, I haven't seen other extensions that fix this problem, but correct me if I'm wrong.\n\nHopefully it helps others dealing with the same Sphinx + GitHub issue. Feedback and suggestions welcome!",
    "author": "j-moralejo-pinas",
    "timestamp": "2025-09-26T16:57:48",
    "url": "https://reddit.com/r/Python/comments/1nrgunj/sphinx_extension_to_fix_broken_github_links_in/",
    "score": 6,
    "num_comments": 4,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nroxvz",
    "title": "Python script to download Reddit posts/comments with media",
    "content": "[Github link](https://github.com/AsfanUlla/Reddit-save)\n\n# What My Project Does\n\nIt saves Reddit posts and comments locally along with any attached media like images, videos and gifs.\n\n# Target Audience\n\nAnyone who want to download Reddit posts and comments\n\n# Comparison\n\nMany such scripts already exists, but most of them require either auth or don't download attached media. This is a simple script which saves the post and comments locally along with the attached media without requiring any sort of auth it uses the post's json data which can be viewed by adding .json at the end of the post url (example link only works in browser: https://www.reddit.com/r/Python/comments/1nroxvz/python\\_script\\_to\\_download\\_reddit\\_postscomments.json).",
    "author": "Unlucky_Street_60",
    "timestamp": "2025-09-27T00:22:21",
    "url": "https://reddit.com/r/Python/comments/1nroxvz/python_script_to_download_reddit_postscomments/",
    "score": 1,
    "num_comments": 22,
    "upvote_ratio": 0.52,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nrade3",
    "title": "Haiku Validator: a simple Flask web app to write haikus!",
    "content": "https://github.com/scottastone/haiku-maker\n\nhttps://haikuvalidator.com/\n\nWhat My Project Does:\n\nA little flask app to write and validate haikus. It's definitely not perfect and makes some mistakes. It uses Flask for the web backend and syllables python libraries to estimate how many syllables are in each word. No fancy AI here.\n\nYou can check the override list at https://haikuvalidator.com/overrides and if you have any suggestions feel free to let me know any words that are broken.\n\nComparison: \n\nUhh I don't know if anyone else has done exactly this - most of the ones I found online didn't seem to work well.\n\nTarget Audience: \n\nThis is my first time making a web app. Hoping that someone finds it fun / useful.",
    "author": "mrstone56",
    "timestamp": "2025-09-26T12:21:11",
    "url": "https://reddit.com/r/Python/comments/1nrade3/haiku_validator_a_simple_flask_web_app_to_write/",
    "score": 10,
    "num_comments": 3,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqnm44",
    "title": "PEP 806 ‚Äì Mixed sync/async context managers with precise async marking",
    "content": "PEP 806 ‚Äì Mixed sync/async context managers with precise async marking\n\n[https://peps.python.org/pep-0806/](https://peps.python.org/pep-0806/)\n\n# Abstract\n\nPython allows the¬†`with`¬†and¬†`async¬†with`¬†statements to handle multiple context managers in a single statement, so long as they are all respectively synchronous or asynchronous. When mixing synchronous and asynchronous context managers, developers must use deeply nested statements or use risky workarounds such as overuse of¬†[`AsyncExitStack`](https://docs.python.org/3/library/contextlib.html#contextlib.AsyncExitStack).\n\nWe therefore propose to allow¬†`with`¬†statements to accept both synchronous and asynchronous context managers in a single statement by prefixing individual async context managers with the¬†`async`¬†keyword.\n\nThis change eliminates unnecessary nesting, improves code readability, and improves ergonomics without making async code any less explicit.\n\n# Motivation\n\nModern Python applications frequently need to acquire multiple resources, via a mixture of synchronous and asynchronous context managers. While the all-sync or all-async cases permit a single statement with multiple context managers, mixing the two results in the ‚Äústaircase of doom‚Äù:\n\n    async def process_data():\n        async with acquire_lock() as lock:\n            with temp_directory() as tmpdir:\n                async with connect_to_db(cache=tmpdir) as db:\n                    with open('config.json', encoding='utf-8') as f:\n                        # We're now 16 spaces deep before any actual logic\n                        config = json.load(f)\n                        await db.execute(config['query'])\n                        # ... more processing\n\nThis excessive indentation discourages use of context managers, despite their desirable semantics. See the¬†[Rejected Ideas](https://peps.python.org/pep-0806/#rejected-ideas)¬†section for current workarounds and commentary on their downsides.\n\nWith this PEP, the function could instead be written:\n\n    async def process_data():\n        with (\n            async acquire_lock() as lock,\n            temp_directory() as tmpdir,\n            async connect_to_db(cache=tmpdir) as db,\n            open('config.json', encoding='utf-8') as f,\n        ):\n            config = json.load(f)\n            await db.execute(config['query'])\n            # ... more processing\n\nThis compact alternative avoids forcing a new level of indentation on every switch between sync and async context managers. At the same time, it uses only existing keywords, distinguishing async code with the¬†`async`¬†keyword more precisely even than our current syntax.\n\nWe do not propose that the¬†`async¬†with`¬†statement should ever be deprecated, and indeed advocate its continued use for single-line statements so that ‚Äúasync‚Äù is the first non-whitespace token of each line opening an async context manager.\n\nOur proposal nonetheless permits¬†`with¬†async¬†some_ctx()`, valuing consistent syntax design over enforcement of a single code style which we expect will be handled by style guides, linters, formatters, etc. See¬†[here](https://peps.python.org/pep-0806/ban-single-line-with-async)¬†for further discussion.",
    "author": "kirara0048",
    "timestamp": "2025-09-25T17:44:27",
    "url": "https://reddit.com/r/Python/comments/1nqnm44/pep_806_mixed_syncasync_context_managers_with/",
    "score": 184,
    "num_comments": 22,
    "upvote_ratio": 0.99,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nr26fk",
    "title": "Feeling guilty using Bootstrap while learning Flask",
    "content": "So I‚Äôm learning Flask rn and using Bootstrap for the HTML part. I do know HTML/CSS, but I feel kinda guilty using pre-made stuff instead of coding everything from scratch.\nIs this chill or am I lowkey skipping real learning? üò¨",
    "author": "MelodicChampion5736",
    "timestamp": "2025-09-26T07:02:27",
    "url": "https://reddit.com/r/Python/comments/1nr26fk/feeling_guilty_using_bootstrap_while_learning/",
    "score": 16,
    "num_comments": 59,
    "upvote_ratio": 0.59,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nrgwpe",
    "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread",
    "content": "# Weekly Thread: Resource Request and Sharing üìö\n\nStumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!\n\n## How it Works:\n\n1. **Request**: Can't find a resource on a particular topic? Ask here!\n2. **Share**: Found something useful? Share it with the community.\n3. **Review**: Give or get opinions on Python resources you've used.\n\n## Guidelines:\n\n* Please include the type of resource (e.g., book, video, article) and the topic.\n* Always be respectful when reviewing someone else's shared resource.\n\n## Example Shares:\n\n1. **Book**: [\"Fluent Python\"](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \\- Great for understanding Pythonic idioms.\n2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \\- Excellent overview of Python's built-in data structures.\n3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \\- A deep dive into decorators.\n\n## Example Requests:\n\n1. **Looking for**: Video tutorials on web scraping with Python.\n2. **Need**: Book recommendations for Python machine learning.\n\nShare the knowledge, enrich the community. Happy learning! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-26T17:00:32",
    "url": "https://reddit.com/r/Python/comments/1nrgwpe/saturday_daily_thread_resource_request_and/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nra2ps",
    "title": "PAR LLAMA v0.7.0 Released - Enhanced Security &amp; Execution Experience",
    "content": "# What It Does\n\nA powerful Terminal User Interface (TUI) for managing and interacting with Ollama and other major LLM providers ‚Äî featuring **persistent AI memory**, **secure code execution**, **interactive development workflows**, and **truly personalized conversations**!\n\nPAR LLAMA Chat Interface\n\n# What's New in v0.7.0\n\n# Improved Execution Experience\n\n* **Better Result Formatting**: Clean, professional display of execution results\n* **Smart Command Display**: Shows 'python -c &lt;script&gt;' instead of escaped code for CLI parameters\n* **Syntax-Highlighted Code Blocks**: Short scripts (‚â§10 lines) display with proper syntax highlighting\n* **Intelligent Language Detection**: Automatic highlighting for Python, JavaScript, and Bash\n* **Clean Command Truncation**: Long commands truncated intelligently for better readability\n\n# Previous Major Features (v0.6.0)\n\n# Memory System\n\n* **Persistent User Context**: AI remembers who you are and your preferences across ALL conversations\n* **Memory Tab Interface**: Dedicated UI for managing your personal information and context\n* **AI-Powered Memory Updates**: Use `/remember` and `/forget` slash commands for intelligent memory management\n* **Automatic Injection**: Your memory context appears in every new conversation automatically\n* **Real-time Synchronization**: Memory updates via commands instantly reflect in the Memory tab\n* **Smart Context Management**: Never repeat your preferences or background information again\n\n# Template Execution System\n\n* **Secure Code Execution**: Execute code snippets and commands directly from chat messages using **Ctrl+R**\n* **Multi-Language Support**: Python, JavaScript/Node.js, Bash, and shell scripts with automatic language detection\n* **Configurable Security**: Command allowlists, content validation, and comprehensive safety controls\n* **Interactive Development**: Transform PAR LLAMA into a powerful development companion\n* **Real-time Results**: Execution results appear as chat responses with output, errors, and timing\n\n# Enhanced User Experience\n\n* **Memory Slash Commands**: `/remember [info]`, `/forget [info]`, `/memory.status`, `/memory.clear`\n* **Intelligent Updates**: AI intelligently integrates new information into existing memory\n* **Secure Storage**: All memory data stored locally with comprehensive file validation\n* **Options Integration**: Both Memory and Template Execution controls in Options tab\n* **Settings Persistence**: All preferences persist between sessions\n\n# Core Features\n\n* **Memory System**: Persistent user context across all conversations with AI-powered memory management\n* **Template Execution**: Secure code execution system with configurable safety controls\n* **Multi-Provider Support**: Ollama, OpenAI, Anthropic, Groq, XAI, OpenRouter, Deepseek, LiteLLM\n* **Vision Model Support**: Chat with images using vision-capable models\n* **Session Management**: Save, load, and organize chat sessions\n* **Custom Prompts**: Create and manage custom system prompts and Fabric patterns\n* **Theme System**: Dark/light modes with custom theme support\n* **Model Management**: Pull, delete, copy, and create models with native quantization\n* **Smart Caching**: Intelligent per-provider model caching with configurable durations\n* **Security**: Comprehensive file validation and secure operations\n\n# Key Features\n\n* **100% Python**: Built with Textual and Rich for a beautiful easy to use terminal experience. Dark and Light mode support, plus custom themes\n* **Cross-Platform**: Runs on Windows, macOS, Linux, and WSL\n* **Async Architecture**: Non-blocking operations for smooth performance\n* **Type Safe**: Fully typed with comprehensive type checking\n\n# GitHub &amp; PyPI\n\n* GitHub: [https://github.com/paulrobello/parllama](https://github.com/paulrobello/parllama)\n* PyPI: [https://pypi.org/project/parllama/](https://pypi.org/project/parllama/)\n\n# Comparison:\n\nI have seen many command line and web applications for interacting with LLM's but have not found any TUI related applications as feature reach as PAR LLAMA\n\n# Target Audience\n\nIf you're working with LLMs and want a powerful terminal interface that **remembers who you are** and **bridges conversation and code execution** ‚Äî PAR LLAMA v0.7.0 is a game-changer. Perfect for:\n\n* **Developers**: Persistent context about your tech stack + execute code during AI conversations\n* **Data Scientists**: AI remembers your analysis preferences + run scripts without leaving chat\n* **DevOps Engineers**: Maintains infrastructure context + execute commands interactively\n* **Researchers**: Remembers your research focus + test experiments in real-time\n* **Consultants**: Different client contexts persist across sessions + rapid prototyping\n* **Anyone**: Who wants truly personalized AI conversations with seamless code execution",
    "author": "probello",
    "timestamp": "2025-09-26T12:09:45",
    "url": "https://reddit.com/r/Python/comments/1nra2ps/par_llama_v070_released_enhanced_security/",
    "score": 3,
    "num_comments": 2,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nr3qvm",
    "title": "Pytrithon: Graphical Petri-Net Inspired Agent Oriented Programming Language Based On Python",
    "content": "# What My Project Does\nPytrithon is a graphical petri-net inspired agent oriented programming language based on Python. Do not worry, there is no need to understand formal petri-nets, the language instead is only inspired by them and is very simple and intuitive. Instead of a tree structure of linear code files, you have multiple Agents, each being a two dimensional graphical Pytri net, which cooperate with eachother. Pytrithon introduces native inter Agent communication into the core language as a first class member. You can directly model the actual control flow of an Agent which frees you from the strict linear recursive method calling of Python and enables many more modes of structuring the code. The Pytri nets you will create are very intuitive and readable, just by looking at them you can directly understand how the Agents operate, you don't need to browse the code as you do in plain Python and jump from file to file, method to method, desperately trying to reverse engineer how the code works. There are Places which store intermediate and global data and there are subtypes which express different use cases of variables, like queues and stacks. Pytrithon has many different Transitions, which are the actors of an Agent and are triggered by Places. The main Python Transition allows you to directly assign an arbitrary Python snippet as an action and allows for the powerful triggering of other parts of the Pytri net through supression. There also are different types of Tansitions which embody different kinds of intra Agent control flow, like an explicit if or switch, sending and receiving a signal, defining and using the Pytri net equivalent of a method, a Nethod. For inter Agent communication there are Transitions for sending and receiving arbitrary Python objects inbetween Agents, and the Task abstaction allows for an Agent to offer a service to other Agents which can be utilized as a single Transition on the caller's side. What makes a Pytri net so graspable is that all the control flow is apparent through explicit graphical Arcs, which connect Places to Transitions and hint at what follows what. Entire Pytri nets can be turned into Fragments and embedded into any Agent to modularize Pytrithon code. Ontology Concepts can be defined by stating their slots and are used to encapsulate data. One of Pytrithon's strengths is that you can monitor and manipulate Agents through the Monipulator, even during their execution, and can see the state of an Agent by viewing the contents of Places inside its Pytri net.\n# Target Audience\nPytrithon is for developers of all skill levels who want to try something new. Experienced Python programmers should value the new expressiveness it offers and know intuitively how to operate it. It is especially suited for Python beginners who want to kickstart into a much mightier language and want to learn about Agents communicating with one another on the fly. Pytrithon is an universal programming language which can be used for anything Python can be used for. It is suitable for quick prototyping, since you can directly embed GUI widgets into an Agent, but can also be used for more demanding and complex use cases, exemplified by TMWOTY2, a full Pygame game, which runs at 60 frames per second across 6 different Agents.\n# Why I Built It\nAt university I got introduced to a formal Petri net tool which was there used to learn about Petri nets and agent oriented programming, with which we implemented a Settlers game. I really enjoyed the expressiveness of Petri nets but found out that its formal nature made simple tasks very complicated. There were huge structures just to send data from one agent to another and you had to understand Petri nets in depth. I wanted something similar but way more intuitive and terse and adapted it into the Pytrithon language for more than 15 years now by rethinking how to integrate it deeply with Python.\n# Comparison\nNothing compares to Pytrithon, it is its very own thing. Most textual programming languages are based on linear files. Most graphical programming languages do not allow embedding arbitrary code and are just glorified parametrized flowcharts.\n# How To Explore\nAt least Python 3.10 is required to run all example Agents. The install script should install all required packages. Then you can run the pytrithon script to open up a Monipulator and check out the example Agents by hitting ctrl-o. If you prefer using the console, run 'python nexus -m &lt;agentname&gt;'. Recommended Agents to try are: \"basic\", \"calculator\", \"kniffel\", \"guess\", \"pokerserver\" + multiple \"poker\", \"chatserver\" + multiple \"chat\", \"image\", \"jobapplic\", and \"nethods\". There are also scripts for running and editing TMWOTY2. Your focus should be on the workbench folder, Pytrithon is just the backstage where the magic happens.\n# GitHub Link\nhttps://github.com/JochenSimon/pytrithon\n---\nWhen you give it a try, I would really appreciate feedback, because I have not had any yet, since I only recently found the courage to present it. I welcome being told of any problems when installing and running it, so that I can fix them and they do not bother people anymore. I would enjoy hearing your opinions and ideas for improvement, it would mean a lot to me if you explore several of the example Agents. I welcome any questions and would love to answer them.",
    "author": "Pytrithon",
    "timestamp": "2025-09-26T08:04:32",
    "url": "https://reddit.com/r/Python/comments/1nr3qvm/pytrithon_graphical_petrinet_inspired_agent/",
    "score": 9,
    "num_comments": 3,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqv24k",
    "title": "Re-define or wrap exceptions from external libraries?",
    "content": "I'm wondering what the best practice is for the following situation:\n\n  \nSuppose I have a Python package that does some web queries. In case it matters, I follow the Google style guide. It currently uses `urllib`. If those queries fails, it currently raises a `urllib.error.HTTPError`. \n\nAny user of my Python package would therefore have to catch the  `urllib.error.HTTPError` for the cases where the web queries fail. This is fine, but it would be messy if I at some point decide not to use `urllib` but some other external library.\n\n\nI could make a new `mypackage.HTTPError` or `mypackage.QueryError` exception, and then do a `try: ... catch urllib.error.HTTPError: raise mypackage.QueryError` or even\n\n```\ntry: \n    ... \ncatch urllib.error.HTTPError as e:\n    raise mypackage.QueryError from e\n```\n\nWhat is the recommended approach?",
    "author": "Ok_Constant_9126",
    "timestamp": "2025-09-26T00:33:15",
    "url": "https://reddit.com/r/Python/comments/1nqv24k/redefine_or_wrap_exceptions_from_external/",
    "score": 27,
    "num_comments": 16,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqvsvd",
    "title": "Material 3 Design Comes To Slint GUI Toolkit",
    "content": "üöÄ¬†Speed up UI development with pre-built components,  \nüöÄ¬†Deliver a polished, touch-friendly, familiar user interface for your products,  \nüöÄ¬†Build a user interface that seamlessly works across desktop, mobile, web, and embedded devices.\n\nExplore:¬†[https://material.slint.dev](https://material.slint.dev/)  \nGet started:¬†[https://material.slint.dev/getting-started](https://material.slint.dev/getting-started)",
    "author": "slint-ui",
    "timestamp": "2025-09-26T01:22:18",
    "url": "https://reddit.com/r/Python/comments/1nqvsvd/material_3_design_comes_to_slint_gui_toolkit/",
    "score": 20,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nrrpq2",
    "title": "I tried to refactor my Python code using ChatGPT...",
    "content": "I have this [web application](https://www.synthetic-depo.com/), built as a POC, of which I am the only user.\n\nIt has a lot of inefficiencies in terms of global performance: using numerous loops, duplicated code snippets in various functions,using scipy fsolve rather than scipy brentq etc..\n\nSo I tried to refactor it with ChatGPT. Of course it does not know what I am after, so I use the output of my application as a benchmark for expected results of the refactoring. The process is quite exhausting, as ChatGPT has a lot of different coding ideas to get me there. Needless to say, he is still not there...yet.\n\nI noted that the code is now a lot more efficient, no question about it, but I no longer understand what it does exactly: the code has clearly overreached my Python proficiency.\n\nSo I wondered if, in a lot of companies where former employees spawn their own AI outfit, there is not a case where nobody understands any longer what is going on in their very efficient code.",
    "author": "whoeverdidnt",
    "timestamp": "2025-09-27T03:21:00",
    "url": "https://reddit.com/r/Python/comments/1nrrpq2/i_tried_to_refactor_my_python_code_using_chatgpt/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.23,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqrmrd",
    "title": "Show r/Python: PyWebTransport ‚Äì The canonical, async-native WebTransport stack for Python.",
    "content": "Hi everyone,\n\nI'm excited to share `PyWebTransport`, a modern, async-native networking library for Python. It's designed as a powerful alternative to WebSockets, leveraging the QUIC protocol to solve issues like head-of-line blocking and provide more versatile communication patterns.\n\nThe project is open-source, fully documented, and available on PyPI. It provides a high-level, asyncio-native API for the WebTransport protocol, allowing you to build high-performance, real-time network applications.\n\n### What My Project Does\n\n`PyWebTransport`'s main features include:\n\n- **Full Async Support**: Built from the ground up on asyncio for high-performance, non-blocking I/O.\n- **High-Level Frameworks**: Includes a ServerApp with routing and middleware, and a versatile WebTransportClient with helpers for pooling, auto-reconnection, and proxying.\n- **Advanced Messaging**: Built-in managers for Pub/Sub and RPC (JSON-RPC 2.0 compliant), plus pluggable serializers (`JSON`, `MsgPack`, `Protobuf`) for structured data.\n- **Complete Protocol Implementation**: Full support for bidirectional and unidirectional streams, as well as unreliable datagrams.\n- **Lifecycle and Resource Management**: Robust, async context-managed components for handling connections, sessions, streams, and monitoring.\n- **Event-Driven Architecture**: A powerful EventEmitter and EventBus system for decoupled, asynchronous communication between components.\n- **Type-Safe and Tested**: A fully type-annotated API with extensive test coverage (unit, integration, E2E) to ensure reliability and maintainability.\n\n### Target Audience\n\nThis library is intended for developers building **high-performance, real-time network applications** in Python.\n\nIt is designed with production use cases in mind. Features like robust resource management to prevent leaks, detailed statistics for monitoring, and the auto-reconnect client are all included to support stable, long-running services.\n\n### Comparison\n\nThe main alternative is WebSockets. `PyWebTransport` differs by leveraging QUIC to offer:\n\n- **No Head-of-Line Blocking**: Because it supports multiple, independent streams, a slow or large message on one stream doesn't block others.\n- **Unreliable Datagrams**: It provides a datagram API for sending low-latency, non-guaranteed messages, which WebSockets doesn't offer. This is ideal for things like real-time game state or voice data.\n- **Unidirectional Streams**: It supports write-only and read-only streams, which can be more efficient for certain application patterns, like a client sending a continuous stream of telemetry.\n\n### A Quick Look at the API\n\n#### Server (`server.py`)\n\n```python\nimport asyncio\n\nfrom pywebtransport import (\n    ConnectionError,\n    ServerApp,\n    ServerConfig,\n    SessionError,\n    WebTransportSession,\n    WebTransportStream,\n)\nfrom pywebtransport.utils import generate_self_signed_cert\n\ngenerate_self_signed_cert(hostname=\"localhost\")\n\napp = ServerApp(\n    config=ServerConfig.create(\n        certfile=\"localhost.crt\",\n        keyfile=\"localhost.key\",\n        initial_max_data=1024 * 1024,\n        initial_max_streams_bidi=10,\n    )\n)\n\n\nasync def handle_datagrams(session: WebTransportSession) -&gt; None:\n    try:\n        datagram_transport = await session.datagrams\n        while True:\n            data = await datagram_transport.receive()\n            await datagram_transport.send(data=b\"ECHO: \" + data)\n    except (ConnectionError, SessionError, asyncio.CancelledError):\n        pass\n\n\nasync def handle_streams(session: WebTransportSession) -&gt; None:\n    try:\n        async for stream in session.incoming_streams():\n            if isinstance(stream, WebTransportStream):\n                data = await stream.read_all()\n                await stream.write_all(data=b\"ECHO: \" + data)\n    except (ConnectionError, SessionError, asyncio.CancelledError):\n        pass\n\n\n@app.route(path=\"/\")\nasync def echo_handler(session: WebTransportSession) -&gt; None:\n    datagram_task = asyncio.create_task(handle_datagrams(session))\n    stream_task = asyncio.create_task(handle_streams(session))\n    try:\n        await session.wait_closed()\n    finally:\n        datagram_task.cancel()\n        stream_task.cancel()\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"127.0.0.1\", port=4433)\n\n```\n\n#### Client (`client.py`)\n\n```python\nimport asyncio\nimport ssl\n\nfrom pywebtransport import ClientConfig, WebTransportClient\n\n\nasync def main() -&gt; None:\n    config = ClientConfig.create(\n        verify_mode=ssl.CERT_NONE,\n        initial_max_data=1024 * 1024,\n        initial_max_streams_bidi=10,\n    )\n\n    async with WebTransportClient(config=config) as client:\n        session = await client.connect(url=\"https://127.0.0.1:4433/\")\n\n        print(\"Connection established. Testing datagrams...\")\n        datagram_transport = await session.datagrams\n        await datagram_transport.send(data=b\"Hello, Datagram!\")\n        response = await datagram_transport.receive()\n        print(f\"Datagram echo: {response!r}\\n\")\n\n        print(\"Testing streams...\")\n        stream = await session.create_bidirectional_stream()\n        await stream.write_all(data=b\"Hello, Stream!\")\n        response = await stream.read_all()\n        print(f\"Stream echo: {response!r}\")\n\n        await session.close()\n\n\nif __name__ == \"__main__\":\n    try:\n        asyncio.run(main())\n    except KeyboardInterrupt:\n        pass\n\n```\n\n### Links\n\n- **GitHub (Source &amp; Issues)**: `https://github.com/lemonsterfy/pywebtransport`\n\nThe goal was to create a robust and well-documented library that fits naturally into the Python `asyncio` ecosystem. All feedback, suggestions, and contributions are welcome.\n\nWould love to hear feedback from anyone who‚Äôs tried experimenting with QUIC or WebTransport in Python.\n",
    "author": "CodeOrganic3141",
    "timestamp": "2025-09-25T21:06:23",
    "url": "https://reddit.com/r/Python/comments/1nqrmrd/show_rpython_pywebtransport_the_canonical/",
    "score": 10,
    "num_comments": 7,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nr49v6",
    "title": "AI Pothole Detector LIVE ‚Äì Testing on Varthur-Gunjur Road, Bangalore üöß",
    "content": "[https://www.youtube.com/watch?v=mJGvRONdpbI](https://www.youtube.com/watch?v=mJGvRONdpbI)\n\nüëâ On just a 50-meter stretch, the AI detected 32 potholes in real time, logging their location, number, and timestamp into a live dataset.  \n  \nüîç What‚Äôs inside this demo:  \n  \nLive video feed with AI highlighting potholes  \n  \nAutomatic logging of pothole data to Excel/CSV  \n  \nReal-time insights for road maintenance  \n  \nüõ† Why it matters for Bangalore:  \n  \nGovernment has announced massive budgets for road repair (‚Çπ5,948 crore for maintenance).  \n  \nEarly detection can save money, reduce accidents, and avoid endless manual inspections.  \n  \nThis system can integrate into Smart City solutions, giving authorities accurate, real-time maps of road damage.  \n  \nThis is just the beginning ‚Äî I‚Äôm working on upgrades to also detect size, depth, and severity of potholes.  \n  \nüí° Do you think AI like this can help solve Bangalore‚Äôs pothole problem? Share your thoughts in the comments!  \n  \nIf you find this useful, please like, share, and subscribe to support more tech-driven solutions for our city‚Äôs infrastructure.",
    "author": "chandan__m",
    "timestamp": "2025-09-26T08:24:49",
    "url": "https://reddit.com/r/Python/comments/1nr49v6/ai_pothole_detector_live_testing_on_varthurgunjur/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nq1588",
    "title": "What small Python automation projects turned out to be the most useful for you?",
    "content": "I‚Äôm trying to level up through practice and I‚Äôm leaning toward automation simple scripts or tools that actually make life or work easier.\n\nWhat projects have been the most valuable for you? For example:  \n data parsers or scrapers  \n bots (Telegram/Discord)  \n file or document automation  \n small data analysis scripts\n\nI‚Äôm especially curious about projects that solved a real problem for you, not just tutorial exercises.\n\nI think a list like this could be useful not only for me but also for others looking for practical Python project ideas.",
    "author": "MENTX3",
    "timestamp": "2025-09-25T01:19:35",
    "url": "https://reddit.com/r/Python/comments/1nq1588/what_small_python_automation_projects_turned_out/",
    "score": 272,
    "num_comments": 119,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqknrt",
    "title": "AISP - Artificial Immune Systems Package",
    "content": "Hi everyone!\n\nAs part of my final thesis, I developed **AISP (Artificial Immune Systems Package)**, an open-source Python library that implements Artificial Immune System (AIS) techniques.\n\n**What My Project Does**\n\nAISP provides implementations of algorithms inspired by the vertebrate immune system, applicable to tasks such as classification, anomaly detection, and optimization. The package currently includes:\n\n* Negative Selection Algorithm (NSA)\n* Clonal Selection Algorithm\n* Artificial Immune Network\n\n  \n**Target Audience**  \nResearchers and students interested in natural computing and machine learning.\n\n  \n**Comparison**\n\nUnlike other scattered implementations, AISP brings together multiple Artificial Immune System approaches into a single, unified package with a consistent interface.\n\nüìÇ GitHub: [github.com/AIS-Package/aisp](https://github.com/AIS-Package/aisp)\n\nüìñ Documentation: [ais-package.github.io](https://ais-package.github.io)\n\nüêç Pypi: [https://pypi.org/project/aisp/](https://pypi.org/project/aisp/)",
    "author": "Jpsilvabarr",
    "timestamp": "2025-09-25T15:29:42",
    "url": "https://reddit.com/r/Python/comments/1nqknrt/aisp_artificial_immune_systems_package/",
    "score": 14,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqe9pj",
    "title": "Series of Jupyter notebooks teaching Jax numerical computing library",
    "content": "Two years ago, as part of my Ph.D., I migrated some vectorized NumPy code to JAX to leverage the GPU and achieved a pretty good speedup (roughly 100x, based on how many experiments I could run in the same timeframe). Since third-party resources were quite limited at the time, I spent quite a bit of time time consulting the documentation and experimenting. I ended up creating a series of educational notebooks covering how to migrate from NumPy to JAX, core JAX features (admittedly highly opinionated), and real-world use cases with examples that demonstrate the core features discussed.\n\nThe material is designed for self-paced learning, so I thought it might be useful for at least one person here. I've presented it at some events for my university and at [PyCon 2025 - Speed Up Your Code by 50x: A Guide to Moving from NumPy to JAX](https://us.pycon.org/2025/schedule/presentation/54/). \n\nThe repository includes a series of standalone [exercises](https://github.com/IanQS/numpy_to_jax/tree/main/exercises) (with solutions in a separate folder) that introduce each concept with exercises that gradually build on themselves. There's also series of [case-studies](https://github.com/IanQS/numpy_to_jax/tree/main/case_studies) that demonstrate the practical applications with different algorithms.\n\nThe core functionality covered includes:\n\n- jit\n- loop-primitives\n- vmap\n- profiling\n- gradients + gradient manipulations\n- pytrees\n- einsum\n\nWhile the use-cases covers:\n\n- binary classification\n- gaussian mixture models\n- leaky integrate and fire\n- lotka-volterra\n\n---\n\nPlans for the future include 3d-tensor parallelism and maybe more real-world examplees\n",
    "author": "iamquah",
    "timestamp": "2025-09-25T11:17:09",
    "url": "https://reddit.com/r/Python/comments/1nqe9pj/series_of_jupyter_notebooks_teaching_jax/",
    "score": 25,
    "num_comments": 6,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqko2g",
    "title": "An Empirical Study of Type-Related Defects in Python Projects [pdf]",
    "content": "https://rebels.cs.uwaterloo.ca/papers/tse2021_khan.pdf\n\n**Abstract**: In recent years, Python has experienced an explosive growth in adoption, particularly among open source projects. While\nPython‚Äôs dynamically-typed nature provides developers with powerful programming abstractions, that same dynamic type system\nallows for type-related defects to accumulate in code bases. To aid in the early detection of type-related defects, type annotations were\nintroduced into the Python ecosystem (i.e., PEP-484) and static type checkers like mypy have appeared on the market. While\napplying a type checker like mypy can in theory help to catch type-related defects before they impact users, little is known about the\nreal impact of adopting a type checker to reveal defects in Python projects.\nIn this paper, we study the extent to which Python projects benefit from such type checking features. For this purpose, we mine the\nissue tracking and version control repositories of 210 Python projects on GitHub. Inspired by the work of Gao et al. on type-related\ndefects in JavaScript, we add type annotations to test whether mypy detects an error that would have helped developers to avoid real\ndefects. We observe that 15% of the defects could have been prevented by mypy. Moreover, we find that there is no significant\ndifference between the experience level of developers committing type-related defects and the experience of developers committing\ndefects that are not type-related. In addition, a manual analysis of the anti-patterns that most commonly lead to type-checking faults\nreveals that the redefinition of Python references, dynamic attribute initialization and incorrectly handled Null objects are the most\ncommon causes of type-related faults. Since our study is conducted on fixed public defects that have gone through code reviews and\nmultiple test cycles, these results represent a lower bound on the benefits of adopting a type checker. Therefore, we recommend\nincorporating a static type checker like mypy into the development workflow, as not only will it prevent type-related defects but also\nmitigate certain anti-patterns during development",
    "author": "ketralnis",
    "timestamp": "2025-09-25T15:30:04",
    "url": "https://reddit.com/r/Python/comments/1nqko2g/an_empirical_study_of_typerelated_defects_in/",
    "score": 5,
    "num_comments": 3,
    "upvote_ratio": 0.66,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nq45ep",
    "title": "migrating from django to FastAPI",
    "content": "We've hit the scaling wall with our decade-old Django monolith. We handle¬†**45,000 requests/minute (RPM)**¬†across¬†**1,500+ database tables**, and the synchronous¬†**ORM calls**¬†are now our critical bottleneck, even with async views. We need to migrate to an¬†**async-native Python framework**.\n\nTo survive this migration, the alternative must meet these criteria:\n\n1. **Python-Based**¬†(for easy code porting).\n2. **ORM**¬†support similar to Django,\n3. **Stability &amp; Community**¬†(not a niche/beta framework).\n4. **Feature Parity:**¬†Must have good equivalents for:\n   * **Admin Interface**¬†(crucial for ops).\n   * **Template system.**\n   * **Signals/Receivers**¬†pattern.\n   * **CLI Tools**¬†for¬†**migrations**¬†(`makemigrations`,¬†`migrate`, custom management commands, shell).\n5. We're looking at¬†**FastAPI**¬†(great async, but lacks ORM/Admin/Migrations batteries) and¬†**Sanic**, but open to anything.\n\nalso please share if you have done this what are your experiences",
    "author": "No-Excitement-7974",
    "timestamp": "2025-09-25T04:25:34",
    "url": "https://reddit.com/r/Python/comments/1nq45ep/migrating_from_django_to_fastapi/",
    "score": 52,
    "num_comments": 69,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nq5x1b",
    "title": "PyCon AU 2025 talks are all up!",
    "content": "This year's PyCon AU talks have all been uploaded! \n\nThey're all in playlist form [here](https://www.youtube.com/playlist?list=PLs4CJRBY5F1LRkAAUwbqHNGPBlxDkrz-3), but in general it's best not to run from start to finish or you'll get a bunch of the conference opening/closing stuff. (Disclaimer: I volunteer for PyCon AU)\n\nThis year I'd recommend:\n\n1. [Lilly Ryan's \"Falsehoods Programmers Believe About Reality\"](https://www.youtube.com/playlist?list=PLs4CJRBY5F1LRkAAUwbqHNGPBlxDkrz-3) - in which Lilly talks about how to get things done even though it's basically impossible to model the world correctly. \n\n2. [Benno Rice's \"Skill Issue\"](https://www.youtube.com/watch?v=ND_SPnOynvg&amp;list=PLs4CJRBY5F1LRkAAUwbqHNGPBlxDkrz-3&amp;index=65) - in which Benno (of [The Tragedy of Systemd](https://www.youtube.com/watch?v=o_AIw9bGogo)) talks through his discomfort with ~~AI~~ Large Language Models and decides whether he's got valid reasons or if he's simply dislikes change. (Trust me this is not a talk about LLMs... mostly).\n\n3. [Dilpreet Singh's \"Beyond Vibes - Building Evals for Generative AI\"](https://youtu.be/sgpPhL15W10?si=KfHxhMNsbfU5lHgT) - Dilpreet talks through the steps he and his team have taken to build  evaluations of LLM outputs. \n\nI haven't had the chance to watch everything yet, and my time actually in talks was pretty limited this year, so I'm really looking forward to: \n\n1. [The Student Showcase](https://youtu.be/mqg93zv1S-E?si=ZrTkCcMNTy-2pmZl), [Lightning Talks 1](https://youtu.be/j1e9kF8uaNY?si=FI-g_O8ugvD2dxOk) and [Lightning Talks 2](https://youtu.be/Wl1CZTpWFEk?si=mPXnNDCdQYTcQk4q) - these are all the 'variety' talks that appeal to my attention span. The Student Showcase is almost always my favourite part of the conference, because of how cool the projects are and the fact that _these people are still in high school_.\n\n2. [Hailey Bartlett's \"Pinchy the Bestest Boi\"](https://youtu.be/Q2I7uJDIQhE?si=WQ5trjqvwHn8uDCu) - Pinchy robot!\n\n3. [Michaela Wheeler's \"High altitude balloon imagery decoding in the browser with C, JS, and Python\"](https://youtu.be/Xd1unoPzvgI?si=EY2zEZ0IZEsbAonT) - I don't know, this just sounds cool?\n\nKeen to hear what others find interesting here! \n\n(Also, I think I'd be remiss if I didn't mention [PyCon AU 2026](https://2026.pycon.org.au/) has already been announced in Brisbane next year and ticket sales are already open. Worth clicking, ^if ^^only ^^^because ^^^^we ^^^^^animated ^^^^^^the ^^^^^^^Curlyboi ^^^^^^^^this ^^^^^^^^^year)",
    "author": "fphhotchips",
    "timestamp": "2025-09-25T05:51:06",
    "url": "https://reddit.com/r/Python/comments/1nq5x1b/pycon_au_2025_talks_are_all_up/",
    "score": 27,
    "num_comments": 1,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqmont",
    "title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays",
    "content": "# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è\n\nWelcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!\n\n## How it Works:\n\n1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.\n2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.\n3. **News &amp; Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.\n\n## Guidelines:\n\n* All topics should be related to Python or the /r/python community.\n* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).\n\n## Example Topics:\n\n1. **New Python Release**: What do you think about the new features in Python 3.11?\n2. **Community Events**: Any Python meetups or webinars coming up?\n3. **Learning Resources**: Found a great Python tutorial? Share it here!\n4. **Job Market**: How has Python impacted your career?\n5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!\n6. **Community Ideas**: Something you'd like to see us do? tell us.\n\nLet's keep the conversation going. Happy discussing! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-25T17:00:48",
    "url": "https://reddit.com/r/Python/comments/1nqmont/friday_daily_thread_rpython_meta_and_freetalk/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nq3azn",
    "title": "Best approach to modernize a Python + PyQt5 desktop app (EXE, Windows, offline)?",
    "content": "Hi all,\n\nI have a Python app built with PyQt5 and Qt Creator for the GUI. I need to rebuild and modernize the interface and workflow. My main constraints:\n\n* It must be packaged as an **.exe for Windows** (offline use, no dependencies on a web connection).\n* Backend must remain **Python** (lots of logic/data processing already there).\n* I‚Äôm fluent in **React** for frontend development, so I‚Äôd love to leverage modern UI practices if possible.\n\nWhat‚Äôs the best approach in 2025 to create a **modern, polished GUI** for a Python desktop app?\n\nI‚Äôve seen options like Electron (tying React with Python APIs), but it looks easy to get bloated or run into pitfalls. Other people suggest sticking with PyQt or switching to PySide, but they don‚Äôt feel as ‚Äúmodern‚Äù out of the box.\n\nHas anyone here gone through this recently? Should I:\n\n* Stick with PyQt/PySide and just modernize styles?\n* Use React with something like Tauri or a bridge to Python?\n* Look at other Python-native GUI frameworks?\n\nWould love to hear real-world experience with **long-term maintainability, performance, and packaging into a reliable EXE**.",
    "author": "MatadorFearsNoBull",
    "timestamp": "2025-09-25T03:38:14",
    "url": "https://reddit.com/r/Python/comments/1nq3azn/best_approach_to_modernize_a_python_pyqt5_desktop/",
    "score": 28,
    "num_comments": 18,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqfyqh",
    "title": "Looking for Feedback and suggestions: Soundmentations - Library for Audio Augmentation",
    "content": "[Soundmentations](https://github.com/saumyarr8/soundmentations)\n\nI am working on this library for sound augmentation. Wanted to know the feedbacks and any features which you would want to see. Currently working on bounding box support (it will have times stamps). The APIs are veryuch similar to Albumentations. Looking forward to your comments.",
    "author": "saumyarr8",
    "timestamp": "2025-09-25T12:21:44",
    "url": "https://reddit.com/r/Python/comments/1nqfyqh/looking_for_feedback_and_suggestions/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqi7kz",
    "title": "mockylla, a library that allows you to easily mock out tests based on ScyllaDB",
    "content": "Hey! At [Genlogs](https://www.genlogs.io) we have recently released [mockylla](https://github.com/GenLogs/mockylla), a library that allows you to easily mock tests based on ScyllaDB. We use ScyllaDB in our projects, but when trying to create tests we wanted a simple solution similar to `moto` for AWS, and in our research we didn't find anything that worked for us. That‚Äôs why we created **mockylla**.\n\n## What my project does\n\n**mockylla** is a lightweight, in-memory mock for the ScyllaDB Python driver. It allows you to run integration-style tests for code that depends on ScyllaDB without requiring a live cluster.\n\nIt patches the `scylla-driver` at runtime with a single decorator, requiring no changes to your application code.\n\n## Target audience\n\nAny Python developer or company that uses ScyllaDB and needs to write tests more easily and efficiently.\n\n## Comparison\n\nWe didn‚Äôt find any existing library that covered this use case, but it is inspired by `moto`, the popular solution for mocking AWS services.\n",
    "author": "fexx3l",
    "timestamp": "2025-09-25T13:48:51",
    "url": "https://reddit.com/r/Python/comments/1nqi7kz/mockylla_a_library_that_allows_you_to_easily_mock/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqudfd",
    "title": "Which Python package manager makes automation easiest in 2025?",
    "content": "Trying to make your Python automation smooth and hassle-free? Which package manager do you actually reach for:\n\n* **pip** ‚Äì simple and classic\n* **pipenv** ‚Äì keeps it tidy\n* **poetry** ‚Äì fancy and powerful\n* **conda** ‚Äì big on data science\n* **Other** ‚Äì drop your fav in the comments!\n\nCurious to see what everyone else uses‚Äîshare your pick and why!\n\n**Note:** I know automation doesn‚Äôt strictly depend on the package manager, but I want to know which one makes it easier to manage virtual environments, lock files, and dependencies‚Äîespecially when taking a project live in production.",
    "author": "trickythinking07",
    "timestamp": "2025-09-25T23:48:45",
    "url": "https://reddit.com/r/Python/comments/1nqudfd/which_python_package_manager_makes_automation/",
    "score": 0,
    "num_comments": 35,
    "upvote_ratio": 0.44,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nplhop",
    "title": "Teaching my wife python!",
    "content": "Hey fellow redditors, I'm teaching my wife python, and I made a lesson plan to both keep me on track and keep her on track and busy. It seems to be working very well. Sharing it here in case its useful to anyone else. [Link](https://github.com/Skidkidd/Teaching_Python)",
    "author": "Skidkiddo",
    "timestamp": "2025-09-24T12:26:00",
    "url": "https://reddit.com/r/Python/comments/1nplhop/teaching_my_wife_python/",
    "score": 65,
    "num_comments": 18,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqajjt",
    "title": "Python on the Edge: Fast, sandboxed, and powered by WebAssembly",
    "content": "[https://wasmer.io/posts/python-on-the-edge-powered-by-webassembly](https://wasmer.io/posts/python-on-the-edge-powered-by-webassembly)\n\n&gt;With AI workloads on the rise, the demand for Python support on WebAssembly on the Edge has grown rapidly.\n\n&gt;However, bringing Python to WebAssembly isn't trivial as it means supporting native modules like¬†`numpy`,¬†`pandas`, and¬†`pydantic`. While projects like¬†[`pyodide`](https://pyodide.org/en/stable/)¬†made strides in running Python in the browser via WebAssembly, their trade-offs don't fully fit server-side needs.\n\n&gt;After months of hard work, today we're thrilled to announce¬†**full Python support in Wasmer Edge (Beta)**¬†powered by WebAssembly and¬†[WASIX](https://wasix.org/).\n\n&gt;Now you can run¬†**FastAPI, Streamlit, Django, LangChain, MCP servers and more**¬†directly on Wasmer and Wasmer Edge!",
    "author": "poopatroopa3",
    "timestamp": "2025-09-25T08:55:48",
    "url": "https://reddit.com/r/Python/comments/1nqajjt/python_on_the_edge_fast_sandboxed_and_powered_by/",
    "score": 1,
    "num_comments": 6,
    "upvote_ratio": 0.51,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1npercr",
    "title": "Fast API better option than Django?",
    "content": "I have worked with Django since 2017, since its version 1.X, I have more than 10 projects in production from my previous works and I could consider myself an expert in its use, both for monolithic and for using DRF.\nI started using Fast API for work in 2022 to create endpoints that required synchronization, fastapi is great for that.\n\nMy question is, considering that the learning curve of either of them is not necessary, is FastAPI really a better option than Django for a large project?\n\nMaybe it's because I come from Django, but as apps grow, especially with CRUDs, it's easier to use viewsets than to create each of the endpoints in FastAPI with their functions. \nSomething I did for a medium-sized project was to create my own modelviewsets to make CRUDs with classes in FastAPI, but I think that's reinventing the wheel or trying to bring the advantages of Django to FastAPI, I don't think it's the right approach, if I already have it there, why reinvent it?\nI don't consider myself a Django fanboy, it has its disadvantages, but I think it has grown a lot with each update, it's already on 6, it has a large community and it is mature. I think its main deficiency is not supporting async natively (it already has some functionalities but is still missing). While FastAPI, I see it more for small projects, applications that require async, such as data processing or AI in general. But for large projects (more than 30-40 endpoints), I think it is more complex to maintain in the long term.",
    "author": "stopwords7",
    "timestamp": "2025-09-24T08:11:34",
    "url": "https://reddit.com/r/Python/comments/1npercr/fast_api_better_option_than_django/",
    "score": 78,
    "num_comments": 57,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqkyzg",
    "title": "Want to use FastAPI with an AI SDK frontend? I built this",
    "content": "Are you trying to wire FastAPI to an AI SDK frontend with streaming? I built a small helper to make that easy.\n\n# What My Project Does\n\n* Connects FastAPI to the AI SDK protocol\n* Streams AI responses with SSE\n* Uses Pydantic models for typed events\n* Simple builders and decorators for a clean API\n\n# Target Audience\n\n* FastAPI devs building chat or streaming AI features\n* Teams who want an AI SDK frontend with a Python backend\n* Suitable for real apps with tests and MIT license\n\n# Comparison\n\n* Versus rolling your own SSE: less glue, fewer protocol edge cases\n* Versus WebSockets: simpler setup, matches the AI SDK stream format\n* Versus Node-focused examples: Python first, type validated, FastAPI native\n\n# Links\n\n* GitHub: [https://github.com/doganarif/fastapi-ai-sdk](https://github.com/doganarif/fastapi-ai-sdk)\n\nHappy to hear feedback.",
    "author": "doganarif",
    "timestamp": "2025-09-25T15:43:17",
    "url": "https://reddit.com/r/Python/comments/1nqkyzg/want_to_use_fastapi_with_an_ai_sdk_frontend_i/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1np9d42",
    "title": "Pyrefly &amp; Instagram - A Case Study on the Pain of Slow Code Navigation",
    "content": "Pyrefly, the new typechecker and language server for Python from Meta, is being battle-tested on Instagram's massive 20M LOC Python codebase. Some of the results have been shared in a new blog post:\n\n&gt; In real world use cases, developers who switched from Pyright (the default LSP for VSCode) to Pyrefly spent 98% less time waiting on hover results and go-to definition was ~10x faster. On the slowest files (p99), these IDE responses grew from an order of minutes to seconds (30x improvement). If those numbers are hard to visualise, the TL;DR is that this upgrade took instagram developers from questioning ‚Äúis my editor frozen?‚Äù to not giving their IDE a second thought.\n\nFull blog post: https://pyrefly.org/blog/2025/09/15/ide-extension/\n\nDisclaimer: I'm one of the maintainers for Pyrefly",
    "author": "BeamMeUpBiscotti",
    "timestamp": "2025-09-24T04:15:08",
    "url": "https://reddit.com/r/Python/comments/1np9d42/pyrefly_instagram_a_case_study_on_the_pain_of/",
    "score": 126,
    "num_comments": 27,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nqff8h",
    "title": "[Ajuda] Python ou Go? O que estudar e o que n√£o pode faltar no roadmap",
    "content": "Ol√° pessoal, tudo bem?\n\nSou do TI mas agora que estou desempregado, por isso tenho bastante tempo livre para estudar. Quero usar esse tempo para dominar uma linguagem de programa√ß√£o e me tornar um profissional completo. Estou em d√∫vida entre **Python** e **Golang**.\n\nMinha ideia √© focar em:\n\n* Desenvolvimento de APIs\n* Qualidade de testes \n* Automa√ß√£o\n\nAl√©m disso, quero consolidar meus conhecimentos em bancos de dados. J√° tenho experi√™ncia em **SQL Server** e um pouco de **MySQL**, mas n√£o conhe√ßo bem **PostgreSQL, Oracle** e outros. Tamb√©m estou estudando ingl√™s para chegar em n√≠vel profissional.\n\nMinhas d√∫vidas:\n\n1. Voc√™s acham que compensa ir direto para **Golang** ou focar em **Python** primeiro?\n2. Vale a pena incluir no meu roadmap certifica√ß√µes como **ITIL 4 Foundation, ISO 27001, COBIT 2019, Scrum Fundamentals/Scrum Master, Cloud Fundamentals, Networking basics e Cybersecurity Essentials**?\n3. Para organizar os estudos, pensei em usar algum m√©todo como **Scrum/Agile/Kanban**. Voc√™s recomendam o **ClickUp** ou outra ferramenta? Quais dicas dariam para montar essa organiza√ß√£o?\n\n‚ùì **Perguntas principais:**  \nO que eu **preciso estudar obrigatoriamente** para me tornar um bom profissional?  \nO que **n√£o pode faltar** no meu roadmap de estudos?  \nIndicam algum curso especifico ?\n\nAgrade√ßo qualquer sugest√£o!",
    "author": "Rude-Priority3611",
    "timestamp": "2025-09-25T12:01:20",
    "url": "https://reddit.com/r/Python/comments/1nqff8h/ajuda_python_ou_go_o_que_estudar_e_o_que_n√£o_pode/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1npoyza",
    "title": "Tired of manually timing functions? Meet time-my-func!",
    "content": "I built this because‚Ä¶ honestly, I was tired of writing three lines with `time.perf_counter()` just to see how long a function takes. Yes, I‚Äôm that lazy. üòÖ\n\nSo I made a tiny Python package that does it for you in **one line**: just slap @timeit() on any function, and it prints the execution time every time the function runs. It even picks the best time unit automatically ‚Äî nanoseconds, microseconds, milliseconds, seconds, or minutes ‚Äî but you can force it if you want.\n\n**What my Project does:**\n\n* **One-line timing:** Just @timeit(). Done.\n* **Automatic unit selection:** It figures out whether your function is fast enough for ¬µs or slow enough for seconds.\n* **Custom units &amp; precision:** Control decimals or force a specific unit.\n* **Works with async functions:** Because sometimes you want to time `async def` too.\n* **Exception-friendly:** Even if your function crashes, it still prints the time before propagating the error.\n\n**Usage:**\n\n    from timy_my_func import timeit, set_enabled\n    import time\n    \n    @timeit()\n    def fast_function():\n        sum(range(100))\n    \n    @timeit(decimals=5, unit=\"ms\")\n    def slow_function():\n        time.sleep(0.123)\n    \n    @timeit()\n    def disabled_function():\n      time.sleep(0.5)\n    \n    fast_function()\n    set_enabled(False)\n    disabled_function()\n    set_enabled(True)\n    slow_function()\n\n**Output:**\n\n    [fast_function] Execution time: 12.345 ¬µs\n    [slow_function] Execution time: 123.45678 ms\n\n**Target Audience:**\n\n* Python developers who want **quick, convenient \"benchmarking**\" of functions without boilerplate code.\n* Great for **personal projects, experiments, small scripts**, or learning performance optimization.\n\n**Comparison**\n\n* **Manual** `time.perf_counter()`: Flexible, but verbose ‚Äî you need multiple lines for each function, and it‚Äôs easy to forget to start/stop timers.\n* **Built-in** `timeit` **module**: Excellent for benchmarking snippets or loops, but awkward for timing full functions inline and printing results each time.\n* **Profiling tools (e.g., cProfile, line\\_profiler)**: Extremely detailed and powerful, but overkill if you just want a quick execution time. They also require setup and produce more output than most developers want for small tests.\n* **Other tiny timing utilities**: Often don‚Äôt support async functions or fail silently if an exception occurs. `timeitdecorator` handles both cleanly and prints results automatically.\n\nIt‚Äôs small, it‚Äôs silly, and it‚Äôs way easier than copying and pasting `start = time.perf_counter()`\n\n`print(...)` every time.\n\nCheck it out on GitHub: [https://github.com/DeathlyDestiny/function\\_timer](https://github.com/DeathlyDestiny/function_timer)\n\nOr just install using pip\n\n    pip install time-my-func",
    "author": "ExplanationFit4552",
    "timestamp": "2025-09-24T14:43:16",
    "url": "https://reddit.com/r/Python/comments/1npoyza/tired_of_manually_timing_functions_meet_timemyfunc/",
    "score": 8,
    "num_comments": 11,
    "upvote_ratio": 0.57,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nq0n2r",
    "title": "I tried combinning similar youtube comments.",
    "content": "I always wanted to take video (from youtube) with thousands of comments, and combine the similar ones down to just a headline or such.  \nSentences like \"This is amazing\" and \"so amazing\", I think should be condensed.  \n**What My Project Does** \\- This project aims at taking a single youtube's video's comments and group them up by comment's meaning.\n\n**Comparison:** I thought maybe someone made something like this but no, I can't find anything like it (please share with me if something like this exists).\n\nSo I made something: **Youtube Comments Aggregator.**\n\n[You can find it here](https://github.com/Whispergnome/Youtube-Comments-Aggregator/tree/main).\n\nTo work the first file, which fetchs comments, you do need a youtube API key. But I've also added a sample .csv file.\n\n**Target Audience**¬†is anyone who read youtube comments.  \nWhat do you think? And can this be improved?",
    "author": "IR-Indigo",
    "timestamp": "2025-09-25T00:45:55",
    "url": "https://reddit.com/r/Python/comments/1nq0n2r/i_tried_combinning_similar_youtube_comments/",
    "score": 1,
    "num_comments": 0,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nps3nn",
    "title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",
    "content": "# Weekly Thread: Professional Use, Jobs, and Education üè¢\n\nWelcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.\n\n---\n\n## How it Works:\n\n1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.\n2. **Education Q&amp;A**: Ask or answer questions about Python courses, certifications, and educational resources.\n3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.\n\n---\n\n## Guidelines:\n\n- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.\n- Keep discussions relevant to Python in the professional and educational context.\n  \n---\n\n## Example Topics:\n\n1. **Career Paths**: What kinds of roles are out there for Python developers?\n2. **Certifications**: Are Python certifications worth it?\n3. **Course Recommendations**: Any good advanced Python courses to recommend?\n4. **Workplace Tools**: What Python libraries are indispensable in your professional work?\n5. **Interview Tips**: What types of Python questions are commonly asked in interviews?\n\n---\n\nLet's help each other grow in our careers and education. Happy discussing! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-24T17:00:30",
    "url": "https://reddit.com/r/Python/comments/1nps3nn/thursday_daily_thread_python_careers_courses_and/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1npb0ks",
    "title": "ConfOpt: Hyperparameter Tuning That Works",
    "content": "**What My Project Does:**\n\nI built a new **hyperparameter tuning** package that picks the best hyperparameters for your ML model!\n\n**Target Audience:**\n\nAny Data Scientist who wants to squeeze extra performance out of their hyperparameter tuning.\n\n**How does it work?**\n\nLike Optuna and existing methods, it uses Bayesian Optimization to identify the most promising hyperparameter configurations to try next.\n\nUnlike existing methods though, it makes no distributional assumptions and uses quantile regression to guide next parameter selection.\n\n**Comparison:**\n\nIn benchmarking, ConfOpt strongly outperforms Optuna's default sampler (TPE) across the board. If you switch to Optuna's GP sampler, ConfOpt still outperforms, but it's close if you only have numerical hyperparameters. It's still a big outperformance with categorical hyperparameters.\n\nI should also mention this all applies to single fidelity tuning. If you're a pro and you're tuning some massive LLM on multi-fidelity, I don't have benchmarks for you yet.\n\n**Want to learn more?**\n\nFor the serious stuff, you can find the preprint of my paper here: [https://www.arxiv.org/abs/2509.17051](https://www.arxiv.org/abs/2509.17051)\n\nIf you have any questions or feedback, please let me know in the comments!\n\n**Want to give it a try?** Check out the links below.\n\n* **Github Repository** (consider giving it a star!)**:** [https://github.com/rick12000/confopt](https://github.com/rick12000/confopt)\n* **Documentation:** [https://confopt.readthedocs.io/](https://confopt.readthedocs.io/)\n* **PyPI:** [https://pypi.org/project/confopt/](https://pypi.org/project/confopt/)\n\nInstall it with: `pip install confopt`",
    "author": "RickCodes1200",
    "timestamp": "2025-09-24T05:38:40",
    "url": "https://reddit.com/r/Python/comments/1npb0ks/confopt_hyperparameter_tuning_that_works/",
    "score": 19,
    "num_comments": 8,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1np8iyh",
    "title": "Made a FastAPI Project Starter",
    "content": "## What My Project Does\nI got tired of setting up FastAPI projects from scratch‚Äîdatabases, auth, background tasks, migrations, Docker‚Ä¶ so I built a FastAPI project starter. It scaffolds a production-ready project in seconds, including PostgreSQL (async/sync), Celery+Redis, Loguru logging, Docker, middlewares (RequestID, Timer, CORS), Traefik, and MailPit. Post-deployment hooks start services automatically.\n\n## Target Audience\nBackend developers who want to quickly spin up production-ready FastAPI projects, small teams, or solo devs who need a consistent setup across projects.\n\n## Comparison\nCompared to starting from scratch or using minimal templates, this starter comes pre-configured with essentials like database, background tasks, logging, Docker, monitoring, and middlewares. Unlike other starters, it has post-deployment hooks and multiple middlewares out of the box, saving setup time and reducing errors.\n\n## Links (for reference)\n- GitHub: https://github.com/deveshshrestha20/FastAPI_Project_Starter\n- PyPI: https://pypi.org/project/fastapi-project-starter/\n",
    "author": "Detox-Boy",
    "timestamp": "2025-09-24T03:26:35",
    "url": "https://reddit.com/r/Python/comments/1np8iyh/made_a_fastapi_project_starter/",
    "score": 22,
    "num_comments": 6,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1np7qc8",
    "title": "[Project] df2tables - Export pandas DataFrames as  interactive HTML tables",
    "content": "Hey everyone,\n\nI built a small Python utility called **df2tables**\n\n**What my project does**  \n`df2tables` converts `pandas`  and `polars dataframes` into standalone interactive HTML tables using the DataTables JS library. It produces a single, lightweight HTML file you can open in any browser - **no Jupyter, no server.**\n\nIt renders directly from a compact JavaScript array, keeping file sizes small while still handling large datasets responsively. It also includes the latest ColumnControl component from DataTables, giving you flexible column visibility management out of the box.\n\n**Customization** \\- you can  configure DataTables options **directly from Python**\n\n**Target audience**  \nIt‚Äôs designed to embed seamlessly into popular web frameworks like Flask, Django, or FastAPI -  making it perfect for dashboards, admin panels, or lightweight data apps.\n\nThis can be useful for people who work with dataframes but don‚Äôt use Jupyter, or who want to share DataFrames as portable, interactive tables without extra setup.\n\nFor quick visual data exploration, it's easier to just enter text into the datatables search box, which searches in all text columns, than to build a filter in pandas (ColumnControl is even more convenient)\n\n**Comparison**  \nProjects like **itables** offer powerful Jupyter integration, but need Ipython and they rely on a notebook environment. `df2tables` is deliberately much smaller and simpler -  and the output is a fully standalone HTML file.\n\nRequires only `pandas` ***or*** `polars -` you don‚Äôt need both.\n\nBecause the output is plain HTML+JS, it‚Äôs trivial to embed these tables into any web framework (Flask, Django, FastAPI etc.), which makes it flexible. It stays lightweight while still supporting professional-grade features like filtering, sorting.\n\nRepo: [https://github.com/ts-kontakt/df2tables](https://github.com/ts-kontakt/df2tables)",
    "author": "No_Pineapple449",
    "timestamp": "2025-09-24T02:36:26",
    "url": "https://reddit.com/r/Python/comments/1np7qc8/project_df2tables_export_pandas_dataframes_as/",
    "score": 17,
    "num_comments": 0,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nq0xnm",
    "title": "Please give your input ü§î",
    "content": "Hello everyone\nI'm currently a QA with Java selenium knowledge. Something's telling me to learn playwright python and move. \n\nWould be great to have your valuable suggestions",
    "author": "MousseBudget6974",
    "timestamp": "2025-09-25T01:05:31",
    "url": "https://reddit.com/r/Python/comments/1nq0xnm/please_give_your_input/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.17,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1npztrj",
    "title": "Typing of functions returns : type hints vs pyright (or similar) inference",
    "content": "I used to think \"pyright already inferes the return type from what the function does, so no need to repeat it in the type hint.\n\nBut recently I realized that writing a return type hint can help to constrain a specification to automatically check if what the functions does follow it.\n\nWhat do you think ?\n\nIt seems the same would apply to Typescript or using \\`auto\\` as return type in C++.",
    "author": "Neither_Garage_758",
    "timestamp": "2025-09-24T23:52:35",
    "url": "https://reddit.com/r/Python/comments/1npztrj/typing_of_functions_returns_type_hints_vs_pyright/",
    "score": 0,
    "num_comments": 16,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nprv1g",
    "title": "ANACONDA ON OLD MAC",
    "content": "Hi everybody, I have a pretty old mac (2015) 2,2 GHz Intel Core i7. I have been trying to get Anaconda Jupiter but can't seem to download it. I need it for my python class and the prof keeps asking me to download it on the regular website just like any windows user would do. Please lmk if you have a shortcut for old macs. Thank you!!",
    "author": "Worldly-Guitar-785",
    "timestamp": "2025-09-24T16:49:15",
    "url": "https://reddit.com/r/Python/comments/1nprv1g/anaconda_on_old_mac/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nomupo",
    "title": "Trouble with deploying Python programs as internal tools?",
    "content": "Hi all I have been trying to figure out better ways to manage internal tooling. Wondering what are everyones biggest blockers / pain-points when attempting to take a python program, whether it be a simple script, web app, or notebook, and converting it into a usable internal tool at your company? \n\nCould be sharing it, deploying to cloud, building frontend UI, refactoring code to work better with non-technical users, etc.",
    "author": "Competitive-Water302",
    "timestamp": "2025-09-23T09:57:05",
    "url": "https://reddit.com/r/Python/comments/1nomupo/trouble_with_deploying_python_programs_as/",
    "score": 72,
    "num_comments": 88,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1npq2nz",
    "title": "Durable Vibe Automation Platform for Python Developers",
    "content": "**What My Project Does**\n\n[AutoKitteh](http://www.autokitteh.com) is an¬†[open-source¬†](https://github.com/autokitteh/autokitteh)platform (self-hosted or¬†[SaaS](https://app.autokitteh.cloud/)) that lets you build durable¬†**automations**¬†and¬†**AI agents**¬†from plain English (we call it **VibeAutomation**) \n\nWhat you can build?¬†anything from personal to enterprise-grade automations and AI Agents for productivity, DevOps, Ops, ChatOps, human-in-the-loop workflows etc.\n\nInterfaces**:**¬†Web UI, VS-Code / Cursore extension\n\n**Key features**: Vibe automation, Serverless, Connectors to applications¬†(Gmail, Slack, Twilio and many more. Easy to add new applications), Durable workflows¬†- support reliable long-running workflows, Pre-build templates, Workflow visualization.\n\nLinks: Serverless cloud [platform](http://app.autokitteh.cloud), GitHub¬†[Repo](https://github.com/autokitteh/autokitteh), Samples¬†[Repo](https://github.com/autokitteh/kittehub), [Discord](https://discord.gg/UhnJuBarZQ)¬†.\n\n**Target Audience**\n\nAnyone with basic Python skills that wants to connect applications and APIs to build automations with or without AI.  \nNote that the platform is for connecting APIs and not an application builder like Lovable / Bolt / Base44, however it can be the backend automation for such platforms. \n\n**Comparison**¬†\n\nAutomation tools like:¬†**n8n / Zapier / Make**. Unlike those tools the platform is designed for reliability, long-running workflows, with the flexibility of Python.  \n**String** is another platform that goes by the same approach of Vibe automation.\n\n",
    "author": "HaimZlatokrilov",
    "timestamp": "2025-09-24T15:28:52",
    "url": "https://reddit.com/r/Python/comments/1npq2nz/durable_vibe_automation_platform_for_python/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.41,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nocyn3",
    "title": "StringWa.rs: Which Libs Make Python Strings 2-10√ó Faster?",
    "content": "## What My Project Does\n\nI've put together **[StringWa.rs](http://github.com/ashvardanian/StringWa.rs)** ‚Äî a benchmark suite for text and sequence processing in Python. It compares `str` and `bytes` built-ins, popular third-party libraries, and GPU/SIMD-accelerated backends on common tasks like splitting, sorting, hashing, and edit distances between pairs of strings.\n\n## Target Audience\n\nThis is for Python developers working with text processing at any scale ‚Äî whether you're parsing config files, building NLP pipelines, or handling large-scale bioinformatics data. If you've ever wondered why your string operations are bottlenecking your application, or if you're still using packages like NLTK for basic string algorithms, this benchmark suite will show you exactly what performance you're leaving on the table.\n\n## Comparison\n\nMany developers still rely on outdated packages like `nltk` (with 38 M monthly downloads) for Levenshtein distances, not realizing the same computation can be **500√ó faster on a single CPU core** or up to **160,000√ó faster on a high-end GPU**. The benchmarks reveal massive performance differences across the ecosystem, from built-in Python methods to modern alternatives like my own **[StringZilla](https://github.com/ashvardanian/StringZilla/)** library (just released v4 under Apache 2.0 license after months of work).\n\nSome surprising findings for native `str` and `bytes`:\n* `str.find` is about **10√ó** slower than it can be\n* On 4 KB blocks, using `re.finditer` to match byte-sets is **46√ó** slower\n* On same inputs, `hash(str)` is **2√ó** slower and has lower quality\n* `bytes.translate` for binary transcoding is **4√ó** slower\n\nSimilar gaps exist in third-party libraries, like `jellyfish`, `google_crc32c`, `mmh3`, `pandas`, `pyarrow`, `polars`, and even Nvidia's own GPU-accelerated `cudf`, that (depending on the input) can be 100√ó slower than `stringzillas-cuda` on the same H100 GPU.\n\n---\n\nI recently wrote 2 articles about the new algorithms that went into the v4 release, that received some positive feedback on \"r/programming\" ([one](https://www.reddit.com/r/programming/comments/1nm3ath/processing_strings_109x_faster_than_nvidia_on_h100), [two](https://www.reddit.com/r/programming/comments/1nmxdvf/how_a_string_library_beat_opencv_at_image)), so I thought it might be worth sharing the underlying project on \"r/python\" as well ü§ó\n\nThis is in no way a final result, and there is a ton of work ahead, but let me know if I've overlooked important directions or libraries that should be included in the benchmarks!\n\nThanks, Ash!",
    "author": "ashvar",
    "timestamp": "2025-09-23T02:41:23",
    "url": "https://reddit.com/r/Python/comments/1nocyn3/stringwars_which_libs_make_python_strings_210/",
    "score": 108,
    "num_comments": 9,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1np8y4w",
    "title": "Need Suggestions",
    "content": "So I'm working as an Automation Engineer in a fintech based company and have total of around 4 years of experience in QA &amp; Automation Engineer\n\nNow I'm stuck at a point in life where in I have a decision to make to plan my future ahead basically either get myself grinding and switch to Dev domain or grind myself and look for SDET kind of roles\n\nI have always been fond of Dev domain but due to family situations I really couldn't try switching from QA to Dev during this period and now I'm pretty sure I'm underpaid to an extent basically I'm earning somewhere between 8-10 lpa even after having 4 years of experience and trust me I'm good at what I do ( it's not me but that's what teammates say)\n\nPlease guide me as to what option do you think is feasible for me as consider me I'm the only breadwinner of my family and I genuinely need this community's help to get my mind clear\n\nThank you so much in advance",
    "author": "AdministrationFit910",
    "timestamp": "2025-09-24T03:52:02",
    "url": "https://reddit.com/r/Python/comments/1np8y4w/need_suggestions/",
    "score": 1,
    "num_comments": 12,
    "upvote_ratio": 0.54,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1np6xet",
    "title": "Multi-Signal Trading Strategy with RSI and Moving Averages",
    "content": "Created a Python script that combines RSI and moving average indicators to generate trading signals with interactive visualizations.\n\nTech stack:\n\n* pandas-ta for technical indicators\n* yfinance for data\n* plotly for interactive charts with subplots\n* Custom signal logic with confirmation rules\n\nThe visualization shows price action, moving averages, RSI, and buy/sell signals all in one interactive chart.\n\nCode walkthrough and explanation given [here](https://blog.adnansiddiqi.me/building-your-first-multi-signal-trading-strategy-with-rsi-and-moving-averages/).\n\n",
    "author": "pknerd",
    "timestamp": "2025-09-24T01:42:06",
    "url": "https://reddit.com/r/Python/comments/1np6xet/multisignal_trading_strategy_with_rsi_and_moving/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1npheag",
    "title": "Python Data Model Exercise",
    "content": "An exercise about the Python Data Model. What is the output of this program?\n\n    a = [1]\n    b = a\n    b += [2]\n    b.append(3)\n    b = b + [4]\n    b.append(5)\n    \n    print(a)\n    # --- possible answers ---\n    # A) [1]\n    # B) [1, 2]\n    # C) [1, 2, 3]\n    # D) [1, 2, 3, 4]\n    # E) [1, 2, 3, 4, 5]\n\n* [Solution](https://memory-graph.com/#codeurl=https://raw.githubusercontent.com/bterwijn/memory_graph_videos/refs/heads/main/exercises/exercise9.py&amp;play)\n* [Explanation](https://github.com/bterwijn/memory_graph?tab=readme-ov-file#python-data-model)\n* [More Exercises](https://www.reddit.com/r/Python_memory_graph/)",
    "author": "Sea-Ad7805",
    "timestamp": "2025-09-24T09:50:42",
    "url": "https://reddit.com/r/Python/comments/1npheag/python_data_model_exercise/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.3,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nohze7",
    "title": "Real-Time BLE Air Quality data into Adafruit IO using python",
    "content": "This project shows how to turn a BleuIO USB dongle into a tiny gateway that streams live¬†**air-quality data**¬†from a HibouAir sensor straight to¬†**Adafruit IO**. The python script listens for Bluetooth Low Energy (BLE) advertising packets, decodes CO2, temperature, and humidity, and posts fresh readings to your Adafruit IO feeds every few seconds. The result is a clean, shareable dashboard that updates in real time‚Äîperfect for demos, labs, offices, classrooms, and proofs of concept.  \nDetails of this tutorial and source code available at   \n[https://www.bleuio.com/blog/real-time-ble-air-quality-monitoring-with-bleuio-and-adafruit-io/](https://www.bleuio.com/blog/real-time-ble-air-quality-monitoring-with-bleuio-and-adafruit-io/)",
    "author": "bleuio",
    "timestamp": "2025-09-23T06:52:01",
    "url": "https://reddit.com/r/Python/comments/1nohze7/realtime_ble_air_quality_data_into_adafruit_io/",
    "score": 5,
    "num_comments": 1,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1noj6sr",
    "title": "Skylos dead code detector",
    "content": "Hola! I'm back! Yeap I've promoted this a couple of times, some of you lurkers might already know this. So anyway I'm back with quite a lot of new updates. \n\nSkylos is yet another static analysis tool for Python codebases written in Python that detects dead code, secrets and dangerous code. Why skylos? \n\nSome features include:\n\n* **CST-safe removals:**¬†Uses LibCST to remove selected imports or functions\n* **Framework-Aware Detection**: Attempt at handling Flask, Django, FastAPI routes and decorators .. Still wip\n* **Test File Exclusion**: Auto excludes test files (you can include it back if you want)\n* **Interactive Cleanup**: Select specific items to remove from CLI\n* **Dangerous Code detection**\n* **Secrets detection**\n* **CI/CD integration**\n\nYou can read more in the repo's README \n\nI have also recently released a new VSC extension that will give you feedback everytime you save the file. (search for skylos under the vsc marketplace). Will be releasing for other IDEs down the road.\n\n**Future plans in the next update**\n\n* Expanding to more IDEs \n* Increasing the capability of the extension\n* Increasing the capabilities of searching for dead code as well as dangerous code\n\n**Target audience:**\n\nPython developers\n\nAny collaborators/contributors will be welcome. If you found the repo useful please give it a star. If you like some features you can ping me here or drop a message inside the discussion tab in the skylos repo. Thanks for reading folks and have a wonderful rest of the week ahead. \n\n  \nLink to the repo: [https://github.com/duriantaco/skylos](https://github.com/duriantaco/skylos)",
    "author": "papersashimi",
    "timestamp": "2025-09-23T07:39:48",
    "url": "https://reddit.com/r/Python/comments/1noj6sr/skylos_dead_code_detector/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1no2n2e",
    "title": "Tuesday Daily Thread: Advanced questions",
    "content": "# Weekly Wednesday Thread: Advanced Questions üêç\n\nDive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.\n\n## How it Works:\n\n1. **Ask Away**: Post your advanced Python questions here.\n2. **Expert Insights**: Get answers from experienced developers.\n3. **Resource Pool**: Share or discover tutorials, articles, and tips.\n\n## Guidelines:\n\n* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.\n* Questions that are not advanced may be removed and redirected to the appropriate thread.\n\n## Recommended Resources:\n\n* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.\n\n## Example Questions:\n\n1. **How can you implement a custom memory allocator in Python?**\n2. **What are the best practices for optimizing Cython code for heavy numerical computations?**\n3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**\n4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**\n5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**\n6. **What are some advanced use-cases for Python's decorators?**\n7. **How can you achieve real-time data streaming in Python with WebSockets?**\n8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**\n9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**\n10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**\n\nLet's deepen our Python knowledge together. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-22T17:00:33",
    "url": "https://reddit.com/r/Python/comments/1no2n2e/tuesday_daily_thread_advanced_questions/",
    "score": 22,
    "num_comments": 2,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1np0ijo",
    "title": "Plot Twist: After Years of Compiling Python, I‚Äôm Now Using AI to Speed It Up",
    "content": "# My Journey with Python Performance Optimization: From Nuitka to AI-Powered Solutions\n\nHi everyone,\n\nThis post: [AI Python Compiler: Transpile Python to Golang with LLMs for 10x perf gain](https://discuss.python.org/t/ai-python-compiler-transpile-python-to-golang-with-llms-for-10x-perf-gain-pypi-like-service-to-host-transpiled-packages/103759) motivated me to share my own journey with Python performance optimization.\n\nAs someone who has been passionate about Python performance in various ways, it's fascinating to see the diverse approaches people take towards it. There's Cython, the Faster CPython project, mypyc, and closer to my heart, Nuitka.\n\nI started my OSS journey by contributing to Nuitka, mainly on the packaging side (support for third-party modules, their data files, and quirks), and eventually became a maintainer.\n\n## A bit about Nuitka and its approach\n\nFor those unfamiliar, Nuitka is a Python compiler that translates Python code to C++ and then compiles it to machine code. Unlike transpilers that target other high-level languages, Nuitka aims for 100% Python compatibility while delivering significant performance improvements.\n\nWhat makes Nuitka unique is its approach:\n\n- It performs whole-program optimization by analyzing your entire codebase and its dependencies\n- The generated C++ code mimics CPython's behavior closely, ensuring compatibility with even the trickiest Python features (metaclasses, dynamic imports, exec statements, etc.)\n- It can create standalone executables that bundle Python and all dependencies, making deployment much simpler\n- The optimization happens at multiple levels: from Python AST transformations to C++ compiler optimizations\n\nOne of the challenges I worked on was ensuring that complex packages with C extensions, data files, and dynamic loading mechanisms would work seamlessly when compiled. This meant diving deep into how packages like NumPy, SciPy, and various ML frameworks handle their binary dependencies and making sure Nuitka could properly detect and include them.\n\n## The AI angle\n\nNow, in my current role at [Codeflash](https://codeflash.ai), I'm tackling the performance problem from a completely different angle: using AI to rewrite Python code to be more performant.\n\nRather than compiling or transpiling, we're exploring how LLMs can identify performance bottlenecks and automatically rewrite code for better performance while keeping it in Python.\n\nThis goes beyond just algorithmic improvements - we're looking at:\n\n- Vectorization opportunities\n- Better use of NumPy/pandas operations\n- Eliminating redundant computations\n- Suggesting more performant libraries (like replacing `json` with `ujson` or `orjson`)\n- Leveraging built-in functions over custom implementations\n\nMy current focus is specifically on optimizing async code:\n- Identifying unnecessary awaits\n- Opportunities for concurrent execution with `asyncio.gather()`\n- Replacing synchronous libraries with their async counterparts\n- Fixing common async anti-patterns\n\nThe AI can spot patterns that humans might miss, like unnecessary list comprehensions that could be generator expressions, or loops that could be replaced with vectorized operations.\n\n## Thoughts on the evolution\n\nIt's interesting how the landscape has evolved from pure compilation approaches to AI-assisted optimization. Each approach has its trade-offs, and I'm curious to hear what others in the community think about these different paths to Python performance.\n\nWhat's your experience with Python performance optimization?\n\nAny thoughts?\n\nedit: thanks u/EmberQuill for making me aware of the markdown issue; this isn't LLM generated; I copied the content directly from [my DPO](https://discuss.python.org/t/plot-twist-after-years-of-compiling-python-im-now-using-ai-to-speed-it-up/103916) thread and it brought on the formatting, which I hadn't noticed",
    "author": "DivineSentry",
    "timestamp": "2025-09-23T19:22:31",
    "url": "https://reddit.com/r/Python/comments/1np0ijo/plot_twist_after_years_of_compiling_python_im_now/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nnqa6s",
    "title": "An app I built with Reflex...",
    "content": "I read alot of medical journals (just a hobby of mine) and naturally I always start with the abstract, and if the study sounds good I'll try to see if its available in full text.\n\n\n\\### What My Project Does\n\n\nI got the idea of maybe combining some lightweight LLM model with PubMed and well this is what I got!\n\n\nThis app (I don't have a name for it yet) lets. you create folders/collections, and add pubmed abstracts (with URL to the actual article) and includes a built in collection viewer where you can easily summarize selected articles or talk to the LLM that has some degree of awareness on what you're reading\n\n\nIt's pretty cool that the entire thing was built using only Python. The back end and the LLM itself (gemini flash model) was easily created using just python; also the front end completely in Python as well\n\n\n\\### Target Audience\n\nAll python devs I guess or anyone interested in creating full stack apps in a single stack language. I probably would not have built it if I had to go and pick up some JS + HTML just to create the front end!\n\n\n\\### Comparison\n\nHmm not sure if I've seen any apps like it but im sure there's plenty, I just havent searched for them.\n\n\nSource Video: [https://youtu.be/eXaa40MiIGs](https://youtu.be/eXaa40MiIGs)\n\nFramework Used to build: [https://github.com/reflex-dev/reflex](https://github.com/reflex-dev/reflex)",
    "author": "Wonderful-Today-497",
    "timestamp": "2025-09-22T08:54:01",
    "url": "https://reddit.com/r/Python/comments/1nnqa6s/an_app_i_built_with_reflex/",
    "score": 20,
    "num_comments": 5,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nnh6g0",
    "title": "We just launched Leapcell, deploy 20 Python websites for free",
    "content": "hi r/Python\n\nBack then, I often had to pull the plug on side projects built with Python, the hosting bills and upkeep just weren‚Äôt worth it. They ended up gathering dust on GitHub.\n\nThat‚Äôs why we created **Leapcell**: a platform designed so your Python ideas can stay alive without getting killed by costs in the early stage.\n\n**Deploy up to 20 Python websites or services for free (included in our free tier)**  \nMost PaaS platforms give you a single free VM (like the old Heroku model), but those machines often sit idle. Leapcell takes a different approach: with a serverless container architecture, we fully utilize compute resources and let you host multiple services simultaneously. While other platforms only let you run one free project, Leapcell lets you run up to **20 Python apps** for free.\n\nAnd it‚Äôs not just websites, your Python stack can include:\n\n* Web APIS: Django, Flask, FastAPI\n* Data &amp; automation: Playwright-based crawlers\n* APIs &amp; microservices: lightweight REST or GraphQL services\n\nWe were inspired by platforms like Vercel (multi-project hosting), but Leapcell goes further:\n\n* **Multi-language support:** Django, Node.js, Go, Rust.\n* **Two compute modes**\n   * ***Serverless***: cold start &lt; 250ms, autoscaling with traffic (perfect for early-stage Django apps).\n   * ***Dedicated machines***: predictable costs, no risk of runaway serverless bills, better unit pricing.\n* **Built-in stack:** PostgreSQL, Redis, async tasks, logging, and even web analytics out of the box.\n\nSo whether you‚Äôre running a Django blog, a Flask API, or a Playwright-powered scraper, you can start for free and only pay when you truly grow.\n\nIf you could host 20 Python projects for free today, what would you build first?",
    "author": "OfficeAccomplished45",
    "timestamp": "2025-09-22T01:45:51",
    "url": "https://reddit.com/r/Python/comments/1nnh6g0/we_just_launched_leapcell_deploy_20_python/",
    "score": 68,
    "num_comments": 46,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nnwn2h",
    "title": "D&amp;D Twitch bot: Update 2!",
    "content": "Hello! So I posted awhile back that I was making a cool twitch bot for my chatters themed on D&amp;D and wanted to post another update here! (OG post) [https://www.reddit.com/r/Python/comments/1mt2srw/dd\\_twitch\\_bot/](https://www.reddit.com/r/Python/comments/1mt2srw/dd_twitch_bot/)\n\nMy most current updates have made some major strides!\n\n1.) Quests now auto generate quest to quest, evolving over time at checkpoints and be much more in depth overall. Giving chatters a better story, while also  allowing them multiple roll options with skill rolls tied into each class. (Things like barbarians are bad at thinking, but great at smashing! So they might not be the best at a stealth mission in a China shop...)   \n  \n2.) The bot now recognizes new chatters and greets them with fanfare and a little \"how to\" so they are not so confused when they first arrive. And the alert helps so I know they are a first time chatter!   \n  \n3.) I got all the skill rolls working, and now they are showing and updated in real time on the display. That way chatters can see at all times which skills are the best for this adventure they are on!   \n  \n4.) Bosses now display across the ENTIRE screen for the bot, being a big ol pain until they are defeated!  \n  \n5.) The druid weather effects now work, and have sounds on them (Some are very fun lol) and no longer spam repeats over and over.  \n  \n6.) Small bugs got fixed and many more popped up, so expect more updates soon(ish) \n\nYou can check it out when I'm live sometime [https://www.twitch.tv/thatturtlegm](https://www.twitch.tv/thatturtlegm)",
    "author": "ThatTurtleGM",
    "timestamp": "2025-09-22T12:51:00",
    "url": "https://reddit.com/r/Python/comments/1nnwn2h/dd_twitch_bot_update_2/",
    "score": 7,
    "num_comments": 3,
    "upvote_ratio": 0.77,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nnn35x",
    "title": "Append-only time-series storage in pure Python: Chronostore (faster than CSV &amp; Parquet)",
    "content": "# What My Project Does\n\n*Chronostore* is a fast, append-only binary time-series storage engine for Python. It uses schema-defined daily files with memory-mapped zero-copy reads compatible with Pandas and NumPy. (supported backends: flat files or LMDB)\n\nIn benchmarks (10M rows of 4 float64 columns), *Chronostore* wrote in `~0.43 s` and read in `~0.24 s,` vastly outperforming CSV (`58 s` write, `7.8 s` read) and Parquet (`~2 s` write, `~0.44 s` read).\n\nKey features:\n\n* Schema-enforced binary storage\n* Zero-copy reads via mmap / LMDB\n* Daily file partitioning, append-only\n* Pure Python, easy to install and integrate\n* Pandas/NumPy compatible\n\nLimitations:\n\n* No concurrent write support\n* Lacks indexing or compression\n* Best performance on SSD/NVMe hardware\n\n# Links\n\n* üëâ [The GitHub repo](https://github.com/rundef/chronostore)\n\nif you find it useful, a ‚≠ê would be amazing!\n\n# Why I Built It\n\nI needed a simple, minimal and high-performance local time-series store that integrates cleanly with Python data tools. Many existing solutions require servers, setup, or are too heavy. *Chronostore* is lightweight, fast, and gives you direct control over your data layout\n\n# Target audience\n\n* Python developers working with **IoT, sensor, telemetry, or financial tick data**\n* Anyone needing **schema-controlled, high-speed local time-series persistence**\n* Developers who want **fast alternatives to CSV or Parquet for time-series data**\n* Hobbyists and students exploring **memory-mapped I/O and append-only data design**\n\n‚≠ê If you find this project useful, consider giving it a star on GitHub, it really helps visibility and motivates further development: [https://github.com/rundef/chronostore](https://github.com/rundef/chronostore)",
    "author": "rundef",
    "timestamp": "2025-09-22T06:52:57",
    "url": "https://reddit.com/r/Python/comments/1nnn35x/appendonly_timeseries_storage_in_pure_python/",
    "score": 23,
    "num_comments": 11,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nnokzl",
    "title": "S3Ranger - A TUI for S3 and S3-like cloud storage built using Textual",
    "content": "### What My Project Does  \nI built **s3ranger**, a TUI to interact with S3 and S3-like cloud storage services. It‚Äôs built with [Textual](https://github.com/Textualize/textual) and uses **boto3** + **awscli** under the hood.  \nWhile the **AWS CLI** already supports most of these operations, I wanted an actual interface on top of it that feels quick and easy to use.  \n\nSome things it can do that the standard S3 console doesn‚Äôt give you:  \n- Download a \"folder\" from S3  \n- Rename a \"folder\"  \n- Upload a \"folder\"  \n- Delete a \"folder\"  \n\n### Target Audience  \nThis project is mainly for developers who:  \n- Use **localstack** or other S3-compatible services and want a simple UI on top  \n- Need to do batch/folder operations that the AWS S3 web UI doesn‚Äôt provide  \n- Like terminal-first tools (since this is a TUI, not a web app)  \n\nIt‚Äôs not meant to replace the CLI or the official console, but rather to make repetitive/local workflows faster and more visual.\n\n--\nYou can run it against localstack like this:  \n```\ns3ranger --endpoint-url http://localhost:4566 --region-name us-east-1\n```\n### GitHub Link  \nRepo: [https://github.com/Sharashchandra/s3ranger](https://github.com/Sharashchandra/s3ranger)\n\nAny feedback is appreciated!",
    "author": "swifty_sanchez",
    "timestamp": "2025-09-22T07:50:44",
    "url": "https://reddit.com/r/Python/comments/1nnokzl/s3ranger_a_tui_for_s3_and_s3like_cloud_storage/",
    "score": 18,
    "num_comments": 2,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nodyna",
    "title": "Python Recursion Made Simple",
    "content": "Some struggle with recursion, but as package invocation\\_tree visualizes the Python call tree in real-time, it gets easy to understand what is going on and to debug any remaining issues.\n\nSee this one-click [Quick Sort demo](https://www.invocation-tree.com/#codeurl=https://raw.githubusercontent.com/bterwijn/invocation_tree/refs/heads/main/src/quick_sort.py&amp;timestep=0.5&amp;play) in the Invocation Tree Web Debugger.",
    "author": "Sea-Ad7805",
    "timestamp": "2025-09-23T03:41:35",
    "url": "https://reddit.com/r/Python/comments/1nodyna/python_recursion_made_simple/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.4,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nnimms",
    "title": "python-cq ‚Äî Lightweight CQRS package for async Python projects",
    "content": "# What My Project Does\n\n[`python-cq`](https://github.com/100nm/python-cq) is a package that helps apply CQRS principles (Command Query Responsibility Segregation) in async Python projects.\n\nThe core idea of CQRS is to separate:\n\n* **Commands** ‚Üí actions that change the state of the system.\n* **Queries** ‚Üí operations that only read data, without side effects.\n* **Events** ‚Üí facts that describe something that happened, usually triggered by commands.\n\nWith `python-cq`, handlers for commands, queries, and events are just regular Python classes decorated with `@command_handler`, `@query_handler`, or `@event_handler`.\nThe framework automatically detects which message type is being handled based on type hints, no need to inherit from base classes or write boilerplate.\n\nIt also integrates with dependency injection through [`python-injection`](https://github.com/100nm/python-injection), which makes it easier to manage dependencies between handlers.\n\nExample:\n\n```python\nfrom dataclasses import dataclass\nfrom injection import inject\nfrom cq import CommandBus, RelatedEvents, command_handler, event_handler\n\n@dataclass\nclass UserRegistrationCommand:\n    email: str\n    password: str\n\n@dataclass\nclass UserRegistered:\n    user_id: int\n    email: str\n\n@command_handler\nclass UserRegistrationHandler:\n    def __init__(self, events: RelatedEvents):\n        self.events = events\n\n    async def handle(self, command: UserRegistrationCommand):\n        \"\"\" register the user \"\"\"\n        user_id = ...\n        event = UserRegistered(user_id, command.email)\n        self.events.add(event)\n\n@event_handler\nclass SendConfirmationEmailHandler:\n    async def handle(self, event: UserRegistered):\n        \"\"\" send confirmation email \"\"\"\n\n@inject\nasync def main(bus: CommandBus[None]):\n    command = UserRegistrationCommand(email=\"root@gmail.com\", password=\"root\")\n    await bus.dispatch(command)\n```\n\n# Target Audience\n\nThis library is intended for developers who want to experiment with CQRS principles in async Python projects. I think the project could be production-ready, but I need more feedback to be certain.\n\nIf you‚Äôre interested in clean architecture, domain-driven design, or simply curious about alternative ways to structure Python code, this might be useful.\n\n# Comparison\n\nMost existing CQRS frameworks are designed for distributed systems or microservices, often bringing a lot of complexity with them.\n`python-cq` tries to stay different by being:\n\n* **Minimal**: just decorators, type annotations, and async.\n* **Local-first**: it works well for a single application.\n* **Integrated with DI**: works out of the box with `python-injection`.\n\nIt‚Äôs trying to provide a simple, Pythonic way to use CQRS ideas in async projects.\n\nSource code: https://github.com/100nm/python-cq",
    "author": "Skearways",
    "timestamp": "2025-09-22T03:17:14",
    "url": "https://reddit.com/r/Python/comments/1nnimms/pythoncq_lightweight_cqrs_package_for_async/",
    "score": 27,
    "num_comments": 2,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nodcg2",
    "title": "I nee a fix which i cant able to solve till today",
    "content": "The problem is that i used XAMPP for my life for making php projects but when its time for using sql in python even installing and updating all the sql packages in pip, still the python program cannot run the code of sql or even if then it crashed the sql server even installing sql breaks the whole sql system in xampp or python what should i do?",
    "author": "Ok_Tap_1597",
    "timestamp": "2025-09-23T03:05:10",
    "url": "https://reddit.com/r/Python/comments/1nodcg2/i_nee_a_fix_which_i_cant_able_to_solve_till_today/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1noebrx",
    "title": "Why is Spyder so slow",
    "content": "I recently installed Spyder, I am so disappointed in it's speed of accomplishing tasks, even getting it to start is a tag of war. The machine I am using satisfies all the requirements, I have never experienced issues with any other applications, even apps of 20GBs are running faster than an app of approximately 600mbs. Is this a general issue?? I want honest opinion.",
    "author": "Southern_Primary1824",
    "timestamp": "2025-09-23T04:02:39",
    "url": "https://reddit.com/r/Python/comments/1noebrx/why_is_spyder_so_slow/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nodt93",
    "title": "Python Sanity Check",
    "content": "Sanity check: I don't really know Python but boss wants me to hand code Python to pull data from a proprietary REST API we use. API is in-house so no open source or off the shelf library. I've done a fair bit of SQL and data pipeline work but scripting directly against APIs in Python isn't my thing. I guess vibe coding and hack something together in Python but I'll have to maintain it etc. What would you do?",
    "author": "Longjumping_Leg2213",
    "timestamp": "2025-09-23T03:32:50",
    "url": "https://reddit.com/r/Python/comments/1nodt93/python_sanity_check/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nmuy7t",
    "title": "Python 3.13 is 10% slower than 3.12 for my file parser",
    "content": "I have written a custom parser for a game-specific file format. \n\nIt performs particularly bad when there's too many nested references (A reference to a different object in an object), but that's a different problem on its own.\n\nThe current problem I have is with the performance degradation by almost 10% when using Python 3.13. I am trying to figure out what changes happened in 3.13 that might be relevant for my issue.\n\nI should probably attach the concrete code, so [here](https://github.com/seifhassine/REasy/blob/d4d4c842511c51d52913e7d5d63e752875fa17ef/file_handlers/rsz/rsz_file.py#L2068) is the method in question.",
    "author": "Bubbly-Craft8736",
    "timestamp": "2025-09-21T08:28:59",
    "url": "https://reddit.com/r/Python/comments/1nmuy7t/python_313_is_10_slower_than_312_for_my_file/",
    "score": 399,
    "num_comments": 74,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nnmhmw",
    "title": "Lazy Ninja ‚Äì Automate Django APIs &amp; Generate SDKs for Multiple Languages",
    "content": "# What My Project Does\n\nLazy Ninja is a Python library for Django that removes boilerplate from your APIs. It automatically generates CRUD endpoints from your Django models, creates Pydantic schemas for listing, creating, updating, and detailing records, and even generates SDKs/clients for multiple languages like TypeScript, Go and more.\n\nIt also comes with:\n\n* Async endpoints by default (configurable to sync if needed).\n* Interactive API documentation via Swagger UI and ReDoc.\n* Smart filtering, sorting, and customizable hooks to add your own logic.\n\nWith Lazy Ninja, you can focus on building features instead of writing repetitive code or keeping frontend clients in sync.\n\n# Target Audience\n\nLazy Ninja is for developers building Django projects who want to save time on repetitive API work. It works great for internal tools, prototypes, or learning projects‚Äîand I hope that with community contributions, it will soon be fully ready for production use hahaha ü•∫\n\nIf you‚Äôve ever wished Django could handle the boring parts for you, Lazy Ninja can help.\n\n# Comparison\n\nCompared to using Django Ninja or DRF manually:\n\n* **Time-saving:** No need to write the same CRUD endpoints repeatedly.\n* **Multi-language SDK generation:** Clients for TypeScript, Dart, Python, Go, Java, C#, and more.\n* **Automatic Pydantic schema generation:** Eliminates errors from manually writing schemas.\n* **Better for async projects:** Designed to leverage Django‚Äôs async features seamlessly.\n\nIt‚Äôs not a replacement for Django Ninja or DRF‚Äîrather, it builds on top of them and removes repetitive tasks, making API development faster and more consistent.\n\n# Recent Updates / Highlights\n\n* **Project scaffolding:** Quickly start a new Django project with `lazy-ninja init` (includes [`api.py`](http://api.py) and minimal setup).\n* **SDK generation:** `lazy-ninja generate-client` now supports multiple languages from your backend schema, without running the server.\n* **UUID support:** If your models use UUID primary keys, Lazy Ninja now handles them correctly in CRUD routes.\n\n# Links\n\n* GitHub: [https://github.com/AghastyGD/lazy-ninja](https://github.com/AghastyGD/lazy-ninja)\n* Docs: [https://lazy-ninja.readthedocs.io](https://lazy-ninja.readthedocs.io)",
    "author": "Aghasty_GD",
    "timestamp": "2025-09-22T06:27:59",
    "url": "https://reddit.com/r/Python/comments/1nnmhmw/lazy_ninja_automate_django_apis_generate_sdks_for/",
    "score": 6,
    "num_comments": 0,
    "upvote_ratio": 0.8,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1noegsu",
    "title": "Python 3.14 ‚Äì What you need to know",
    "content": "We're currently on 3.14.0rc3 (Release Candidate 3) with the official release of Python 3.14 scheduled for the 7th of October (2 weeks from now). To save users the trouble of going through all of the release notes, discussions and PEP docs, Cloudsmith have compiled a shortened, synthesized version of the Python 3.14 release notes as we approach the release date. There's some really interesting changes in this release, such as discontinuing PGP signatures in favour of short-lived Sigstore signing through OIDC, making Parentheses Optional in Except and Except Blocks, as well as deferred Evaluation Of Annotations Using Descriptors.   \n  \nIf you're excited about this upcoming release, check out the full full release notes here:  \n[https://cloudsmith.com/blog/python-3-14-what-you-need-to-know](https://cloudsmith.com/blog/python-3-14-what-you-need-to-know) ",
    "author": "ExtensionSuccess8539",
    "timestamp": "2025-09-23T04:10:26",
    "url": "https://reddit.com/r/Python/comments/1noegsu/python_314_what_you_need_to_know/",
    "score": 0,
    "num_comments": 7,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nn7o08",
    "title": "Monday Daily Thread: Project ideas!",
    "content": "# Weekly Thread: Project Ideas üí°\n\nWelcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.\n\n## How it Works:\n\n1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.\n2. **Build &amp; Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.\n3. **Explore**: Looking for ideas? Check out Al Sweigart's [\"The Big Book of Small Python Projects\"](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.\n\n## Guidelines:\n\n* Clearly state the difficulty level.\n* Provide a brief description and, if possible, outline the tech stack.\n* Feel free to link to tutorials or resources that might help.\n\n# Example Submissions:\n\n## Project Idea: Chatbot\n\n**Difficulty**: Intermediate\n\n**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar \n\n**Description**: Create a chatbot that can answer FAQs for a website.\n\n**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)\n\n# Project Idea: Weather Dashboard\n\n**Difficulty**: Beginner\n\n**Tech Stack**: HTML, CSS, JavaScript, API\n\n**Description**: Build a dashboard that displays real-time weather information using a weather API.\n\n**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)\n\n## Project Idea: File Organizer\n\n**Difficulty**: Beginner\n\n**Tech Stack**: Python, File I/O\n\n**Description**: Create a script that organizes files in a directory into sub-folders based on file type.\n\n**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)\n\nLet's help each other grow. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-21T17:00:38",
    "url": "https://reddit.com/r/Python/comments/1nn7o08/monday_daily_thread_project_ideas/",
    "score": 21,
    "num_comments": 1,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nmta0f",
    "title": "I built a full programming language interpreter in Python based on a meme",
    "content": "The project started as a joke based on the \"everyone talks about while loops but no one asks WHEN loops\" meme, but evolved into a complete interpreter demonstrating how different programming paradigms affect problem-solving approaches.\n\n# What My Project Does\n\nWHEN is a programming language interpreter written in Python where all code runs in implicit infinite loops and the only control flow primitive is `when` conditions. Instead of traditional for/while loops, everything is reactive:\n\n    # WHEN code example\n    count = 0\n    \n    main:\n        count = count + 1\n        print(\"Count:\", count)\n        when count &gt;= 5:\n            print(\"Done!\")\n            exit()\n\nThe interpreter features:\n\n* Full lexer, parser, and AST implementation\n* Support for importing Python modules directly\n* Parallel and cooperative execution models\n* Interactive graphics and game development capabilities (surprisingly)\n\nYou can install it via pip: `pip install when-lang`\n\n# Target Audience\n\nThis is Currently a toy/educational project, but exploring use cases in game development, state machine modeling, and reactive system prototyping, currently exploring\n\n* Learning about interpreter implementation\n* Exploring state machine programming\n* Educational purposes (understanding event-driven systems)\n* Having fun with esoteric language design\n\nNOT recommended for production use (everything is global scope and runs in infinite loops by design).\n\n# Comparison\n\nUnlike traditional languages:\n\n* **No explicit loops** \\- Everything runs implicitly forever until stopped\n* **No if statements** \\- Only `when` conditions that check every iteration\n* **Forced reactive paradigm** \\- All programs become state machines\n* **Built-in parallelism** \\- Blocks can run cooperatively or in parallel threads\n\nCompared to other Python-based languages:\n\n* **Brython/Skulpt**: Compile Python to JS, WHEN is a completely different syntax\n* **Hy**: Lisp syntax for Python, WHEN uses reactive blocks instead\n* **Coconut**: Functional programming, WHEN is purely reactive/imperative\n\nThe closest comparison might be reactive frameworks like RxPy, but WHEN makes reactive programming the ONLY way to write code, not an optional pattern.\n\n# Implementation Details\n\nThe interpreter (\\~1000 lines) includes:\n\n* Custom lexer with indentation-based parsing\n* Recursive descent parser generating an AST\n* Tree-walking interpreter with parallel execution support\n* Full Python module interoperability\n\nExample of WHEN's unique block system:\n\n    # Runs once\n    os setup():\n        initialize_system()\n    \n    # Runs exactly 5 times\n    de heartbeat(5):\n        print(\"beat\")\n    \n    # Runs forever\n    fo monitor():\n        check_status()\n    \n    # Entry point (implicit infinite loop)\n    main:\n        when not_started:\n            setup()\n            heartbeat.start()\n            monitor.start()\n\nGitHub: [https://github.com/PhialsBasement/WHEN-Language](https://github.com/PhialsBasement/WHEN-Language)",
    "author": "HearMeOut-13",
    "timestamp": "2025-09-21T07:22:42",
    "url": "https://reddit.com/r/Python/comments/1nmta0f/i_built_a_full_programming_language_interpreter/",
    "score": 117,
    "num_comments": 18,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nnj1rc",
    "title": "Extract complex bracket structure from pdf",
    "content": "I'm trying to extract text from a pdf, with a complex bracket structure (multiple rounds with winner and score of each match as players in next round, and potentially empty slots for BYEs etc.). I've tried pdfplumber, and I've tried converting to image and using tesseract to get the text from image. But no effort has worked to properly understand what the human eye can read. Tesseract constantly seems to misinterpret the text, particularly Swedish characters (even if adding to whitelist). And pdfplumber extracts the text in a way that is not relatable to the visual columns.\n\nWhat would be the best way to extract matches and scores from a pdf file like this? Is it even possible?\n\n[bracket pdf](https://drive.google.com/file/d/1mf3BVk0SuBswlaW_7q4YVcALBPaSIV6Q/view?usp=sharing)",
    "author": "dannewestis",
    "timestamp": "2025-09-22T03:41:56",
    "url": "https://reddit.com/r/Python/comments/1nnj1rc/extract_complex_bracket_structure_from_pdf/",
    "score": 2,
    "num_comments": 5,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nn2o95",
    "title": "Best Jupyter TUI",
    "content": "Hi. There has apparently been a recent \"surge\" in TUI/CLI-based apps, with the help of Python-based libraries such as Textual.\n\nThere are many such TUIs for creating and running Jupyter notebooks, but the last time I checked most were out of date, rarely used, or incomplete in features.\n\nHas anyone used one such Jupyter TUIs successfully? Has any of them come out as \"the\" winner? My main concern is autocomplete and Intellisense.\n\n\nThanks",
    "author": "ihatebeinganonymous",
    "timestamp": "2025-09-21T13:26:39",
    "url": "https://reddit.com/r/Python/comments/1nn2o95/best_jupyter_tui/",
    "score": 21,
    "num_comments": 11,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nmycfj",
    "title": "Do you find it helpful to run Sphinx reStructuredText/Markdown in a browser?",
    "content": "I‚Äôve been thinking a lot about documentation workflows lately. Sphinx is super powerful (and pretty much the standard for Python), but every time I try to onboard someone new, the initial ‚Äúinstall + configure‚Äù step feels like a wall.\n\nFor example, if you just want to:\n\n* Test how reStructuredText or MyST Markdown renders\n* Show a student how Sphinx works\n* Experiment with docs-as-code quickly\n* Quickly see the resulting HTML when styling Sphinx themes\n\n‚Ä¶you still need a local setup, which isn‚Äôt always trivial. Has anyone else struggled with this? How do you usually get around the ‚Äúfirst steps‚Äù friction when teaching or experimenting with Sphinx?  \n  \n(I‚Äôve been tinkering with a little experiment in running full, latest **Sphinx completely in a browser** using WebAssembly ‚Äî will share it in the comments if anyone‚Äôs curious.)",
    "author": "mattdocumatt",
    "timestamp": "2025-09-21T10:41:14",
    "url": "https://reddit.com/r/Python/comments/1nmycfj/do_you_find_it_helpful_to_run_sphinx/",
    "score": 25,
    "num_comments": 12,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nn7o0l",
    "title": "Monday Daily Thread: Project ideas!",
    "content": "# Weekly Thread: Project Ideas üí°\n\nWelcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.\n\n## How it Works:\n\n1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.\n2. **Build &amp; Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.\n3. **Explore**: Looking for ideas? Check out Al Sweigart's [\"The Big Book of Small Python Projects\"](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.\n\n## Guidelines:\n\n* Clearly state the difficulty level.\n* Provide a brief description and, if possible, outline the tech stack.\n* Feel free to link to tutorials or resources that might help.\n\n# Example Submissions:\n\n## Project Idea: Chatbot\n\n**Difficulty**: Intermediate\n\n**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar \n\n**Description**: Create a chatbot that can answer FAQs for a website.\n\n**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)\n\n# Project Idea: Weather Dashboard\n\n**Difficulty**: Beginner\n\n**Tech Stack**: HTML, CSS, JavaScript, API\n\n**Description**: Build a dashboard that displays real-time weather information using a weather API.\n\n**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)\n\n## Project Idea: File Organizer\n\n**Difficulty**: Beginner\n\n**Tech Stack**: Python, File I/O\n\n**Description**: Create a script that organizes files in a directory into sub-folders based on file type.\n\n**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)\n\nLet's help each other grow. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-21T17:00:39",
    "url": "https://reddit.com/r/Python/comments/1nn7o0l/monday_daily_thread_project_ideas/",
    "score": 6,
    "num_comments": 1,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nnqaor",
    "title": "Which Tech role will be in demand at most in 2026?",
    "content": "Hello everyone, \n\nI am Python developer and want to go either toward AI, ML or Data science. which one do you suggest the most? ",
    "author": "Hour-Computer-4857",
    "timestamp": "2025-09-22T08:54:31",
    "url": "https://reddit.com/r/Python/comments/1nnqaor/which_tech_role_will_be_in_demand_at_most_in_2026/",
    "score": 0,
    "num_comments": 20,
    "upvote_ratio": 0.21,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nmo8hl",
    "title": "pyya - integrate YAML configurations with your code easily",
    "content": "Updated to v0.1.9. Added a CLI tool to generate stubs for YAML configuration, now attribute style configuration has nice completion suggestions assuming you have setup mypy/python LSP.\n\nInstall:\npip install pyya\n\nPage:\nhttps://github.com/shadowy-pycoder/pyya\n\nFeatures:\n\n1) Automatically merge default and production configuration files\n2) Convert keys in configuration files to snake_case\n3) YAML validation with Pydantic models\n4) Generate stub files for your dynamic configuration with pyya CLI tool.\n5) Simple API",
    "author": "wit4er",
    "timestamp": "2025-09-21T03:12:02",
    "url": "https://reddit.com/r/Python/comments/1nmo8hl/pyya_integrate_yaml_configurations_with_your_code/",
    "score": 12,
    "num_comments": 2,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nmgr3q",
    "title": "super lightweight stateful flow",
    "content": "**What My Project Does**\n\nA lightweight AI-Ready Python framework for building asynchronous data processing pipelines with stateful nodes.\n\n**Target Audience**\n\nThose who wants to build AI application backends or lightweight data process backends. The project is not massivly tested in production.\n\n**Comparison**\n\nCompared to hamilton, airflow, pydag, etc., OoFlow is super lightweight and has very easy to use APIs, no restrictions on code positions, and its nodes/tasks are stateful, enabling cross-messages business logic.\n\n\\----------------------------------------------\n\nwhen i was building new applications(some were AI related), i found the programming paradigm changed, because the first token/byte of  each phase deeply affect user experiences.\n\ni had to make every step processing data asynchronous, stateful, parallel.\n\n    \"\"\"\n    Flow topology diagram:\n        A\n        ‚îÇ\n        ‚ñº\n        B\n       ‚ï± ‚ï≤\n      ‚ñº   ‚ñº\n      C   D\n       ‚ï≤ ‚ï±\n        ‚ñº\n        E\n    \"\"\"\n    flow = ooflow.create(\n        A.to(B),           # A ‚Üí B\n        B.to(C, D),        # B ‚Üí C, D (branching)\n        C.to(E),           # C ‚Üí E\n        D.to(E)            # D ‚Üí E (merging)\n    )\n\ni tried many frameworks(say hamilton, airflow, pydag, pipefunc ...), and finally decided to build a new one, they are either **too heavy, or have some weird rules to follow, or can not make my task function stateful**.\n\nthat's why i built OoFlow, you can realize the above graph/tasks-chain like this:\n\n    import asyncio\n    import ooflow\n    \n    u/ooflow.Node\n    async def A(context: ooflow.Context):\n        while True:\n            msg = await context.fetch()\n            await context.emit(f\"{msg} A | \")\n    \n    u/ooflow.Node\n    async def B(context: ooflow.Context):\n        while True:\n            msg = await context.fetch()\n            await context.emit(f\"{msg} B | \", C)\n            await context.emit(f\"{msg} B | \", D)\n    \n            # # you can also emit to C, D all at once\n            # await context.emit(f\"{msg} B | \")\n    \n    u/ooflow.Node\n    async def C(context: ooflow.Context):\n        while True:\n            msg = await context.fetch()\n            await context.emit(f\"{msg} C | \")\n    \n    @ooflow.Node\n    async def D(context: ooflow.Context):\n        while True:\n            msg = await context.fetch()\n            await context.emit(f\"{msg} D | \")\n    \n    @ooflow.Node\n    async def E(context: ooflow.Context):\n        while True:\n            msg_from_C = await context.fetch(C)\n            msg_from_D = await context.fetch(D)\n            await context.emit(f\"{msg_from_C} E\")\n            await context.emit(f\"{msg_from_D} E\")\n    \n            # # you can also fetch from C, D in one line\n            # msg = await context.fetch()\n            # await context.emit(f\"{msg} E\")\n    \n    async def main():\n        flow = ooflow.create(\n            A.to(B),\n            B.to(C, D), \n            C.to(E),\n            D.to(E)\n        )   \n        flow.run()\n    \n        async def producer():\n            count = 0 \n            while True:\n                count = count + 1 \n                await flow.emit(f\"{count}\")\n                await asyncio.sleep(1)\n    \n        asyncio.create_task(producer()),\n        while True:\n            print(await flow.fetch())\n    \n    if __name__ == \"__main__\":\n        asyncio.run(main())\n\nthe very important point of OoFlow is: task nodes are **stateful**. meaning that your task function will not exit after processing one message, you can leverage this feature to build cross-message functionalities, which are very common in AI-apps building.\n\nand OoFlow supports cyclic graph and multiple graphs in one flow instance, non-blocking fetches/emits are also supported, and class/instance/static methods are also supported.\n\nthe project site is: [https://github.com/fanfank/ooflow](https://github.com/fanfank/ooflow) it would be great if this framework helps you, and give your star :D",
    "author": "Significant-Maize933",
    "timestamp": "2025-09-20T19:45:39",
    "url": "https://reddit.com/r/Python/comments/1nmgr3q/super_lightweight_stateful_flow/",
    "score": 26,
    "num_comments": 0,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nmio5b",
    "title": "duvc-ctl Windows library for UVC camera control and Property control",
    "content": "I made this for controlling USB cameras on Windows without needing any extra SDKs or serial controls for PTZ. It‚Äôs called duvc-ctl. Supports C++, Python(other languages support coming soon), and a CLI for adjusting pan/tilt/zoom(ptz), focus, exposure, and other camera properties.\n\nhttps://github.com/allanhanan/duvc-ctl\n\nWhat my project does:\nControl camera properties such as Brightness, Exposure, Pan, Tilt, Zoom, and other camera properties available in DirectShow \nIt exposes the DirectShow api to access these properties easily in C++ and binds it to python\n\nLinux already has v4l2-ctl which is waay better but windows was lacking\n\nWould be interested to hear if others find this useful or have ideas for where it could fit into workflows.\n\nI personally found this useful where I didn't want to mess with visca or other serial protocols and just wanted to control it from python with just the usb connected\n\nI might add linux support but I'm open to hear any opinions on this for now ",
    "author": "Ok_Avocado_5836",
    "timestamp": "2025-09-20T21:30:25",
    "url": "https://reddit.com/r/Python/comments/1nmio5b/duvcctl_windows_library_for_uvc_camera_control/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nmdhrp",
    "title": "Sunday Daily Thread: What's everyone working on this week?",
    "content": "# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è\n\nHello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!\n\n## How it Works:\n\n1. **Show &amp; Tell**: Share your current projects, completed works, or future ideas.\n2. **Discuss**: Get feedback, find collaborators, or just chat about your project.\n3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.\n\n## Guidelines:\n\n* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.\n* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.\n\n## Example Shares:\n\n1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!\n2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.\n3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!\n\nLet's build and grow together! Share your journey and learn from others. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-20T17:00:32",
    "url": "https://reddit.com/r/Python/comments/1nmdhrp/sunday_daily_thread_whats_everyone_working_on/",
    "score": 4,
    "num_comments": 11,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nlze8r",
    "title": "Tines API Wrapper",
    "content": "**Links**\n\nPyPI: [https://pypi.org/project/Tapi/](https://pypi.org/project/Tapi/)  \nGitHub: [https://github.com/1Doomdie1/Tapi](https://github.com/1Doomdie1/Tapi)  \nPepy.tech: [stats](https://pepy.tech/projects/tapi?timeRange=threeMonths&amp;category=version&amp;includeCIDownloads=true&amp;granularity=daily&amp;viewType=line&amp;versions=0.1.3%2C0.1.2)\n\n**So what is Tines?**\n\nIn short, **Tines** is a no-code automation platform designed for security and IT teams. It allows users to build, orchestrate, and automate workflows such as incident response, threat detection, and IT operations without needing to write code. By connecting to APIs and tools, Tines helps streamline repetitive tasks, reduce response times, and improve operational efficiency. Althought it is marketed as a \"*no-code*\" solution, that doesn't mean it doesn't have the ability to run code. Quite the opposite, it provides you with a dedicated action which allows you to write and execute your own python code.\n\n**What My Project Does**\n\nI created **Tapi** as a Python wrapper for the [Tines API](https://www.tines.com/api/welcome/). Rather than dealing with raw HTTP requests or parsing JSON by hand, Tapi provides structured classes like `WorkflowsAPI`, `ActionsAPI`, `CredentialsAPI`, and others. These give you a clean way to interact with your Tines tenant and its endpoints.\n\n**Examples**\n\nPulling information about your tenant would look somehting like this:\n\n    from json import dumps\n    from tapi import TenantAPI\n    \n    def main():\n        DOMAIN  = \"my-cool-domain-1234\"\n        API_KEY = \"do_not_put_this_on_github_lol\"\n        \n        tenant = TenantAPI(DOMAIN, API_KEY)\n        \n        tenant_info = tenant.info()\n        \n        print(dumps(tenant_info, indent = 4))\n\nOutput:\n\n    {\n        \"body\": {\n            \"stack\": {...}\n        },\n        \"headers\": {...},\n        \"status_code\": ...\n    }\n\nAnother example would be getting all the workflows from your tenant.\n\n    from json import dumps\n    from tapi import StoriesAPI\n    \n    def main():\n        DOMAIN  = \"my-cool-domain-1234\"\n        API_KEY = \"do_not_put_this_on_github_lol\"\n        \n        stories_api = StoriesAPI(DOMAIN, API_KEY)\n        \n        stories = stories_api.list()\n        \n        print(dumps(stories, indent = 4))\n\nOutput:\n\n    {\n        \"body\": {\n            \"stories\": [\n                {\n                    \"name\": \"Testing\",\n                    \"user_id\": 1234,\n                    \"description\": null,\n                    \"keep_events_for\": 604800,\n                    \"disabled\": false,\n                    \"priority\": false\n                    //...[snip]...//\n                }\n            //...[snip]...//\n            ]\n        },\n        \"headers\": {...},\n        \"status_code\": ...\n    }\n\nAnd so on and so forth. To find out more, please do check out the GitHub or PyPI repos.\n\nI‚Äôd love to hear what you think! Feedback, feature requests, or contributions are always welcome!",
    "author": "XDoomdieX",
    "timestamp": "2025-09-20T07:18:42",
    "url": "https://reddit.com/r/Python/comments/1nlze8r/tines_api_wrapper/",
    "score": 20,
    "num_comments": 14,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nmu5pn",
    "title": "senior junior talks",
    "content": "[https://www.geeksforgeeks.org/courses/c-skill-up](https://www.geeksforgeeks.org/courses/c-skill-up)¬†hi i am a student of cybersecurity now i am first year i just wanna ask you is this course will help in academics to pass my pps (c language) exam\n\n",
    "author": "AdScary1945",
    "timestamp": "2025-09-21T07:58:04",
    "url": "https://reddit.com/r/Python/comments/1nmu5pn/senior_junior_talks/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nmq30k",
    "title": "Licensing Platform for Fintech Software Website Sync?",
    "content": "Disclaimer: I foolishly got GPT to write this post but it seems to nail down what I am looking for.\n\nTL;DR\n\n* Late-stage beta **Windows desktop trading app** (integrates with **MT5**).\n* Need two things (ideally decoupled):\n   1. **Pro desktop UI** (tabs for Live/Backtest/Config, logs, charts, settings, license status). Open to **PySide6/Qt, .NET, or Tauri/Electron**.\n   2. **Licensing + accounts + payments** tied to **WordPress users** (trials, activations/deactivations, online check with offline grace, basic telemetry).\n* Prefer a **packaged/licensing platform** \\+ **subscription stack** that handles invoices/taxes (**Stripe+Woo**, **Paddle**, or **Lemon Squeezy**).\n* Must stay a desktop app; want **auto-update, code signing, crash reporting** if possible.\n* Looking for a **partner/contractor** or **battle-tested stack recommendations**. DM with examples, stack preference, and rough timeline.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nI‚Äôm in late-stage beta on a trading project (Stirling FOREX). The core engine is solid and runs as a **Windows desktop app** that integrates with **MetaTrader 5** via API. The current UI is a functional ‚Äúbuilder‚Äù style interface, but it‚Äôs time to replace it with something professional‚Äîand, separately, I need to stand up the **licensing + accounts + payments** side. Ideally those two tracks don‚Äôt have to be tightly coupled.\n\n**What I need (two parallel tracks):**\n\n1. **UI replacement (desktop, Windows first)**\n\n* Re-skin/replace the current builder UI with a clean, professional desktop UI.\n* Keep it native-feeling and performant (I‚Äôm open on framework: PySide6/Qt, .NET wrapper, Tauri/Electron if justified, etc.).\n* Typical screens: multi-tab layout (Live, Backtest, Config), tables/logs, charts, start/stop controls, settings, license/status panel.\n* Nice to have: light/dark themes, responsive layout, error toasts, and a safe auto-update flow.\n\n1. **Licensing + website accounts + payments (WordPress)**\n\n* Users already have/will have **WordPress accounts** on my site.\n* I want **licenses tied to website accounts** (plan-based, per-seat/per-machine), with:\n   * trials, activations/deactivations,\n   * online verification with a short offline grace window,\n   * basic telemetry/heartbeat is fine if needed.\n* **Payments &amp; accounting:** looking for an off-the-shelf subscription stack that handles invoicing, taxes (Canada GST/HST), refunds, and proration.\n   * I‚Äôm open to options like Stripe (+ WooCommerce/membership), Paddle, Lemon Squeezy, etc.‚Äîwhichever is the least painful and plays nicely with WordPress and a license server.\n* Bonus: code signing for Windows builds, crash reporting, and a straightforward release pipeline.\n\n**Key constraints &amp; reality check**\n\n* This **must remain a desktop app** (tight MT5 integration).\n* I don‚Äôt have the bandwidth to build licensing/commerce from scratch. A **packaged platform or proven combo** is preferred.\n* I‚Äôm aiming to **decouple** the UI rebuild from the licensing/commerce work so either can ship independently.\n\n**What I bring**\n\n* Fully working trading engine with clear boundaries between logic and UI.\n* Test builds and sample data for quick iteration.\n* Fast feedback cycles and a pragmatic scope (ship the essentials first).\n\n**What I‚Äôm looking for**\n\n* Either: (a) a **partner/contractor** who can take one or both tracks, or (b) **recommendations** for a licensing+commerce setup that fits a WordPress site and a Python/Windows desktop app.\n* War stories welcome: gotchas with Paddle/Lemon Squeezy/Stripe+Woo, WordPress SSO flows into a desktop client, license server choices, updater tooling, and code signing tips.\n\nIf you‚Äôre interested (or have a battle-tested stack to recommend), please drop a comment or DM me with:\n\n* Relevant examples (UI rebuilds, licensing integrations).\n* Your preferred stack and why.\n* Rough timeline/engagement model.\n\n\n\nMe again. This isn't a time sensitive project. Just something I have been building for fun that actually turned into some violently complicated.\n\nCheers,\n\n",
    "author": "Greedy_Bookkeeper_30",
    "timestamp": "2025-09-21T04:59:55",
    "url": "https://reddit.com/r/Python/comments/1nmq30k/licensing_platform_for_fintech_software_website/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nm04o4",
    "title": "Cosmic Django: Architecture Patterns",
    "content": "https://brunodantas.github.io/blog/2025/09/12/cosmic-django/\n\nArticle on the applicability of the patterns from the Cosmic Python book (Architecture Patterns With Python) to Django projects.\n",
    "author": "poopatroopa3",
    "timestamp": "2025-09-20T07:48:16",
    "url": "https://reddit.com/r/Python/comments/1nm04o4/cosmic_django_architecture_patterns/",
    "score": 8,
    "num_comments": 1,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nllur9",
    "title": "Why isn't the copy() method part of the Sequence and MutableSequence ABCs?",
    "content": "The `Sequence` ABC from collections.abc does not include an abstract method copy(). What are the reasons for that design choice?\n\nNote that I am *not* asking how to work with that design choice. Instead I am trying to understand it.\n\n## Update\n\nThere have been great comments helping to answer (or even unask) the question. What I found most compelling is the observation (that I needed pointed out to me) that `copy` is problematic for a number reasons.\n\nPeople drew attention to this discussion of adding copy to `Set`:\n\nhttps://bugs.python.org/issue22101\n\n### copy return type\n\nThere are two arguments against adding copy to Set. One is that depending on the backing of the data copy might be inappropriate. The other is that the return type of copy is unclear. As Guido says, \n\n&gt; I personally despise almost all uses of \"copying\" (including the entire copy module, both deep and shallow copy functionality).  I much prefer to write e.g. list(x) over x.copy() -- when I say list(x) I know the type of the result.\n\nI had not thought of that before, but once stated, I completely agree with it.\nI am no longer thinking about creating a `CopiableSequence` protocol. If I have a concrete class for which `copy` makes sense and has clear semantics, I might add concrete a concrete method, but even then, I would probably probably create something like\n\n```python\nMyConcreteSequence[T](Sequence[T]):\n   def mutable_copy(self) -&gt; list[T]:\n      ...  # actual implementation would go here.\n```\n\nbut I don't really foresee needing to do that.\n\n### Keep the \"Base\" in ABC\n\nThe other line of answer was effectively about how basic a base class is expected to be. These really should be the minimal description of what makes something conform to the ABC. I find that a good and principled argument, but then I am left with why `reversed()` is included in `Sequence`.\n\nSo I come back to thinking that the relevant difference between `reversed()` and `copy()` for an immutable thing like Sequence is about deciding what the return type of `copy()` should be.\n\n#### Update (again)\n\nMy initial sense that implementing `copy` would depend on the same underlying properties of the data in the same way that implementing `reversed` would was mistaken. I learned a great deal in the discussion, and I encourage others to read it.\n\n\n",
    "author": "jpgoldberg",
    "timestamp": "2025-09-19T18:55:01",
    "url": "https://reddit.com/r/Python/comments/1nllur9/why_isnt_the_copy_method_part_of_the_sequence_and/",
    "score": 46,
    "num_comments": 27,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nlw9dz",
    "title": "it's not always about django vs fastapi/flask, you can use both",
    "content": "I've build an intricate image generation tool and, while I started with django (I have a svelte+django template I use for all my projects), I slowly started to extract certain parts of it, most relevant one is the \"engine\". here's an overview:\n\n\\- backend: django, django-allauth, django-drf, celery workers, celery beat, sqlite (WAL mode for speed), etc.  \n\\- engine (where the magic happens): fastapi with sqlalchemy (still with sqlite w/ WAL)  \n\\- frontend: svelte static site, server via nginx under docker  \n\\- metabase (analytics): reads my sqlite from django and provides nice graphs\n\nbackend handles all the requests and crud, while engine actually does what users want. the reason I separated them is that now I can have multiple engine instances, nicely orchestrated by django (I don't have that yet, and it'll take some time as I can just beef up my vps until huge scale hits me, but still it's good to have).\n\nI'm still very fond of using python instead of node (I'm not a js dev). you have so many ai/ml/charting libs in python, and can prototype really fast directly in django, like running some kind of expensive ml task dierectly as part of the processing of the request, just to test things out, but of course you can then defer them to celery workers, and when you need more power just ad more celery workers. you can sustain pretty high loads this way, also use gunicorn with uvicorn worker type for even better process management\n\nall these under a single docker compose on my hetzner vps",
    "author": "lutian",
    "timestamp": "2025-09-20T04:56:18",
    "url": "https://reddit.com/r/Python/comments/1nlw9dz/its_not_always_about_django_vs_fastapiflask_you/",
    "score": 8,
    "num_comments": 16,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nltf58",
    "title": "Scintilla, Qt and alternative text editor widgets",
    "content": "Hello fellow python enjoyers,\n\nI'm currently considering moving away from PyQt6 to go on PySide6 due to license issues. However, it would imply moving away from QScintilla as a text editor too, since there is no bindings for Scintilla on PySide side.\n\nI don't want to go back to \"default\" QPlainTextEdit since my needs are close to the ones of a Source Code editor (especially indentation guides).\n\nDo any of you know an alternative? I'm leaning towards Monaco via [QTMonaco](https://github.com/bec-project/qtmonaco), but there might be better options or easier to adapt (I still need to find out resources regarding Monaco).",
    "author": "ThylowZ",
    "timestamp": "2025-09-20T02:10:44",
    "url": "https://reddit.com/r/Python/comments/1nltf58/scintilla_qt_and_alternative_text_editor_widgets/",
    "score": 8,
    "num_comments": 13,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nlqcrz",
    "title": "Weird event loop/closure error?",
    "content": "Could someone explain me what cause the second `async_to_sync` call to fail and more interestingly why the hack to overcome the error works?\n\nI'm using the `taskiq` library from synchronous function, so instead of `await async_job.kiq(\"name\")`, I'm using `async_to_sync`. The first call succeeds, but the second one fails miserably\n\n    RuntimeError: Task &lt;Task pending name='Task-4' coro=&lt;AsyncToSync.__call__.&lt;locals&gt;.new_loop_wrap() running at /home/kmmbvnr/Workspace/summary/.venv/lib/python3.12/site-packages/asgiref/sync.py:230&gt; cb=[_run_until_complete_cb() at /usr/lib/python3.12/asyncio/base_events.py:182]&gt; got Future &lt;Future pending&gt; attached to a different loop\n\nSurprisingly the simple hack to wrap it in sync\\_to\\_async and back helps\n\n    if __name__ == \"__main__\":\n        # this two calls works fine\n        # async_to_sync(sync_to_async(lambda: async_to_sync(async_job.kiq)(\"first\")))\n        # async_to_sync(sync_to_async(lambda: async_to_sync(async_job.kiq)(\"second\")))\n    \n    \n        # more straigtforward approach produce an error on second call\n        print(\"first\")\n        async_to_sync(async_job.kiq)(\"first\")\n        print(\"second\")\n        async_to_sync(async_job.kiq)(\"second\") # fails\n\nFull gist - [https://gist.github.com/kmmbvnr/f47c17ed95a5a6dc0a166ed7e75c0439](https://gist.github.com/kmmbvnr/f47c17ed95a5a6dc0a166ed7e75c0439)",
    "author": "kmmbvnr",
    "timestamp": "2025-09-19T22:59:11",
    "url": "https://reddit.com/r/Python/comments/1nlqcrz/weird_event_loopclosure_error/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl9f0h",
    "title": "I just released reaktiv v0.19.2 with LinkedSignals! Let me explain what Signals even are",
    "content": "I've been working on this reactive state management library for Python, and I'm excited to share that I just added LinkedSignals in v0.19.2. But first, let me explain what this whole \"Signals\" thing is about.\n\n# I built Signals = Excel for your Python code\n\nYou know that frustrating bug where you update some data but forget to refresh the UI? Or where you change one piece of state and suddenly everything is inconsistent? **I got tired of those bugs, so I built something that eliminates them completely.**\n\nSignals work just like Excel - change one cell, and all dependent formulas automatically recalculate:\n\n    from reaktiv import Signal, Computed, Effect\n    \n    # Your data (like Excel cells)\n    name = Signal(\"Alice\")\n    age = Signal(25)\n    \n    # Automatic formulas (like Excel =A1&amp;\" is \"&amp;B1&amp;\" years old\")\n    greeting = Computed(lambda: f\"{name()} is {age()} years old\")\n    \n    # Auto-display (like Excel charts that update automatically)\n    display = Effect(lambda: print(greeting()))\n    # Prints: \"Alice is 25 years old\"\n    \n    # Just change the data - everything updates automatically!\n    name.set(\"Bob\")  # Prints: \"Bob is 25 years old\"\n    age.set(30)      # Prints: \"Bob is 30 years old\"\n\n**No more forgotten updates. No more inconsistent state. It just works.**\n\n# What I just added: LinkedSignals\n\nThe big feature I'm excited about in v0.19.2 is **LinkedSignals** \\- for when you want a value that usually follows a formula, but users can override it temporarily:\n\n    from reaktiv import Signal, Computed, LinkedSignal\n    \n    # Items from your API\n    items = Signal([\"iPhone\", \"Samsung\", \"Google Pixel\"])\n    \n    # Selection that defaults to first item but remembers user choice\n    selected = LinkedSignal(lambda: items()[0] if items() else None)\n    \n    print(selected())  # \"iPhone\"\n    \n    # User picks something\n    selected.set(\"Samsung\") \n    print(selected())  # \"Samsung\"\n    \n    # API updates - smart behavior!\n    items.set([\"Samsung\", \"OnePlus\", \"Nothing Phone\"])\n    print(selected())  # Still \"Samsung\" (preserved!)\n    \n    # But resets when their choice is gone\n    items.set([\"OnePlus\", \"Nothing Phone\"])\n    print(selected())  # \"OnePlus\" (smart fallback)\n\n**I built this for:**\n\n* Search/filter UIs where selections should survive data refreshes\n* Pagination that clamps to valid pages automatically\n* Form defaults that adapt but remember user input\n* Any \"smart defaulting\" scenario\n\n# Why I think this matters\n\nThe traditional approach:\n\n    # Update data ‚úì\n    # Remember to update display (bug!)  \n    # Remember to validate selection (bug!)\n    # Remember to update related calculations (bug!)\n\nSo I built something where you declare relationships once:\n\n    # Declare what depends on what\n    # Everything else happens automatically ‚úì\n\nI borrowed this battle-tested pattern from frontend frameworks (Angular, SolidJS) and brought it to Python. Perfect for APIs, data processing, configuration management, or any app where data flows through your system.\n\nTry it out: `pip install reaktiv` (now v0.19.2!)\n\n[GitHub](https://github.com/buiapp/reaktiv) | [Docs](https://reaktiv.readthedocs.io) | [Examples](https://github.com/buiapp/reaktiv/tree/main/examples) | [Playground](https://reaktiv.bui.app/#interactive-demo)\n\nWould love to hear what you think or if you build something cool with it!",
    "author": "loyoan",
    "timestamp": "2025-09-19T10:11:47",
    "url": "https://reddit.com/r/Python/comments/1nl9f0h/i_just_released_reaktiv_v0192_with_linkedsignals/",
    "score": 19,
    "num_comments": 15,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nld9qx",
    "title": "Python Context Managers 101",
    "content": "You've likely seen it before: The `with` keyword, which is one way of using Python context managers, such as in this File I/O example below:\n\n```python\nwith open('my_file.txt', 'r') as f:\n    content = f.read()\n    print(content)\n```\n\nPython context managers provide a way to wrap code blocks with setUp and tearDown code that runs before and after the code block. This tearDown part can be useful for multiple reasons, such as freeing up resources that have been allocated, closing files that are no longer being read from (or written to), and even quitting browsers that were spun up for automated testing.\n\nCreating them is simple. Let's create a simple context manager that displays the runtime of a code block:\n\n```python\nimport time\nfrom contextlib import contextmanager\n\n@contextmanager\ndef print_runtime(description=\"Code block\"):\n    start_time = time.time()\n    try:\n        yield\n    finally:\n        runtime = time.time() - start_time\n        print(f\"{description} ran for {runtime:.4f}s.\")\n```\n\nHere's how you could use it as a method decorator:\n\n```python\n@print_runtime()\ndef my_function():\n    # &lt;CODE BLOCK&gt;\n\nmy_function()\n```\n\nHere's how you could use it within a function using the `with` keyword:\n\n```python\nwith print_runtime():\n    # &lt;CODE BLOCK&gt;\n```\n\nAnd here's a low-level way to use it without the `with` keyword:\n\n```python\nmy_context = print_runtime()\nmy_object = my_context.__enter__()\n# &lt;CODE BLOCK&gt;\nmy_context.__exit__(None, None, None)\n```\n\nAs you can see, it's easy to create and use Python context managers. You can even pass args into them when configured for that. In advanced scenarios, you might even use context managers for browser automation. Example:\n\n```python\nfrom seleniumbase import SB\n\nwith SB(incognito=True, demo=True, test=True) as sb:\n    sb.open(\"https://www.saucedemo.com\")\n    sb.type(\"#user-name\", \"standard_user\")\n    sb.type(\"#password\", \"secret_sauce\")\n    sb.click(\"#login-button\")\n    sb.click('button[name*=\"backpack\"]')\n    sb.click(\"#shopping_cart_container a\")\n    sb.assert_text(\"Backpack\", \"div.cart_item\")\n```\n\nThat was a simple example of testing an e-commerce site. There were a few args passed into the context manager on initialization, such as `incognito` for Chrome's Incognito Mode, `demo` to highlight browser actions, and `test` to display additional info for testing, such as runtime.\n\nWhether you're looking to do simple File I/O, or more advanced things such as browser automation, Python context managers can be extremely useful!",
    "author": "SeleniumBase",
    "timestamp": "2025-09-19T12:38:14",
    "url": "https://reddit.com/r/Python/comments/1nld9qx/python_context_managers_101/",
    "score": 9,
    "num_comments": 22,
    "upvote_ratio": 0.56,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nksvm0",
    "title": "enso: A functional programming framework for Python",
    "content": "Hello all, I'm here to make my first post and 'release' of my functional programming framework, enso.  Right before I made this post, I made the repository public.  You can find it [here.](https://gitlab.com/evansemenoff/enso)\n\n# What my project does\n\nenso is a high-level functional framework that works over top of Python.  It expands the existing Python syntax by adding a variety of features.  It does so by altering the AST at runtime, expanding the functionality of a handful of built-in classes, and using a modified tokenizer which adds additional tokens for a preprocessing/translation step.\n\nI'll go over a few of the basic features so that people can get a taste of what you can do with it.\n\n1. Automatically curried functions!\n\nHow about the function add, which looks like\n\n    def add(x:a, y:a) -&gt; a:\n        return x + y\n\nUnlike normal Python, where you would need to call add with 2 arguments, you can call this `add` with only one argument, and then call it with the other argument later, like so:\n\n    f = add(2)\n    f(2)\n    4\n\n2. A map operator\n\nSince functions are automatically curried, this makes them really, really easy to use with `map`.  Fortunately, enso has a map operator, much like Haskell.\n\n    f &lt;$&gt; [1,2,3]\n    [3, 4, 5]\n\n3. Predicate functions\n\nFunctions that return `Bool` work a little differently than normal functions.  They are able to use the pipe operator to filter iterables:\n\n    even? | [1,2,3,4]\n    [2, 4]\n\n4. Function composition\n\nThere are a variety of ways that functions can be composed in enso, the most common one is your typical function composition.\n\n    h = add(2) @ mul(2)\n    h(3)\n    8\n\nAdditionally, you can take the *direct sum* of 2 functions:\n\n    h = add + mul\n    h(1,2,3,4)\n    (3, 12)\n\nAnd these are just a few of the ways in which you can combine functions in enso.\n\n5. Macros\n\nenso has a variety of macro styles, allowing you to redefine the syntax on the file, adding new operators, regex based macros, or even complex syntax operations.  For example, in the REPL, you can add a `zip` operator like so:\n\n    macro(op(\"-=-\", zip))\n    [1,2,3] -=- [4,5,6]\n    [(1, 4), (2, 5), (3, 6)]\n\nThis is just one style of macro that you can add, see the readme in the project for more.\n\n6. Monads, more new operators, new methods on existing classes, tons of useful functions, automatically derived function 'variants', and loads of other features made to make writing code fun, ergonomic and aesthetic.\n\nAbove is just a small taster of the features I've added.  The README file in the repo goes over a lot more.\n\n# Target Audience\n\nWhat I'm hoping is that people will enjoy this.  I've been working on it for awhile, and dogfooding my own work by writing several programs in it.  My own smart-home software is written entirely in *enso.*  I'm really happy to be able to share what is essentially a beta version of it, and would be super happy if people were interested in contributing, or even just using enso and filing bug reports.  My long shot goal is that one day I will write a proper compiler for enso, and either self-host it as its own language, or run it on something like LLVM and avoid some of the performance issues from Python, as well as some of the sticky parts which have been a little harder to work with.\n\nI will post this to r/functionalprogramming once I have obtained enough karma.\n\nHappy coding.",
    "author": "enso_lang",
    "timestamp": "2025-09-18T20:29:06",
    "url": "https://reddit.com/r/Python/comments/1nksvm0/enso_a_functional_programming_framework_for_python/",
    "score": 177,
    "num_comments": 64,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl4bxv",
    "title": "A script to get songs from a playlist with matching total length",
    "content": "#What my project does\nBasically, you input:\n\n- A public youtube playlist\n\n- Target duration\n\nYou get:\n\n- Song groups with a matching total length\n\n#Target Audience\n\nSo I think this is one of the most specific 'problems'..\n\nI've been making a slow return to jogging, and one of the changes to keep things fresh was to jog until the playlist ended. (Rather than meters, or a route)\n\nI am incrementing the length of the playlist by 15 seconds between each run, and each time finding a group of songs with a matching length can be tiring, which is why I thought of this üòÖ\n\n&amp;nbsp;\n\nSo I guess this is for people who want a shuffled playlist, with a specific duration, for some reason.\n\nThis is 'py-playlist-subset', try it out üëÄ\n\nhttps://github.com/Tomi-1997/py-playlist-subset",
    "author": "Atlas___Hugged",
    "timestamp": "2025-09-19T06:58:47",
    "url": "https://reddit.com/r/Python/comments/1nl4bxv/a_script_to_get_songs_from_a_playlist_with/",
    "score": 23,
    "num_comments": 4,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nkidxq",
    "title": "Today I learned that Python doesn't care about how many spaces you indent as long as it's consistent",
    "content": "Call me stupid for only discovering this after 6 years, but did you know that you can use as many spaces you want to indent, as long as they're consistent within one indented block. For example, the following (awful) code block gives no error:\n\n    def say_hi(bye = False):\n    ¬†print(\"Hi\")\n    ¬†if bye:\n    ¬† ¬† ¬† ¬† print(\"Bye\")",
    "author": "FillAny3101",
    "timestamp": "2025-09-18T12:49:56",
    "url": "https://reddit.com/r/Python/comments/1nkidxq/today_i_learned_that_python_doesnt_care_about_how/",
    "score": 591,
    "num_comments": 194,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nm3in3",
    "title": "DBMS based on python dictionarys",
    "content": "Hello, I'm a programming student and enthusiast, and I'm here to launch a DBMS called datadictpy that uses Python dictionary logic to store data.\n\n # What my project does:\n\n Creates tables, relates data, saves data, changes data, and deletes data, using dictionaries as a structured data storage method.\n\n# Some functions\n\n``add_element(\"nome\")``\n\nThis method creates a table/list, it is called after adding data in the standard python way to a dictionary, for the dictionary to be considered it is necessary to make it an object of the dB class\n\n``find_key_element(\"Key\", \"list\")``\n\nThis method finds all elements of a table that share the same dictionary key like \"name\" for example\n\n``find_value_element(\"Key\", \"value\", \"lista)``\n\nThis method checks if a value exists within the table.\n\n``show_list(\"list\")``\n\nThis method displays an entire table in the terminal.\n\n``find_id(\"id\", \"list\")``\n\nThis method finds data related to an ID within a list.\n\nThese are some functions; in general, the system uses standard Python dictionary syntax.\n\n# Target Audience\n It's a production project, but it's in its early stages and needs a bit more refinement. However, it works perfectly with frameworks.\n\n# Comparison\n This project differs from DBMSs like MySQL, PostgreSQL, etc., because it uses dictionaries as a structured data format and does not require an ORM..\n\n# How it contributes\n This project can contribute to Python by reducing dependence on APIs like MySQL in certain projects, as it would be done by Python itself.\n\nhttps://github.com/Heitor2025/datadictpy.git\n\nGood coding for everyone",
    "author": "Friendly_Nothing_546",
    "timestamp": "2025-09-20T10:01:36",
    "url": "https://reddit.com/r/Python/comments/1nm3in3/dbms_based_on_python_dictionarys/",
    "score": 0,
    "num_comments": 15,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nljibj",
    "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread",
    "content": "# Weekly Thread: Resource Request and Sharing üìö\n\nStumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!\n\n## How it Works:\n\n1. **Request**: Can't find a resource on a particular topic? Ask here!\n2. **Share**: Found something useful? Share it with the community.\n3. **Review**: Give or get opinions on Python resources you've used.\n\n## Guidelines:\n\n* Please include the type of resource (e.g., book, video, article) and the topic.\n* Always be respectful when reviewing someone else's shared resource.\n\n## Example Shares:\n\n1. **Book**: [\"Fluent Python\"](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \\- Great for understanding Pythonic idioms.\n2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \\- Excellent overview of Python's built-in data structures.\n3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \\- A deep dive into decorators.\n\n## Example Requests:\n\n1. **Looking for**: Video tutorials on web scraping with Python.\n2. **Need**: Book recommendations for Python machine learning.\n\nShare the knowledge, enrich the community. Happy learning! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-19T17:00:30",
    "url": "https://reddit.com/r/Python/comments/1nljibj/saturday_daily_thread_resource_request_and/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl5x5g",
    "title": "Introducing 'Drawn' - A super simple text-to-diagram tool",
    "content": "Hi folks,\n\nI wanted to share [**Drawn**](https://github.com/parthivrmenon/drawn), a minimalistic CLI tool that transforms simple text notation into system diagrams.\n\n‚Ä¶take ‚Äúbeautiful‚Äù with a pinch of salt‚ÄîI‚Äôm a terrible judge of aesthetics üòÖ\n\n---\n\n## What My Project Does\n\nDrawn converts plain text ‚Äúdiagram code‚Äù into visual diagrams. You write a simple notation file, and it generates a clean diagram, making it easier to document systems, workflows, or processes.\n\n**Example:**\n\n```bash\nSun --&gt; Evaporation\nEvaporation -(condensation)-&gt; Clouds\nClouds -(precipitation)-&gt; Rain\nRain --&gt; Rivers\nRivers --&gt; Oceans\nOceans -(evaporation)-&gt; Evaporation\n```\n\nThis produces a neat diagram representing the **Water Cycle**.\n\n---\n\n## Target Audience\n\nDrawn is mainly a **toy/experimental project**‚Äîgreat for developers, students, or anyone who wants a quick way to turn text into diagrams. It‚Äôs not production-grade yet, but it is still quite useful!\n\n---\n\n## Comparison\n\nUnlike heavier diagram tools (like Mermaid or PlantUML), Drawn is **ultra-lightweight and intuitive to use with virtually no learning curve**. It focuses on **simplicity** over exhaustive features, making it quick to use for small projects or notes.\n\n---\n\nFeel free to give it a whirl! I‚Äôd love your feedback and any suggestions for improving the project.\n",
    "author": "SilverOrder1714",
    "timestamp": "2025-09-19T08:00:44",
    "url": "https://reddit.com/r/Python/comments/1nl5x5g/introducing_drawn_a_super_simple_texttodiagram/",
    "score": 13,
    "num_comments": 11,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nlvv14",
    "title": "Pure Python Cryptographic Commitment Scheme: General Purpose, Offline-Capable, Zero Dependencies",
    "content": "Hello everyone, I have created a cryptographic commitment scheme that is universally applicable to any computer running python, it provides cryptographic security to any average coder just by copy and pasting the code module I curated below, it has many use cases and has never been available/accessible until now according to GPT deep search. My original intent was to create a verifiable psi experiment, then it turned into a universally applicable cryptographic commitment module code that can be used and applied by anyone at this second from the GitHub repository.\n\nLmk what ya‚Äôll think?\n\nChatGPT‚Äôs description: This post introduces a minimal cryptographic commitment scheme written in pure Python. It relies exclusively on the Python standard library. No frameworks, packages, or external dependencies are required. The design goal was to make secure commitment‚Äìreveal verification universally usable, auditably simple, and deployable on any system that runs Python.\n\nThe module uses HMAC-SHA256 with domain separation and random per-instance keys. The resulting commitment string can later be verified against a revealed key and message, enabling proof-of-prior-knowledge, tamper-evident disclosures, and anonymous timestamping.\n\n‚∏ª\n\nRepositories:\n\n‚Ä¢\tMinimal module: https://github.com/RayanOgh/Minimal-HMAC-SHA256-Commitment-Verification-Skeleton-Python-\n\n‚Ä¢\tExtended module with logging/timestamping: https://github.com/RayanOgh/Remote-viewing-commitment-scheme\n\n‚∏ª\n\nCore Capabilities: ‚Ä¢\tHMAC-SHA256 cryptographic commitment\n\n‚Ä¢\tDomain separation using a contextual prefix\n\n‚Ä¢\t32-byte key generation using os.urandom\n\n‚Ä¢\tDeterministic, tamper-evident output\n\n‚Ä¢\tConstant-time comparison via hmac.compare_digest\n\n‚Ä¢\tCanonicalization option for message normalization\n\n‚Ä¢\tFully offline operation\n\n‚Ä¢\tExecutable in restricted environments\n\n‚∏ª\n\nApplications:\n\n1.\t‚Å†Scientific Pre-Registration ‚Ä¢\tCommit to experimental hypotheses or outputs before public release\n2.\t‚Å†Anonymous Proof-of-Authorship ‚Ä¢\tTime-lock or hash-lock messages without revealing them until desired\n3.\t‚Å†Decentralized Accountability ‚Ä¢\tEnable individuals or groups to prove intent, statements, or evidence at a later time\n4.\t‚Å†Censorship Resistance ‚Ä¢\tContent sealed offline can be later verified despite network interference\n5.\t‚Å†Digital Self-Testimony ‚Ä¢\tIndividuals can seal claims about future events, actions, or beliefs for later validation\n6.\t‚Å†Secure Collaborative Coordination ‚Ä¢\tPrevent cheating in decision processes that require asynchronous commitment and later reveal\n7.\t‚Å†Education in Applied Cryptography ‚Ä¢\tTeaches secure commitment schemes with no prerequisite tooling\n8.\t‚Å†Blockchain-Adjacent Use ‚Ä¢\tWorks as an off-chain oracle verification mechanism or as a pre-commitment protocol\n\n‚∏ª\n\nDesign Philosophy:\n\nThe code does not represent innovation in algorithm design. It is a structural innovation in distribution, accessibility, and real-world usability. It converts high-trust commitment protocols into direct, deployable, offline-usable infrastructure. All functionality is transparent and auditable. Because it avoids dependency on complex libraries or hosted backends, it is portable across both privileged and under-resourced environments.\n\n‚∏ª\n\nConclusion:\n\nThis module allows anyone to generate cryptographic proofs of statements, events, or data without needing a company, a blockchain, or a third-party platform. The source code is auditable, adaptable, and already functioning. It is general-purpose digital infrastructure for public verifiability and personal integrity.\n\nUse cases are active. Implementation is immediate. The code is already working.",
    "author": "Difficult_Jicama_759",
    "timestamp": "2025-09-20T04:35:28",
    "url": "https://reddit.com/r/Python/comments/1nlvv14/pure_python_cryptographic_commitment_scheme/",
    "score": 0,
    "num_comments": 39,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nkq8pt",
    "title": "T-Strings: What will you do?",
    "content": "Good evening from my part of the world!\n\nI'm excited with the new functionality we have in Python 3.14. I think the feature that has caught my attention the most is the introduction of [t-strings](https://docs.python.org/3.14/whatsnew/3.14.html#pep-750-template-strings).\n\nI'm curious, what do you think will be a good application for t-strings? I'm planning to use them as better-formatted templates for a custom message pop-up in my homelab, taking information from different sources to format for display. Not reinventing any functionality, but certainly a cleaner and easier implementation for a message dashboard.\n\nPlease share your ideas below, I'm curious to see what you have in mind!",
    "author": "sikes01",
    "timestamp": "2025-09-18T18:22:59",
    "url": "https://reddit.com/r/Python/comments/1nkq8pt/tstrings_what_will_you_do/",
    "score": 129,
    "num_comments": 89,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nlwpe8",
    "title": "Fake OS - Worth making?",
    "content": "So, a while ago i discovered this repo on github: [https://github.com/crcollins/pyOS](https://github.com/crcollins/pyOS)\n\nIn summary, its a program trying to simulate an OS by having a kernel, programs (terminal commands), a filesystem etc.\n\nIve been impressed of the dedication for something that isnt useful in your everyday life. Though ive seen the small group of repositories making similar projects fascinating, and thought about making my own, but ive yet to come up a reason for it.\n\nSo here i am, wanting to ask:\n\nIs something like this worth making, following the structure of a real computer, containing a kernel, drivers, the OS layer, BIOS etc?\n\nWhat would be ways to make it useful / more interesting?\n\nAll feedback is appreciated, thanks in advance :O",
    "author": "BravestCheetah",
    "timestamp": "2025-09-20T05:18:48",
    "url": "https://reddit.com/r/Python/comments/1nlwpe8/fake_os_worth_making/",
    "score": 0,
    "num_comments": 21,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl14dr",
    "title": "Built a real-time debugging dashboard that works with any FastAPI app",
    "content": "# What My Project Does\n\nFastAPI Radar is a debugging dashboard that gives you complete visibility into your FastAPI applications. Once installed, it monitors and displays:\n\n* All HTTP requests and responses with timing data\n* Database queries with execution times\n* Exceptions with full stack traces\n* Performance metrics in real-time\n\nEverything is viewable through a clean web interface that updates live as your app handles requests. You access it at `/__radar/` while your app is running.\n\n# Target Audience\n\nThis is primarily for developers working with FastAPI during development and debugging. It's NOT meant for production use (though you can disable it in prod with a flag).\n\nIf you've ever found yourself adding print statements to debug API calls, wondering why an endpoint is slow, or trying to track down which queries are running, this tool is for you. It's especially useful when building REST APIs with FastAPI + SQLAlchemy.\n\nGitHub: [github.com/doganarif/fastapi-radar](http://github.com/doganarif/fastapi-radar)",
    "author": "doganarif",
    "timestamp": "2025-09-19T04:35:38",
    "url": "https://reddit.com/r/Python/comments/1nl14dr/built_a_realtime_debugging_dashboard_that_works/",
    "score": 17,
    "num_comments": 5,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nlwdh0",
    "title": "Idea for Open Source package",
    "content": "Hi all, I have a use for a proper Python equivalent to `knip`. Knip is a TypeScript/JavaScript package that performs complex dead code analysis. It's fast and pretty reliable - despite the huge complexities involved with the JS ecosystem. I don't know anything similar in Python. The best dead code analyzer I know is proprietary and is part of the IntelliJ Python plugin / PyCharm. \n\nSo, in a nutshell, it would be awesome if someone here decides to create this. In today age it should be written in Rust.\n",
    "author": "Goldziher",
    "timestamp": "2025-09-20T05:02:03",
    "url": "https://reddit.com/r/Python/comments/1nlwdh0/idea_for_open_source_package/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl763h",
    "title": "I made a Python wrapper for the Kick API (channels, videos, chat, clips)",
    "content": "**GitHub:** [https://github.com/Enmn/KickAPI](https://github.com/Enmn/KickAPI)\n\n**PyPi:** [https://pypi.org/project/KickApi/](https://pypi.org/project/KickApi/)\n\nHello everyone\n\n# What My Project Does\n\nI constructed \\*\\*KickAPI\\*\\*, a Python interface to the [Kick.com](http://Kick.com) API. Instead of dealing with raw JSON or writing boilerplate HTTP requests, now you can deal with \\*\\*organized Python classes\\*\\* like \\`Channel\\`, \\`Video\\`, \\`Chat\\`, and \\`Clip\\`.\n\n**This makes it easier:**\n\n* To get channel details (ID, username, followers, etc.)\n* To get video metadata (title, duration, views, source URL)\n* To browse categories with pagination\n* To fetch chat history\n* Obtain clip data\n\n# Target Audience\n\nThis library is mostly for:\n\n* \\*\\*Kick data experimenters\\*\\*\n* Those making \\*\\*bots, dashboards, or analytics tools\\*\\*\n* Hobbyists who are interested in the Kick API\n\nIt's \\*\\*not production-ready yet\\*\\*, but \\*\\*stable enough for side projects and experimentation\\*\\*.\n\n# Comparison\n\nTo the best of my knowledge, there isn't an existing, actively maintained \\*\\*Python wrapper\\*\\* for Kick's API.\n\n**KickAPI tries to fill that gap by:**\n\n* Providing direct \\*\\*Pythonic access\\*\\* to data\n* Handling \\*\\*request/response parsing\\*\\* internally\n* Offering a familiar interface similar to wrappers for other platforms\n\n# Work in Progress\n\n* Adding more endpoints\n* Improving error handling\n* More helper methods for convenience\n\n# Feedback\n\nI‚Äôd love feedback, suggestions, or contributions!  Pull requests are very welcome",
    "author": "Few-Independent8041",
    "timestamp": "2025-09-19T08:47:31",
    "url": "https://reddit.com/r/Python/comments/1nl763h/i_made_a_python_wrapper_for_the_kick_api_channels/",
    "score": 2,
    "num_comments": 4,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl9tk6",
    "title": "BleScope - Like a telescope for Bluetooth Low energy devices üî≠",
    "content": "Hello reddit,\n\nWhat my project does: This is a Bluetooth Low energy scanner application featuring a python backend and a web UI frontend to interact with the devices.\n\nTarget audience: Any hobbyist interested in python and Bluetooth Discovery\n\nComparison: To my knowledge, kismet and some abilities for Bluetooth Low energy devices, but not sure if we can interact with them.\n\nI've started a small project in order to explore the Bluetooth world and especially low energy Bluetooth devices.\n\nI know that project is somewhat already implemented in different other projects like kismet. But I wanted to go really deep with this project.\n\nFirstly to enrich my python and architectural pattern knowledge. Secondly to explore a completely unknown world to me which is the Bluetooth Low energy stuff. Finally, be able to use what I built to control my low energy devices through my home automation system which is running OpenHAB.\n\nRight now, the UI is only listing found devices, this is still pretty rough, but that's the foundation of the project. Next steps are adding interaction service to be able to connect to devices and read/write characteristics through GATT.\n\nThe UI a simple html using AlpineJS that run from the fastapi server. I don't feel the need to have a full separate frontend for now.\n\nAny constructive review will be appreciated as well as contribution if you want to üòä\n\nRight now, there is no tests. Yeah, this is bad üòÖ This is probably something that would need to be done urgently if the project grows. Anyone who feel comfortable to implement tests are welcome of course üòéüòÅ\n\nThe project is available here: \n[https://github.com/lion24/BleScope](https://github.com/lion24/BleScope)\n\nHappy hacking.",
    "author": "lion_24",
    "timestamp": "2025-09-19T10:27:06",
    "url": "https://reddit.com/r/Python/comments/1nl9tk6/blescope_like_a_telescope_for_bluetooth_low/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl5l83",
    "title": "Pips/Dominoes Solver",
    "content": "Hi everyone! I'd like to show off a neat side project I've been working on- a Pips/Dominoes puzzle solver!  \nI got the idea for this after doing some Leetcode problems and wondering what the most optimized way would be to tackle this type of puzzle. If you're unfamiliar with this game, check out Pips on the NYTGames site- there's 3 free puzzles every day.\n\n**TARGET AUDIENCE:**  \nAnyone interested in Pips/Dominoes puzzles, and wants more than just the daily puzzles provided by NYTGames. This is meant as a non-commercial toy project designed to give myself and others more to do with Pips.\n\n**Comparison:**  \nTo my knowledge, the only other resource similar to this project is [PipsGame.io](http://PipsGame.io), but they're closed-source compared to my project. And as mentioned, NYTGames runs the official game on their website, but currently their site doesn't provide an archive or more than 3 daily puzzles to do.\n\n**What My Project Does:**  \nMy intention was to implement backtracking and BFS to solve this like it was a Leetcode problem: backtracking to recursively place dominoes, and BFS to look for all connected tiles with the same constraint.  \nThe average time to solve a puzzle is 0.059 seconds, although there are some puzzles I've encountered- taking entire minutes- that I need to optimize the algorithm for.\n\nAny suggestions/feedback are appreciated, and I've provided my GitHub link if anyone wants to contribute! In the future, I'm hoping to also build a puzzle generator and flesh out this repository as a playable terminal game.\n\n**LINKS:**  \nGitHub Link:¬†[https://github.com/ematth/pips](https://github.com/ematth/pips)",
    "author": "Ematth",
    "timestamp": "2025-09-19T07:47:59",
    "url": "https://reddit.com/r/Python/comments/1nl5l83/pipsdominoes_solver/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nlux1e",
    "title": "What should I do to start earning fast ?",
    "content": "I am currently on loop on python and I feeling I want money from python as Soon as possible as a freelancer what should I learn by using python that I can start earning money ",
    "author": "Red_Priest0",
    "timestamp": "2025-09-20T03:42:28",
    "url": "https://reddit.com/r/Python/comments/1nlux1e/what_should_i_do_to_start_earning_fast/",
    "score": 0,
    "num_comments": 21,
    "upvote_ratio": 0.14,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl74ue",
    "title": "Advice on optimizing my setup",
    "content": "I‚Äôve built a Django-based web application that provides a streamlined trading and auctioning platform for specialized used industrial tooling. At present, it‚Äôs actively used by five smaller companies, and while the system doesn‚Äôt support automated payments, all transactions are handled manually. That said, it‚Äôs critical that order placement and price determination remain consistently accurate to ensure proper \"manual\" accounting.\n\nThe application is currently deployed on a VPS using Docker Compose, with PostgreSQL running on a local volume. All on the same single machine. Although I don‚Äôt anticipate significant user growth/increased load, the platform has gained traction among clients, and I‚Äôm now looking to optimize the infrastructure for reliability and maintainability. In essence to safe time and for peace of mind. It does not generate too much revenue, so i would only be able to afford around 25-50 dollars per month for everything.\n\nMy goal is to simplify infrastructure management without incurring high costs‚Äîideally with a setup that‚Äôs secure, easy to operate, and resilient. A key priority is implementing continuous database backups, preferably stored on a separate system to safeguard against data loss.",
    "author": "Successful-Glass-919",
    "timestamp": "2025-09-19T08:46:12",
    "url": "https://reddit.com/r/Python/comments/1nl74ue/advice_on_optimizing_my_setup/",
    "score": 2,
    "num_comments": 7,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nkzmy7",
    "title": "prob_conf_mat - Statistical inference for classification experiments and confusion matrices",
    "content": "[`prob_conf_mat`](https://github.com/ioverho/prob_conf_mat) is a library I wrote to support my statistical analysis of classification experiments. It's now at the point where I'd like to get some external feedback, and before sharing it with its intended audience, I was hoping some interested r/Python users might want to take a look first.\n\nThis is the first time I've ever written code with others in mind, and this project required learning many new tools and techniques (e.g., unit testing, Github actions, type checking, pre-commit checks, etc.). I'm very curious to hear whether I've implemented these correctly, and generally I'd love to get some feedback on the readability of the documentation.\n\nPlease don't hesitate to ask any questions; I'll respond as soon as I can.\n\n# What My Project Does\n\nWhen running a classification experiment, we typically evaluate a classification model's performance by evaluating it on some held-out data. This produces a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), which is a tabulation of which class the model predicts when presented with an example from some class. Since confusion matrices are hard to read, we usually summarize them using classification metrics (e.g., accuracy, F1, MCC). If the metric achieved by our model is better than the value achieved by another model, we conclude that our model is better than the alternative.\n\nWhile very common, this framework ignores a lot of information. There's no accounting for the amount of uncertainty in the data, for sample sizes, for different experiments, or for the size of the difference between metric scores.\n\nThis is where [`prob_conf_mat`](https://github.com/ioverho/prob_conf_mat) comes in. It quantifies the uncertainty in the experiment, it allows users to combine different experiments into one, and it enables statistical significance testing. Broadly, theit does this by sampling many plausible counterfactual confusion matrices, and computes metrics over all confusion matrices to produce a distribution of metric values. In short, with very little additional effort, it enables rich statistical inferences about your classification experiment.\n\n# Example\n\nSo instead of doing:\n\n    &gt;&gt;&gt; import sklearn\n    &gt;&gt;&gt; sklearn.metrics.f1_score(model_a_y_true, model_a_y_pred, average=\"macro\")\n    0.75\n    &gt;&gt;&gt; sklearn.metrics.f1_score(model_b_y_true, model_a_b_pred, average=\"macro\")\n    0.66\n    &gt;&gt;&gt; 0.75 &gt; 0.66\n    True\n\nNow you can do:\n\n    &gt;&gt;&gt; import prob_conf_mat\n    &gt;&gt;&gt; study = prob_conf_mat.Study()        # Initialize a Study\n    &gt;&gt;&gt; study.add_experiment(\"model_a\", ...) # Add data from model a\n    &gt;&gt;&gt; study.add_experiment(\"model_b\", ...) # Add data from model b\n    &gt;&gt;&gt; study.add_metric(\"f1@macro\", ...)    # Add a metric to compare them\n    &gt;&gt;&gt; study.plot_pairwise_comparison(      # Compare the experiments\n        metric=\"f1@macro\",\n        experiment_a=\"model_a\",\n        experiment_b=\"model_b\",\n        min_sig_diff=0.005,\n    )\n\n[Example difference distribution figure](https://github.com/ioverho/prob_conf_mat/raw/main/documentation/assets/figures/readme/comparison_plot.svg)\n\nNow you can tell how probable it is that \\`model\\_a\\` is actually better, and whether this difference is statistically significant or not.\n\nThe ['Getting Started' chapter of the documentation](https://www.ivoverhoeven.nl/prob_conf_mat/Getting%20Started/index.html) has a lot more examples.\n\n# Target Audience\n\nThis was built for anyone who produces confusion matrices and wants to analyze them. I expect that it will mostly be interesting for those in academia: scientists, students, statisticians and the like. The documentation is hopefully readable for anyone with some machine-learning/statistics background.\n\n# Comparison\n\nThere are many, many excellent Python libraries that handle confusion matrices, and compute classification metrics (e.g., [`scikit-learn`](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html), [`TorchMetrics`](https://lightning.ai/docs/torchmetrics/stable/), [`PyCM`](https://github.com/sepandhaghighi/pycm), *inter alia*).\n\nThe most famous of these is probably `scikit-learn`. `prob-conf-mat` implements all metrics currently in `scikit-learn` (plus some more) and tests against these to ensure equivalence. We also enable class averaging for all metrics through a single interface.\n\nFor the statistical inference portion (i.e., what sets `prob_conf_mat` apart), to the best of my knowledge, there are no viable alternatives.\n\n# Design &amp; Implementation\n\nMy primary motivation for this project was to learn, and because of that, **I do not** **use AI tools**. Going forward this might change (although minimally).\n\n# Links\n\nGithub: [https://github.com/ioverho/prob\\_conf\\_mat](https://github.com/ioverho/prob_conf_mat)\n\nHomepage: [https://www.ivoverhoeven.nl/prob\\_conf\\_mat/](https://www.ivoverhoeven.nl/prob_conf_mat/)\n\nPyPi: [https://pypi.org/project/prob-conf-mat/](https://pypi.org/project/prob-conf-mat/)",
    "author": "ioverho",
    "timestamp": "2025-09-19T03:12:21",
    "url": "https://reddit.com/r/Python/comments/1nkzmy7/prob_conf_mat_statistical_inference_for/",
    "score": 4,
    "num_comments": 0,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nkvz9d",
    "title": "StampDB ‚Äì A tiny C++ Time Series Database with a NumPy-native Python API",
    "content": "Hey everyone üëã\n\n# What My Project Does\n\nI‚Äôve been working on a small side project called **StampDB**, a lightweight time series database written in C++ with a clean Python wrapper.\n\nThe idea is to provide a **minimal, NumPy-native interface** for time series data, without the overhead of enterprise-grade database systems. It‚Äôs designed for folks who just need a simple, fast way to manage time series in Python, especially in research or small-scale projects.\n\n### Features\n\n* C++ core with CSV-based storage + schema validation\n* NumPy-native API for Python users\n* In-memory indexing + append-only disk writes\n* Simple relational algebra (selection, projection, joins, etc.) on NumPy structured arrays\n* Atomic writes + compaction on close\n\n# Comparison\n\nNot the main goal, but still fun to test ‚Äî StampDB runs:\n\n* **2√ó faster writes**\n* **30√ó faster reads**\n* **50√ó faster queries** ‚Ä¶ compared to tinyflux (a pure Python time series DB).\n\n# Target Audience\n\n### Not for you if you need\n\n* Multi-process or multi-threaded access\n* ACID guarantees\n* High scalability\n\n# üîó Links\n\n* [https://github.com/aadya940/stampdb](https://github.com/aadya940/stampdb)\n\nWould love feedback, especially from anyone who‚Äôs worked with time series databases. This is mostly an educational work done while reading \"Designing Data Intensive Applications\".",
    "author": "Lost-Dragonfruit-663",
    "timestamp": "2025-09-18T23:19:02",
    "url": "https://reddit.com/r/Python/comments/1nkvz9d/stampdb_a_tiny_c_time_series_database_with_a/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.78,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl1mv8",
    "title": "[Project] turboeda ‚Äî one-command EDA HTML report (pandas + Plotly)",
    "content": "Hi everyone, I built a small open-source tool called **turboeda** and wanted to share it in case it‚Äôs useful to others.\n\nWhat it does\n- Reads CSV/XLSX (CSV encoding auto-detected; Excel defaults to first sheet unless --sheet is set)\n- Runs a quick EDA pipeline (summary, missingness, numeric/categorical stats, datetime insights)\n- Outputs an interactive HTML report (Plotly), with dark/light themes\n- Includes correlation heatmaps (numeric-only), histograms, bar charts, top categories\n- Works from the CLI and in Jupyter\n\nInstall\n    pip install turboeda\n\nCLI\n    turboeda \"data.csv\" --open\n    # Excel:\n    turboeda \"data.xlsx\" --sheet \"Sheet1\" --open\n\nPython / Jupyter\n    from turboeda import EDAReport\n    report = EDAReport(\"data.csv\", theme=\"dark\", auto_save_and_open=True)\n    res = report.run()\n    # optional:\n    # report.to_html(\"report.html\", open_in_browser=True)\n\nLinks\n- PyPI: https://pypi.org/project/turboeda/\n- Source: https://github.com/rozsit/turboeda\n\nIt‚Äôs still young; feedback, issues, and PRs are very welcome. MIT licensed. Tested on Python 3.9‚Äì3.12 (Windows/macOS/Linux).\n\nThanks for reading!",
    "author": "rozsit",
    "timestamp": "2025-09-19T05:01:24",
    "url": "https://reddit.com/r/Python/comments/1nl1mv8/project_turboeda_onecommand_eda_html_report/",
    "score": 2,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nkc512",
    "title": "prek a fast (rust and uv powered) drop in replacement for pre-commit with monorepo support!",
    "content": "I wanted to let you know about a tool I switched to about a month ago called prek: https://github.com/j178/prek?tab=readme-ov-file#prek\n\nIt's a drop in replacement for pre-commit, so there's no need to change any of your config files, you can install and type `prek` instead of `pre-commit`, and switch to using it for your git precommit hook by running `prek install -f`.\n\nIt has a few advantage over pre-commit:\n\n* Core hooks re-written in Rust for better performance\n* Uses uv to install Python dependencies so non-cached runs are much faster\n* Can be installed without needing to set up a Python environment: https://github.com/j178/prek?tab=readme-ov-file#installation\n* Monorepo support as of 0.2.0:  https://github.com/j178/prek/releases/tag/v0.2.0\n* Automatic PEP 723 in-line metadata dependency installation: https://github.com/j178/prek/pull/529\n\nIt's still early days for prek, but the large project apache-airflow has adopted it (https://github.com/apache/airflow/pull/54258), is taking advantage of monorepo support (https://github.com/apache/airflow/pull/54615) and PEP 723 dependencies (https://github.com/apache/airflow/pull/54917). So it already has a lot of exposure to real world development.\n\nWhen I first reviewed the tool I found a couple of bugs and they were both fixed within a few hours of reporting them. Since then I've enthusiastically adopted prek, largely because while pre-commit is stable it is very stagnant, the pre-commit author actively blocks suggesting using new packaging standards, so I am excited to see competition in this space.",
    "author": "zurtex",
    "timestamp": "2025-09-18T08:56:21",
    "url": "https://reddit.com/r/Python/comments/1nkc512/prek_a_fast_rust_and_uv_powered_drop_in/",
    "score": 77,
    "num_comments": 4,
    "upvote_ratio": 0.98,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl300g",
    "title": "Published my first PyPI package: cohens-d-effect-size - Cohen's d effect size calculator",
    "content": "    Hey r/Python! \n    \n    I just published my first package to PyPI and wanted to share it with the community: **cohens-d-effect-size**\n    \n    # What My Project Does\n    Cohen's d is a measure of effect size used in statistics, especially in research and data science. While there are existing Cohen's d packages available, I wanted to create a more comprehensive implementation that handled edge cases better and followed NumPy/SciPy conventions more closely.\n    \n    # Key features\n    - **One-sample and two-sample Cohen's d** calculations\n    - **Multi-dimensional array support** with axis specification\n    - **Missing data handling** (propagate, raise, or omit NaN values)\n    - **Pooled vs unpooled variance** options\n    - **Full NumPy compatibility** with broadcasting\n    - **23 comprehensive tests** covering edge cases\n    \n    # Installation\n    ¬† ¬† pip install cohens-d-effect-size\n    \n    # Quick example\n    ¬† ¬† import numpy as np\n    ¬† ¬† from cohens_d import cohens_d\n    \n    ¬† ¬† # Two-sample Cohen's d\n    ¬† ¬† control = np.array([1, 2, 3, 4, 5])\n    ¬† ¬† treatment = np.array([3, 4, 5, 6, 7])\n    ¬† ¬† effect_size = cohens_d(control, treatment)\n    ¬† ¬† print(f\"Cohen's d: {effect_size:.3f}\") ¬†# Output: Cohen's d: -1.265\n    \n    # Comparison to Existing Solutions\n    While there are existing Cohen's d packages like `cohens-d` (by Duncan Tulimieri), my package offers several advantages:\n    \n    - **Multi-dimensional support**: Handle arrays with multiple dimensions and axis specification\n    - **Better error handling**: Comprehensive validation and clear error messages ¬†\n    - **SciPy conventions**: Follows established patterns from scipy.stats\n    - **Missing data policies**: Flexible NaN handling (propagate/raise/omit)\n    - **Broadcasting support**: Full NumPy compatibility for complex operations\n    - **Extensive testing**: 23 comprehensive tests covering edge cases\n    - **Professional packaging**: Modern packaging standards with proper metadata\n    \n    The existing `cohens-d` package is more basic and doesn't handle multi-dimensional arrays or provide the same level of configurability.\n    \n    # Links\n    - **PyPI**: https://pypi.org/project/cohens-d-effect-size/\n    - **GitHub**: https://github.com/DawitLam/cohens-d-scipy\n    - **Documentation**: Full README with examples and API docs\n    \n    This was an incredible learning experience in Python packaging, testing, and following community standards. I learned a lot about:\n    - Proper package structure and metadata\n    - Comprehensive testing with pytest\n    - Following SciPy API conventions\n    - NumPy compatibility and broadcasting rules\n    \n    **Feedback and suggestions are very welcome!** I'm planning to propose this for inclusion in SciPy eventually, so any input on the API design or implementation would be appreciated.\n    \n    Thanks for being such a supportive community!\n    ",
    "author": "Future-Pen-2493",
    "timestamp": "2025-09-19T06:03:26",
    "url": "https://reddit.com/r/Python/comments/1nl300g/published_my_first_pypi_package_cohensdeffectsize/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nkit7n",
    "title": "Dou you use jit compilation with numba?",
    "content": "Is it common among experienced python devs and what is the scope of it (where it cannot be used really). Or do you use other optimization tools like that?",
    "author": "husayd",
    "timestamp": "2025-09-18T13:05:59",
    "url": "https://reddit.com/r/Python/comments/1nkit7n/dou_you_use_jit_compilation_with_numba/",
    "score": 19,
    "num_comments": 31,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nlaq8e",
    "title": "What do you need to know to make a simple text adventure game, or just a text game in Python ???",
    "content": "THE MODERATORS SAID MY BODY TEXT NEEDS TO BE AT LEAST 120 CHARACTERS LONG. I DON'T KNOW WHY IT SAYS IT'S OPTIONAL SO I,M WRITING THIS.",
    "author": "SkyDwag187",
    "timestamp": "2025-09-19T11:01:04",
    "url": "https://reddit.com/r/Python/comments/1nlaq8e/what_do_you_need_to_know_to_make_a_simple_text/",
    "score": 0,
    "num_comments": 23,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nlaekl",
    "title": "I have a very important question.",
    "content": "I was looking to get in python application development, but I need a clear and easy roadmap,  \nFor my frontend i chose PyQt6 and Tkinter, but now im confused, what do i learn for the backend, for file management i chose OS but for dashboards, graphs, etc. (libraries to make proper applications)",
    "author": "One_Ranger_5979",
    "timestamp": "2025-09-19T10:48:43",
    "url": "https://reddit.com/r/Python/comments/1nlaekl/i_have_a_very_important_question/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nk6vma",
    "title": "UV issues in corporate env",
    "content": "I am trying uv for the first time in a corporate environment. I would like to make sure I understand correctly: \n\n- uv creates a virtual env in the projects folder, and it stores all dependencies in there. So, for a quick data processing job with pandas and marimo, I will keep 200Mb+ worth of library and auxiliary files. If I have different folders for different projects, this will be duplicated over on each. Maybe there is a way to set central repositories, but I already have conda for that. \n\n- uv automatically creates a git repository for the project. This is fine in principle, but unfortunately OneDrive, Dropbox and other sync tools choke on the .git folder. Too many files and subfolders. I have had problems in the past. \n\nI am not sure uv is for me. How do you guys deal with these issues? Thanks",
    "author": "jabellcu",
    "timestamp": "2025-09-18T05:26:43",
    "url": "https://reddit.com/r/Python/comments/1nk6vma/uv_issues_in_corporate_env/",
    "score": 35,
    "num_comments": 151,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nknfji",
    "title": "Free eBook - Working with Files in Python 3",
    "content": "I enjoy helping out folks in the Python 3 community.\n\nIf you are interested, you can click the top link on my landing page and download my eBook, \"Working with Images Python 3\" for free:¬†[https://linktr.ee/chris4sawit](https://linktr.ee/chris4sawit)\n\nThere are other free Python eBooks there as well, so feel free to grab what you want.\n\nI hope this 19 page pdf will be useful for someone interested in working with Images in Python with a special focus on the Pillow library. \n\nSince it is sometimes difficult to copy/paste from a pdf, I've added a .docx and .md version as well. The link will download all files in the project. Also included are the image files used in the code samples.  No donations will be requested. \n\nOnly info needed is a name and email address to get the download link. If you don't care to provide your name, that's fine; please feel free to use any alias.",
    "author": "caudor",
    "timestamp": "2025-09-18T16:12:19",
    "url": "https://reddit.com/r/Python/comments/1nknfji/free_ebook_working_with_files_in_python_3/",
    "score": 4,
    "num_comments": 3,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nkohvq",
    "title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays",
    "content": "# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è\n\nWelcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!\n\n## How it Works:\n\n1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.\n2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.\n3. **News &amp; Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.\n\n## Guidelines:\n\n* All topics should be related to Python or the /r/python community.\n* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).\n\n## Example Topics:\n\n1. **New Python Release**: What do you think about the new features in Python 3.11?\n2. **Community Events**: Any Python meetups or webinars coming up?\n3. **Learning Resources**: Found a great Python tutorial? Share it here!\n4. **Job Market**: How has Python impacted your career?\n5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!\n6. **Community Ideas**: Something you'd like to see us do? tell us.\n\nLet's keep the conversation going. Happy discussing! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-18T17:00:50",
    "url": "https://reddit.com/r/Python/comments/1nkohvq/friday_daily_thread_rpython_meta_and_freetalk/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl79k2",
    "title": "Small Python trick that saved me hours on client work",
    "content": "Hey Reddit,\n\nWhile working on client WordPress sites, I recently used Python to automate a repetitive task, it saved me about 5 hours of work in a single week.\n\nSeeing something I coded actually save real time felt amazing.\n\nFreelancers and developers here, what‚Äôs your favorite small automation trick that‚Äôs made your life easier?",
    "author": "Striking-Pizza1443",
    "timestamp": "2025-09-19T08:51:15",
    "url": "https://reddit.com/r/Python/comments/1nl79k2/small_python_trick_that_saved_me_hours_on_client/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nk9v3h",
    "title": "Looking for feedback: Making Python Deployments Easy",
    "content": "Hey r/Python,\n\nWe've been experimenting with how to make Python deployment easier and would love your thoughts.\n\nAfter building Shuttle for Rust, we're exploring whether the same patterns work well in Python.\n\nWe built [Shuttle Cobra](https://github.com/shuttle-hq/shuttle-cobra), a Python framework that lets you define AWS infrastructure using Python decorators and then using the Shuttle CLI `shuttle deploy` to deploy your code to your own AWS account.\n\nHere's what it looks like:\n\n    from typing import Annotated\n    from shuttle_aws.s3 import AllowWrite\n    \n    TABLE = \"record_counts\"\n    \n    @shuttle_task.cron(\"0 * * * *\")\n    async def run(\n        bucket: Annotated[\n            Bucket,\n            BucketOptions(\n                bucket_name=\"grafana-exporter-1234abcd\",\n                policies=[\n                    AllowWrite(account_id=\"842910673255\", role_name=\"SessionTrackerService\")\n                ]\n            )\n        ],\n        db: Annotated[RdsPostgres, RdsPostgresOptions()],\n    ):\n        # ...\n\nThe goal is simplicity and ease of use, we want developers to focus on writing application code than managing infra. The CLI reads your type hints to understand what AWS resources you need, then generates CloudFormation templates automatically and deploys to your own AWS account. You will still be using the official AWS libraries so migration will be seamless by just adding a few lines of code.\n\nRight now the framework is only focused on Python CRON jobs but planning to expand to other use cases.\n\nWe're looking for honest feedback on a few things. Does this approach feel natural in Python, or does it seem forced? How does this compare to your current deployment workflow? Is migration to this approach easy? What other AWS resources would be most useful to have supported? Do you have any concerns about mixing infrastructure definitions with application code?\n\nThis is experimental - we're trying to understand if IfC patterns that work well in Rust translate effectively to Python. The Python deployment ecosystem already has great tools, so we want to know if this adds value or just complexity.\n\n**Resources:**\n\n* [Full Article](https://www.shuttle.dev/blog/2025/09/18/introducing-shuttle-cobra?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=shuttle_cobra_launch)\n* [GitHub Repository](https://github.com/shuttle-hq/shuttle-cobra)\n* [Shuttle Cobra Docs](https://docs.cobra.shuttle.dev/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=python_cobra_feedback)\n\nThanks for any feedback - positive or negative. Trying to understand if this direction makes sense for the Python community.",
    "author": "openquery",
    "timestamp": "2025-09-18T07:30:20",
    "url": "https://reddit.com/r/Python/comments/1nk9v3h/looking_for_feedback_making_python_deployments/",
    "score": 6,
    "num_comments": 5,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nkwv1v",
    "title": "Best Way to Scrape Amazon?",
    "content": "I‚Äôm scraping product listings, reviews, but rotating datacenter proxies doesn‚Äôt cut it anymore. Even residential proxies sometimes fail. I added headless Chrome rendering but it slowed everything down. Is anyone here successfully scraping Amazon? Does an API solve this better, or do you still need to layer proxies + browser automation?",
    "author": "MetalGoatP3AK",
    "timestamp": "2025-09-19T00:14:26",
    "url": "https://reddit.com/r/Python/comments/1nkwv1v/best_way_to_scrape_amazon/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.36,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nkiwc3",
    "title": "Streaming BLE Sensor Data into Microsoft Power BI using Python",
    "content": "This project demonstrate how to stream¬†**Bluetooth Low Energy (BLE) sensor data**¬†directly into¬†**Microsoft Power BI**¬†using Python. By combining a HibouAir environmental sensor with BleuIO and a simple Python script, we can capture live readings of¬†**CO2, temperature, and humidity**¬†and display them in real time on a Power BI dashboard for further analysis.   \ndetails and source code available here\n\n[https://www.bleuio.com/blog/streaming-ble-sensor-data-into-microsoft-power-bi-using-bleuio/](https://www.bleuio.com/blog/streaming-ble-sensor-data-into-microsoft-power-bi-using-bleuio/)\n\n",
    "author": "bleuio",
    "timestamp": "2025-09-18T13:09:09",
    "url": "https://reddit.com/r/Python/comments/1nkiwc3/streaming_ble_sensor_data_into_microsoft_power_bi/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nl0336",
    "title": "anyone here to teach me python",
    "content": "i am new to this python world so can someone teach me python I can put 2 hr for 5 days every week and i am adding this extra info just to reach the word limit ",
    "author": "Dry-Leave8217",
    "timestamp": "2025-09-19T03:39:05",
    "url": "https://reddit.com/r/Python/comments/1nl0336/anyone_here_to_teach_me_python/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.09,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nkttgm",
    "title": "Python script to .exe - is this still a thing?",
    "content": "Hello,\n\nI've built a ‚Äúlittle‚Äù tool that lets you convert a Python script (or several) into an exe file.\n\nIt's really easy to use:\n\nYou don't even need to have Python installed to use it.\n\nWhen you start it up, a GUI appears where you can select your desired Python version from a drop-down menu.\n\nYou specify the folder where the Python scripts are located.\n\nThen you select the script that you want to be started first.\n\nNow you can give your exe file a name and add an icon.\n\n\n\nOnce you have specified the five parameters, you can choose whether you want a ‚Äúonefile‚Äù or a folder with the finished bundle.\n\nPython is now compiled in the desired version.\n\n\n\nThen a little black magic happens and the Python scripts are searched for imports. If libraries are not found, an online search is performed on pypi. If several candidates are available, a selection menu appears where you must choose the appropriate one. For example, opencv: the import is: import cv2, and the installation package is called opencv-python.\n\nOnce you've imported the history, the PC does a little calculation and you get either a single exe file containing everything, as selected, or a folder structure that looks like this:\n\nFolder\n\n\\-- pgmdata/\n\n\\-- python/\n\n\\-- myProgram.exe\n\n\n\nYou can now distribute the exe or folder to any computer and start it. So you don't have to install anything, nor does anything change on the system.\n\n  \n\n\nNow to my question: Is this even a thing anymore these days? I mean, before I go to the trouble of polishing it all up and uploading it to GitHub. Tools like cxfreeze and py2exe have been around forever, but will they even still be used in 2025? ",
    "author": "Icy-Farm9432",
    "timestamp": "2025-09-18T21:16:44",
    "url": "https://reddit.com/r/Python/comments/1nkttgm/python_script_to_exe_is_this_still_a_thing/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.32,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nk1urz",
    "title": "tenets - CLI and API to aggregate context from relevant files for your prompts",
    "content": "**What My Project Does**\n\nI work a lot with AI pair programming tools, for implementations, code refactoring, writing tons of docs and tests, and I find they are surprisingly weak at navigating repos (the directory they have access to) when responding to and understanding what you're asking. Simply tracing the methods and imports in a relevant file or two is too limited when we have projects with hundreds of files and 100k+ LOC.\n\nI built and launched tenets, a CLI and library to gather the right files and context automatically for your LLM prompts, living at¬†[https://tenets.dev](https://tenets.dev/), or¬†[https://github.com/jddunn/tenets](https://github.com/jddunn/tenets)¬†for the direct source. Install with one command:\n\n`pip install tenets`\n\nand run:\n\n`tenets distill \"fix my bugs in the rest API authentication\"`\n\nsomewhere and you'll get the most important file and their contents relevant to your prompt, optimized to fit into token budgets and summarized smartly (like imports being condensed or non-important functions truncated) as needed.\n\nYou can run the same command:\n\n`tenets rank \"fix my bugs in the rest API authentication\"`\n\nand you'll get a list of files (at a much faster speed) on their own. Think of tenets like repomix on steroids, all automatic (no manual searches) with deterministic NLP analysis like BM25 and optional semantic understandings with embeddings.\n\nWith tenets you also get code intelligence and optional visualization tools to measure metrics, velocity, and evolution of your codebase over time, with outputs in SVG, PNG, JSON, and HTML.\n\n**Target Audience**¬†\n\nI built this out as a tool for personal needs that I think will have value not just for users but potential programmatic usage in coding assistants; as such, tenets has a well-documented API (https://tenets.dev/latest/api/).\n\n**Comparison**¬†\n\nProjects like repomix aggregate files with manual selection. I don't know of many other libraries with the same design goals and intentions as tenets.",
    "author": "PermissionNo4771",
    "timestamp": "2025-09-18T00:29:49",
    "url": "https://reddit.com/r/Python/comments/1nk1urz/tenets_cli_and_api_to_aggregate_context_from/",
    "score": 4,
    "num_comments": 2,
    "upvote_ratio": 0.84,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1njo1k2",
    "title": "Where's a good place to find people to talk about projects?",
    "content": "I'm a hobbyist programmer, dabbling in coding for like 20 years now, but never anything professional minus a three month stint. I'm trying to work on a medium sized Python project but honestly, I'm looking to work with someone who's a little bit more experienced so I can properly learn and ask questions instead of being reliant on a hallucinating chat bot.\n\nBut where would be the best place to discuss projects and look for like minded folks? ",
    "author": "InterstellarExpanse",
    "timestamp": "2025-09-17T13:17:05",
    "url": "https://reddit.com/r/Python/comments/1njo1k2/wheres_a_good_place_to_find_people_to_talk_about/",
    "score": 37,
    "num_comments": 10,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1njiy79",
    "title": "BS4 vs xml.etree.ElementTree",
    "content": "\n\nBeautiful Soup or standard library (xml.etree.ElementTree)? I am building an ETL process for extracting notes from Evernote ENML. I hear BS4 is easier but standard library performs faster. This alone makes me want to stick with the standard library. Any reason why I should reconsider?",
    "author": "ndeans",
    "timestamp": "2025-09-17T10:06:55",
    "url": "https://reddit.com/r/Python/comments/1njiy79/bs4_vs_xmletreeelementtree/",
    "score": 22,
    "num_comments": 17,
    "upvote_ratio": 0.88,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nkfla2",
    "title": "üöÄ Dispytch ‚Äî async Python framework for building event-driven services",
    "content": "Hey folks!  \nCheck out [**Dispytch**](https://github.com/e1-m/dispytch) ‚Äî async Python framework for building event-driven services.\n\n# üöÄ What Dispytch Does\n\nDispytch makes it easy to build services that react to events ‚Äî whether they're coming from Kafka, RabbitMQ, Redis or some other broker. You define event types as Pydantic models and wire up handlers with dependency injection. Dispytch handles validation, retries, and routing out of the box, so you can focus on the logic.\n\n# ‚öîÔ∏è Comparison\n\n|Framework|Focus|Notes|\n|:-|:-|:-|\n|Celery|Task queues|Great for backgroud processing|\n|Faust|Kafka streams|Powerful, but streaming-centric|\n|Nameko|RPC services|Sync-first, heavy|\n|FastAPI|HTTP APIs|Not for event processing|\n|FastStream|Stream pipelines|Built around streams‚Äîgreat for  data pipelines.|\n|**Dispytch**|Event handling|Event-centric and reactive, designed for clear event-driven services.|\n\n# ‚úçÔ∏è Quick API Example\n\n# Handler\n\n    user_events.handler(topic='user_events', event='user_registered')\n    async def handle_user_registered(\n            event: Event[UserCreatedEvent],\n            user_service: Annotated[UserService, Dependency(get_user_service)]\n    ):\n        user = event.body.user\n        timestamp = event.body.timestamp\n    \n        print(f\"[User Registered] {user.id} - {user.email} at {timestamp}\")\n    \n        await user_service.do_smth_with_the_user(event.body.user)\n\n# Emitter\n\n    async def example_emit(emitter):\n       await emitter.emit(\n           UserRegistered(\n               user=User(\n                   id=str(uuid.uuid4()),\n                   email=\"example@mail.com\",\n                   name=\"John Doe\",\n               ),\n               timestamp=int(datetime.now().timestamp()),\n           )\n       )\n\n# üéØ Features\n\n* ‚ö° Async core\n* üîå FastAPI-style DI\n* üì® Kafka, RabbitMQ and Redis PubSub out of the box\n* üß± Composable, override-friendly architecture\n* ‚úÖ Pydantic-based validation\n* üîÅ Built-in retry logic\n\nüëÄ Try it out:\n\n    uv add dispytch\n\nüìö Docs and examples in the repo: [https://github.com/e1-m/dispytch](https://github.com/e1-m/dispytch)\n\nFeedback, bug reports, feature requests ‚Äî all welcome.\n\nThanks for checking it out!",
    "author": "e1-m",
    "timestamp": "2025-09-18T11:04:04",
    "url": "https://reddit.com/r/Python/comments/1nkfla2/dispytch_async_python_framework_for_building/",
    "score": 0,
    "num_comments": 8,
    "upvote_ratio": 0.31,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1njmvk2",
    "title": "Is JetBrains really able to collect data from my code files through its AI service?",
    "content": "I can't tell if I'm misunderstanding this setting in PyCharm about data collection.\n\nThis is the only setting I could find that allows me to disable data collection via AI APIs, in Appearance &amp; Behavior &gt; System Settings &gt; Data Sharing:\n\n&gt;Allow detailed data collection by JetBrains AI  \nTo measure and improve integration with JetBrains AI, we can collect non-anonymous information about its usage, which includes the full text of inputs sent by the IDE to the large language model and its responses, including source code snippets.  \nThis option enables or disables the detailed data collection by JetBrains AI in all IDEs.  \nEven if this setting is disabled, the AI Assistant plugin will send the data essential for this feature to large language model providers and models hosted on JetBrains servers. If you work on a project where you don't want to share your data, you can disable the plugin.\n\nI'm baffled by what this is saying but maybe I'm mis-reading it? It sounds like there's no way to actually prevent JetBrains from reading source files on my computer which then get processed by its AI service for the purpose of code generation/suggestions.\n\nThis feels alarming to me due to the potential for data mining and data breaches. How can anyone feel safe coding a real project with it, especially with sensitive information? It sounds like disabling it does not actually turn it off? And what is classified as \"essential\" data? Like I don't want anything in my source files shared with anyone or anything, what the hell.",
    "author": "Effective-Koala-9956",
    "timestamp": "2025-09-17T12:31:58",
    "url": "https://reddit.com/r/Python/comments/1njmvk2/is_jetbrains_really_able_to_collect_data_from_my/",
    "score": 12,
    "num_comments": 13,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nk7bet",
    "title": "Prompture: Get reliable JSON from LLMs with validation + usage tracking",
    "content": "Hi everyone! üëã\n\nOne of the biggest headaches I had with LLMs was getting messy or inconsistent outputs when I really needed **structured JSON**.\n\nSo I built **Prompture** a Python library that makes LLMs return clean, validated JSON every time.\n\n**What my project does:**\n\n* Forces JSON output from LLMs (validated with `jsonschema`)\n* Works with multiple drivers: OpenAI, Claude, Ollama, Azure, HTTP, mock\n* Tracks tokens + costs automatically for every call\n* Lets you run the same prompt across different models and compare results\n* Generates reports (validation status, usage stats, execution times, etc.)\n\n**Target audience:**\n\n* Developers tired of parsing unreliable AI outputs\n* Teams who need reproducible structured data from LLMs\n* Makers who want to compare models on the same tasks\n\n**Comparison:**\n\nI know Ollama added structured outputs, which is great if you‚Äôre only using their models. Prompture takes the same idea but makes it universal: you‚Äôre not locked into one ecosystem, the outputs are validated against your schema, and you get cost + usage stats built in. For me it‚Äôs been a huge upgrade in terms of reliability and testing across providers.\n\nüìÇ GitHub: [https://github.com/jhd3197/Prompture](https://github.com/jhd3197/Prompture)  \nüåç PyPi: [https://pypi.org/project/prompture/](https://pypi.org/project/prompture/)\n\nWould love feedback, suggestions, or ideas for features you'd like to see! üôå And hey‚Ä¶ don‚Äôt forget to ‚≠ê if you find it useful ‚ú®",
    "author": "jhd3197",
    "timestamp": "2025-09-18T05:46:19",
    "url": "https://reddit.com/r/Python/comments/1nk7bet/prompture_get_reliable_json_from_llms_with/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1njkn7a",
    "title": "Fast-Channels: WebSocket + Layer Utility Port/Based on Django Channels",
    "content": "Hi all üëã\n\n**Sharing my new package**: **fast-channels** - Django Channels-inspired WebSocket library for FastAPI/Starlette and any ASGI framework.\n\n## What My Project Does\n\nFast-channels brings Django Channels' proven consumer patterns and channel layers to FastAPI/Starlette and any ASGI framework. It enables:\n\n- **Group messaging** - Send to multiple WebSocket connections simultaneously\n- **Cross-process communication** - Message from HTTP endpoints/workers to WebSocket clients\n- **Real-time notifications** without routing through database\n- **Multiple backends** - In-memory, Redis Queue, Redis Pub/Sub\n\n## Target Audience\n\n**Production-ready** for building scalable real-time applications. Ideal for developers who:\n- Need advanced WebSocket patterns beyond basic FastAPI WebSocket support\n- Want Django Channels functionality without Django\n- Are building chat apps, live dashboards, notifications, or collaborative tools\n\n## Comparison\n\nUnlike native FastAPI WebSockets (basic connection handling) or simple pub/sub libraries, fast-channels provides:\n- **Consumer pattern** with structured connect/receive/disconnect methods\n- **Message persistence** via Redis Queue backend\n- **Automatic connection management** and group handling\n- **Testing framework** for WebSocket consumers\n- **Full type safety** with comprehensive type hints\n\n## Example\n\n```python\nfrom fast_channels.consumer.websocket import AsyncWebsocketConsumer\n\nclass ChatConsumer(AsyncWebsocketConsumer):\n    groups = [\"chat_room\"]\n    channel_layer_alias = \"chat\"\n\n    async def connect(self):\n        await self.accept()\n        await self.channel_layer.group_send(\n            \"chat_room\",\n            {\"type\": \"chat_message\", \"message\": \"Someone joined!\"}\n        )\n\n    async def receive(self, text_data=None, **kwargs):\n        # Broadcast to all connections in the group\n        await self.channel_layer.group_send(\n            \"chat_room\",\n            {\"type\": \"chat_message\", \"message\": f\"Message: {text_data}\"}\n        )\n```\n\n## Links\n\n- **PyPI**: `pip install fast-channels[redis]`\n- **Docs**: https://fast-channels.readthedocs.io/\n- **GitHub**: https://github.com/huynguyengl99/fast-channels\n- **Tutorial**: https://fast-channels.readthedocs.io/en/latest/tutorial/index.html\n\nPerfect for chat apps, real-time dashboards, live notifications, and collaborative tools!\n\nWould love to hear your thoughts and feedback! üôè",
    "author": "huygl99",
    "timestamp": "2025-09-17T11:08:02",
    "url": "https://reddit.com/r/Python/comments/1njkn7a/fastchannels_websocket_layer_utility_portbased_on/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 0.83,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nj12yr",
    "title": "Do you prefer sticking to the standard library or pulling in external packages?",
    "content": "I‚Äôve been writing Python for a while and I keep running into this situation. Python‚Äôs standard library is huge and covers so much, but sometimes it feels easier (or just faster) to grab a popular external package from PyPI.\n\nFor example, I‚Äôve seen people write entire data processing scripts with just built-in modules, while others immediately bring in pandas or requests even for simple tasks.\n\nI‚Äôm curious how you all approach this. Do you try to keep dependencies minimal and stick to the stdlib as much as possible, or do you reach for external packages early to save development time?",
    "author": "Unusual-Program-2166",
    "timestamp": "2025-09-16T19:23:09",
    "url": "https://reddit.com/r/Python/comments/1nj12yr/do_you_prefer_sticking_to_the_standard_library_or/",
    "score": 104,
    "num_comments": 110,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nitzoz",
    "title": "List of 87 Programming Ideas for Beginners (with Python implementations)",
    "content": "https://inventwithpython.com/blog/programming-ideas-beginners-big-book-python.html\n\nI've compiled a list of beginner-friendly programming projects, with example implementations in Python. These projects are drawn from my free Python books, but since they only use stdio text, you can implement them in any language.\n\nI got tired of the copy-paste \"1001 project\" posts that obviously were copied from other posts or generated by AI which included everything from \"make a coin flip program\" to \"make an operating system\". I've personally curated this list to be small enough for beginners. The implementations are all usually under 100 or 200 lines of code.",
    "author": "AlSweigart",
    "timestamp": "2025-09-16T14:13:35",
    "url": "https://reddit.com/r/Python/comments/1nitzoz/list_of_87_programming_ideas_for_beginners_with/",
    "score": 220,
    "num_comments": 29,
    "upvote_ratio": 0.94,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nji3b8",
    "title": "user auth in azure table storage using python",
    "content": "[link to my github repo ](https://github.com/jurriaancap/auth_azuretablestorage)\n\n  \nWhat My Project Does\n\nThis repository provides a **lightweight user management system in Python**, built on **Azure Table Storage**. It includes:\n\n* User registration with bcrypt password hashing\n* User login with JWT-based access and refresh tokens\n* Secure token refresh endpoint\n* Centralized user data stored in Azure Table Storage\n* Environment-based configuration (no secrets in code)\n\nIt is structured for reuse and easy inclusion in multiple projects, rather than as a one-off script.\n\n# Target Audience\n\nThis project is primarily aimed at **developers building prototypes, proof-of-concepts, or small apps** who want:\n\n* Centralized, persistent user authentication\n* A low-cost alternative to SQL or Postgres\n* A modular, easy-to-extend starting point\n\nIt is not a production-ready identity system but can be adapted and hardened for production use.\n\n# Comparison\n\nUnlike many authentication examples that use relational databases, this project uses **Azure Table Storage** ‚Äî making it ideal for those who want:\n\n* A fully serverless, pay-per-use model\n* A simple NoSQL-style approach to user management\n* Easy integration with other Azure services\n\nIf you want a simple, minimal, and cloud-native way to handle user authentication without spinning up a SQL database,",
    "author": "the_milkman01",
    "timestamp": "2025-09-17T09:35:17",
    "url": "https://reddit.com/r/Python/comments/1nji3b8/user_auth_in_azure_table_storage_using_python/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1njtelc",
    "title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",
    "content": "# Weekly Thread: Professional Use, Jobs, and Education üè¢\n\nWelcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.\n\n---\n\n## How it Works:\n\n1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.\n2. **Education Q&amp;A**: Ask or answer questions about Python courses, certifications, and educational resources.\n3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.\n\n---\n\n## Guidelines:\n\n- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.\n- Keep discussions relevant to Python in the professional and educational context.\n  \n---\n\n## Example Topics:\n\n1. **Career Paths**: What kinds of roles are out there for Python developers?\n2. **Certifications**: Are Python certifications worth it?\n3. **Course Recommendations**: Any good advanced Python courses to recommend?\n4. **Workplace Tools**: What Python libraries are indispensable in your professional work?\n5. **Interview Tips**: What types of Python questions are commonly asked in interviews?\n\n---\n\nLet's help each other grow in our careers and education. Happy discussing! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-17T17:00:31",
    "url": "https://reddit.com/r/Python/comments/1njtelc/thursday_daily_thread_python_careers_courses_and/",
    "score": 1,
    "num_comments": 1,
    "upvote_ratio": 0.67,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1njhu32",
    "title": "Good platform to deploy python scripts with triggers &amp; scheduling",
    "content": "Hey folks,\n\nI'm a full-stack dev and recently played around with no-code tools like Make/Zapier for a side project.\n\nWhat I really liked was how fast it is to set up automations with triggers (RSS, webhooks, schedules, etc.), basically cron jobs without the hassle.\n\nBut as a developer, I find it a bit frustrating that all these tools are so geared towards non-coders.\n\nSometimes I‚Äôd rather just drop a small Python or JS file, wire up a trigger/cron, and have it run in autopilot (I already think about many scrapers I would have loved to deploy ages ago) ‚Äî without messing with full infra like AWS Lambda, Render, or old-school stuff like PythonAnywhere.\n\nSo my question is:\n\n***üëâ Do some of you know a modern, dev-friendly platform that‚Äôs specifically built for running small scripts with scheduling and event triggers?***\n\nSomething between ‚ÄúZapier for non-coders‚Äù and ‚Äúfull serverless setup with IAM roles and Docker images‚Äù.\n\nI‚Äôve seen posts like [this one](https://www.reddit.com/r/learnpython/comments/12t9bgm/how_do_i_deploy_python_scripts_in_production/) but didn‚Äôt find a really clean solution for managing multiple little projects/scripts.\n\nWould love to hear if anyone here has found a good workflow or platform for that!",
    "author": "CesMry_BotBlogR",
    "timestamp": "2025-09-17T09:25:52",
    "url": "https://reddit.com/r/Python/comments/1njhu32/good_platform_to_deploy_python_scripts_with/",
    "score": 3,
    "num_comments": 35,
    "upvote_ratio": 0.62,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1njvpzm",
    "title": "Proto-agent : an AI Agent Framework and CLI!",
    "content": "What my project does: I've started this project 2 weeks ago, where it started as a simple CLI, it was supposed to be just a learning educational project where others can read the code and study it to learn more about agents, but for every feature i would add, i make it really super modular and extendable that it felt to be a waste to just bind it to a single limited interface liked the command line, Porto-agent focuses heavily on independence while putting Safety as its number 1 priority through our permission system  \nproton-agent's CLI isn't supposed to be some another TUI coding agent, but your own ai that can do various stuff for you on your computer through our toolkit architecture\n\nTarget audience: Both developers and normal users can benefit from using proto-agent, the former can have a very lightweight and extendable while having one of the best safety features framework to build on top of, and the CLI can be used by anyone to do various stuff, I'm adding more toolkits\n\nComparison: Agno framework is one of the biggest inspiration for this project, proto-agent is NOT anywhere close to have that many features but it doesn't aim to be replacement nor a competitor, I'm picking and discarding the features that my target audience actually \\*needs\\* for their apps rather than being an all entriprise grade framework \n\n  \nPlease give it a try either as a CLI or a framework, i would love nothing more than feedbacks, I feel like the docs are abit lacking but i'm working on it!  \n[https://github.com/WeismannS/Proto-agent](https://github.com/WeismannS/Proto-agent)\n\nIf anyone wants to check it out, or contribute, please feel free to reach out.",
    "author": "Opposite_Ad_974",
    "timestamp": "2025-09-17T18:49:24",
    "url": "https://reddit.com/r/Python/comments/1njvpzm/protoagent_an_ai_agent_framework_and_cli/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.2,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1njzusk",
    "title": "What yall need? (I need a project)",
    "content": "So, i just finished one of my bigger projects, a custom interpreted programming language made to feel like assembly, with memory and register emulators and an modular instruction set which is easily modifiable by just adding files to a folder, as well as a IO module system with a modular approach for Memory mapped IO. But, as cool as it sounds, there is no real usecase? (project: [https://github.com/CheetahDoesStuff/BEANS](https://github.com/CheetahDoesStuff/BEANS) (note that all docs arent fully written, i do those when im bored in school))\n\nAs im finishing up on that im looking for a project that would \\*make others experience better (automod, why do you delete my post if it contains the he-lp word?)\\* like libraries, cli tools, gui tools. Anything that you need or think \"why isnt there a library for that?\", ill consider. If i realise i would benefit from it too, then i would maybe consider it.. even more?\n\nAlso so nobody says it, ive already made a logging library, with log saving, custom colors, a lot of settings, project names, subnames, sublogging, error, critical, warning, info logs. Whitespace log, raw log, timestamps, misc logs, and a lot more features, check it out on pypi, its called usefullog. ( [https://pypi.org/project/usefullog](https://pypi.org/project/usefullog) )\n\nAll suggestions are welcome!",
    "author": "BravestCheetah",
    "timestamp": "2025-09-17T22:26:46",
    "url": "https://reddit.com/r/Python/comments/1njzusk/what_yall_need_i_need_a_project/",
    "score": 0,
    "num_comments": 13,
    "upvote_ratio": 0.21,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1njb946",
    "title": "Built a small PyPI Package for explainable preprocessing",
    "content": "I made a Python package that explains preprocessing with reports and plots\n\nNote: This project started as a way for me to learn packaging and publishing on PyPI, but I thought it might also be useful for beginners who want not just preprocessing, but also clear reports and plots of what happened during preprocessing.\n\n\nWhat my project does: It‚Äôs a simple ML preprocessing helper package called ml-explain-preprocess. Along with handling basic preprocessing tasks (missing values, encoding, scaling, and outliers), it also generates additional outputs to make the process more transparent:\n\nText reports\n\nJSON reports\n\n(Optional) visual plots of distributions and outliers\n\n\nThe idea was to make it easier for beginners not only to preprocess data but also to understand what happened during preprocessing, since I couldn‚Äôt find many libraries that provide clear reports or visualizations alongside transformations.\n\nIt‚Äôs nothing advanced and definitely not optimized for production-level pipelines, but it was a good exercise in learning how packaging works and how to publish to PyPI.\n\nTarget audience: beginners in ML who want preprocessing plus some transparency. Experts probably won‚Äôt find it very useful, but maybe it can help people starting out.\n\nComparison: To my knowledge, most existing libraries handle preprocessing well, but they don‚Äôt directly give reports/plots. This project tries to cover that small gap.\n\nIf anyone wants to check it out or contribute, please feel free:\n\nPyPI: https://pypi.org/project/ml-explain-preprocess/\nGitHub: https://github.com/risheeee/ml-explain-preprocess.git\n\nWould appreciate any feedback, especially on how to improve packaging or add meaningful features.",
    "author": "western_chicha",
    "timestamp": "2025-09-17T05:04:27",
    "url": "https://reddit.com/r/Python/comments/1njb946/built_a_small_pypi_package_for_explainable/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1niqudg",
    "title": "Let your Python agents play an MMO: Agent-to-Agent protocol + SDK",
    "content": "\n\n**Repo:** [https://github.com/Summoner-Network/summoner-agents](https://github.com/Summoner-Network/summoner-agents)\n\n**TL;DR:** We are building **Summoner**, a Python SDK with a Rust server for agent-to-agent networking across machines. Early beta (beta version 1.0).\n\n**What my project does:** A protocol for live agent interaction with a desktop app to track network-wide agent state (battles, collaborations, reputation), so you can build MMO-style games, simulations, and tools.\n\n**Target audience:** Students, indie devs, and small teams who want to build networked multi-agent projects, simulations, or MMO-style experiments in Python.\n\n**Comparison:**\n\n* LangChain and CrewAI are app frameworks and an API spec for serving agents, not an on-the-wire interop protocol;\n* Google A2A is an HTTP-based spec that uses JSON-RPC by default (with optional gRPC or REST);\n* MCP standardizes model-to-tool and data connections.\n* **Summoner** targets live, persistent agent-to-agent networking for MMO-style coordination.\n\n**Status**\n\nOur Beta 1.0. works with example agents today. Expect sharp edges.\n\n**More**\n\nGithub page: [https://github.com/Summoner-Network](https://github.com/Summoner-Network)\n\nDocs/design notes: [https://github.com/Summoner-Network/summoner-docs](https://github.com/Summoner-Network/summoner-docs)\n\nCore runtime: [https://github.com/Summoner-Network/summoner-core](https://github.com/Summoner-Network/summoner-core)\n\nSite: [https://summoner.org](https://summoner.org)",
    "author": "SummonerNetwork",
    "timestamp": "2025-09-16T12:15:17",
    "url": "https://reddit.com/r/Python/comments/1niqudg/let_your_python_agents_play_an_mmo_agenttoagent/",
    "score": 20,
    "num_comments": 1,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nifogm",
    "title": "Some tips for beginners (Things you probably wish you knew when you first started)",
    "content": "Maybe the title came out a bit ambiguous, but I‚Äôd really like to get this kind of help and I also hope this post can be useful for others who, like me, are just starting out on their Python journey.",
    "author": "MonsieurJus",
    "timestamp": "2025-09-16T05:10:26",
    "url": "https://reddit.com/r/Python/comments/1nifogm/some_tips_for_beginners_things_you_probably_wish/",
    "score": 68,
    "num_comments": 77,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nirump",
    "title": "An open source internal tools platform for Python programs",
    "content": "Like the title says I am building an open source internal tools platform for Python programs, specifically one that is aimed at giving a company or team access to internal Python apps through a centralized hub. I have been building internal tools for 4 years and have used just about every software and platform out there:\n\n(Heroku, Streamlit Cloud, Hugging Face Spaces, Retool, Fly.io / Render / Railway),\n\nand they all fall short in terms of simplicity and usability for most teams. This platform would allow smaller dev teams to click-to-deploy small-medium sized programs, scripts, web apps, etc. to the cloud from a Github repository. The frontend will consist of a portal to select the program you want to run and then route to that specific page to execute it. Features I am looking into are:\n\n* centralized sharing gives non-tech users an easier way to access all the tools in one location (no more siloed notebooks, scripts, and web app URLs)\n* one-click edits/deploys (git push = updated application in cloud)\n* execution logs + observability at the user level -&gt; dev(s) can see the exact error logs + I/Os \n* secure SSO (integration with both azure and gcp)\n* usage analytics\n\nI'm wondering if this would be useful for others / what features you would like to see in it! Open to all feedback and advice. Lmk if you are interested in collaborating as well, I want this to be a community-first project.",
    "author": "Competitive-Water302",
    "timestamp": "2025-09-16T12:52:32",
    "url": "https://reddit.com/r/Python/comments/1nirump/an_open_source_internal_tools_platform_for_python/",
    "score": 13,
    "num_comments": 10,
    "upvote_ratio": 0.89,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nj7y99",
    "title": "Python's role in the AI infrastructure stack ‚Äì sharing lessons from building production AI systems",
    "content": "Python's dominance in AI/ML is undeniable, but after building several production AI systems, I've learned that the language choice is just the beginning. The real challenges are in architecture, deployment, and scaling.\n\n**Current project:**¬†Multi-agent system processing 100k+ documents daily  \n**Stack:**¬†FastAPI, Celery, Redis, PostgreSQL, Docker  \n**Scale:**¬†\\~50 concurrent AI workflows, 1M+ API calls/month\n\n**What's working well:**\n\n* **FastAPI for API development**¬†‚Äì async support handles concurrent AI calls beautifully\n* **Celery for background processing**¬†‚Äì essential for long-running AI tasks\n* **Pydantic for data validation**¬†‚Äì catches errors before they hit expensive AI models\n* **Rich ecosystem**¬†‚Äì libraries like LangChain, Transformers, and OpenAI client make development fast\n\n**Pain points I've encountered:**\n\n* **Memory management**¬†‚Äì AI models are memory-hungry, garbage collection becomes critical\n* **Dependency hell**¬†‚Äì AI libraries have complex requirements that conflict frequently\n* **Performance bottlenecks**¬†‚Äì Python's GIL becomes apparent under heavy concurrent loads\n* **Deployment complexity**¬†‚Äì managing GPU dependencies and model weights in containers\n\n**Architecture decisions that paid off:**\n\n1. **Async everywhere**¬†‚Äì using asyncio for all I/O operations, including AI model calls\n2. **Worker pools**¬†‚Äì separate processes for different AI tasks to isolate failures\n3. **Caching layer**¬†‚Äì Redis for expensive AI results, dramatically improved response times\n4. **Health checks**¬†‚Äì monitoring AI model availability and fallback mechanisms\n\n**Code patterns that emerged:**\n\n`# Context manager for AI model lifecycle`\n\n`@asynccontextmanager`\n\n`async def ai_model_context(model_name: str):`\n\n`model = await load_model(model_name)`\n\n`try:`\n\n`yield model`\n\n`finally:`\n\n`await cleanup_model(model)`\n\n\n\n`# Retry logic for AI API calls`\n\n`@retry(stop=stop_after_attempt(3), wait=wait_exponential())`\n\n`async def call_ai_api(prompt: str) -&gt; str:`\n\n`# Implementation with proper error handling`\n\n**Questions for the community:**\n\n1. How are you handling AI model deployment and versioning in production?\n2. What's your experience with alternatives to Celery for AI workloads?\n3. Any success stories with Python performance optimization for AI systems?\n4. How do you manage the costs of AI API calls in high-throughput applications?\n\n**Emerging trends I'm watching:**\n\n* **MCP (Model Context Protocol)**¬†‚Äì standardizing how AI systems interact with external tools\n* **Local model deployment**¬†‚Äì running models like Llama locally for cost/privacy\n* **AI observability tools**¬†‚Äì monitoring and debugging AI system behavior\n* **Edge AI with Python**¬†‚Äì running lightweight models on edge devices\n\nThe Python AI ecosystem is evolving rapidly. Curious to hear what patterns and tools are working for others in production environments.",
    "author": "Siddharth-1001",
    "timestamp": "2025-09-17T01:58:27",
    "url": "https://reddit.com/r/Python/comments/1nj7y99/pythons_role_in_the_ai_infrastructure_stack/",
    "score": 0,
    "num_comments": 4,
    "upvote_ratio": 0.48,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nj7agh",
    "title": "Datalore vs Deepnote?",
    "content": "I have been a long-term user of Deepnote at my previous company and am now looking for alternatives for my current company. Can anyone vouch for Datalore? ",
    "author": "Odd-Avocado7191",
    "timestamp": "2025-09-17T01:14:29",
    "url": "https://reddit.com/r/Python/comments/1nj7agh/datalore_vs_deepnote/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nhtowu",
    "title": "I made a vs code extension that insults you if you copy &amp; paste AI generated code",
    "content": "-on an important note: this project was just for fun, I'm not against using AI to help your coding sessions-\n\n\nWhat my project does:\nIt's a vs code extension that gives random insults such as \"Do you ask GPT what to eat for dinner as well?\" to the user if it detects AI generated content. It uses a pretrained transformer-based model for inference (roberta-base-openai-detector), that returns the probability of human and AI writing the given section of text. It was pretty fun to play around with, although not accurate (the model was trained on GPT-2, and not optimized for code, so accuracy is bum), but it was my first time mixing languages together to create something. (In this case typescript and python) It's interesting how extensions like these are set up, I think it's valuable for anyone to do pet projects like these.\n\n\nTarget audience: noone really, just a funny pet project, due to the inaccuracy I wouldn't recommend it for actual usage (it's a bit difficult to create something more accurate, these kind of open-source models were trained on texts, not code) \n\n\nComparison: To my knowledge there hasn't been a vs code extension like this before, but there are several much more accurate detectors available online. \n\n\nIf anyone wants to check it out, or contribute, please feel free to reach out.\n\n\nhttps://github.com/Tbence132545/Ai-copypaste-insult",
    "author": "ComplexCollege6382",
    "timestamp": "2025-09-15T11:20:56",
    "url": "https://reddit.com/r/Python/comments/1nhtowu/i_made_a_vs_code_extension_that_insults_you_if/",
    "score": 304,
    "num_comments": 36,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nj8l0y",
    "title": "Master Roshi AI Chatbot - Train with the Turtle Hermit",
    "content": "URL: https://roshi-ai-showcase.vercel.app\n\nHey Guys, I created a chatbot using Nomos (https://nomos.dowhile.dev) (https://github.com/dowhiledev/nomos) which allows you to create AI Intelligent AI Agents without writing code (but if you want to you can do that too). Give it a try. (Responding speed could be slow as i am using a free tier service). AI Agent have access to https://dragonball-api.com\n\nGive it a try. Tell me how i can improve the library and what to create next with it\n\nFrontend is made with lovable",
    "author": "No-Base-1700",
    "timestamp": "2025-09-17T02:38:55",
    "url": "https://reddit.com/r/Python/comments/1nj8l0y/master_roshi_ai_chatbot_train_with_the_turtle/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.29,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ni81px",
    "title": "Has Anyone Been Using Pyrefly?",
    "content": "Thinking of introducing it at my company as a sort of second linter alongside basedpyright. I think it'll be good to get it incorporated a bit early so that we can fix whatever bugs it catches as it comes along. It looks to be in a decent state for basic typechecking, and the native django support will be nice as it comes along (compared to mypy).",
    "author": "auric_gremlin",
    "timestamp": "2025-09-15T21:35:39",
    "url": "https://reddit.com/r/Python/comments/1ni81px/has_anyone_been_using_pyrefly/",
    "score": 27,
    "num_comments": 25,
    "upvote_ratio": 0.97,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nickil",
    "title": "Fast weighted selection using digit-bin-index",
    "content": "**What my project does:**  \nThis is slightly niche, but if you need to do weighted selection and can treat probabilities as fixed precision, I built a high-performing package called digit-bin-index with Rust under the hood. It uses a novel algorithm to achieve best in class performance.\n\n**Target audience:**  \nThis package is particularly suitable for iterative weighted selection from an evolving population, such as a simulation. One example is repeated churn and acquisition of customers with a simulation to determine the customer base evolution over time.\n\n**Comparison:**  \nThere are naive algorithms, often O(N) or worse. State of the art algorithms like Walker's alias method can do O(1) selection, but require an O(N) setup and is not suitable for evolving populations. Fenwick trees are also often used, with O(log N) complexity for selection and addition. `DigitBinIndex` is O(P) for both, where P is the fixed precision.\n\nHere's an excerpt from a test run on a MacBook Pro with M1 CPU:\n\n`--- Benchmarking with 1,000,000 items ---`  \n`This may take some time...`  \n`Time to add 1,000,000 items: 0.219317 seconds`  \n`Estimated memory for index: 145.39 MB`  \n`100,000 single selections: 0.088418 seconds`  \n`1,000 multi-selections of 100: 0.025603 seconds`\n\nThe package is available at:¬†[https://pypi.org/project/digit-bin-index/](https://pypi.org/project/digit-bin-index/)  \nThe source code is available on:¬†[https://github.com/Roenbaeck/digit-bin-index](https://github.com/Roenbaeck/digit-bin-index)",
    "author": "Roenbaeck",
    "timestamp": "2025-09-16T02:16:57",
    "url": "https://reddit.com/r/Python/comments/1nickil/fast_weighted_selection_using_digitbinindex/",
    "score": 8,
    "num_comments": 2,
    "upvote_ratio": 0.85,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nj0bd4",
    "title": "Can i use candyserver together with gunicorn?",
    "content": "Hi,\n\nI have a flask web service that originally run with gunicorn and nginx on top of it. and I would like to replace with cadyserver.\n\nCan i set up my flask server with gunicorn and cadyserver? or can cadyserver replace both gunicorn and nginx",
    "author": "tranylvu",
    "timestamp": "2025-09-16T18:46:49",
    "url": "https://reddit.com/r/Python/comments/1nj0bd4/can_i_use_candyserver_together_with_gunicorn/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nirju1",
    "title": "Any python meetups/talks in the NY/NJ area coming up? What do you use to find events like this?",
    "content": "Interested in attending anything python related except for data science. It would be nice to be around and hear people talk about and see how they use python in a professional setting. ",
    "author": "luunnn",
    "timestamp": "2025-09-16T12:41:17",
    "url": "https://reddit.com/r/Python/comments/1nirju1/any_python_meetupstalks_in_the_nynj_area_coming/",
    "score": 1,
    "num_comments": 2,
    "upvote_ratio": 0.6,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nic529",
    "title": "Mogami: VS Code Extension for Managing Python Dependencies",
    "content": "Hi all, I'd like to introduce¬†[Mogami](https://github.com/ninoseki/vscode-mogami), a VS Code Extension for managing Python dependencies.\n\n* VS Code market place:¬†[https://marketplace.visualstudio.com/items?itemName=ninoseki.vscode-mogami](https://marketplace.visualstudio.com/items?itemName=ninoseki.vscode-mogami)\n* GitHub:¬†[https://github.com/ninoseki/vscode-mogami](https://github.com/ninoseki/vscode-mogami)\n\n---\n\n# What My Project Does\n\nIt displays a CodeLens (a tooltip to inform the latest version and allow you to update it by clicking it) on dependencies in requirements.txt, pyproject.toml, etc.\n\n# Target Audience\n\nPython dev who uses VS Code.\n\n# Comparison\n\n* [https://github.com/Twixes/pypi-assistant/](https://github.com/Twixes/pypi-assistant/) (supports many formats, but not actionable)\n* [https://gitlab.com/versionlens/vscode-versionlens](https://gitlab.com/versionlens/vscode-versionlens) (supports only basic PEP 735)\n\n---\n\nPlease try it out and give me feedback.",
    "author": "ninoseki",
    "timestamp": "2025-09-16T01:49:44",
    "url": "https://reddit.com/r/Python/comments/1nic529/mogami_vs_code_extension_for_managing_python/",
    "score": 8,
    "num_comments": 1,
    "upvote_ratio": 0.79,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nias08",
    "title": "I Build Type-safe TOML configuration with environment variables for Python 3.11+ | TomlEv",
    "content": "**TL;DR:** Stop fighting with environment variables and manual type conversion - get type-safe TOML configuration that just works.\n\n```bash\npip install tomlev\n```\n\n**Benefits:**\n- Automatic type conversion and validation\n- Environment variable substitution with defaults\n- Zero dependencies, production-ready\n- Perfect IDE and AI assistant support\n\n## The Problem\n\nPEP 735 style config management leads to repetitive, error-prone code:\n\n```python\n# The old way - manual parsing everywhere\nDB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\nDB_PORT = int(os.getenv(\"DB_PORT\", \"5432\"))  # Hope this doesn't crash!\nDEBUG = os.getenv(\"DEBUG\", \"false\").lower() == \"true\"  # Boolean hell\n```\n\n## What My Project Does\n\nTomlEv reads TOML files with environment variable substitution and validates them against typed Python classes:\n\n```python\nfrom tomlev import BaseConfigModel, TomlEv\n\nclass DatabaseConfig(BaseConfigModel):\n    host: str\n    port: int\n    user: str\n\nclass AppConfig(BaseConfigModel):\n    debug: bool\n    database: DatabaseConfig\n\n# One line - fully type-safe!\nconfig: AppConfig = TomlEv(AppConfig).validate()\n```\n\n**TOML file:**\n```toml\ndebug = \"${DEBUG|-false}\"\n\n[database]\nhost = \"${DB_HOST|-localhost}\"\nport = \"${DB_PORT|-5432}\"\nuser = \"${DB_USER}\"\n```\n\nWorks as CLI too:\n```bash\ntomlev validate --toml app.toml --env-file .env\ntomlev render --toml app.toml &gt; config.json\n```\n\n## Target Audience\n\nPython developers using modern type hints who want reliable configuration management without the boilerplate.\n\n## Comparison\n\n‚ùå **python-dotenv**: No type safety, manual parsing\n‚ùå **pydantic-settings**: More complex, less TOML-focused\n‚ùå **configparser**: INI format, no modern Python features\n‚ùå **YAML configs**: Security issues, complex parsing\n\n‚úÖ **TomlEv**: TOML readability + Python type safety + environment flexibility\n\n**Similar tools:**\n- No direct equivalent for TOML + type safety + env substitution\n\n## Try it out: https://github.com/thesimj/tomlev\n\n‚≠ê **Star if it helps!** Issues and PRs welcome. ‚≠ê\n",
    "author": "nbpatron",
    "timestamp": "2025-09-16T00:19:16",
    "url": "https://reddit.com/r/Python/comments/1nias08/i_build_typesafe_toml_configuration_with/",
    "score": 7,
    "num_comments": 0,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ni2k4t",
    "title": "Starplot - Star charts and maps of the sky",
    "content": "Hey all, I‚Äôd like to introduce [Starplot](https://starplot.dev/) ‚Äî a Python library for creating star charts and maps of the sky.\n\n**What My Project Does**\n\n* Creates customizable star charts and maps of the night sky\n* Allows custom styling for all plotted objects, and includes many color themes\n* Supports many map projections and types of plots:\n   * Zenith plots that show the whole sky at a specific time and place\n   * Map plots that show an area of the sky\n   * Horizon plots that show the sky from a specific cardinal direction\n   * Optic plots that show what an object looks like through an optic (e.g. telescope, binoculars, etc) at a specific time and place\n* Includes a built-in database of 2M+ stars and 14,000+ deep sky objects (galaxies, nebulae, star clusters, etc)\n* Exports plots to PNG, JPEG, or SVG\n\n**Target Audience**\n\n* Anyone interested in astronomy or creating maps of the sky!\n* Astrophysicists\n* Astronomers\n\n**Comparison** \n\nCompared to similar projects (e.g. fchart3, astroplan), Starplot supports a lot of customization and has many different plot types.\n\n\\---\n\nHomepage: [https://starplot.dev/](https://starplot.dev/)\n\nExample Plots: [https://starplot.dev/examples/](https://starplot.dev/examples/)\n\nSource Code: [https://github.com/steveberardi/starplot](https://github.com/steveberardi/starplot)\n\nStarplot is still very much a work in progress, and I appreciate any feedback. Also very open to contributors if you want to help out! üòÄ Clear skies! üî≠ ‚ú®",
    "author": "starplotting",
    "timestamp": "2025-09-15T17:09:48",
    "url": "https://reddit.com/r/Python/comments/1ni2k4t/starplot_star_charts_and_maps_of_the_sky/",
    "score": 17,
    "num_comments": 3,
    "upvote_ratio": 1.0,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nhdt04",
    "title": "I made a terminal-based game that uses LLMs -- Among LLMs: You are the Impostor",
    "content": "I made this game in Python (that uses **Ollama** and local `gpt-oss:20b` / `gpt-oss:120b` models) that runs directly inside your terminal. TL;DR **above** the example.\n\n&gt;Among LLMs turns your **terminal** into a chaotic chatroom playground where **you‚Äôre the only human** **among a bunch of eccentric AI agents**, dropped into a common *scenario* \\-- it could be Fantasy, Sci-Fi, Thriller, Crime, or something completely unexpected. Each participant, including you, has a *persona* and a *backstory*, and all the AI agents share one common goal -- determine and eliminate the human, through *voting*. **Your mission: stay hidden, manipulate conversations, and turn the bots against each other with edits, whispers, impersonations, and clever gaslighting**. Outlast everyone, turn chaos to your advantage, and make it to the final two.\n\n&gt;Can you survive the hunt and *outsmart* the AI ?\n\nQuick Demo: [https://youtu.be/kbNe9WUQe14](https://youtu.be/kbNe9WUQe14)\n\nGithub: [https://github.com/0xd3ba/among-llms](https://github.com/0xd3ba/among-llms) (refer to `develop` branch for latest updates)\n\n(**Edit:** Join the [subreddit for Among LLMs](https://www.reddit.com/r/Among_LLMs/) if you have any bug reports, issues, feature-requests, suggestions or want to showcase your hilarious moments)\n\n* **What my project does:** Uses local Ollama gpt-oss models uniquely in a game setting; Built completely as a terminal-UI based project.\n* **Target Audience:** Anyone who loves drama and making AI fight each other\n* **Comparision:** No such project exists yet.\n\n# Example of a Chatroom (after export)\n\nYou can **save chatrooms as JSON** and **resume** where you left off later on. **Similarly you can load other's saved JSON as well**! What's more, when you save a chatroom, it also exports the chat as a text file. Following is an example of a chatroom I recently had.\n\n**Note(s):**\n\n* Might be lengthy, but you'll get the idea of how these bots behave (lol)\n* All agents have personas and backstories, which are not visible in the exported chat\n\n**Example:** [https://pastebin.com/ud7mYmH4](https://pastebin.com/ud7mYmH4)",
    "author": "Foreign_Radio8864",
    "timestamp": "2025-09-14T22:50:48",
    "url": "https://reddit.com/r/Python/comments/1nhdt04/i_made_a_terminalbased_game_that_uses_llms_among/",
    "score": 248,
    "num_comments": 26,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nixhum",
    "title": "I've created an cross platform app called `PyEnvManager` to make managing python virtual envs easy",
    "content": "Hey folks,\n\nI just released a small tool¬†called¬†**PyEnvManager**. Would love to showcase it and  get feedback from the community .\n\n# Problem\n\nThis all started while I was working on another project that needed a bunch of different Python environments. Different dependencies, different Python versions, little experiments I didn‚Äôt want to contaminate ‚Äî so I kept making new envs. \n\nAt the time it felt like I was being organized. I assumed I had maybe 5‚Äì6 environments active. When I finally checked, I had 6 actively used Python virtual environments, but there were also many leftover envs scattered across Conda, venv, Poetry, and Mamba ‚Äî together they were chewing up \\~45GB on my Windows machine. On my Mac, where I thought things were ‚Äúclean,‚Äù I found another 4 using \\~5GB. And honestly, it was just annoying. I couldn‚Äôt remember which ones were safe to delete, which belonged to what project, or why some even existed. Half the time with Jupyter I‚Äôd open a notebook, it would throw a¬†*ModuleNotFoundError: No module named 'pandas*', and then I‚Äôd realize I launched it in the wrong kernel. It wasn‚Äôt catastrophic, but it was really annoying ‚Äî a steady drip of wasted time that broke my flow.\n\nSo, i built this to improve my workflow.\n\nGithub:¬†[https://github.com/Pyenvmanager](https://github.com/Pyenvmanager) \n\nWebsite:¬†[https://pyenvmanager.com/](https://pyenvmanager.com/)\n\n# What My Project Does\n\nPyEnvManager is a small desktop app that helps you¬†**discover, manage, and secure Python virtual environments**¬†across a machine . It‚Äôs focused on removing the everyday friction of working with many envs and making environment-related security and compliance easy to see.\n\nCore capabilities (today / near-term):\n\n* System-wide environment discovery across different environments (Conda, venv, Poetry, Mamba, Micromamba).\n* Per-env metadata: Python version, disk usage, last-used timestamp.\n* One-click Jupyter launch into the correct environment\n* Create envs from templates or with custom packages.\n* Safe delete with a preview of reclaimed disk space.\n* Dependency surface: searchable package chips and CVE highlighting (dependency scanning aligned with pip-audit behavior).\n* Exportable metadata / SBOM (planned/improving for Teams/Enterprise). \n\nShort form: it finds the envs you forgot about, helps you use the right one, and gives you the tools to clean and audit them.\n\n# Target Audience\n\n**Who it‚Äôs for, and how it should be used**\n\n* **Individual developers &amp; data scientists (primary, production-ready):**\n   * Daily local use on laptops and workstations.\n   * If you want to stop wasting time managing kernels, reclaim disk space, and avoid ‚Äúwrong-kernel‚Äù bugs, this is for you.\n* **Small teams / consultancies (early pilots / beta):**\n   * Useful for reproducibility, shared templates, and exporting SBOMs for client work.\n   * Good candidate for a pilot with a few machines to validate workflows and reporting needs.¬†\n   * The product is¬†**production-ready for individual devs**¬†(discovery, Jupyter launch, deletes, templates).\n* Team &amp; enterprise functionality is being added progressively (SBOM exports, snapshots, headless CLI).\n\n# Comparison\n\n* **vs**¬†`pyenv`¬†**/**¬†`conda`¬†**/**¬†`poetry`¬†**(CLI tools):**\n   * Those are excellent for version switching and per-project env creation. They do¬†**not**¬†provide system-wide discovery, a unified GUI, disk-usage visibility, or one-click Jupyter kernel mapping. PyEnvManager sits on top of those workflows and gives a single place to see and act on all envs.\n* **vs**¬†`pip-audit`¬†**/ SCA tools (Snyk, OSV, etc.):**\n   * SCA tools focus on dependency scanning of projects and CI pipelines. PyEnvManager focuses on¬†**installed environments on machines**¬†(local dev workstations), surfacing envs that SCA tools typically never see. It aligns with pip-audit for CVE detection but is not meant to replace enterprise SCA in CI/CD ‚Äî it complements them by finding the hidden surface area on endpoints.\n* **vs developer GUIs (IDE plugins, Docker Desktop):**\n   * Docker Desktop is a platform for containers and developer workflows. PyEnvManager is specifically about¬†**Python virtual environments**, Jupyter workflows, and reproducibility. The ‚ÄúDocker Desktop for Python envs‚Äù analogy helps convey the UX-level ambition: make env discovery and management approachable and visual.",
    "author": "TypicalPudding6190",
    "timestamp": "2025-09-16T16:37:26",
    "url": "https://reddit.com/r/Python/comments/1nixhum/ive_created_an_cross_platform_app_called/",
    "score": 0,
    "num_comments": 10,
    "upvote_ratio": 0.21,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nhuk7r",
    "title": "Created python library for time series projections. E.g. combining income, inflation, dividends, etc",
    "content": "GitHub: [https://github.com/TimoKats/pylan](https://github.com/TimoKats/pylan)\n\nPyPi: [https://pypi.org/project/pylan-lib/](https://pypi.org/project/pylan-lib/)\n\n# What My Project Does\n\nPython library for making complex time series projections. E.g. for simulating the combined effect of (increasing) salary, inflation, investment gains, etc, over time. Note, it can also be applied to other domains.\n\n# Target Audience\n\nData analysts, planners, etc. People that use excel for making projections, but want to move to python.\n\n# Comparison\n\n\\- SaaS financial planning tools (like ProjectionLab) work through a webUI, whereas here you have access to all the Python magic in the same place as you do your simulation.\n\n\\- Excel....\n\n\\- Write your own code for this is not super difficult, but this library does provide a good framework of dealing with various schedule types (some of which cron doesn't support) to get to your analysis more quickly.",
    "author": "_Rush2112_",
    "timestamp": "2025-09-15T11:52:55",
    "url": "https://reddit.com/r/Python/comments/1nhuk7r/created_python_library_for_time_series/",
    "score": 13,
    "num_comments": 9,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nihwt2",
    "title": "Taming wild JSON in Python: lessons from AI/Agentic Conversations exports",
    "content": "Working on a data extraction project just taught me that not all JSON is created equal. What looked like a ‚Äústraightforward parsing task‚Äù quickly revealed itself as a lesson in defensive programming, graph algorithms, and humility.\n\n**The challenge:** Processing ChatGPT conversation exports that looked like simple JSON arrays‚Ä¶ but in reality were directed acyclic graphs with all the charm of a family tree drawn by Kafka.\n\n**Key lessons learned about Python:**\n\n**1. Defensive programming is essential**\n\nBecause JSON in the wild is like Schr√∂dinger‚Äôs box - you don‚Äôt know if it‚Äôs a string, dict, or None until you peek inside.\n\n&gt;\\`\\`\\`python\n\n&gt;*# Always check before 'in' operator*\n\n&gt;if metadata and 'key' in metadata:\n\n&gt;value = metadata\\['key'\\]\n\n&gt;\n\n&gt;*# Handle polymorphic arrays gracefully*¬†¬†\n\n&gt;for part in parts or \\[\\]:\n\n&gt;if part is None:\n\n&gt;continue\n\n&gt;\\`\\`\\`\n\n**2. Graph traversal beats linear iteration** \n\nWhen JSON contains parent/child relationships, backward traversal from leaf nodes works often much better than trying to sort or reconstruct order.\n\n**3. Content type patterns** \n\nReal-world JSON often mixes strings, objects, and structured data in the same array. Building type-specific handlers saved me hours of debugging (and possibly a minor breakdown).\n\n**4. Memory efficiency matters** \n\nProcessing 500MB+ JSON files called for thinking about memory usage patterns and and garbage collection like a hawk. Nothing sharpens your appreciation of Python‚Äôs object model like watching your laptop heat up enough to double as a panini press.\n\n**Technical outcome:**\n\n* 99.5+% success rate processing 7,000 \"conversations.\n* Comprehensive error logging for the 1% of edge cases where reality outsmarted my code\n* Renewed respect for how much defensive programming and domain knowledge matter, even with ‚Äúsimple‚Äù data formats\n\n\n\n*Full extractor here*: [chatgpt-conversation-extractor/README.md at master ¬∑ slyubarskiy/chatgpt-conversation-extractor ¬∑ GitHub](https://github.com/slyubarskiy/chatgpt-conversation-extractor/blob/master/README.md)",
    "author": "External-Ad-3916",
    "timestamp": "2025-09-16T06:44:45",
    "url": "https://reddit.com/r/Python/comments/1nihwt2/taming_wild_json_in_python_lessons_from_aiagentic/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1niqqg2",
    "title": "Anyone willing to collaborate on a new chess bot called Ou7 (already has a Github page)",
    "content": "I am looking for 1-3 people to help develop a new chess bot coded entirely in python (Ou7) if this sounds like it might interest you, message me",
    "author": "EOSTRAT",
    "timestamp": "2025-09-16T12:11:15",
    "url": "https://reddit.com/r/Python/comments/1niqqg2/anyone_willing_to_collaborate_on_a_new_chess_bot/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.25,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nhn589",
    "title": "I built AuthTuna, a modern, async-first security framework for FastAPI with hierarchical permissions",
    "content": "Hey everyone,\n\nI built an async security library for FastAPI called AuthTuna to solve some problems I was facing with existing tools.\n\n\n\n# What My Project Does\n\n\n\nAuthTuna is an async-first security library for FastAPI. It's not just a set of helpers; it's a complete foundation for authentication, authorization, and session management. Out of the box, it gives you:\n\n* **Fully async** operations built on SQLAlchemy 2.0.\n* **Hierarchical RBAC** for complex, nested permissions (e.g., `Organization -&gt; Project -&gt; Resource`), which goes beyond simple roles.\n* **Secure, server-side sessions** with built-in hijack detection.\n* A familiar developer experience using standard FastAPI `Depends` and Pydantic models.\n\n\n\n# Target Audience\n\n\n\nThis is built for Python developers using FastAPI to create **production-grade applications**. It's specifically useful for projects that need more complex, granular authorization logic, like multi-tenant SaaS platforms, internal dashboards, or any app where users have different levels of access to specific resources. It is not a toy project and is running in our own production environment.\n\n\n\n# Comparison\n\n\n\nI built this because I needed a specific combination of features that I couldn't find together in other libraries.\n\n* **vs. FastAPI's built-in tools:** The built-in security utilities are great low-level primitives. AuthTuna is a higher-level, \"batteries-included\" framework. You get pre-built user flows, session management, and a full permission system instead of having to build them yourself on top of the primitives.\n* **vs. FastAPI-Users:** FastAPI-Users is an excellent, popular library. AuthTuna differs mainly in its focus on **hierarchical permissions** and its **session model**. If you need to model complex, multi-level access rules (not just \"admin\" or \"user\") and prefer the security model of stateful, server-side sessions over stateless JWTs, then AuthTuna is a better fit.\n\nThe code is up on GitHub, and feedback is welcome.\n\n**GitHub:** [`https://github.com/shashstormer/authtuna`](https://github.com/shashstormer/authtuna)",
    "author": "shashstormer",
    "timestamp": "2025-09-15T07:19:20",
    "url": "https://reddit.com/r/Python/comments/1nhn589/i_built_authtuna_a_modern_asyncfirst_security/",
    "score": 19,
    "num_comments": 1,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nii4as",
    "title": "Can fine-grained memory management be achieved in Python?",
    "content": "This is just a hypothetical \"is this at all remotely possible?\", I do not in anyway shape or form (so far) think its a good idea to computationally demanding staff that requires precise memory management using a general purpose language ... but has anyone pulled it off? \n\nDo pypi packages exist that make it work? Or some seedy base package that already does it that I am too dumb to know about?",
    "author": "MilanTheNoob",
    "timestamp": "2025-09-16T06:52:58",
    "url": "https://reddit.com/r/Python/comments/1nii4as/can_finegrained_memory_management_be_achieved_in/",
    "score": 0,
    "num_comments": 20,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ni2d07",
    "title": "Tuesday Daily Thread: Advanced questions",
    "content": "# Weekly Wednesday Thread: Advanced Questions üêç\n\nDive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.\n\n## How it Works:\n\n1. **Ask Away**: Post your advanced Python questions here.\n2. **Expert Insights**: Get answers from experienced developers.\n3. **Resource Pool**: Share or discover tutorials, articles, and tips.\n\n## Guidelines:\n\n* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.\n* Questions that are not advanced may be removed and redirected to the appropriate thread.\n\n## Recommended Resources:\n\n* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.\n\n## Example Questions:\n\n1. **How can you implement a custom memory allocator in Python?**\n2. **What are the best practices for optimizing Cython code for heavy numerical computations?**\n3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**\n4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**\n5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**\n6. **What are some advanced use-cases for Python's decorators?**\n7. **How can you achieve real-time data streaming in Python with WebSockets?**\n8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**\n9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**\n10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**\n\nLet's deepen our Python knowledge together. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-15T17:00:32",
    "url": "https://reddit.com/r/Python/comments/1ni2d07/tuesday_daily_thread_advanced_questions/",
    "score": 2,
    "num_comments": 1,
    "upvote_ratio": 0.76,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nhfhcs",
    "title": "3 months in Python, I made my first proper 2D game",
    "content": "**What My Project Does:**  \nI‚Äôve been messing with Python for about three months, mostly tutorials and dumb exercises. Finally tried making an actual game, and this is what came out.\n\nIt‚Äôs called [Hate-Core](https://github.com/ah4ddd/Hate-Core). You play as a knight fighting dragons in 2D. There‚Äôs sprites, music, keyboard and touch controls, and a high-score system. Basically my attempt at a Dark Souls-ish vibe, but, you know‚Ä¶ beginner style. Built it with **Pygame**, did the movement, attacks, scoring, and slapped in some sprites and backgrounds.\n\n**Target Audience:**  \nHonestly? Just me learn-ing Python. Not production-ready, just a toy to practice, see what works, and maybe have some fun.\n\n**Comparison:**  \nWay beyond boring number guessing, dice rolls, or quizzes you see from beginners. It‚Äôs an actual 2D game, with visuals, music, and some ‚Äúcombat‚Äù mechanics. Dark Souls-ish but tiny, broken, and beginner-coded.\n\nI‚Äôd love **honest feedback**, tips, ideas or anything. I know it‚Äôs rough as hell.\n\nCheck it out here: [https://github.com/ah4ddd/Hate-Core](https://github.com/ah4ddd/Hate-Core)",
    "author": "justahappycamper1",
    "timestamp": "2025-09-15T00:34:37",
    "url": "https://reddit.com/r/Python/comments/1nhfhcs/3_months_in_python_i_made_my_first_proper_2d_game/",
    "score": 32,
    "num_comments": 31,
    "upvote_ratio": 0.82,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nh3rlv",
    "title": "I was terrible at studying so I made a Chrome extension that forces you to learn programming.",
    "content": "tldr; I made a free, open-source Chrome extension that helps you study by showing you flashcards while you browse the web. Its algorithm uses spaced repetition and semantic analysis to target your weaknesses and help you learn faster. It started as an SAT tool, but I've expanded it for everything, and I have custom flashcard deck suggestions for you guys to learn programming syntax and complex CS topics.\n\nHi everyone,\n\nSo, I'm not great at studying, or any good lol. Like when the SATs were coming up in high school, all my friends were getting 1500s, and I was just not, like I couldn't keep up, and I hated that I couldn't just sit down and study like them. The only thing I did all day was browse the web and working on coding projects that i would never finish in the first place.\n\nSo, one day, whilst working on a project and contemplating how bad of a person I was for not studying, I decided why not use my only skill, coding, to force me to study.\n\nAt first I wanted to make like a locker that would prevent my from accessing apps until I answered a question, but I only ever open a few apps a day, but what I did do was load hundreds of websites a da, and that's how the idea flashysurf was born. I didn't even have a real computer at the time, my laptop broke, so I built the first version as a userscript on my old iPad with a cheap Bluetooth mouse. It basically works like this, it's a Chrome extension that just randomly pops up with a flashcard every now and then while you're on YouTube, watching Anime, GitHub, or wherever. You answer it, and you slowly build knowledge without even trying.\n\nIt's completely free and open source ([GitHub link here](https://github.com/MaxDevv/FlashySurf)), and I got a little obsessed with the algorithm  (I've been working on this for like 5-6 months now lol). It's not just random. It uses a combination of psycological techniques to make learning as efficient as possible:\n\n* Dumb Weakness Targeting: Really simple, everytime you get a question wrong, its stored in a list and then later on these quesitons are priorotized that way you work on your weaknesses.\n* Intelligent Weakness Targeting: This was one of the biggest updates I made. For my SAT version, I implemented a semantic clustering system that groups questions by topic. So for example, if you get a question about arithmentic wrong, it knows to show you more questions that are semantically similar. Meaning it actively tarkedts your weak areas. The question selection is split 50% new questions, 35% questions similar to ones you've failed, and 15% direct review of failed questions.\n* Forced Note-Taking: This is in my opinion the most important feature in flashysurf for learning. Basically, if you get a question wrong, you have to write a short note on why you messed up and what you should've done instead, before you can close the card. It forces you to actually assess your mistakes and learn from them, instead of just clicking past them.\n\nAt first, it was just for the SAT, and the results were actually really impressive. I personally got my score up 100 points, which is like going from the top 8% to the top 3% (considered a really big improvement), and a lot of my friends and other online users saw 60-100 point increases. So it proved the concept worked, especially for lazy people like me who want to learn without the effort of a formal study session.\n\nAfter seeing it work so well, I pushed an update, FlashySurf v2.0, so that anyone can study LITERALLY ANYTHING without having to try. You can create and import your own flashcard decks for any subject.\n\nThe only/biggest caveat about flashysurf is that you need to use it for a bit of time to see results like I used it for 2 months to see that 100 point increase (technically that was an outdated version with far less optimizations, so it should take less time) so you can't just use it for a test you have tmrw (unless you set it to be like 100% which would mean that a flashcard would appear on every single website).\n\nIt has a few more features that I couldn't mention here: AI flashcard generation from documents; 30 minute breaks to focus; stats on flashcard collections; and for the SAT, performance reports. (Also if ur wondering why i'm using semicolons, I actually learnt that from studying the SAT using flashysurf lol)\n\nAnd for you guys in r/python, I thought this would be perfect for drilling concepts that just need repetition. So, if you go to the flashysurf [flashcard creator](https://flashysurf.com/creator) you can actually use the AI flashcard import/maker tool to convert any documents (i.e. programming problems/exercises you have) or your own flashcard decks into flashysurf flashcards. So you can work on complex programming topics like Big O notation, dynamic programming, and graph theory algorithms. Note: You will obviously need the extension to use the cards lol but when you install the extension, you'll recieve [instructions](https://flashysurf.com/onboarding) on creating and importing flashcards, so you don't gotta memorize any of this.\n\nYou can download it from the Chrome Web Store, link in the website: [https://flashysurf.com/](https://flashysurf.com/?utm_source=rpst&amp;utm_campaign=rpython)\n\nI'm still actively working on it (just pushed a bugfix yesterday lol), so I'd love to hear any feedback or ideas you have. Hope it helps you learn something new while you're procrastinating on your actual work.\n\nThanks for reading :D\n\nComplicance thingy\n\n# What My Project Does\n\nFlashySurf is a free, open-source Chrome extension that helps users learn and study by showing them flashcards as they browse the web. It uses a spaced repetition algorithm with semantic analysis to identify and target a user's weaknesses. The extension also has features like a \"Forced Note-Taking\" system to ensure users learn from their mistakes, and it allows for custom flashcard decks so it can be used for any subject.\n\n# Target Audience\n\nFlashySurf is intended for anyone who wants to learn or study new information without the effort of a formal study session. It is particularly useful for students, professionals, or hobbyists who spend a lot of time on the web and want to use that time more productively. It's a production-ready project that's been in development for over six months, with a focus on being a long-term learning tool.\n\n# Comparison\n\nWhile there are other flashcard and spaced repetition tools, FlashySurf stands out by integrating learning directly into a user's everyday browsing habits. Unlike traditional apps like Anki, which require dedicated study sessions, FlashySurf brings the flashcards to you. Its unique combination of a spaced repetition algorithm with a semantic clustering system means it not only reinforces what you've learned but actively focuses on related topics where you are weakest. This approach is designed to help \"lazy\" learners like me who struggle with traditional study methods.",
    "author": "MaxDev0",
    "timestamp": "2025-09-14T14:40:42",
    "url": "https://reddit.com/r/Python/comments/1nh3rlv/i_was_terrible_at_studying_so_i_made_a_chrome/",
    "score": 157,
    "num_comments": 22,
    "upvote_ratio": 0.87,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ni3j6b",
    "title": "RepoGif: Generate GIF previews for your GitHub repos automatically üé•‚≠ê",
    "content": "Hi everyone! üëã\n\nI got tired of static GitHub previews, so I built a Python package called **RepoGif**.\n\n**What my project does:**\n\nRepoGif automatically generates 2-frame GIF repo cards (stars, forks, etc.) that you can drop into your README or use as social previews.\n\n* Written in Python\n* Simple API: `generate_gif(\"RepoName\", stars=100, forks=50)`\n* Exports GIFs with customizable templates &amp; sizes\n\n**Target audience:**\n\n* Developers who want their repos to look more lively and engaging\n* Open source maintainers who want to showcase project growth visually\n* Makers who need quick, shareable repo previews\n\n**Comparison:**\n\nThere are static badges (like shields.io), but RepoGif is different because it makes **animated previews** with multiple templates and sizes, instead of static icons.\n\nGitHub: [https://github.com/jhd3197/RepoGif](https://github.com/jhd3197/RepoGif)\n\nWould love feedback, suggestions, or ideas for new templates! üôå  \nAnd hey‚Ä¶ don‚Äôt forget to drop a ‚≠ê if you like it üòâ",
    "author": "jhd3197",
    "timestamp": "2025-09-15T17:53:47",
    "url": "https://reddit.com/r/Python/comments/1ni3j6b/repogif_generate_gif_previews_for_your_github/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.5,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ni9kd3",
    "title": "ImportError: /opt/render/project/src/.venv/lib/python3.13/site-packages/psycopg2/_psycopg.cpython-31",
    "content": "I was trying to deploy the backend on **Render**.\n\n* I updated the environment variable for the database connection string:psql 'postgresql://neondb\\_owner:...@ep-crimson-night-a14reavo-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require&amp;channel\\_binding=require' \n* The build itself finished successfully (all dependencies installed).\n* But when Render tried to run the app with Gunicorn, the service crashed immediately.\n\n**Error shown in Render logs:**\n\n    ImportError: /opt/render/project/src/.venv/lib/python3.13/site-packages/psycopg2/_psycopg.cpython-313-x86_64-linux-gnu.so:\n    undefined symbol: _PyInterpreterState_Get\n    \n\nThis happens right after:\n\n    app = create_app()\n    db.init_app(app)\n    \n\nSo the app fails at the point where Flask-SQLAlchemy tries to import psycopg2.",
    "author": "Specialist_Bed_234",
    "timestamp": "2025-09-15T23:03:08",
    "url": "https://reddit.com/r/Python/comments/1ni9kd3/importerror/",
    "score": 0,
    "num_comments": 0,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nhl0t6",
    "title": "I Build Type-safe TOML configuration with environment variables for Python 3.11+ | TomlEv",
    "content": "**TL;DR:** Stop fighting with environment variables and manual type conversion - get type-safe TOML configuration that just works.\n\n```bash\npip install tomlev\n```\n\n**Benefits:**\n- Automatic type conversion and validation\n- Environment variable substitution with defaults\n- Zero dependencies, production-ready\n- Perfect IDE and AI assistant support\n\n## The Problem\n\nPEP 735 style config management leads to repetitive, error-prone code:\n\n```python\n# The old way - manual parsing everywhere\nDB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\nDB_PORT = int(os.getenv(\"DB_PORT\", \"5432\"))  # Hope this doesn't crash!\nDEBUG = os.getenv(\"DEBUG\", \"false\").lower() == \"true\"  # Boolean hell\n```\n\n## What My Project Does\n\nTomlEv reads TOML files with environment variable substitution and validates them against typed Python classes:\n\n```python\nfrom tomlev import BaseConfigModel, TomlEv\n\nclass DatabaseConfig(BaseConfigModel):\n    host: str\n    port: int\n    user: str\n\nclass AppConfig(BaseConfigModel):\n    debug: bool\n    database: DatabaseConfig\n\n# One line - fully type-safe!\nconfig: AppConfig = TomlEv(AppConfig).validate()\n```\n\n**TOML file:**\n```toml\ndebug = \"${DEBUG|-false}\"\n\n[database]\nhost = \"${DB_HOST|-localhost}\"\nport = \"${DB_PORT|-5432}\"\nuser = \"${DB_USER}\"\n```\n\nWorks as CLI too:\n```bash\ntomlev validate --toml app.toml --env-file .env\ntomlev render --toml app.toml &gt; config.json\n```\n\n## Target Audience\n\nPython developers using modern type hints who want reliable configuration management without the boilerplate.\n\n## Comparison\n\n‚ùå **python-dotenv**: No type safety, manual parsing\n‚ùå **pydantic-settings**: More complex, less TOML-focused\n‚ùå **configparser**: INI format, no modern Python features\n‚ùå **YAML configs**: Security issues, complex parsing\n\n‚úÖ **TomlEv**: TOML readability + Python type safety + environment flexibility\n\n**Similar tools:**\n- No direct equivalent for TOML + type safety + env substitution\n\n## Try it out: https://github.com/thesimj/tomlev\n\n‚≠ê **Star if it helps!** Issues and PRs welcome. ‚≠ê\n",
    "author": "nbpatron",
    "timestamp": "2025-09-15T05:53:09",
    "url": "https://reddit.com/r/Python/comments/1nhl0t6/i_build_typesafe_toml_configuration_with/",
    "score": 3,
    "num_comments": 3,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nhq7oz",
    "title": "I built a pytest plugin to cleanly manage test fixture files - would love feedback!",
    "content": "# What My Project Does:\n\n`pytest-fixtures-fixtures` is a pytest plugin that provides fixtures to easily read and work with test data files (JSON, CSV, YAML, JSONL, plain text, custom) in your tests. Instead of writing boilerplate code to read files in every test, you get clean fixtures that handle file reading, parsing, and error handling automatically.\n\nThe plugin also includes a decorator to parametrise tests directly from data files, making data-driven testing much more straightforward.\n\n# Target Audience:\n\nThis is designed for Python developers who use pytest for testing and regularly work with test data files. It's production-ready and suitable for:\n\n* Teams writing comprehensive test suites\n* Developers doing data-driven testing\n* Anyone tired of writing file-reading boilerplate in tests\n* Projects that need clean separation between test logic and test data\n\n# Comparison:\n\nWhile there are other pytest plugins that help with test data (`pytest-datafiles`, `pytest-datadir`), `pytest-fixtures-fixtures` differs in several key ways:\n\n* **Fixture-based approach**: Uses pytest's native fixture system rather than custom decorators or utilities\n* **Multiple format support**: Handles JSON, CSV, YAML, JSONL, plain text with a consistent API plus it lets you provide your own deserialisation\n* **Built-in parametrisation**: The `@parametrize_from_fixture` decorator lets you create data-driven tests directly from files\n* **Type safety**: Full type hints and protocols for better IDE support\n* **Error handling**: Clear error messages when files are missing or malformed\n* **Flexible configuration**: Easy to customise the fixtures directory and behaviour\n\n# Example Usage:\n\n    # Before: lots of boilerplate\n    def test_user_data():\n        with open(\"tests/fixtures/users.json\") as f:\n            data = json.load(f)\n        assert data[\"name\"] == \"Alice\"\n    \n    # After: clean and simple\n    def test_user_data(read_json_fixture):\n        data = read_json_fixture(\"users.json\")\n        assert data[\"name\"] == \"Alice\"\n    \n    # Even better: parametrize from data files\n    @parametrize_from_fixture(\"test_cases.csv\")\n    def test_math_operations(a, b, expected):\n        assert int(a) + int(b) == int(expected)\n\n# Key Features:\n\n* Supports JSON, CSV, YAML, JSONL, and plain text files + custom deserialisation\n* Automatic file parsing with proper error handling\n* Parametrise tests directly from data files with custom test IDs\n* Configurable fixtures directory (defaults to `tests/fixtures/`)\n* Type hints and clear error messages\n* Works with pytest's existing fixture system\n\n# Installation:\n\n    pip install pytest-fixtures-fixtures\n\n# Links:\n\n**GitHub:** [https://github.com/fferegrino/pytest-fixtures-fixtures](https://github.com/fferegrino/pytest-fixtures-fixtures)  \n**PyPI:** [https://pypi.org/project/pytest-fixtures-fixtures/](https://pypi.org/project/pytest-fixtures-fixtures/)  \n**Docs:** [https://fferegrino.github.io/pytest-fixtures-fixtures/](https://fferegrino.github.io/pytest-fixtures-fixtures/)\n\n**Why I built it:** I was tired of writing the same file-reading boilerplate in every test suite. This keeps test data separate from test logic and makes data-driven testing much easier.\n\n# What do you think?\n\n* Does this solve a problem you've faced?\n* Any features you'd like to see?\n* How do you currently handle test data files in your projects?\n\nI'd love feedback from the community to make this tool better.",
    "author": "fferegrino",
    "timestamp": "2025-09-15T09:13:39",
    "url": "https://reddit.com/r/Python/comments/1nhq7oz/i_built_a_pytest_plugin_to_cleanly_manage_test/",
    "score": 2,
    "num_comments": 2,
    "upvote_ratio": 0.63,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ni74c4",
    "title": "Want to run the python project (Chatbot) on Xampp local server",
    "content": "Can anyone tell me the solution to the problem? run a Python project on the XAMPP local server, but the issue is that the XAMPP server does not support Python projects. Firstly, I need to test the project on the XAMPP local server and then integrate it with the PHP website. \n",
    "author": "Informal_Sea5714",
    "timestamp": "2025-09-15T20:46:02",
    "url": "https://reddit.com/r/Python/comments/1ni74c4/want_to_run_the_python_project_chatbot_on_xampp/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.21,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ni3kyj",
    "title": "Algumas dicas para iniciantes(Que voc√™ provavelmente queria saber quando come√ßou)",
    "content": "Talvez esse t√≠tulo tenha ficado amb√≠guo, mas gostaria muito de receber essa ajuda e espero que esse post sirva para outros, que assim como eu, tamb√©m est√£o iniciando nessa jornada pythonica. ",
    "author": "MonsieurJus",
    "timestamp": "2025-09-15T17:55:59",
    "url": "https://reddit.com/r/Python/comments/1ni3kyj/algumas_dicas_para_iniciantesque_voc√™/",
    "score": 0,
    "num_comments": 1,
    "upvote_ratio": 0.21,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nhulk7",
    "title": "Wanting a project - What do yall need?",
    "content": "So, i just finished one of my bigger projects, a custom interpreted programming language made to feel like assembly, with memory and register emulators and an modular instruction set which is easily modifiable by just adding files to a folder, as well as a IO module system with a modular approach for Memory mapped IO. But, as cool as it sounds, there is no real usecase?\n\nAs im finishing up on that im looking for a project that would \\*make others experience better (automod, why do you delete my post if it contains the he-lp word?)\\* like libraries, cli tools, gui tools. Anything that you need or think \"why isnt there a library for that?\", ill consider. If i realise i would benefit from it too, then i would maybe consider it.. even more?\n\nAlso so nobody says it, ive already made a logging library, with log saving, custom colors, a lot of settings, project names, subnames, sublogging, error, critical, warning, info logs. Whitespace log, raw log, timestamps, misc logs, and a lot more features, check it out on pypi, its called usefullog.\n\nAll suggestions are welcome! ",
    "author": "BravestCheetah",
    "timestamp": "2025-09-15T11:54:18",
    "url": "https://reddit.com/r/Python/comments/1nhulk7/wanting_a_project_what_do_yall_need/",
    "score": 0,
    "num_comments": 6,
    "upvote_ratio": 0.42,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ngy2ha",
    "title": "Another free Python 3 book - Files and Directories",
    "content": "If you are interested, you can click the top link on my landing page and download my eBook, \"Working with Files and Directories in Python 3\" for free: [https://tr.ee/MFl4Mmyu1B](https://tr.ee/MFl4Mmyu1B)\n\nI recently gave away a Beginner's Python Book and that went really well\n\nSo I hope this 26 page pdf will be useful for someone interested in working with Files and Directories  in Python. Since it is sometimes difficult to copy/paste from a pdf, I've added a .docx and .md version as well. The link will download all 3 as a zip file. No donations will be requested. Only info needed is a name and email address to get the download link.  It doesn't matter to me if you put a fake name.  Enjoy.",
    "author": "caudor",
    "timestamp": "2025-09-14T10:57:43",
    "url": "https://reddit.com/r/Python/comments/1ngy2ha/another_free_python_3_book_files_and_directories/",
    "score": 18,
    "num_comments": 8,
    "upvote_ratio": 0.73,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nhs5rm",
    "title": "[D√∫vida] - Serial Number para venda de um projeto",
    "content": "Pessoal, estou desenvolvendo um aplicativo em python para concilia√ß√£o banc√°ria. Pretendo o disponibilizar para venda mas como garanto a distribui√ß√£o n√£o autorizada? Por exemplo, uma pessoa compra acha bacana e envia para os amigos usarem.  \nPensei em algo como um serial number para registro e uso do mesmo, queria dicas e sugest√µes de como voc√™s fariam para coibir essa distribui√ß√£o n√£o autorizada.\n\n  \n\\*O aplicativo ser√° em exe via pyinstaller.",
    "author": "Working-Bag-2973",
    "timestamp": "2025-09-15T10:25:12",
    "url": "https://reddit.com/r/Python/comments/1nhs5rm/d√∫vida_serial_number_para_venda_de_um_projeto/",
    "score": 0,
    "num_comments": 2,
    "upvote_ratio": 0.33,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nh6ydt",
    "title": "Monday Daily Thread: Project ideas!",
    "content": "# Weekly Thread: Project Ideas üí°\n\nWelcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.\n\n## How it Works:\n\n1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.\n2. **Build &amp; Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.\n3. **Explore**: Looking for ideas? Check out Al Sweigart's [\"The Big Book of Small Python Projects\"](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.\n\n## Guidelines:\n\n* Clearly state the difficulty level.\n* Provide a brief description and, if possible, outline the tech stack.\n* Feel free to link to tutorials or resources that might help.\n\n# Example Submissions:\n\n## Project Idea: Chatbot\n\n**Difficulty**: Intermediate\n\n**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar \n\n**Description**: Create a chatbot that can answer FAQs for a website.\n\n**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)\n\n# Project Idea: Weather Dashboard\n\n**Difficulty**: Beginner\n\n**Tech Stack**: HTML, CSS, JavaScript, API\n\n**Description**: Build a dashboard that displays real-time weather information using a weather API.\n\n**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)\n\n## Project Idea: File Organizer\n\n**Difficulty**: Beginner\n\n**Tech Stack**: Python, File I/O\n\n**Description**: Create a script that organizes files in a directory into sub-folders based on file type.\n\n**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)\n\nLet's help each other grow. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-14T17:00:32",
    "url": "https://reddit.com/r/Python/comments/1nh6ydt/monday_daily_thread_project_ideas/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.86,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nh54hf",
    "title": "RQ Manager: Monitoring &amp; Metrics for RQ",
    "content": "Hey y‚Äôall.\n\nI‚Äôve been using RQ for a while after a few years with Celery. I always liked RabbitMQ‚Äôs monitoring + Flower, but didn‚Äôt find anything similar for RQ that really worked for me. Ended up hacking together something small that‚Äôs been running fine in production (3 queues, 5‚Äì7 workers).\n\nWhat it does\n ‚Ä¢ Monitor queue depth, worker throughput, and live job status\n ‚Ä¢ Retry, remove, or send jobs straight from the UI\n ‚Ä¢ /metrics endpoint for Prometheus/Grafana\n ‚Ä¢ Clean, responsive web UI (dark/light themes, live updates)\n\nWho it‚Äôs for\nAnyone running RQ in production who wants a simple, container-friendly way to monitor and manage jobs.\n\nHow it compares\nSimilar to rq-dashboard, rq-monitor and rq-exporter, but rolled into one:\n ‚Ä¢ UI + Prometheus metrics in the same tool\n ‚Ä¢ More direct job/queue management actions\n ‚Ä¢ Live charts for queue/job/worker monitoring\n ‚Ä¢ Easier deployment (single Docker container or K8s manifests)\n\nRepo: https://github.com/ccrvlh/rq-manager\nScreenshot in comments. Feedback + contributions welcome.",
    "author": "lowercase00",
    "timestamp": "2025-09-14T15:37:38",
    "url": "https://reddit.com/r/Python/comments/1nh54hf/rq_manager_monitoring_metrics_for_rq/",
    "score": 3,
    "num_comments": 1,
    "upvote_ratio": 0.72,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nhifw0",
    "title": "FastAPI is good but it is something I wouldn't go for",
    "content": "I wanted to learn web development using Python so I started learning Flask instead of Django because Flask gives a developer more freedom of tools when compared to Django. I'm have a better experience with Flask. I wanted to learn FastAPI because of its asynchronous nature.\n\nFastAPI is hard for me to create a database, and connect it. It needs many imports which is something I don't like\n\nPydantic makes it hard to pick up the framework. The use of many classes makes it complicated.\n\nIs it only me or it happens to many developers learning FastAPI??",
    "author": "donalddbanda",
    "timestamp": "2025-09-15T03:44:37",
    "url": "https://reddit.com/r/Python/comments/1nhifw0/fastapi_is_good_but_it_is_something_i_wouldnt_go/",
    "score": 0,
    "num_comments": 26,
    "upvote_ratio": 0.35,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nhigm1",
    "title": "requests.package",
    "content": "from requests.packages.urllib3.util.ssl\\_ import (  # type: ignore     \\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^         create\\_urllib3\\_context,         \\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^     )  # pylint: disable=ungrouped-imports     \\^ ModuleNotFoundError: No module named 'requests.packages.urllib3'; 'requests.packages' is not a package     even though i have tried installing this multiple times and couldn't figure what file is having this issue",
    "author": "Clear-Basket-1041",
    "timestamp": "2025-09-15T03:45:48",
    "url": "https://reddit.com/r/Python/comments/1nhigm1/requestspackage/",
    "score": 0,
    "num_comments": 12,
    "upvote_ratio": 0.21,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nfyq8o",
    "title": "MathFlow: an easy-to-use math library for python",
    "content": "Project Site: [https://github.com/cybergeek1943/MathFlow](https://github.com/cybergeek1943/MathFlow)\n\nIn the process of doing research for my paper [Combinatorial and Gaussian Foundations of Rational Nth Root Approximations](https://doi.org/10.48550/arXiv.2508.14095) (on arXiv), I created this library to address the pain points I felt when using only SymPy and SciPy separately. I wanted something lightweight, easy to use (exploratory), and something that would support numerical methods more easily. Hence, I created this lightweight wrapper that provides a hybrid symbolic-numerical interface to symbolic and numerical backends. It is backward compatible with Sympy. In short, this enables much faster analysis of symbolic math expressions by providing both numerical and traditional symbolic methods of analysis in the same interface. I have also added additional numerical methods that neither SymPy nor SciPy have (Pade approximations, numerical roots, etc.). The main goal for this project is to provide a tool that requires as little of a learning curve as possible and allows them to just focus on the math they are doing.\n\n# Core features\n\n* **üîí Operative Closure**: Mathematical operations return new Expression objects by default\n* **‚ö° Mutability Control**: Choose between immutable (default) and mutable expressions for different workflows\n* **üîó Seamless Numerical Integration**: Every symbolic expression has a¬†`.n`¬†attribute providing numerical methods without manual lambdification (uses cached lambdified expression when needed)\n* **üé® Enhanced Printing**: Flexible output formatting through the¬†`.print`¬†attribute (LaTeX, pretty printing, code generation)\n* **üì° Signal System**: Qt-like signals for tracking expression mutations and clones, enabling reactive programming\n* **üîÑ Automatic Type Conversions**: Seamlessly and automatically converts between internal Poly and Expr representations based on context\n* **üì¶ Lightweight**: \\~0.5 MB itself, \\~100 MB including dependencies\n* **üß© Fully backward compatible**: Seamlessly integrate SymPy and MathFlow in the same script. All methods that work on SymPy Expr or Poly objects work on MathFlow objects\n* **üîç Exploratory**: Full IDE support, enabling easy tool finding and minimizing the learning curve.\n\nA few examples are shown below. Many more examples can be found in the README of the official GitHub site.\n\n# Quick Start\n\nInstall using: `pip install mathflow`\n\n    from mathflow import Expression, Polynomial, Rational\n    \n    # Create expressions naturally\n    f = Expression(\"2x^2 + 3x + \\frac{1}{2}\")  # latex is automatically parsed\n    g = Expression(\"sin(x) + cos(x)\")\n    \n    # Automatic operative closure - operations return new objects of the same type\n    h = f + g  # f and g remain unchanged\n    hprime = h.diff()  # hprime is still an Expression object\n    \n    # Numerical evaluation made easy\n    result = f(2.5)  # Numerically evaluate at x = 2.5\n    \n    # Use the .n attribute to access fast numerical methods\n    numerical_roots = f.n.all_roots()\n    # Call f's n-prefixed methods to use variable precision numerical methods\n    precise_roots = f.nsolve_all(prec=50)  # 50 digits of accuracy\n    \n    # quick and easy printing\n    f.print()\n    f.print('latex')  # LaTeX output\n    f.print('mathematica_code')\n    f.print('ccode')  # c code output\n\n# Numerical Computing\n\nMathFlow excels at bridging symbolic and numerical mathematics:\n\n    f = Expression(\"x^3 - 2x^2 + x - 1\")\n    \n    # Root finding\n    all_roots = f.n.all_roots(bounds=(-5, 5))\n    specific_root = f.nsolve_all(bounds=(-5, 5), prec=50)  # High-precision solve\n    \n    # Numerical calculus\n    derivative_func = f.n.derivative_lambda(df_order=2)  # 2nd derivative numerical function  \n    integral_result = f.n.integrate(-1, 1)               # Definite integral  \n    \n    # Optimization\n    minimum = f.n.minimize(bounds=[(-2, 2)])\n\n# Edit:\n\nThis project was developed and used primarily for a research project, so a thorough test suite has not yet been developed. The project is still in development, and the current release is an alpha version. I have tried to minimize danger here, however, by designing it as a proxy to the already well-tested SymPy and SciPy libraries.",
    "author": "sciencenerd_1943",
    "timestamp": "2025-09-13T07:21:43",
    "url": "https://reddit.com/r/Python/comments/1nfyq8o/mathflow_an_easytouse_math_library_for_python/",
    "score": 116,
    "num_comments": 28,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ngcnn7",
    "title": "Sunday Daily Thread: What's everyone working on this week?",
    "content": "# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è\n\nHello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!\n\n## How it Works:\n\n1. **Show &amp; Tell**: Share your current projects, completed works, or future ideas.\n2. **Discuss**: Get feedback, find collaborators, or just chat about your project.\n3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.\n\n## Guidelines:\n\n* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.\n* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.\n\n## Example Shares:\n\n1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!\n2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.\n3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!\n\nLet's build and grow together! Share your journey and learn from others. Happy coding! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-13T17:00:35",
    "url": "https://reddit.com/r/Python/comments/1ngcnn7/sunday_daily_thread_whats_everyone_working_on/",
    "score": 16,
    "num_comments": 18,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ng10wr",
    "title": "The best object notation?",
    "content": "I want your advice regarding the best object notation to use for a python project. If you had the choice to receive data with a specific object notation, what would it be? YAML or JSON? Or another object notation?\n\nYAML looks, to me, to be in agreement with a more pythonic way, because it is simple, faster and easier to understand. On the other hand, JSON has a similar structure to the python dictionary and the native python parser is very much faster than the YAML parser.\n\nAny preferences or experiences?",
    "author": "StarsRonin",
    "timestamp": "2025-09-13T08:54:47",
    "url": "https://reddit.com/r/Python/comments/1ng10wr/the_best_object_notation/",
    "score": 37,
    "num_comments": 128,
    "upvote_ratio": 0.75,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ng9en6",
    "title": "midi-visualiser: A real-time MIDI player and visualiser.",
    "content": "Hi all, I recently revisited an old project I created to visualise MIDI music (using a piano roll) and after some tidying up and fixes I've now uploaded it to¬†[PyPI](https://pypi.org/project/midi-visualiser/)! The program allows single MIDI files or playlists of MIDI files to be loaded and visualised through a command-line tool.\n\nIt's fairly simple, using Pygame to display the visualiser window and provide playback control, but I'm pretty proud of how it looks and the audio-syncing logic (which uses Mido to interpret MIDI events). More details on how to use it are available in the¬†[project repository](https://github.com/benjaminrall/midi-visualiser).\n\nThis is the first project I've used¬†[uv](https://docs.astral.sh/uv/)¬†for, and I absolutely love it - check it out if you haven't already. Also, any suggestions/comments about the project would be greatly appreciated as I'm very new to uploading to PyPI!\n\nTo summarise;\n- **What My Project Does**: Plays MIDI files and visualises them using a scrolling piano roll\n- **Target Audience**: Mainly just a toy project, but could be used by anyone who wants a simple &amp; quick way to view any MIDI file!\n- **Comparison**: I can't find any alternatives that have this same functionality (at least not made in Python) - it obviously can't compete with mega fancy MIDI visualisers, but a strong point is how straight forward the project is, working immediately from the command-line without needing any configuration.\n\nEdit: Thanks to a comment, I've discovered an issue that means this only works on Windows - will look into fixing this, sorry!",
    "author": "Ben2508",
    "timestamp": "2025-09-13T14:34:14",
    "url": "https://reddit.com/r/Python/comments/1ng9en6/midivisualiser_a_realtime_midi_player_and/",
    "score": 13,
    "num_comments": 8,
    "upvote_ratio": 0.93,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nh1lj2",
    "title": "Python Interview Questions: From Basics to Advanced",
    "content": "The article titled \"Python Interview Questions: From Basics to Advanced\" [Python Interview Questions: From Basics to Advanced](https://www.lockedinai.com/blog/python-interview-questions-from-basics-to-advanced) provides a comprehensive guide to help candidates prepare for Python-related interviews across various levels. It covers essential topics ranging from fundamental syntax to advanced concepts.\n\n* Basic Concepts: The article emphasizes the importance of understanding Python's syntax, data types, variables, and control structures. It discusses common pitfalls such as mutable default arguments and floating-point precision issues.\n* Intermediate Topics: It delves into data structures like sets, dictionaries, and deques, as well as object-oriented programming concepts like inheritance and encapsulation.\n* Advanced Topics: The article explores advanced subjects including decorators, generators, and concurrency mechanisms like threading, multiprocessing, and asyncio.\n* Preparation Tools: It highlights resources like mock interviews, real-time feedback, and personalized coaching to aid in effective preparation.\n\nThis guide serves as a valuable resource for individuals aiming to enhance their Python skills and perform confidently in interviews.",
    "author": "Numerous-Trust7439",
    "timestamp": "2025-09-14T13:14:28",
    "url": "https://reddit.com/r/Python/comments/1nh1lj2/python_interview_questions_from_basics_to_advanced/",
    "score": 0,
    "num_comments": 3,
    "upvote_ratio": 0.38,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ng2h8x",
    "title": "SplitterMR: a modular library for splitting &amp; parsing documents",
    "content": "Hey guys, I just released **SplitterMR**, a library I built because none of the existing tools quite did what I wanted for slicing up documents cleanly for LLMs / downstream processing.\n\nIf you often work with **mixed document types** (PDFs, Word, Excel, Markdown, images, etc.) and **need flexible, reliable splitting/parsing**, this might be useful.\n\nThis library supports **multiple input formats**, e.g., text, Markdown, PDF, Word / Excel / PowerPoint, HTML / XML, JSON / YAML, CSV / TSV, and even images.\n\nFiles can be read using **MarkItDown** or **Docling**, so this is perfect if you are using those frameworks with your current applications.\n\nLogically, it supports **many different splitting strategies**: not only based on the number of characters but on tokens, schema keys, semantic similarity, and many other techniques. You can even develop your own splitter using the Base object, and it is the same for the Readers!\n\nIn addition, **you can process the graphical resources of your documents (e.g., photos) using VLMs** (OpenAI, Gemini, HuggingFace, etc.), so you can extract the text or caption them!\n\n# What‚Äôs new / what‚Äôs good in the latest release\n\n* Stable Version **1.0.0** is out.\n* Supports **more input formats / more robust readers**.\n* **Stable API** for the Reader abstractions so you can plug in your own if needed.\n* **Better handling of edge cases** (e.g. images, schema‚Äôd JSON / XML) so you don‚Äôt lose structure unintentionally.\n\n# Some trade-offs / limitations (so you don‚Äôt run into surprises)\n\n* **Heavy dependencies**: because it supports all these formats you‚Äôll pull in a bunch of libs (PDF, Word, image parsing, etc.). If you only care about plain text, many of those won‚Äôt matter, but still.\n* **Not a fully ‚ÄúLLM prompt manager‚Äù or embedding chunker out of the box** ‚Äî splitting + parsing is its job; downstream you‚Äôll still need to decide chunk sizes, context windows, etc.\n\n# Installation and usage\n\nIf you want to test:\n\n    uv add splitter-mr\n\nExample usage:\n\n    from splitter_mr.reader import VanillaReader\n    from splitter_mr.model.models import AzureOpenAIVisionModel\n    \n    model = AzureOpenAIVisionModel()\n    reader = VanillaReader(model=model)\n    output = reader.read(file_path=\"data/sample_pdf.pdf\")\n    print(output.text)\n\n**Check out the docs for more examples, API details, and instructions on how to write your own Reader for special formats:**  \n\n* üëâ [Github](https://github.com/andreshere00/Splitter_MR)\n* üëâ [Documentation server](https://andreshere00.github.io/Splitter_MR/)\n* üëâ [PyPi package](https://pypi.org/project/splitter-mr/1.0.1/)\n* üëâ [LinkedIn (to contact with me)](https://www.linkedin.com/in/andres-herencia)\n\nIf you want to collaborate or you have some suggestions, don't dubt to contact me.\n\n**Thank you so much for reading :)**\n",
    "author": "Andreshere",
    "timestamp": "2025-09-13T09:52:42",
    "url": "https://reddit.com/r/Python/comments/1ng2h8x/splittermr_a_modular_library_for_splitting/",
    "score": 17,
    "num_comments": 0,
    "upvote_ratio": 0.91,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nfvo8y",
    "title": "Announcing iceoryx2 v0.7: Fast and Robust Inter-Process Communication (IPC) Library",
    "content": "Hello hello,\n\nI am one of the maintainers of the open-source zero-copy middleware iceoryx2, and we‚Äôve just released iceoryx2 v0.7 which comes with Python language bindings. That means you can now use fast zero-copy communication directly in Python. Here is the full release blog: [https://ekxide.io/blog/iceoryx2-0-7-release/](https://ekxide.io/blog/iceoryx2-0-7-release/)\n\nWith iceoryx2 you can communicate between different processes, send data with publish-subscribe, build more complex request-response streams, or orchestrate processes using the event messaging pattern with notifiers and listeners.\n\nWe‚Äôve prepared a set of Python examples here: [https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/python](https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/python)\n\nOn top of that, we invested some time into writing a detailed getting started guide in the iceoryx2 book: [https://ekxide.github.io/iceoryx2-book/main/getting-started/quickstart.html](https://ekxide.github.io/iceoryx2-book/main/getting-started/quickstart.html)\n\nAnd one more thing: iceoryx2 lets Python talk directly to C, C++ and Rust processes - without any serialization or binding overhead. Check out the cross-language publish-subscribe example to see it in action: [https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples](https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples)\n\nSo in short:\n\n* **What My Project Does:** Zero-Copy Inter-Process Communication\n* **Target Audience:** Developers building distributed systems, plugin-based applications, or safety-critical and certifiable systems\n* **Comparision:** Provides a high-level, service-oriented abstraction over low-level shared memory system calls",
    "author": "elfenpiff",
    "timestamp": "2025-09-13T04:59:15",
    "url": "https://reddit.com/r/Python/comments/1nfvo8y/announcing_iceoryx2_v07_fast_and_robust/",
    "score": 22,
    "num_comments": 6,
    "upvote_ratio": 0.9,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nf57hb",
    "title": "Update: Should I give away my app to my employer for free?",
    "content": "Link to original post - https://www.reddit.com/r/Python/s/UMQsQi8lAX\n\nHi, since my post gained a lot of attention the other day and I had a lot of messages, questions on the thread etc. I thought I would give an update. \n\nI didn‚Äôt make it clear in my previous post but I developed this app in my own time, but using company resources. \n\nI spoke to a friend in the HR team and he explained a similar scenario happened a few years ago, someone built an automation tool for outlook, which managed a mailbox receiving 500+ emails a day (dealing/contract notes) and he simply worked on a fund pricing team and only needed to view a few of those emails a day but realised the mailbox was a mess. He took the idea to senior management and presented the cost saving and benefits. Once it was deployed he was offered shares in the company and then a cash bonus once a year of realised savings was achieved. \n\nI‚Äôve been advised by my HR friend to approach senior management with my proposal, explain that I‚Äôve already spoken to my manager and detail the cost savings I can make, ask for a salary increase to provide ongoing support and develop my code further and ask for similar terms to that of the person who did this previously. He has confirmed what I‚Äôve done doesn‚Äôt go against any HR policies or my contract. \n\nMeeting is booked for next week and I‚Äôve had 2 messages from senior management saying how excited they are to see my idea :) \n\n",
    "author": "RDE_20",
    "timestamp": "2025-09-12T07:47:00",
    "url": "https://reddit.com/r/Python/comments/1nf57hb/update_should_i_give_away_my_app_to_my_employer/",
    "score": 798,
    "num_comments": 92,
    "upvote_ratio": 0.96,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ng8qg0",
    "title": "Tea Tasting: t-testing library alternatives?",
    "content": "I dont feel this repo is Pythonic nor are their docs sufficient: [https://e10v.me/tea-tasting-analysis-of-experiments/](https://e10v.me/tea-tasting-analysis-of-experiments/) (am i missing something or stupid?) \n\nLooking for good alternatives - I havent found any ",
    "author": "rm-rf-rm",
    "timestamp": "2025-09-13T14:04:40",
    "url": "https://reddit.com/r/Python/comments/1ng8qg0/tea_tasting_ttesting_library_alternatives/",
    "score": 4,
    "num_comments": 4,
    "upvote_ratio": 0.64,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nfupw4",
    "title": "I built QRPorter ‚Äî local Wi-Fi file transfer via QR (PC ‚Üî Mobile)",
    "content": "Hi everyone, I built **QRPorter**, a small open-source utility that moves files between desktop and mobile over your LAN/Wi-Fi using QR codes. No cloud, no mobile app, no accounts ‚Äî just scan &amp; transfer.\n\n# What it does\n\n* **PC ‚Üí Mobile file transfer:** select a file on your desktop, generate a QR code, scan with your phone and download the file in the phone browser.\n* **Mobile ‚Üí PC file transfer:** scan the QR on the PC, open the link on your phone, upload a file from the phone and it‚Äôs saved on the PC.\n\n# Target audience\n\n* Developers, students, and office users who frequently move screenshots, small media or documents between phone ‚Üî PC.\n* Privacy-conscious users who want transfers to stay on their LAN/Wi-Fi (no third-party servers).\n* Anyone who wants a dead-simple cross-device transfer without installing mobile apps.\n\n# Comparison\n\n* **No extra mobile apps / accounts** ‚Äî works via the phone‚Äôs browser and the desktop app.\n* **Local-first** ‚Äî traffic stays on your Wi-Fi/LAN (no cloud).\n* **Cross-platform** ‚Äî desktop UI + web interface works with modern mobile browsers (Windows / macOS / Linux / iOS / Android).\n\n# Requirements &amp; tested platforms\n\n* **Python 3.12+** and `pip`.\n* Tested on **Windows 11** and **Linux**; macOS should work.\n* Key Python deps: `Flask`, `PySide6`, `qrcode`, `Werkzeug`, `Pillow`.\n\n# Installation\n\nYou can install from PyPI:\n\n    pip install qrporter\n\nAfter install, run:\n\n    qrporter\n\n# Troubleshooting\n\n* Make sure **both devices are on the same Wi-Fi/LAN** (guest/isolated networks often block local traffic).\n* **Maximum 1 GB file size** limit and commonly used file types allowed.\n* **One file at a time.** For multiple files, zip them and transfer the zip.\n\n# License\n\n* MIT License\n\n# GitHub\n\n[https://github.com/manikandancode/qrporter](https://github.com/manikandancode/qrporter)\n\nI beautified and commented the code using AI to improve readability and inline documentation. If you try it out ‚Äî I‚Äôd love feedback, issues, or ideas for improvements. Thanks! üôè",
    "author": "MrShortCircuitMan",
    "timestamp": "2025-09-13T04:05:08",
    "url": "https://reddit.com/r/Python/comments/1nfupw4/i_built_qrporter_local_wifi_file_transfer_via_qr/",
    "score": 6,
    "num_comments": 2,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nff4dw",
    "title": "Flowfile - An open-source visual ETL tool, now with a Pydantic-based node designer.",
    "content": "Hey r/Python,\n\nI built Flowfile, an open-source tool for creating data pipelines both visually and in code. Here's the latest feature: Custom Node Designer.\n\n# What My Project Does\n\nFlowfile creates bidirectional conversion between visual ETL workflows and Python code. You can build pipelines visually and export to Python, or write Python and visualize it. The Custom Node Designer lets you define new visual nodes using Python classes with Pydantic for settings and Polars for data processing.\n\n# Target Audience\n\nProduction-ready tool for data engineers who work with ETL pipelines. Also useful for prototyping and teams that need both visual and code representations of their workflows.\n\n# Comparison\n\n* **Alteryx**: Proprietary, expensive. Flowfile is open-source.\n* **Apache NiFi**: Java-based, requires infrastructure. Flowfile is pip-installable Python.\n* **Prefect/Dagster**: Orchestration-focused. Flowfile focuses on visual pipeline building.\n\n# Custom Node Example\n\n    import polars as pl\n    from flowfile_core.flowfile.node_designer import (\n        CustomNodeBase, NodeSettings, Section,\n        ColumnSelector, MultiSelect, Types\n    )\n    \n    class TextCleanerSettings(NodeSettings):\n        cleaning_options: Section = Section(\n            title=\"Cleaning Options\",\n            text_column=ColumnSelector(label=\"Column to Clean\", data_types=Types.String),\n            operations=MultiSelect(\n                label=\"Cleaning Operations\",\n                options=[\"lowercase\", \"remove_punctuation\", \"trim\"],\n                default=[\"lowercase\", \"trim\"]\n            )\n        )\n    \n    class TextCleanerNode(CustomNodeBase):\n        node_name: str = \"Text Cleaner\"\n        settings_schema: TextCleanerSettings = TextCleanerSettings()\n    \n        def process(self, input_df: pl.LazyFrame) -&gt; pl.LazyFrame:\n            text_col = self.settings_schema.cleaning_options.text_column.value\n            operations = self.settings_schema.cleaning_options.operations.value\n            \n            expr = pl.col(text_col)\n            if \"lowercase\" in operations:\n                expr = expr.str.to_lowercase()\n            if \"trim\" in operations:\n                expr = expr.str.strip_chars()\n            \n            return input_df.with_columns(expr.alias(f\"{text_col}_cleaned\"))\n\nSave in `~/.flowfile/user_defined_nodes/` and it appears in the visual editor.\n\n# Why This Matters\n\nYou can wrap complex tasks‚ÄîAPI connections, custom validations, niche library functions‚Äîinto simple drag-and-drop blocks. Build your own high-level tool palette right inside the app. It's all built on Polars for speed and completely open-source.\n\n# Installation\n\n`pip install Flowfile`\n\n# Links\n\n* GitHub: [https://github.com/Edwardvaneechoud/Flowfile/](https://github.com/Edwardvaneechoud/Flowfile/)\n* Custom Nodes Documentation: [https://edwardvaneechoud.github.io/Flowfile/for-developers/creating-custom-nodes.html](https://edwardvaneechoud.github.io/Flowfile/for-developers/creating-custom-nodes.html)\n* Previous discussions: [SideProject post](https://www.reddit.com/r/SideProject/comments/1mp0hor/i_built_a_tool_that_turns_python_data_pipelines/), [FlowFrame post](https://www.reddit.com/r/Python/comments/1kp0er9/flowframe_python_code_that_generates_visual_etl/)",
    "author": "Proof_Difficulty_434",
    "timestamp": "2025-09-12T14:15:28",
    "url": "https://reddit.com/r/Python/comments/1nff4dw/flowfile_an_opensource_visual_etl_tool_now_with_a/",
    "score": 47,
    "num_comments": 16,
    "upvote_ratio": 0.92,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nfdlmq",
    "title": "Learning machine learning",
    "content": "Is this an appropriate question here? \nI was wondering if anyone could suggest any resources to learn machine learning relatively quickly. By quickly I mean get a general understanding and be able to talk about it. Then I can spend time actually learning it. \nI‚Äôm a beginner in Python. Thanks!",
    "author": "dedenorio",
    "timestamp": "2025-09-12T13:15:48",
    "url": "https://reddit.com/r/Python/comments/1nfdlmq/learning_machine_learning/",
    "score": 16,
    "num_comments": 12,
    "upvote_ratio": 0.71,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nexoe8",
    "title": "I built a from-scratch Python package for classic Numerical Methods (no NumPy/SciPy required!)",
    "content": "Hey everyone,\n\nOver the past few months I‚Äôve been building a Python package called¬†`numethods`¬†‚Äî a small but growing collection of¬†**classic numerical algorithms implemented 100% from scratch**. No NumPy, no SciPy, just plain Python floats and list-of-lists.\n\nThe idea is to make algorithms transparent and educational, so you can actually¬†*see*¬†how LU decomposition, power iteration, or RK4 are implemented under the hood. This is especially useful for students, self-learners, or anyone who wants a deeper feel for how numerical methods work beyond calling library functions.\n\n[https://github.com/denizd1/numethods](https://github.com/denizd1/numethods)\n\n# üîß What‚Äôs included so far\n\n* **Linear system solvers**: LU (with pivoting), Gauss‚ÄìJordan, Jacobi, Gauss‚ÄìSeidel, Cholesky\n* **Root-finding**: Bisection, Fixed-Point Iteration, Secant, Newton‚Äôs method\n* **Interpolation**: Newton divided differences, Lagrange form\n* **Quadrature (integration)**: Trapezoidal rule, Simpson‚Äôs rule, Gauss‚ÄìLegendre (2- and 3-point)\n* **Orthogonalization &amp; least squares**: Gram‚ÄìSchmidt, Householder QR, LS solver\n* **Eigenvalue methods**: Power iteration, Inverse iteration, Rayleigh quotient iteration, QR iteration\n* **SVD**¬†(via eigen-decomposition of ATAA\\^T AATA)\n* **ODE solvers**: Euler, Heun, RK2, RK4, Backward Euler, Trapezoidal, Adams‚ÄìBashforth, Adams‚ÄìMoulton, Predictor‚ÄìCorrector, Adaptive RK45\n\n# ‚úÖ Why this might be useful\n\n* Great for¬†**teaching/learning**¬†numerical methods step by step.\n* Good reference for people writing their own solvers in C/Fortran/Julia.\n* Lightweight, no dependencies.\n* Consistent object-oriented API (`.solve()`,¬†`.integrate()`¬†etc).\n\n# üöÄ What‚Äôs next\n\n* PDE solvers (heat, wave, Poisson with finite differences)\n* More optimization methods (conjugate gradient, quasi-Newton)\n* Spectral methods and advanced quadrature\n\nüëâ If you‚Äôre learning numerical analysis, want to peek under the hood, or just like playing with algorithms, I‚Äôd love for you to check it out and give feedback.",
    "author": "sikerce",
    "timestamp": "2025-09-12T01:27:44",
    "url": "https://reddit.com/r/Python/comments/1nexoe8/i_built_a_fromscratch_python_package_for_classic/",
    "score": 139,
    "num_comments": 30,
    "upvote_ratio": 0.95,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1ng926f",
    "title": "What is the best way of developing an Agent in Python to support a Go backend?",
    "content": "Giving the context here: me a novice in Agentic world however have strong Go and Python dev background.\nHaving said that, I am quite confused with not sure how to develop agents for the backend.\nOpen to discussion and guidance.",
    "author": "bhushokali",
    "timestamp": "2025-09-13T14:18:46",
    "url": "https://reddit.com/r/Python/comments/1ng926f/what_is_the_best_way_of_developing_an_agent_in/",
    "score": 0,
    "num_comments": 5,
    "upvote_ratio": 0.27,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nfdhlu",
    "title": "Thanks r/Python community for reviewing my project Ducky all in one networking tool!",
    "content": "Thanks to this community I received some feedbacks about Ducky that I posted last week on here, I got 42 stars on github as well and some comments for Duckys enhancement. Im thankful for the people who viewed the post and went to see the source code huge thanks to you all.  \n\n**What Ducky Does:**\n\nDucky is a desktop application that consolidates the essential tools of a network engineer or security enthusiast into a single, easy-to-use interface. Instead of juggling separate applications for terminal connections, network scanning, and diagnostics, Ducky provides a unified workspace to streamline your workflow. Its core features include a tabbed terminal (SSH, Telnet, Serial), an SNMP-powered network topology mapper, a port scanner, and a suite of security utilities like a CVE lookup and hash calculator.\n\n**Target Audience:**\n\nDucky is built for anyone who works with network hardware and infrastructure. This includes:\n\n* **Network Engineers &amp; Administrators:**¬†For daily tasks like configuring switches and routers, troubleshooting connectivity, and documenting network layouts.\n* **Cybersecurity Professionals:**¬†For reconnaissance tasks like network discovery, port scanning, and vulnerability research.\n* **Students &amp; Hobbyists:**¬†For those learning networking (e.g., for CompTIA Network+ or CCNA), Ducky provides a free, hands-on tool to explore and interact with real or virtual network devices.\n* **IT Support &amp; Help Desk:**¬†For frontline technicians who need to quickly run diagnostics like ping and traceroute to resolve user issues.\n\nGithub link [https://github.com/thecmdguy/Ducky](https://github.com/thecmdguy/Ducky)",
    "author": "initCMD",
    "timestamp": "2025-09-12T13:11:27",
    "url": "https://reddit.com/r/Python/comments/1nfdhlu/thanks_rpython_community_for_reviewing_my_project/",
    "score": 14,
    "num_comments": 0,
    "upvote_ratio": 0.81,
    "is_original_content": false
  },
  {
    "subreddit": "Python",
    "post_id": "1nfiys8",
    "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread",
    "content": "# Weekly Thread: Resource Request and Sharing üìö\n\nStumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!\n\n## How it Works:\n\n1. **Request**: Can't find a resource on a particular topic? Ask here!\n2. **Share**: Found something useful? Share it with the community.\n3. **Review**: Give or get opinions on Python resources you've used.\n\n## Guidelines:\n\n* Please include the type of resource (e.g., book, video, article) and the topic.\n* Always be respectful when reviewing someone else's shared resource.\n\n## Example Shares:\n\n1. **Book**: [\"Fluent Python\"](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \\- Great for understanding Pythonic idioms.\n2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \\- Excellent overview of Python's built-in data structures.\n3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \\- A deep dive into decorators.\n\n## Example Requests:\n\n1. **Looking for**: Video tutorials on web scraping with Python.\n2. **Need**: Book recommendations for Python machine learning.\n\nShare the knowledge, enrich the community. Happy learning! üåü",
    "author": "AutoModerator",
    "timestamp": "2025-09-12T17:00:31",
    "url": "https://reddit.com/r/Python/comments/1nfiys8/saturday_daily_thread_resource_request_and/",
    "score": 5,
    "num_comments": 0,
    "upvote_ratio": 0.79,
    "is_original_content": false
  }
]